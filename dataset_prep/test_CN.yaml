papers:
- title: "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native\
    \ RAN Protocols"
  abstract: 'Integrating large AI models (LAMs) into 6G mobile networks promises to

    redefine protocol design and control-plane intelligence by enabling autonomous,

    cognitive network operations. While industry concepts, such as ETSI''s

    Experiential Networked Intelligence (ENI), envision LAM-driven agents for

    adaptive network slicing and intent-based management, practical implementations

    still face challenges in protocol literacy and real-world deployment. This

    paper presents an end-to-end demonstration of a LAM that generates

    standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as

    part of control-plane procedures inside a gNB. We treat RRC messaging as a

    domain-specific language and fine-tune a decoder-only transformer model (LLaMA

    class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages

    linearized to retain their ASN.1 syntactic structure before standard byte-pair

    encoding tokenization. This enables combinatorial generalization over RRC

    protocol states while minimizing training overhead. On 30k field-test

    request-response pairs, our 8 B model achieves a median cosine similarity of

    0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a

    zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural

    and semantic RRC fidelity. Overall, our results show that LAMs, when augmented

    with Radio Access Network (RAN)-specific reasoning, can directly orchestrate

    control-plane procedures, representing a stepping stone toward the AI-native

    air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for

    future AI-native wireless standards.'
  url: http://arxiv.org/abs/2505.16821v1
  keywords: 6G, Radio Resource Control, protocol learning, AI-Native Air Interface,
    Large AI Model
  document: "#### I. INTRODUCTION\n\nLarge AI Models (LAMs) are poised to transform\
    \ wireless network design by enabling autonomous, cognitive capabilities that\
    \ go beyond the scope of traditional optimization methods. The forthcoming sixth\
    \ generation (6G) of mobile networks is expected to incorporate Artificial Intelligence\
    \ (AI) methods not merely as auxiliary optimisation tools but also as integral\
    \ design primitives of the AI-Native Air Interface (AI-AI). Recent proposals for\
    \ an AI-AI posit that Machine Learning (ML) models could co-design Physical Layer\
    \ (PHY) - and Medium Access Control (MAC) procedures [\\[1\\]](#page-11-0), thereby\
    \ enabling radio stacks that adapt automatically to dynamic service requirements,\
    \ resource constraints, and propagation conditions [\\[2\\]](#page-11-1). The\
    \ rapid evolution of LAMs [\\[3\\]](#page-11-2), [\\[4\\]](#page-11-3) strengthens\
    \ this vision—when endowed with multi-modal perception and reasoning, such models\
    \ become plausible agents for protocol-centric tasks that demand both linguistic\
    \ competence and structured decision making. This raises a timely research challenge:\
    \ determining the extent to which LAMs can directly engage in the generation and\
    \ interpretation of control signalling in Radio Access Network (RAN). Can LAMs\
    \ achieve sufficient protocol literacy for RRC emulation? What finetuning strategies\
    \ are effective? What is the performance fidelity of such a system? These are\
    \ the research questions this paper intends to answer.\n\n1\n\nOne particularly\
    \ compelling target for AI intervention is the Radio Resource Control (RRC) layer\
    \ of 5G New Radio (NR). RRC is the Layer 3 (L3) of the cellular network. It orchestrates\
    \ connection establishment, mobility and configuration between User Equipment\
    \ (UE) and the Next-Generation Node B (gNB). Conceptually, RRC messages form a\
    \ specialised, domain-specific language; learning this language is analogous to\
    \ learning syntax and semantics in natural-language processing. Treating RRC message\
    \ as such positions LAM-based sequence models as natural emulators of the RRC\
    \ layer. Demonstrating accurate RRC emulation would constitute an early, concrete\
    \ instantiation of AI-AI principles, with direct implications for the design of\
    \ the 6G control plane.\n\nIndustry roadmaps reinforce this trend. The recent\
    \ ETSI Experiential Networked Intelligence (ENI)-051 report introduced an Multi-Agent\
    \ Core Architecture (AI-Core), where multiple heterogeneous agents—each supported\
    \ by a Domain-Specific Large AI Model (AgentGPT) and a larger, shared knowledge\
    \ model (i.e., the Telecom Knowledge Large AI Model (NetGPT))—jointly manage next-generation\
    \ network slicing [\\[5\\]](#page-11-4). Within this framework, an *Actor* component\
    \ translates high-level agent decisions into concrete protocol messages. Emulating\
    \ RRC via LAMs directly aligns with the Actor's mission and thus provides an empirical\
    \ testbed for the ENI vision, bridging theoretical architecture with practical\
    \ implementation.\n\nThe scope of LAM-enabled control extends beyond RRC emulation\
    \ [\\[6\\]](#page-11-5). A closely related application is Root Cause Analysis\
    \ (RCA) of RRC traces and broader RAN telemetry, where a model fine-tuned on historical\
    \ logs could infer latent failure modes and recommend remedial actions. Such actions\
    \ are still manually done today. Additional opportunities include LAM-driven anomaly\
    \ detection, adaptive L3 resource control, and intent-based radio configuration.\
    \ Although these topics lie outside the scope of the present study, they illustrate\
    \ the\n\nZ. Liu and X. Chu are with the School of Electrical and Electronic Engineering,\
    \ The University of Sheffield, Sheffield S10 2TN, U.K. (e-mail: {ziming.liu,x.chu}@sheffield.ac.uk).\n\
    \nB. Liu and A. Valcarce are with Nokia Bell Labs, 12 Rue ´ Jean Bart, 91300 Massy,\
    \ France (e-mail: {bryan.liu,alvaro.valcarce rial }@nokia-bell-labs.com).\n\n\
    breadth of potential once protocol literacy is achieved by a Large Language Model\
    \ (LLM).\n\nCan the protocol literacy of a LAM—specifically, its ability to parse\
    \ and generate 5G RRC messages—be systematically improved, and through which fine-tuning\
    \ strategies? This paper answers this question through a novel two-phase methodology\
    \ that (i) imparts RRC knowledge to a decoder-only LAM via supervised fine-tuning\
    \ enhanced with Low-Rank Adaptation (LoRA), and (ii) evaluates the resulting model's\
    \ fidelity in emulating RRC procedures under realistic network scenarios. The\
    \ findings offer evidence for the feasibility of LAM-based RRC and identify the\
    \ conditions under which such models may contribute towards the AI-AI in forthcoming\
    \ 6G systems.\n\nWithout loss of generality, this work focuses on *decoder-only*\
    \ transformer architectures (e.g., LLaMA-class models [\\[7\\]](#page-11-6)).\
    \ Autoregressive decoding matches the turn-taking structure of RRC exchanges and\
    \ permits key–value caching, which curtails inference latency and memory footprint,\
    \ both critical for deployment at the gNB. Decoder-only models thereby strike\
    \ a favourable balance between capacity and computational efficiency, especially\
    \ when combined with parameter-efficient fine-tuning techniques such as LoRA [\\\
    [8\\]](#page-11-7). LoRA freezes the original model weights and introduces low-rank\
    \ update matrices, reducing trainable parameters by orders of magnitude while\
    \ maintaining expressiveness; the approach mitigates both the cost of domain adaptation\
    \ and the runtime overhead of inference, two primary barriers to operational use.\n\
    \nTraining LAMs for specialist telecom domains demands curated datasets that capture\
    \ the combinatorial breadth of protocol states, while deployment must meet stringent\
    \ latency budgets to avoid impairing control-plane responsiveness. Addressing\
    \ these issues entails advances in data-efficient learning, model compression,\
    \ and hardware-accelerated execution—topics that are treated in subsequent sections.\n\
    \n#### II. SYSTEM MODEL\n\nThis section delineates the architectural and operational\
    \ principles underlying the proposed LAM-integrated gNB. We first describe the\
    \ system architecture, detailing the functional decomposition of the disaggregated\
    \ gNB and the embedding of a LAM within the RRC layer, while ensuring adherence\
    \ to standardized interface definitions. Then, the model-driven control-plane\
    \ workflow is examined, encompassing the reception and contextual interpretation\
    \ of uplink RRC messages and the autoregressive synthesis of standards-compliant\
    \ downlink responses.\n\n# *A. Architecture and Functional Integration*\n\nThe\
    \ proposed Large RRC Model integrates a decoder-only transformer-based LAM into\
    \ the RRC layer of a disaggregated gNB, as depicted in Fig. [1.](#page-1-0) Following\
    \ the 3rd Generation Partnership Project (3GPP) NG-RAN standard, the gNB is partitioned\
    \ into a Central Unit (CU) and a Distributed Unit (DU), with the CU further subdivided\
    \ into separate controlplane Control Plane of Central Unit (CU-CP) and user-plane\
    \ User Plane of Central Unit (CU-UP) entities. The CU-CP hosts the RRC layer,\
    \ responsible for essential control-plane tasks such as connection establishment,\
    \ radio bearer management, and mobility decisions. Replacing the traditional rule-based\
    \ logic, the embedded RRC LLM serves as an intelligent decision engine within\
    \ the RRC, producing control messages and associated protocol decisions via autoregressive\
    \ inference. Conversely, the CU-UP encompasses user-plane layers, primarily Service\
    \ Data Adaptation Protocol (SDAP) and userplane Packet Data Convergence Protocol\
    \ (PDCP), that manage the transport of user data. The DU, hosting lower-layer\
    \ (e.g., Radio Link Control (RLC), MAC, and PHY layers) protocols, interfaces\
    \ directly with the Remote Radio Heads (RRHs) performing Radio Frequency (RF)\
    \ and low-level physical-layer tasks. Fig. [2](#page-2-0) illustrates all these\
    \ interfaces and functions. It's important to note that integrating a LLM at the\
    \ RRC level is intended to preserve the external protocol behavior, leaving northbound\
    \ and lateral interfaces to peer nodes unaltered, thus ensuring interoperability\
    \ with existing network deployments.\n\n![](_page_1_Figure_9.jpeg)\n\n<span id=\"\
    page-1-0\"></span>Fig. 1. High-level concept of an AI-native gNB-side RRC layer\
    \ powered by a LLM, illustrating its core inputs, outputs, learning source, and\
    \ interaction with the protocol stack.\n\nAs in standard 5G New Radio (NR), uplink\
    \ RRC messages from UEs, such as Connection Requests or Measurement Reports, arrive\
    \ at the CU-CP via Control-Plane Interface between CU-CP and DU (F1-C), where\
    \ they are processed by the embedded RRC LLM, replacing the conventional RRC layer\
    \ implementation. The model interprets these inputs contextually, considering\
    \ ongoing connection states, historical interactions, and dynamic network conditions\
    \ to generate suitable downlink RRC responses (e.g., Connection Setup or Mobility\
    \ Reconfiguration). The generated messages maintain compliance with standardized\
    \ 3GPP encoding, thus transparently replacing the message generation logic. Downlink\
    \ responses are then transmitted back to the UE through the DU via the same interface.\n\
    \n![](_page_2_Figure_0.jpeg)\n\n<span id=\"page-2-0\"></span>Fig. 2. Low-level\
    \ architecture of an NR gNB disaggregating an LLM-based RRC layer.\n\n# *B. LLM-Driven\
    \ Signaling Workflow and Model Operation*\n\nInteractions between the RRC layer\
    \ and the user-plane modules are similarly orchestrated. When the Large RRC Model\
    \ decides to initiate or modify data radio bearers—potentially based on inferred\
    \ context or changing service demands—it signals these decisions to the CU-UP\
    \ over the Interface between CU-CP and CU-UP (E1). The CU-UP subsequently configures\
    \ the corresponding SDAP and PDCP entities, enabling adaptive management of the\
    \ user-plane traffic. Meanwhile, data-plane traffic flows remain unmodified and\
    \ proceed independently over the established user-plane interfaces (User-Plane\
    \ Interface between CU-UP and DU (F1-U), User-Plane Interface between RRH and\
    \ DU (F2-U)).\n\nFig. [3](#page-3-0) illustrates the fine-tuning and inference\
    \ workflow specific to the proposed RRC LLM. The decoder-only transformer is taught\
    \ RRC domain knowledge using supervised fine-tuning on a curated dataset of historical\
    \ RRC message traces collected from a real-world 5G network. This specialized\
    \ fine-tuning imparts explicit protocol knowledge to the model, enabling it to\
    \ emulate standard RRC behaviors accurately during live inference. Operationally,\
    \ the trained model functions continuously within the CU-CP, dynamically generating\
    \ control-plane messages as new UE requests are received.\n\n# III. FINE-TUNING\
    \ OF LLM FOR RRC MESSAGE PROCESSING\n\nOur approach consists of *dataset construction*,\
    \ *LoRA-based supervised fine-tuning*, and *latency-aware evaluation*. First,\
    \ raw 5G-NR traces are filtered and reorganised into strict onerequest–one-response\
    \ question–answer pairs (Section [III-A1\\)](#page-2-1). Second, a LLaMA3-8B backbone\
    \ is adapted with low-rank adapters and trained on this corpus to internalise\
    \ RRC syntax and semantics (Section [III-B1\\)](#page-3-1). Finally, the resulting\
    \ RRC-LLM is benchmarked in an end-to-end stand-alone (SA) testbed, where we measure\
    \ BERT-based cosine similarity against ground-truth replies and on-device response\
    \ latency (Section [IV\\)](#page-6-0).\n\n# *A. Dataset Construction*\n\n<span\
    \ id=\"page-2-1\"></span>*1) Data Collection and Pre-processing:* The data used\
    \ in this work originates from field test logs of 5G New Radio (5GNR) RRC sessions.\
    \ As these traces include non-RRC messages, such as those related to the Access\
    \ and Mobility Management Function (AMF)—they require preprocessing to isolate\
    \ the relevant RRC-specific exchanges. To enable effective use in LLM training,\
    \ the data is cleaned and reorganized to exclude non-RRC entries and meta-data.\
    \ Fig. [4](#page-2-2) illustrates a portion of a processed RRC session, showing\
    \ only the RRC-specific messages after the removal of unrelated protocol content.\n\
    \n![](_page_2_Figure_10.jpeg)\n\n<span id=\"page-2-2\"></span>Fig. 4. Example\
    \ of a pre-processed RRC trace segment. Messages are structured into Question\
    \ (Q) / Answer (A) pairs for LLM training, where 'Q' denotes the input RRC message(s)\
    \ (X) and 'A' the target response (Y). Note the consolidation of multiple physical\
    \ messages (e.g., *rrcReconfigurationComplete* and *measurementReport*) into a\
    \ single logical request, labeled here as Q4(concat) and Q6(concat).\n\n![](_page_3_Figure_0.jpeg)\n\
    \n<span id=\"page-3-0\"></span>Fig. 3. Data flow for fine-tuning and inference\
    \ in the LLM-based RRC system.\n\nThe RRC-LLM module in Fig. [2](#page-2-0) handles\
    \ tasks related to RRC signalling and connection management. While the current\
    \ implementation primarily focuses on RRC interactions, additional messages from\
    \ the core network can be appended as context to the LLM inputs. Historical RRC\
    \ exchanges can also be appended to enrich the model's contextual understanding.\n\
    \nTo facilitate instruction fine-tuning, a strictly one-questionone-answer labeled\
    \ dataset is required, where each \"question\" corresponds to an uplink RRC message\
    \ input, and each \"answer\" corresponds to the downlink RRC message output. In\
    \ the original traces, request-reply pairs are frequently interleaved due to chronological\
    \ logging, and multiple RRC procedures may overlap—a common occurrence in real\
    \ network traces. To address this issue, UE-specific signalling exchanges are\
    \ extracted by isolating one RRC procedure at a time. By filtering out AMF-related\
    \ messages and consolidating only the RRC-relevant information, the dataset is\
    \ reorganized to meet the one-question-one-answer standard. Specifically, all\
    \ RRC request messages within a session are classified as a consolidated \"request,\"\
    \ and all RRC response messages are classified as a consolidated \"response.\"\
    \n\n#### *B. Fine-Tuning Procedure*\n\nA pre-trained LLM is fine-tuned using supervised\
    \ learning on the processed dataset. This allows the model to learn RRCspecific\
    \ message structures, improving its ability to generate compliant responses. The\
    \ objective is to make the model's responses closely match the ground truth annotations\
    \ in terms of structure and content, thereby enhancing its reliability in RRC\
    \ message generation tasks.\n\n<span id=\"page-3-1\"></span>*1) Training and Inference\
    \ Procedure:* The system implements a structured, LLM-based training and inference\
    \ process for handling RRC messages in a 5G network, as outlined in Fig. [3.](#page-3-0)\
    \ It begins with an *Input Sentence* derived from a linearized RRC message, preserving\
    \ essential syntactic and semantic relationships to streamline protocol message\
    \ processing.\n\nDuring the *Forward Pass*, Byte Pair Encoding (BPE) tokenizes\
    \ the input into variable-length sequences of subword tokens [\\[9\\]](#page-11-8).\
    \ These sequences are then padded to a standardized length before being processed\
    \ by the fine-tuned model πθref . Low-Rank Adaptation (LoRA) weights supplement\
    \ base weights via low-rank matrices, enhancing model adaptability. The cross-entropy\
    \ loss is computed between the predicted and ground truth token distributions,\
    \ quantifying the discrepancy at each timestep and guiding the model to better\
    \ capture RRCspecific semantic patterns.\n\nIn the *Backward Pass*, gradients\
    \ for LoRA parameters are computed via backpropagation, prompting weight updates\
    \ that minimize prediction errors [\\[10\\]](#page-11-9). The Adam optimizer is\
    \ applied to these LoRA-specific parameters, iterating to align the model's output\
    \ with the expected RRC responses.\n\nFinally, the *Output Sentence* represents\
    \ the generated downlink response for the given uplink RRC message.\n\nTABLE I\
    \ TRAINING CONFIGURATION\n\n<span id=\"page-3-2\"></span>\n\n| Parameter     \
    \              | Value            |\n|-----------------------------|------------------|\n\
    | Model                       | LLaMA3-8B        |\n| Max Sequence Length    \
    \     | 4096             |\n| Training Precision          | bf16             |\n\
    | Device                      | CUDA             |\n| Batch Size             \
    \     | 2                |\n| Epochs                      | 18               |\n\
    | Gradient Accumulation Steps | 1                |\n| Optimizer              \
    \     | AdamW            |\n| Learning Rate               | 2e-5             |\n\
    | Loss Function               | CrossEntropyLoss |\n\nTable [I](#page-3-2) summarizes\
    \ the key hyperparameters and configura-\n\ntions used during the supervised fine-tuning\
    \ of the LLaMA3- 8B model [\\[11\\]](#page-11-10). These settings ensure that\
    \ the model is trained under optimal resource constraints while preserving fidelity\
    \ to the original RRC message structures.\n\n<span id=\"page-4-1\"></span>*2)\
    \ Tokenization and Sequence Generation from RRC Messages:* At present, each complete\
    \ RRC message (whether a request or a response) is composed of multiple fields\
    \ containing specific parameters and information elements as defined by 5G NR\
    \ standards. These fields are systematically extracted and linearized into a flat\
    \ text representation while preserving their hierarchical and semantic relationships.\
    \ The linearized text forms a \"Sentence\", defined here as a single, coherent\
    \ textual representation that encapsulates the entire meaning of the RRC message.\n\
    \nThis Sentence is tokenized using the same Byte Pair Encoding (BPE) tokenizer\
    \ employed during the pre-training of the LLaMA3-8B model, ensuring consistency\
    \ in token representation. In this context, a \"sequence\" refers to the ordered\
    \ set of BPE-derived tokens. This methodology ensures that structural and contextual\
    \ information within the RRC message fields is preserved, enabling the language\
    \ model to learn the intricate dependencies among different fields.\n\n*Key Definitions:*\n\
    \n• Input (X): The input Sentence represented as a sequence of tokens\n\n$$X =\
    \ \\{x\\_1, x\\_2, \\dots, x\\_S\\},\\tag{1}$$\n\nobtained using the BPE tokenizer.\n\
    \n• Label (Y ): The target Sentence corresponding to X, represented as\n\n$$Y\
    \ = \\{y\\_1, y\\_2, \\dots, y\\_T\\},\\tag{2}$$\n\nalso tokenized by BPE.\n\n\
    • Inference Output (Y ′ ): The Sentence generated by the model during inference,\n\
    \n$$Y' = \\{y\\_1', y\\_2', \\dots, y\\_{T'}'\\},\\tag{3}$$\n\nobtained by decoding\
    \ the output tokens.\n\nHere, S, T, and T ′ denote the number of tokens in the\
    \ input sequence X, in the target sequence Y , and in the modelgenerated output\
    \ Y ′ , respectively.\n\n*Example of BPE Tokenization:* Consider an excerpt from\
    \ an RRC message, expressed in human-readable ASN.1 encoding. The hierarchical\
    \ structure of the message is first converted into a linear text Sentence. Each\
    \ word or parameter is then split into subword units via BPE, merging frequently\
    \ co-occurring characters to form more compact tokens.\n\nAs shown in Table [II,](#page-4-0)\
    \ each token corresponds to a meaningful subword (or word) from the original RRC\
    \ message fields. These tokens maintain critical semantic relationships and are\
    \ suitable for processing by the LLM. By defining the sequence in this manner,\
    \ both the syntactic and semantic content of RRC messages are captured in a machine-learning-friendly\
    \ format.\n\n*Integration with LLaMA:* To efficiently process text, we employ\
    \ LLaMA's built-in BPE tokenizer. This approach:\n\n- Converts RRC Sentences into\
    \ sequences of token IDs via BPE.\n- Maps tokens {xi} to unique integer IDs {id(xi)}.\n\
    \n<span id=\"page-4-0\"></span>TABLE II EXAMPLE OF TOKEN AND TOKEN ID LOOKUP TABLE\n\
    \n| Token   | Token ID | Token   | Token ID |\n|---------|----------|---------|----------|\n\
    | me      | 1 047    | Serving | 110 058  |\n| as      | 288      | Cell    |\
    \ 5 346    |\n| Results | 12 928   | serv    | 2 096    |\n| meas    | 9 802 \
    \   | 0       | 15       |\n| Id      | 906      | rs      | 15 181   |\n|   \
    \      | 220      | rp      | 49 866   |\n| 1       | 16       | 50      | 1 434\
    \    |\n| Result  | 2 769    | rq      | 116 596  |\n| 10      | 702      | -\
    \       | 533      |\n\n• Feeds these token IDs as inputs to the LLaMA model for\
    \ training or inference.\n\nIt is worth noting that, consistent with standard\
    \ BPE practice, this tokenizer represents numerical fields as sequences of digits\
    \ or sub-word tokens, rather than treating them as atomic numerical entities.\
    \ This direct usage of LLaMA's tokenizer ensures compatibility with existing model\
    \ infrastructure and preserves crucial domain-specific details within the RRC\
    \ messages. Consequently, the LLM can learn from, and eventually generate, properly\
    \ tokenized protocol messages, thereby aiding enhanced RRC in telecommunication\
    \ systems.\n\n*3) QA-Modeling of RRC Procedures:* This models RRC request-response\
    \ exchanges as question-answering (QA) tasks. To illustrate this, consider the\
    \ following conditional probability distribution between an RRC request (X) and\
    \ its corresponding response (Y ):\n\n$$P(Y \\mid X; \\theta) = \\prod\\_{t=1}^{T}\
    \ P(y\\_t \\mid y\\_{$$\n\nwhere θ is the set of all trainable model parameters,\
    \ y<sup>t</sup> denotes the token generated at time step t, and y<t = {y1, . .\
    \ . , yt−1} denotes the tokens generated before time step t. By explicitly modeling\
    \ P(Y | X; θ), the model incorporates the input context X throughout the generation\
    \ process, leading to outputs that remain closely aligned with the given RRC request.\n\
    \nIn our supervised fine-tuning (SFT) framework, the SFT model π<sup>θ</sup>ref\
    \ is trained to estimate this conditional probability distribution, capturing\
    \ the relationships in the labeled dataset. Specifically, during training, the\
    \ model is presented with input sequences X (i.e., uplink RRC requests) alongside\
    \ their corresponding target sequences Y (i.e., downlink responses). By minimizing\
    \ the negative log-likelihood of {(X, Y )} pairs, the SFT model π<sup>θ</sup>ref\
    \ learns how to generate responses Y ′ conditional on inputs X, effectively mastering\
    \ the QA-style mappings inherent in RRC request-response protocols.\n\n#### *C.\
    \ Model and Training Formulation*\n\n*a) Dataset definition:* Let the raw trace\
    \ corpus be Traw = {τ (j)}<sup>M</sup> <sup>j</sup>=1, where each τ (j) is a complete\
    \ base-station log and M = |Traw| is therefore the total number of such logs.\
    \ A deterministic preprocessing function F : Traw → {(X, Y )}—performing *filtering*,\
    \ *de-duplication*, and *chronological re-ordering*—yields the supervised training\
    \ set D = {(X(i) , Y (i) )} N <sup>i</sup>=1, where N = |D| denotes the number\
    \ of aligned uplink request–downlink response pairs obtained after preprocessing\
    \ and X(i) , Y (i) are defined in Sec. [III-B2.](#page-4-1)\n\n*b) Supervised\
    \ fine-tuning:* A copy of the foundation model is initialised as πθref = LoRA(W0;\
    \ A, B), with the resulting low-rank update given by W<sup>θ</sup> = W<sup>0</sup>\
    \ + AB, where:\n\n- W<sup>0</sup> ∈ R d×k is the *frozen* weight matrix from the\
    \ base model (e.g., a query, key, or value projection);\n- A ∈ R d×r and B ∈ R\
    \ r×k are the *trainable* low-rank factors;\n- d is the input (row) dimension,\
    \ typically equal to the model's hidden size;\n- k is the output (column) dimension\
    \ of the original weight;\n- r ≪ min{d, k} is the chosen rank of the LoRA update.\n\
    \nThe objective of SFT is to maximize the conditional likelihood:\n\n$$\\begin{split}\
    \ \\mathcal{L}\\_{\\text{SFT}}(\\theta\\_{\\text{ref}}) &= -\\frac{1}{N} \\sum\\\
    _{i=1}^{N} \\log P(Y^{(i)} \\mid X^{(i)}; \\theta\\_{\\text{ref}}) \\\\ &= \\\
    frac{1}{N} \\sum\\_{i=1}^{N} \\sum\\_{t=1}^{T^{(i)}} \\text{CE} \\{ y\\_t^{(i)},\
    \ \\pi\\_{\\theta\\_{\\text{ref}}}(y\\_t \\mid y\\_{$$\n\nwhere CE y (i) t , πθref\
    \ (y<sup>t</sup> | y<t, X(i) ) is the cross-entropy loss,and T (i) is the length\
    \ (in BPE tokens, including the explicit end-of-sequence marker) of the i-th target\
    \ sequence Y (i) .\n\n*c) Inference.:* Let X be an unseen uplink RRC message and\
    \ let π<sup>θ</sup> denote the fine-tuned model whose parameters θ were obtained\
    \ after supervised fine-tuning. The model generates a downlink response by greedy\
    \ decoding,\n\n$$\\hat{Y} = \\arg\\max\\_{Y} \\pi\\_{\\theta}(Y \\mid X), \\tag{6}$$\n\
    \nand transmits Yˆ as the downlink RRC message.\n\nAlgorithm 1 RRC-LLM Training\
    \ and Inference Pipeline Require: Raw RRC traces Traw, pre-trained model πθinit\
    \ Ensure: Fine-tuned model π<sup>θ</sup> ∗ 1: Phase 1: Dataset Construction 2:\
    \ D ← Preprocess(Traw) {Filter, align, and extract (X, Y ) pairs} 3: for all (X(i)\
    \ , Y (i) ) ∈ D do 4: X(i) ← BPE\\_Tokenize(X(i) ) 5: Y (i) ← BPE\\_Tokenize(Y\
    \ (i) ) 6: end for 7: Phase 2: Supervised Fine-Tuning (SFT) 8: π<sup>θ</sup>ref\
    \ ← LoRA\\_Init(π<sup>θ</sup>init) 9: for e = 1 to ESFT do 10: for all (X(i) ,\
    \ Y (i) ) ∈ D do 11: LSFT ← − log π<sup>θ</sup>ref(Y (i) | X(i) ) 12: Update θref\
    \ using Adam on LSFT 13: end for 14: end for 15: Phase 3: Inference 16: Given\
    \ new uplink message X, compute\n\n# 17: Yˆ ← arg max<sup>Y</sup> πθ(Y | X)\n\n\
    18: return Yˆ\n\n# *D. Training Configuration and Convergence Analysis*\n\nDuring\
    \ the supervised fine-tuning of the LLaMA3-8B model, we observed that the training\
    \ loss rapidly decreased within the first few hundred steps, reaching a relatively\
    \ stable range by step 200.\n\nFig. [5](#page-5-0) shows the log–log training-loss\
    \ curve. Within the first 1000 steps, the EMA-smoothed loss plunges from ∼1.0\
    \ to the 10−2–10−<sup>3</sup> range, while the raw trace—though noisy—exhibits\
    \ diminishing spikes, indicating increasing model stability. After step 8 400,\
    \ improvements level off; therefore, the checkpoint marked by the vertical red\
    \ dashed line is chosen as a compromise between rapid convergence and the risk\
    \ of overfitting.\n\n![](_page_5_Figure_20.jpeg)\n\n<span id=\"page-5-0\"></span>Fig.\
    \ 5. Log–log plot of the per-step training loss. The pale blue trace is the raw\
    \ loss at each optimizer step, and the dark blue curve is an exponential moving\
    \ average (EMA) that smooths out high-frequency fluctuations. The vertical red\
    \ dashed line marks the epoch that is selected for downstream evaluation.\n\n\
    ![](_page_5_Figure_22.jpeg)\n\n<span id=\"page-5-1\"></span>Fig. 6. Mean training\
    \ loss for each 1200-step epoch. Loss falls rapidly in the first few epochs, reaches\
    \ its minimum just before the chosen epoch (red dashed line), and thereafter fluctuates\
    \ around a low, stable plateau as the model nears convergence.\n\nGiven the scale\
    \ of the LLAMA3-8B backbone and the high memory footprint of long-sequence RRC\
    \ samples, we adopt a pragmatic checkpointing approach that balances training\
    \ efficiency with generalisation. Instead of running full validation passes in\
    \ parallel—an option that would roughly double GPU hours and exhaust the testbed's\
    \ memory budget—we monitor the exponential-moving average of the training loss\
    \ and freeze the model once that curve stabilises (Fig. [5\\)](#page-5-0). Generalisation\
    \ is then evaluated offline on a disjoint trace suite using the Sentence-similarity\
    \ framework of Section [IV.](#page-6-0) This two-stage procedure keeps computational\
    \ overheads manageable while still providing a reliable, out-of-sample measure\
    \ of model quality.\n\nFig. [6](#page-5-1) further supports this choice: the average\
    \ loss per epoch decreases from approximately 0.035 in epoch 1 to around 0.012\
    \ just before the selected epoch. Beyond this point, additional fine-tuning yields\
    \ only marginal improvements, with the loss fluctuating between 0.015 and 0.022.\
    \ This plateau indicates that the model's parameters have largely stabilized,\
    \ making the selected epoch an effective trade-off between training time and performance\
    \ on the RRC task.\n\nThe \"loss–step\" and \"loss–epoch\" views together indicate\
    \ three distinct training phases. Phase I (steps 0–1 k, ≈ 0.1 epoch) shows a two-order-of-magnitude\
    \ drop—from ≈ 1 to 10<sup>−</sup><sup>2</sup>–10<sup>−</sup><sup>3</sup>% demonstrating\
    \ how quickly a LoRA-adapted LLaMA-3 internalises the RRC domain once exposed\
    \ to only a handful of mini-batches. Phase II (steps 1 k–8.4 k, epochs 1– 7) is\
    \ characterised by a gentler slope plus gradually shrinking step-to-step variance;\
    \ spikes visible in the raw trace stem from the tiny effective batch size (2 ×\
    \ 4096 tokens, no gradient accumulation) but are damped in the EMA curve. Phase\
    \ III (≥ 8.4 k steps, epochs ≥ 7) plateaus around a mean crossentropy of 0.012;\
    \ further optimisation oscillates within 0.015– 0.022 without consistent improvement.\n\
    \nSelecting the red-line checkpoint therefore (i) captures 95 %+ of the attainable\
    \ loss reduction, (ii) prevents wasteful computation beyond the point of diminishing\
    \ returns, and (iii) limits the risk of over-fitting that can arise when finetuning\
    \ with low-rank adapters on a narrow protocol corpus. The remainder of this paper\
    \ reports results using this 7-epoch model.\n\n#### IV. PERFORMANCE AND EVALUATION\n\
    \n# <span id=\"page-6-0\"></span>*A. Protocol-fidelity Metrics*\n\nWe compare\
    \ sentence similarity using BERT embeddings and cosine similarity to assess model\
    \ performance [\\[12\\]](#page-11-11). The LLM-generated responses are compared\
    \ with actual base station replies, ensuring protocol response accuracy.\n\n*a)\
    \ BERT-based Similarity Computation:* To evaluate model performance, we apply\
    \ sentence similarity analysis based on BERT embeddings and cosine similarity.\
    \ The responses generated by the LLM are compared against actual base station\
    \ replies to ensure compliance with protocol specifications and response accuracy.\
    \ A high-level overview of this sentence similarity approach is presented in Figure\
    \ [7.](#page-6-1)\n\nFormally, for each sample i in the dataset:\n\n• Target Sentence\
    \ Y (i) : The reference sentence provided in the dataset.\n\n![](_page_6_Figure_10.jpeg)\n\
    \n<span id=\"page-6-1\"></span>Fig. 7. Illustration of using BERT to encode ground\
    \ truth RRC downlink messages and LLM-generated responses, followed by cosine\
    \ similarity calculation for performance evaluation.\n\n• Generated Sentence Y\
    \ ′(i) : The output produced by the fine-tuned model πθref given input X(i) .\n\
    \nWe use a pre-trained BERT model BERT<sup>ϕ</sup> with fixed parameters ϕ to\
    \ obtain contextual embeddings for each token in the sentences. Let\n\n$$\\begin{aligned}\
    \ E^{(i)} &= \\text{BERT}\\_{\\phi} \\{ Y^{(i)} \\} = [e\\_1^{(i)}, e\\_2^{(i)},\
    \ \\dots, e\\_{T^{(i)}}^{(i)}], \\\\ E^{\\prime (i)} &= \\text{BERT}\\_{\\phi}\
    \ \\{ Y^{\\prime (i)} \\} = [e\\_1^{\\prime (i)}, e\\_2^{\\prime (i)}, \\dots,\
    \ e\\_{T^{\\prime (i)}}^{\\prime (i)}]. \\end{aligned} \\tag{7}$$\n\nTo obtain\
    \ fixed-size sentence embeddings, we apply a simple pooling operation:\n\n$$\\\
    begin{aligned} u^{(i)} &= \\frac{1}{T^{(i)}} \\sum\\_{t=1}^{T^{(i)}} e\\_t^{(i)},\
    \ \\\\ v^{(i)} &= \\frac{1}{T'^{(i)}} \\sum\\_{t=1}^{T'^{(i)}} e\\_t'^{(i)}. \\\
    end{aligned} \\tag{8}$$\n\nThe cosine similarity between the pooled sentence embeddings\
    \ u (i) and v (i) is then calculated as:\n\n$$Z^{(i)} = \\cos(\\theta^{(i)}) =\
    \ \\frac{u^{(i)} \\cdot v^{(i)}}{||u^{(i)}|| \\, ||v^{(i)}||},\\tag{9}$$\n\nsimilarity\
    \ score Z (i) ∈ [−1, 1]\n\n# *B. Datasets*\n\nWe evaluate RRC-LLM on two complementary\
    \ corpora that balance real-world complexity with fully open access. We captured\
    \ a total of 2 524 Radio Resource Control (RRC) sessions, amounting to 30 247\
    \ *dialog pairs*, on a 5GNR network. Measurements were collected with off-the-shelf\
    \ smartphones from multiple major UE vendors and gNB deployed by a single infrastructure\
    \ supplier. Only metadata essential for protocol analysis (timestamp, direction,\
    \ and cell identifier) was retained, and all vendor and location details were\
    \ anonymised. Owing to confidentiality reasons, this dataset is not publicly released.\n\
    \nTo complement the proprietary log with fully reproducible data, we also generated\
    \ a synthetic corpus using our open-source nrRRC\\_Simulator [\\[13\\]](#page-11-12).\
    \ The resulting RRC dataset [\\[14\\]](#page-11-13) contains paired uplink and\
    \ downlink messages produced by a standards-compliant LTE core and eNodeB stack\
    \ built on SRSRAN. This simulator primarily captures RRC messages related to connection\
    \ setup, and transitions between IDLE and ACTIVE states under constant radio conditions.\
    \ Each sample comprises: (i) an ISO-8601 timestamp, (ii) message direction (*UL*/*DL*),\
    \ (iii) the ASN.1-encoded RRC frame, and (iv) simulator ground-truth labels (e.g.\
    \ layer-3 procedure name). The dataset is released under the Apache 2.0 licence\
    \ and can be obtained from the Hugging Face Hub [\\[14\\]](#page-11-13).\n\n####\
    \ *C. Inference Results*\n\nEmpirical analysis demonstrates that our fine-tuned\
    \ RRC-LLM model generates responses with significantly improved fidelity to real-world\
    \ RRC messages compared to a baseline Llama model. Our approach achieves higher\
    \ BERT-based similarity scores, reduces inference latency, and aligns better with\
    \ real-world RRC message patterns.\n\n![](_page_7_Figure_4.jpeg)\n\n<span id=\"\
    page-7-0\"></span>Fig. 8. Histogram of Similarity Scores\n\n*a) Comparative Analysis\
    \ of Similarity Distributions:* Figure [8](#page-7-0) illustrates the distribution\
    \ of similarity scores. Yellow segments correspond to the original Llama model,\
    \ while blue segments correspond to the fine-tuned RRC-LLM. Notable observations\
    \ include:\n\n- Two-Peak Distribution (RRC-LLM). Fine-tuning creates an additional\
    \ peak at high similarity (0.8–1.0), reflecting improved alignment with the intended\
    \ RRC protocol format.\n- Higher Overall Scores. The fine-tuned RRC-LLM clusters\
    \ more frequently around higher similarity scores, indicating enhanced structural\
    \ fidelity and reduced generation of non-protocol text.\n- Baseline Llama Limitations.\
    \ The original Llama tends to generate mid-range similarity outputs (around 0.5–0.6),\n\
    \nThe baseline Llama exhibits a median similarity of 0.60, whereas the fine-tuned\
    \ RRC-LLM reaches a median of 0.97. This constitutes an absolute increase of 0.37—a\
    \ relative gain of approximately 61%—highlighting the substantial improvement\
    \ in structural fidelity delivered by the fine-tuning process.\n\n*b) Baseline\
    \ comparison with public LLMs:* To contextualize the performance of our fine-tuned\
    \ RRC-LLM, we conducted a zero-shot evaluation using four publicly available LLMs[1](#page-7-1)\
    \ : Gemini 1.5-Flash, GPT-4o, Claude 3.5 Sonnet v2, and o3-mini. Table [III](#page-8-0)\
    \ presents the median cosine similarity scores for these public LLMs on subsets\
    \ of both the synthetic and field-collected datasets, alongside the available\
    \ scores for our RRC-LLM.\n\nRRC-LLM was trained and frozen before the syntheticsimulator\
    \ benchmark was finalised; consequently, that row in Table [III](#page-8-0) is\
    \ left blank. Because the simulator covers only a narrow slice of RRC procedures,\
    \ we focused our evaluation budget on the far larger and more diverse fieldcollected\
    \ corpus (30,247 request-response pairs), which offers a stronger indicator of\
    \ real-world performance. All subsequent analysis—including the similarity-score\
    \ distribution in Figure [8—](#page-7-0)therefore reflects this production-grade\
    \ dataset, where RRC-LLM achieves a median cosine similarity of 0.97, comfortably\
    \ outperforming every public baseline. A full rerun on the synthetic set is planned\
    \ for a future release.\n\n*Evaluation Process*: The zero-shot assessment for\
    \ the public LLMs (Gemini 1.5-Flash, GPT-4o, Claude 3.5 Sonnet v2, and o3-mini)\
    \ was performed for efficiency on a focused test set. This set consisted of 10\
    \ unique uplink-downlink (UL-DL) Radio Resource Control (RRC) message pairs from\
    \ our synthetic simulator dataset and an additional 10 unique UL-DL pairs from\
    \ our field-collected dataset. Each model was initially prompted with the instruction:\
    \ *\"You are playing the role of a base station and attempting to respond to an\
    \ uplink RRC message with an appropriate downlink RRC message.\"* Subsequently,\
    \ individual uplink (UL) RRC messages from this 20-pair combined test set were\
    \ provided one at a time, and the model's generated downlink (DL) RRC message\
    \ was collected for analysis.\n\n*Qualitative Observations*: Beyond the quantitative\
    \ similarity scores, several qualitative behaviors were noted during these evaluations.\n\
    \n- A common characteristic across all evaluated public LLMs was the inclusion\
    \ of explanatory \"wraparound\" text accompanying their generated RRC replies.\
    \ As this text is extraneous to the protocol message itself and can be trivially\
    \ removed, we processed the outputs to extract the pure ASN.1 RRC message for\
    \ assessment.\n- GPT-4o exhibited a notable inconsistency in maintaining its assigned\
    \ role as a base station. While it correctly responded with a DL RRC message to\
    \ the first UL message, it subsequently appeared to 'forget' its role,\n\n<span\
    \ id=\"page-7-1\"></span><sup>1</sup>Model snapshots: GPT-4o (OpenAI, Snapshot\
    \ 2024-11-20) [\\[15\\]](#page-11-14), o3-mini (OpenAI, Snapshot 2025-01-31) [\\\
    [16\\]](#page-11-15), Gemini 1.5-Flash (Google DeepMind, Snapshot 2024-09-24)\
    \ [\\[17\\]](#page-11-16), and Claude 3.5 Sonnet (Anthropic, Snapshot 2024-10-22)\
    \ [\\[18\\]](#page-11-17).\n\ndefaulting to commenting on or describing the content\
    \ of later UL messages rather than generating appropriate DL RRC replies. This\
    \ behavior likely impacted its overall similarity score (as seen in Table [III\\\
    )](#page-8-0).\n\n• In contrast, Claude 3.5 Sonnet demonstrated a surprisingly\
    \ nuanced understanding of RRC procedures. For example, it was the only public\
    \ model among those tested that correctly identified scenarios where no downlink\
    \ reply is necessary, such as after receiving an *RRCReconfigurationComplete*\
    \ message. Furthermore, Claude 3.5 Sonnet displayed an advanced level of contextual\
    \ understanding by, for instance, generating a *UECapabilityEnquiry* message when\
    \ it detected discrepancies in RRC transaction identifiers between past and current\
    \ UL RRC messages. This level of protocol awareness contributes to its strong\
    \ performance reflected in Table [III\\)](#page-8-0).\n\nThese qualitative insights\
    \ highlight the varying degrees of protocol literacy and contextual reasoning\
    \ exhibited by generalpurpose LLMs when applied to the specialized domain of RRC\
    \ message generation without specific fine-tuning. While some models show promise,\
    \ specialized models like our RRC-LLM, extensively trained on 30,247 field data\
    \ pairs and evaluated (as shown in Figure [8\\)](#page-7-0), demonstrate superior\
    \ and more consistent protocol literacy.\n\n<span id=\"page-8-0\"></span>TABLE\
    \ III MEDIAN COSINE SIMILARITY ON THE SIMULATOR DATASET AND FIELD-COLLECTED DATASET\
    \ MEASUREMENTS.\n\n| Model                | Synthetic data | Field data |\n|----------------------|----------------|------------|\n\
    | Gemini 1.5-Flash     | 0.690          | 0.585      |\n| GPT-4o             \
    \  | 0.710          | 0.496      |\n| Claude 3.5 Sonnet v2 | 0.728          |\
    \ 0.768      |\n| GPT-o3-mini          | 0.678          | 0.705      |\n| Original\
    \ Llama 3-8B  | 0.637          | 0.600      |\n| RRC-LLM (Ours)       | N.A. \
    \          | 0.970      |\n\n*c) Similarity Responses:* Table [IV](#page-9-0)\
    \ summarises three representative cases. In the *high-similarity* row (*sim. =\
    \ 0.9991*), the fine-tuned RRC-LLM reproduces the reference message almost verbatim,\
    \ diverging only in metadata such as timestamps; these negligible differences\
    \ (≤0.1 %) leave protocol fidelity intact. The *medium-similarity* example (*sim.\
    \ = 0.92*) reveals a different behaviour: while the generated message retains\
    \ the overall hierarchy, the model inserts an unsolicited securityConfig{...}\
    \ fragment. The addition is syntactically legal and does not break ASN.1 encoding,\
    \ yet it was not present in the ground-truth label—hence the reduced similarity.\
    \ Finally, the *low-similarity* row (*sim. = 0.68*) shows the baseline result\
    \ show emitting free-form commentary interleaved with partial RRC fields and even\
    \ reversing the message direction, all of which markedly depress the score.\n\n\
    Overall, the fine-tuned RRC-LLM tends to preserve structure, including the sequential\
    \ field order consistent with the ASN.1 definitions as learned from the linearized\
    \ training data; discrepancies at the medium-similarity level typically stem from\
    \ extra but valid sub-blocks such as securityConfig. In contrast, the untuned\
    \ LLaMA often drifts into humanlanguage prose, explaining its consistently lower\
    \ similarity distribution.\n\n*d) Timestamp Handling:* Our system uses timestamps\
    \ from uplink RRC messages as additional input features, which helps the model\
    \ maintain ordering and context during training. While the model also generates\
    \ timestamps in its output for engineering convenience, these do not materially\
    \ affect the similarity assessment.\n\nOverall, fine-tuning the RRC-LLM has introduced\
    \ a highsimilarity peak, improved protocol adherence, and produced a more diverse\
    \ similarity distribution, underscoring the model's superior performance on structured\
    \ protocol tasks.\n\n#### *D. Training and Inference Cost*\n\n*a) Hardware Topology\
    \ and Parallelism.:* All experiments were conducted on nodes equipped with two\
    \ NVIDIA A100 GPUs (80GB PCIe), each interconnected via NVLink and connected to\
    \ the host through a PCIe switch. We employed tensor parallelism to split model\
    \ weights across the two devices, enabling pipelined execution without full model\
    \ replication. This topology sustained up to 300 GB/s intra-node bandwidth and\
    \ preserved throughput across long RRC sequences.\n\nIn tensor parallelism, large\
    \ matrix multiplications are divided at the tensor level across multiple devices.\
    \ During each forward and backward pass, intermediate activations and gradients\
    \ are exchanged synchronously between GPUs. While PCIe supported host-GPU data\
    \ flow, NVLink was the primary communication path between GPUs for high-bandwidth\
    \ peerto-peer transfers during collective operations (e.g., all-reduce). This\
    \ setup allowed for efficient compute scaling but introduced tight coupling—any\
    \ delay on one GPU could block progress on the other, making training particularly\
    \ vulnerable to communication faults and imbalances.\n\n*b) Latency and Inference\
    \ Throughput:* Table [V](#page-10-0) details the system specifications used throughout\
    \ training and inference. Training consumed up to 110 GB of total GPU memory across\
    \ the two A100 cards, while inference in GGUF format required approximately 29\
    \ GB per thread. Each GPU operated under CUDA 12, with tensor parallelism executed\
    \ entirely ondevice to minimize CPU-GPU communication latency.\n\nTo reduce computational\
    \ resource usage during inference, we adopted llama.cpp, a lightweight C++-based\
    \ runtime optimized for quantized LLMs. Compared to PyTorch-based inference, this\
    \ approach significantly reduced VRAM consumption, requiring only 29 GB per thread\
    \ in GGUF format—and lowered GPU utilization. This design choice aligns with the\
    \ practical constraints of real-world deployment scenarios, where minimizing hardware\
    \ requirements is crucial. In our experiments, it enabled us to assess whether\
    \ the current setup could realistically meet the latency constraints imposed by\
    \ mobile network standards. During fine-tuning, we used PyTorch Distributed (torch.distributed)\
    \ to partition the model across two A100 GPUs, maximizing memory efficiency and\
    \ parallelism.\n\nThe median inference time per RRC message is 6.9 seconds, with\
    \ an average of 10.4 seconds. This falls short of real-time operation but remains\
    \ practical for protocol emulation, offline testing, and system debugging. For\
    \ reference, typical values for RRC timers (e.g. T300, T301) are in the order\
    \ of milliseconds.\n\nTABLE IV EXAMPLES OF RRC-LLM RESPONSES ACROSS SIMILARITY\
    \ LEVELS\n\n<span id=\"page-9-0\"></span>\n\n| High-Similarity (Similarity = 0.9991)\
    \             |                                                   |  |\n|---------------------------------------------------|---------------------------------------------------|--|\n\
    | True Label: RRC Message                           | RRC-LLM (Similarity = 0.9991)\
    \                     |  |\n|                                                \
    \   |                                                   |  |\n| Time :       \
    \                                     | Time :                               \
    \             |  |\n| 2024 −04 −15                                      | 2024\
    \ −04 −15                                      |  |\n| 1 1 : 3 4 : 3 8 . 4 6 9\
    \ 8 9 2                     | 1 1 : 3 4 : 3 8 . 4 6 9 9 8 2                  \
    \   |  |\n| Message name :                                    | Message name :\
    \                                    |  |\n| r r c S e t u p                 \
    \                  | r r c S e t u p                                   |  |\n\
    | I n t e r f a c e :                               | I n t e r f a c e :    \
    \                           |  |\n| F1 E1                                    \
    \         | F1 E1                                             |  |\n| P r o t\
    \ o c o l :                                 | P r o t o c o l :              \
    \                   |  |\n| nrRrc                                            \
    \ | nrRrc                                             |  |\n| Time           \
    \                                   | Time                                   \
    \           |  |\n| d i f f :                                         | d i f\
    \ f :                                         |  |\n| 0 0 0 0 : 0 0 : 0 0 . 0\
    \ 0 1 8 9 8                 | 0 0 0 0 : 0 0 : 0 0 . 0 0 1 9 8 8              \
    \   |  |\n| {                                                 | {            \
    \                                     |  |\n| Message body : DL−CCCH−Message :\
    \                  | Message body : DL−CCCH−Message :                  |  |\n\
    | {                                                 | {                      \
    \                           |  |\n| message                                  \
    \         | message                                           |  |\n| c1     \
    \                                           | c1                             \
    \                   |  |\n| :                                                \
    \ | :                                                 |  |\n| r r c S e t u p\
    \                                   | r r c S e t u p                        \
    \           |  |\n| :                                                 | :    \
    \                                             |  |\n| r r c − T r a n s a c t\
    \ i o n I d e n t i f i e r | r r c − T r a n s a c t i o n I d e n t i f i e\
    \ r |  |\n| 0 ,                                               | 0 ,          \
    \                                     |  |\n| c r i t i c a l E x t e n s i o\
    \ n s               | c r i t i c a l E x t e n s i o n s               |  |\n\
    | r r c S e t u p :                                 | r r c S e t u p :      \
    \                           |  |\n| {                                        \
    \         | {                                                 |  |\n| r a d i\
    \ o B e a r e r C o n f i g                 | r a d i o B e a r e r C o n f i\
    \ g                 |  |\n| {                                                \
    \ | {                                                 |  |\n| srb −ToAddModList\
    \ {                               | srb −ToAddModList {                      \
    \         |  |\n| {                                                 | {      \
    \                                           |  |\n| srb − I d e n t i t y    \
    \                         | srb − I d e n t i t y                            \
    \ |  |\n| 1                                                 | 1              \
    \                                   |  |\n| }                                \
    \                 | }                                                 |  |\n|\
    \ }                                                 | }                      \
    \                           |  |\n| .                                        \
    \         | .                                                 |  |\n\nMedium-Similarity\
    \ (Similarity = 0.92)\n\n```\nTrue Label: RRC Message RRC-LLM (Similarity = 0.92)\n\
    ```\n} ,\n\n```\n. . .\n                 h e a d e r C o m p r e s s i o n notUsed\
    \ : NULL,\n                 s t a t u s R e p o r t R e q u i r e d t r u e\n\
    \                } ,\n                t − R e o r d e r i n g ms100\n        \
    \      }\n            }\n          }\n         } ,\n         measConfig {\n  \
    \         measObjectToAddModList {\n            {\n              m e a s O b j\
    \ e c t I d 1 ,\n              measObject measObjectNR : {\n                 s\
    \ s b F r e q u e n c y 634080 ,\n                 s s b S u b c a r r i e r S\
    \ p a c i n g kHz30 ,\n                 smtc1 {\n                   p e r i o\
    \ d i c i t y A n d O f f s e t s f 2 0 : 0 ,\n                   d u r a t i\
    \ o n s f 5\n                 } ,\n. . .\n                                   \
    \                                                . . .\n                     \
    \                                                                            \
    \    h e a d e r C o m p r e s s i o n notUsed : NULL\n                      \
    \                                                                            \
    \  } ,\n                                                                     \
    \                               t − R e o r d e r i n g ms100\n              \
    \                                                                            \
    \        }\n                                                                 \
    \                               }\n                                          \
    \                                                     } ,\n                  \
    \                                                                            \
    \ s e c u r i t y C o n f i g {\n                                            \
    \                                                    s e c u r i t y A l g o r\
    \ i t h m C o n f i g {\n                                                    \
    \                                              c i p h e r i n g A l g o r i t\
    \ h m nea2 ,\n                                                               \
    \                                   i n t e g r i t y P r o t A l g o r i t h\
    \ m n i a 2\n                                                                \
    \                                }\n                                         \
    \                                                     }\n                    \
    \                                                                         } ,\n\
    \                                                                            \
    \                 measConfig {\n                                             \
    \                                                  measObjectToAddModList {\n\
    \                                                                            \
    \                    {\n                                                     \
    \                                             m e a s O b j e c t I d 1 ,\n  \
    \                                                                            \
    \                    measObject measObjectNR : {\n                           \
    \                                                                          s s\
    \ b F r e q u e n c y 634080 ,\n                                             \
    \                                                        s s b S u b c a r r i\
    \ e r S p a c i n g kHz30 ,\n                                                \
    \                                                     smtc1 {\n              \
    \                                                                            \
    \             p e r i o d i c i t y A n d O f f s e t s f 2 0 : 0 ,\n        \
    \                                                                            \
    \                   d u r a t i o n s f 5\n```\n# Low-Similarity (Similarity =\
    \ 0.68)\n\n| True Label: RRC Message                                         \
    \  | RRC-LLM (Similarity = 0.68)                                             \
    \                          |\n|-------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n\
    |                                                                   |        \
    \                                                                            \
    \               |\n| Time :<br>2024 −04 −15<br>1 1 : 3 3 : 1 0 . 6 9 0 3 8 8 \
    \          | I t<br>l o o k s<br>l i k e<br>t h e r e ' s<br>a n o t h e r RRC\
    \ r e c o n f i g u r a t i o n   |\n| Message name :<br>r r c R e l e a s e \
    \                            | Complete<br>message<br>b e i n g<br>s e n t ! \
    \                                                    |\n| I n t e r f a c e :<br>F1\
    \ E1                                      | Message name :<br>r r c R e c o n\
    \ f i g u r a t i o n C o m p l e t e                             |\n| P r o t\
    \ o c o l :<br>nrRrc                                        | Time :<br>2024 −04\
    \ −15<br>1 1 : 3 3 : 0 0 . 2 4 3 9 9 7                                       \
    \    |\n| Time<br>d i f f :<br>0 0 0 0 : 0 0 : 1 0 . 6 3 3 8 3 2            |\
    \ I n t e r f a c e :<br>F1 E1                                               \
    \                       |\n| {<br>Message body : DL−DCCH−Message :           \
    \                  | P r o t o c o l :<br>nrRrc                              \
    \                                          |\n| {<br>message<br>c1<br>:<br>r r\
    \ c R e l e a s e<br>:               | Time<br>d i f f :<br>0 0 0 0 : 0 0 : 0\
    \ 0 . 1 8 7 4 4 1                                            |\n| r r c − T r\
    \ a n s a c t i o n I d e n t i f i e r<br>0 ,          | {<br>Message body :\
    \ UL−DCCH−Message :                                                          \
    \   |\n| c r i t i c a l E x t e n s i o n s<br>r r c R e l e a s e :<br>{ | message<br>c1<br>:<br>r\
    \ r c R e c o n f i g u r a t i o n C o m p l e t e<br>:<br>{               |\n\
    | }                                                                 | r r c −\
    \ T r a n s a c t i o n I d e n t i f i e r<br>3 ,                           \
    \               |\n| }                                                       \
    \          | c r i t i c a l E x t e n s i o n s<br>r r c R e c o n f i g u r\
    \ a t i o n C o m p l e t e :<br>{ |\n| }                                    \
    \                             | }                                            \
    \                                                     |\n| .                 \
    \                                                | }                         \
    \                                                                        |\n|\
    \                                                                   | }      \
    \                                                                            \
    \               |\n|                                                         \
    \          | .                                                               \
    \                                  |\n\n. . .\n\n<span id=\"page-10-0\"></span>TABLE\
    \ V SERVER SPECIFICATIONS FOR MODEL TRAINING AND INFERENCE\n\n| Component    \
    \        | Specification                   |\n|----------------------|---------------------------------|\n\
    | CPU                  | 52-core hyper-threaded          |\n| RAM            \
    \      | 512 GB                          |\n| GPU                  | 2 × NVIDIA\
    \ A10080GB PCIe NVLink |\n| GPU VRAM (Inference) | ∼29 GB per thread (GGUF format)\
    \ |\n| GPU VRAM (Training)  | ∼110 GB across two cards        |\n| CUDA Version\
    \         | v12                             |\n\n![](_page_10_Figure_2.jpeg)\n\
    \n<span id=\"page-10-1\"></span>Fig. 9. Distribution of Sentence Generation Times\n\
    \nFuture work will explore smaller and more efficient models to speed this up.\
    \ Figure [9](#page-10-1) shows the distribution of sentence generation times,\
    \ where longer examples correlate with multifield expansions and nested structures.\n\
    \n# *E. Discussion*\n\nFine-tuning LLAMA-8B on the RRC corpus surfaced a critical—albeit\
    \ rare—edge case: a small subset of controlplane exchanges, most notably the elaborate\
    \ rrcSetup procedures, exceed the model's 8,192-token context window. While the\
    \ base model supports sequences up to 8,192 tokens during inference, we fine-tuned\
    \ it using a 4,096-token limit (Table [I\\)](#page-3-2) to ensure training stability\
    \ and efficient GPU memory usage. This discrepancy reflects a deliberate trade-off:\
    \ shorter training windows reduce memory fluctuation and mitigate outof-memory\
    \ risks, while inference utilizes the full window to accommodate longer messages.\n\
    \nRather than discarding these over-length examples, we retained them by splitting\
    \ each message at a logical boundary and inserting a dedicated *segmentation marker*.\
    \ This marker signals to the model that reasoning continues in the next segment,\
    \ ensuring that every RRC transaction is fully represented during both training\
    \ and inference. Only a small fraction of messages required such segmentation.\n\
    \nDuring inference, we apply an identical routine. The model first consumes the\
    \ leading segment terminated by the marker; the subsequent segment (and, on rare\
    \ occasions, a third) is then supplied with the marker prepended, and the partial\
    \ generations are concatenated. This pragmatic workflow avoids architectural modifications\
    \ and guarantees that no generation call breaches the positional-embedding ceiling.\
    \ The trade-off is that each segment is decoded without direct visibility of its\
    \ predecessors, so perfect discourse continuity cannot be ensured.\n\nEmpirically,\
    \ the stitched outputs remain syntactically valid but occasionally exhibit duplicated\
    \ fields or minor inconsistencies—a manifestation of the lost global context that\
    \ a full-window transformer would capture. While such deviations are unacceptable\
    \ in a live control plane, their impact on our offline evaluation proved negligible\
    \ given the scarcity of over-length messages. Nevertheless, the study exposes\
    \ an intrinsic limitation of fixed-window architectures when they confront protocol\
    \ stacks with unbounded message length.\n\n# V. CONCLUSION AND FUTURE WORK\n\n\
    This paper has investigated the question of whether a decoder-only Large AI Model\
    \ (LAM) can acquire 5G NR RRC *protocol literacy*. By fine-tuning an 8-billion-parameter\
    \ LLaMA model with LoRA adapters, our RRC-LLM achieved approximately a 61% relative\
    \ gain in median cosine similarity with ground-truth RRC messages compared to\
    \ a zero-shot LLM baseline. The model now reproduces hierarchical field dependencies\
    \ in RRC messages, resolves partial inputs, and generates standards-compliant\
    \ responses, providing empirical evidence that LAM-based RRC emulation is a viable\
    \ building block for AI-native air interfaces and industry visions such as ETSI\
    \ ENI 051's AI-Core.\n\nOperational challenges. Despite the performance gains,\
    \ three issues still impede the real-time execution of this model and deployment\
    \ at scale: (i) computation and energy overhead (ii) autoregressive latency that\
    \ strains sub-millisecond control budgets; and (iii) finite context windows that\
    \ truncate multi-step procedures requiring long RRC message sequences.\n\nFuture\
    \ work. The following is a non-exhaustive list of the most promising research\
    \ lines on the path towards AI-native RRC layers:\n\n- 1) *Extended-context modelling:*\
    \ To support full-length RRC exchanges without truncation, extended-context mechanisms\
    \ will be needed, including memory-augmented transformers with cached activations,\
    \ hierarchical segment encoders, and efficient attention variants such as Longformer,\
    \ BigBird, RoPE-extended LLaMA, and ALiBi [\\[19\\]](#page-11-18)–[\\[24\\]](#page-12-0).\
    \ Through this combined approach, coherence and field-level fidelity are expected\
    \ to be preserved across long control-plane sequences.\n- 2) *Retrieval-Augmented\
    \ Generation (RAG):* A nonparametric memory will be attached to the RRC-LLM, indexing\
    \ clause-level 3GPP specifications (Rel. 15–19), curated traces, and streaming\
    \ telemetry. During inference, relevant passages will be retrieved via a dual-encoder\
    \ ranker, re-scored by a crossencoder, and injected into the prompt under a fixed\
    \ template [\\[25\\]](#page-12-1). To capture long-range dependencies such as\
    \ cascading timers, a Graph-RAG pipeline can be used to enable, enabling structured\
    \ retrieval of interconnected subgraphs [\\[26\\]](#page-12-2).\n\n3) *Model compression\
    \ and hierarchical control:* Inference efficiency can be further enhanced through\
    \ knowledge distillation, quantization [\\[27\\]](#page-12-3), and the adoption\
    \ of protocol-aware lightweight architectures. To achieve sub-second inference\
    \ latency, hierarchical model designs could be explored, wherein compact front-end\
    \ agents respond rapidly to RRC events, while larger backend models perform higher-level\
    \ reasoning. This design remains compatible with orchestration frameworks such\
    \ as ETSI ENI-051 [\\[5\\]](#page-11-4), though the emphasis is placed on reducing\
    \ latency and improving resource utilization efficiency.\n\nThe proposed efficiency\
    \ measures—model compression, hierarchical agent architectures, and long-context\
    \ techniques [\\[28\\]](#page-12-4)—are expected to reconcile the RRC-LLM's message-level\
    \ fidelity with the tight latency and energy envelopes of 5G RAN. In practical\
    \ terms, they align with emerging multi-agent AI designs (e.g., those sketched\
    \ in ETSI ENI-051) by translating high-level cognitive decisions into standards-compliant\
    \ RRC messages without exceeding sub-millisecond control budgets. By curbing runtime\
    \ overhead while maintaining accuracy over extended RRC sessions, these measures\
    \ move LAM-based emulation from proof-of-concept toward deployment inside modular,\
    \ AI-native base stations, thereby realizing the introductory claim that LAM can\
    \ serve as integral design primitives of the AI-AI rather than merely auxiliary\
    \ optimizers.\n\nLooking beyond the 5G horizon, further acceleration and robustness\
    \ will be pursued through computation-efficient backbones such as state-space\
    \ models, aggressive quantisation, speculative decoding, and sparse activation.\
    \ When combined with the aforementioned toolkit, these advances are projected\
    \ to elevate LAM-driven RRC from laboratory prototype to realtime, energy-aware\
    \ operation in ultra-dense, low-latency 6G networks where multi-modal, self-explaining\
    \ agents co-design physical- and MAC-layer procedures. In this way, the research\
    \ trajectory outlined here closes the loop between the conceptual promise of an\
    \ AI-AI and its concrete instantiation in nextgeneration RAN deployments.\n\n\
    #### APPENDIX A\n\n# SYSTEM INTERRUPTIONS AND RELIABILITY BOTTLENECKS.\n\nOut\
    \ of 73 full training attempts totaling 178.6 GPUhours, only 8 completed without\
    \ interruption. The remaining 65 failures—recovered via checkpoint resumes—introduced\
    \ significant wall-clock delays and impacted epoch scheduling variance.\n\nAs\
    \ shown in Table [VI,](#page-11-19) the most common failure categories were host-level\
    \ faults—including SSD stalls and NCCL watchdog timeouts—followed by GPU memory\
    \ overflows and hardware issues. SSD interruptions, accounting for 16.9% of failures,\
    \ resulted from limited local disk capacity during frequent incremental checkpointing\
    \ and data flushing. We mitigated this by expanding storage in later runs. GPU-side\
    \ issues, including CUDA out-of-memory (OOM) errors and device instability, made\
    \ up 21.6% of interruptions. These were\n\nTABLE VI INTERRUPTIONS BY COMPONENT\n\
    \n<span id=\"page-11-19\"></span>\n\n| Component             | Category      \
    \  | Interruptions | %    |\n|-----------------------|-----------------|---------------|------|\n\
    | Software Bug          | Host            | 8             | 12.3 |\n| Faulty GPU\
    \            | GPU Maintenance | 7             | 10.8 |\n| SSD issue         \
    \    | Maintenance     | 11            | 16.9 |\n| NCCL Watchdog Timeout | Host\
    \            | 9             | 13.8 |\n| CUDA OOM              | GPU         \
    \    | 7             | 10.8 |\n| Other                 | Maintenance     | 23\
    \            | 35.4 |\n\ngenerally easier to isolate and recover from. A further\
    \ 35.4% of cases could not be conclusively categorized and are labeled as \"Other.\"\
    \n\n# REFERENCES\n\n- <span id=\"page-11-0\"></span>[1] A. Valcarce and J. Hoydis,\
    \ \"Toward joint learning of optimal mac signaling and wireless channel access,\"\
    \ *IEEE Trans. Cogn. Commun. and Networking*, vol. 7, no. 4, pp. 1233–1243, Dec.\
    \ 2021.\n- <span id=\"page-11-1\"></span>[2] J. Hoydis, F. A. Aoudia, A. Valcarce,\
    \ and H. Viswanathan, \"Toward a 6G AI-Native Air Interface,\" *IEEE Communications\
    \ Magazine*, vol. 59, no. 5, pp. 76–81, May 2021.\n- <span id=\"page-11-2\"></span>[3]\
    \ OpenAI, \"Gpt-4o system card,\" [https://openai.com/index/](https://openai.com/index/gpt-4o-system-card/)\
    \ [gpt-4o-system-card/,](https://openai.com/index/gpt-4o-system-card/) 2024, accessed:\
    \ 2025-05-13.\n- <span id=\"page-11-3\"></span>[4] G. Team, R. Anil, and S. Borgeaud,\
    \ \"Gemini: A family of highly capable multimodal models,\" 2025.\n- <span id=\"\
    page-11-4\"></span>[5] ETST, \"Experiential Networked Intelligence (ENI); Study\
    \ on AI Agents based Next-generation Network Slicing,\" ETSI, Tech. Rep. GR ENI\
    \ 051 V4.1.1, Feb. 2025.\n- <span id=\"page-11-5\"></span>[6] 3GPP, \"NR; Radio\
    \ Resource Control (RRC); Protocol specification,\" 3GPP, Tech. Rep. TS 38.331,\
    \ 2024, available at https://www.3gpp.org.\n- <span id=\"page-11-6\"></span>[7]\
    \ H. Touvron, T. Lavril, G. Izacard, and et al., \"LLaMA: Open and Efficient Foundation\
    \ Language Models,\" *arXiv:2302.13971*, 2023.\n- <span id=\"page-11-7\"></span>[8]\
    \ E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen, \"\
    LoRA: Low-Rank Adaptation of Large Language Models,\" *arXiv:2106.09685*, 2021.\n\
    - <span id=\"page-11-8\"></span>[9] R. Sennrich, B. Haddow, and A. Birch, \"Neural\
    \ machine translation of rare words with subword units,\" *arXiv:1508.07909*,\
    \ 2016.\n- <span id=\"page-11-9\"></span>[10] D. P. Kingma and J. Ba, \"Adam:\
    \ A method for stochastic optimization,\" *arXiv: 1412.6980*, 2017.\n- <span id=\"\
    page-11-10\"></span>[11] AI@Meta, \"Llama 3 model card,\" *Unpublished work*,\
    \ 2024. [Online]. Available: [https://github.com/meta-llama/llama3/blob/main/](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)\
    \ MODEL [CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)\n\
    - <span id=\"page-11-11\"></span>[12] N. Reimers and I. Gurevych, \"Sentence-bert:\
    \ Sentence embeddings using siamese bert-networks,\" 2019. [Online]. Available:\
    \ [https:](https://arxiv.org/abs/1908.10084) [//arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)\n\
    - <span id=\"page-11-12\"></span>[13] Z. Liu, \"nrRRC Simulator: A RRC protocol\
    \ emulator and data collection toolkit,\" [https://github.com/EE-zim/nrRRC](https://github.com/EE-zim/nrRRC_Simulator)\
    \ Simulator, 2025, git commit main@HEAD; accessed 2025-05-13.\n- <span id=\"page-11-13\"\
    ></span>[14] ——, \"RRC dataset for radio resource control conversations,\" [https://](https://huggingface.co/datasets/EEzim/RRC)\
    \ [huggingface.co/datasets/EEzim/RRC,](https://huggingface.co/datasets/EEzim/RRC)\
    \ 2025, accessed 2025-05-13.\n- <span id=\"page-11-14\"></span>[15] OpenAI, \"\
    Chatgpt-4o model card (gpt-4o-2024-11-20),\" [https://platform.](https://platform.openai.com/docs/models/gpt-4o)\
    \ [openai.com/docs/models/gpt-4o,](https://platform.openai.com/docs/models/gpt-4o)\
    \ 2024, accessed: 2025-05-13.\n- <span id=\"page-11-15\"></span>[16] OpenAI, \"\
    o3-mini: A Cost-Efficient Reasoning Model,\" [https://openai.](https://openai.com/index/openai-o3-mini/)\
    \ [com/index/openai-o3-mini/,](https://openai.com/index/openai-o3-mini/) Jan 2025.\n\
    - <span id=\"page-11-16\"></span>[17] Google DeepMind, \"Gemini 1.5 Flash (gemini-1.5-flash-002)\
    \ Model Card,\" [https://cloud.google.com/vertex-ai/generative-ai/docs/](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/1-5-flash)\
    \ [models/gemini/1-5-flash,](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/1-5-flash)\
    \ Sep 2024.\n- <span id=\"page-11-17\"></span>[18] Anthropic, \"Claude 3.5 Sonnet\
    \ v2 Documentation,\" [https://www.](https://www.anthropic.com/news/claude-3-5-sonnet)\
    \ [anthropic.com/news/claude-3-5-sonnet,](https://www.anthropic.com/news/claude-3-5-sonnet)\
    \ Jun 2024.\n- <span id=\"page-11-18\"></span>[19] J. W. Rae, A. Potapenko, C.\
    \ Fernando, and T. Lillicrap, \"Compressive transformers for long-range sequence\
    \ modelling,\" in *International Conference on Learning Representations (ICLR)*,\
    \ 2020.\n- [20] J. Zhang, K. A. Afshar, A. Trischler, and J. C. K. Cheung, \"\
    Hierarchical transformers are more efficient for long document processing,\" in\
    \ *Conference on Empirical Methods in Natural Language Processing (EMNLP)*, 2022.\n\
    - [21] I. Beltagy, M. E. Peters, and A. Cohan, \"Longformer: The longdocument\
    \ transformer,\" in *Findings of ACL*, 2020.\n- [22] M. Zaheer, G. Guruganesh,\
    \ A. Dubey, J. Ainslie, C. Alberti, S. Onta n'on, P. Pham, A. Ravula, Q. Wang,\
    \ L. Yang, and A. Ahmed, \"Big Bird: Transformers for longer sequences,\" in *Advances\
    \ in Neural Information Processing Systems (NeurIPS)*, 2020.\n- [23] J. Su, Y.\
    \ Lu, S. Pan, B. Wen, and Y. Liu, \"Roformer: Enhanced transformer with rotary\
    \ position embedding,\" in *Findings of ACL*, 2021.\n- <span id=\"page-12-0\"\
    ></span>[24] O. Press, N. Smith, and O. Levy, \"Train short, test long: Attention\
    \ with linear biases enables input length extrapolation,\" *arXiv:2108.12409*\
    \ , 2022.\n- <span id=\"page-12-1\"></span>[25] R. Zhang, H. Du, Y. Liu, D. Niyato,\
    \ J. Kang, S. Sun, X. S. Shen, and H. V. Poor, \"Interactive AI with retrieval-augmented\
    \ generation for next generation networking,\" *IEEE Network*, vol. 38, no. 6,\
    \ pp. 414–424, Nov. 2024.\n- <span id=\"page-12-2\"></span>[26] B. Peng, Y. Zhu,\
    \ Y. Liu, X. Bo, H. Shi, C. Hong, Y. Zhang, and S. Tang, \"Graph retrieval-augmented\
    \ generation: A survey,\" *arXiv:2408.08921* , 2024.\n- <span id=\"page-12-3\"\
    ></span>[27] G. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in\
    \ a neural network,\" *arXiv:1503.02531*, 2015.\n- <span id=\"page-12-4\"></span>[28]\
    \ Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, R. Salakhutdinov, and Q. V. Le, \"Transformer-xl:\
    \ Attentive language models beyond a fixed-length context,\" in *Proceedings of\
    \ the 57th Annual Meeting of the Association for Computational Linguistics (ACL)*,\
    \ 2019, pp. 2978–2988."
- title: 'SONIC: Cost-Effective Web Access for Developing Countries'
  abstract: 'Over 2.6 billion people remain without access to the Internet in 2025.
    This

    phenomenon is especially pronounced in developing regions, where cost and

    infrastructure limitations are major barriers to connectivity. In response, we

    design SONIC, a low-cost, scalable data delivery system that builds on existing

    infrastructures: FM radio for downlink broadcasting, and SMS for personalized

    uplink. SONIC is motivated by the widespread availability of FM radio and SMS

    infrastructure in developing regions, along with embedded FM radio tuners in

    affordable mobile phones. SONIC offers several innovations to effectively

    transmit Web content over sound over FM radio, in a reliable and compressed

    form. For example, we transmit pre-rendered webpages and leverage pixel

    interpolation to recover errors at the receiver. We further modify Android to

    offer a simpler deployment pipeline, supporting a wide range of devices. We

    deployed SONIC at an FM radio station in Cameroon for six weeks with 30

    participants. Our results demonstrate a sustained downlink throughput of 10

    kbps, less than 20% loss for a majority of transmissions with signal strength

    above -90 dbM, and a strong user engagement across both Web browsing and

    ChatGPT interactions.'
  url: http://arxiv.org/abs/2505.16519v1
  keywords: ''
  document: '# SONIC: Cost-Effective Web Access for Developing Countries


    *Ayush Pandey*† *, Rohail Asim*† *, Jean Louis K. E. Fendji*‡ *, Talal Rahwan*†
    *, Matteo Varvello*§ *, Yasir Zaki*† †*New York University Abu Dhabi,* ‡*University
    of Ngaoundere,* §*Nokia Bell Labs*


    #### Abstract


    Over 2.6 billion people remain without access to the Internet in 2025. This phenomenon
    is especially pronounced in developing regions, where cost and infrastructure
    limitations are major barriers to connectivity. In response, we design SONIC,
    a low-cost, scalable data delivery system that builds on existing infrastructures:
    FM radio for downlink broadcasting, and SMS for personalized uplink. SONIC is
    motivated by the widespread availability of FM radio and SMS infrastructure in
    developing regions, along with embedded FM radio tuners in affordable mobile phones.
    SONIC offers several innovations to effectively transmit Web content over sound
    over FM radio, in a reliable and compressed form. For example, we transmit pre-rendered
    webpages and leverage pixel interpolation to recover errors at the receiver. We
    further modify Android to offer a simpler deployment pipeline, supporting a wide
    range of devices. We deployed SONIC at an FM radio station in Cameroon for six
    weeks with 30 participants. Our results demonstrate a sustained downlink throughput
    of 10 kbps, less than 20% loss for a majority of transmissions with signal strength
    above -90 dbM, and a strong user engagement across both Web browsing and ChatGPT
    interactions.


    #### 1 Introduction


    The internet has fundamentally reshaped societies worldwide, driving economic
    growth, fostering new industries, and becoming indispensable in education, healthcare,
    and employment. Yet, despite its profound influence, 32.4%—over 2.6 billion people—remain
    disconnected in 2025 [\[67\]](#page-14-0). This *digital divide* is not only due
    to a lack of infrastructure. According to GSMA Intelligence [\[40\]](#page-13-0),
    3.1 billion people live within the coverage area of a mobile broadband network
    but remain disconnected because they cannot afford the necessary devices and data
    plans to get online.


    The offline population is especially concentrated in developing regions. For example,
    India has 651 million people (44.7%) offline, Pakistan 137 million (54.3%), and
    Nigeria


    128 million (54.6%) [\[67\]](#page-14-0). In the Central African Republic and
    South Sudan, over 84% of the population remains without internet access [\[67\]](#page-14-0).
    The consequences of being "unconnected" go beyond missed economic opportunities;
    it severely limits access to essential services such as education, healthcare,
    and financial resources, further exacerbating existing inequalities.


    Several initiatives have attempted to bridge this gap. Starlink [\[15\]](#page-12-0)
    provides global coverage through low earth orbit (LEO) satellites. Google''s Project
    Loon [\[8\]](#page-12-1) sought to deliver internet via high-altitude balloons,
    while Facebook''s Aquila [\[10\]](#page-12-2) aimed to use solar-powered drones
    (both ultimately discontinued due to maintenance costs and scalability challenges).
    Project Taara [\[17\]](#page-12-3) is a recent initiative that transmits data
    using laser beams over long distances (20 km) at high speeds (20 Gbps). Unfortunately,
    these efforts remain prohibitively expensive in most developing regions. For example,
    in Zambia, classified as one of the UN''s least developed countries, Starlink''s
    \$40 monthly subscription (plus \$180 hardware fee) is prohibitively expensive
    compared to the country''s \$108 monthly GNI per capita.


    In this paper, we build on the preliminary work by [\[57\]](#page-14-1) which
    introduced an initial framework to transmit simplified webpages over FM radio.
    Webpages are pre-rendered as images which are then modulated into audio signals
    and transmitted via FM radio. We advance [\[57\]](#page-14-1) as follows:


    System Architecture. We design the full software architecture of the SONIC server
    and Android client, which includes modules for content rendering, prefetching,
    encoding, transmission, decoding, and error correction.


    FM Tuning on Android. We leverage the internal FM tuner found in many Android
    smartphones to receive and decode SONIC transmissions. We do this by modifying
    LineageOS, an open-source Android operating system, to enable programmatic access
    to the FM hardware, allowing other apps to control FM chip tuning and access the
    received audio stream directly, without needing to root the device.


    LLM Support. We extend SONIC''s functionality to support LLM interactions over
    FM radio. We use the same audio mod-


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Network performance of MTN, the best available mobile network provider
    at our deployment site in Cameroon.


    ulation pipeline to allow users to query models like ChatGPT via SMS and receive
    coherent responses without internet access. We show that LLM responses, due to
    their smaller size, can be transmitted significantly faster than full webpages.


    Real-world Deployment. We report on a six-week deployment at a live FM radio station
    in Cameroon. During this period, 30 participants used the SONIC app to request
    webpages and interact with ChatGPT via SMS. Our evaluation shows that SONIC can
    sustain a transmission rate of 10 kbps, with stable reception achievable at a
    Received Signal Strength Indicator (RSSI) value up to -90 dBM. Mean decoding accuracy
    remained at 71% under real-world conditions.


    #### <span id="page-1-2"></span>2 Motivation


    Affordability and QoE in Rural Areas. Mobile internet adoption is rapidly increasing
    in low- and middle-income countries [\[40\]](#page-13-0), yet ensuring affordability
    and a good quality of experience (QoE) remains a challenge. In many remote areas,
    even when mobile broadband is available, data costs are prohibitively high relative
    to average income levels. To assess affordability and QoE in a rural setting,
    we conduct network performance measurements and a survey at the SONIC deployment
    site in Cameroon.


    We select MTN [\[53\]](#page-14-2), the best available mobile network provider
    in the area, and purchase its monthly data plan, which offers 9.2 GB for \$14.
    Following the methodology in [\[71\]](#page-15-0), we use mtr [\[45\]](#page-13-1),
    a tool for measuring latency and tracing network paths to examine routing changes
    and packet losses. Additionally, we employ Speedtest CLI [\[55\]](#page-14-3),
    a command-line tool for measuring latency, download, and upload speeds. We also
    evaluate web performance using Google Lighthouse [\[1\]](#page-11-0), an automated
    tool that provides key website performance metrics, including Speed Index (SI),
    First Contentful Paint (FCP), and Largest Contentful Paint (LCP). Given the data
    limits of our mobile internet plan, we conduct mtr and Speedtest measurements
    once every three hours throughout our deployment. For Lighthouse evaluations,
    we rely on data collected in the wild as a response to real SONIC users (see Table
    [1\)](#page-8-0).


    Figure [1\(](#page-1-0)a) shows the cumulative distribution function


    <span id="page-1-1"></span>![](_page_1_Figure_8.jpeg)


    Figure 2: Barriers to Web access from speed and cost. The main plot shows CDF
    of how SONIC users agree with avoiding websites due to slow internet. The inset
    shows the % of users indicating that their browsing is restricted by data costs.


    (CDF) of average round-trip time (RTT) towards popular content providers (Amazon,
    Facebook, and Google), and DNS operators (Cloudflare and Google). The figure shows
    RTTs higher than 100ms for most measurements and providers, with the exception
    of Google. As observed in [\[70\]](#page-15-1), this is due to Google footprint
    in Africa which was also confirmed by our path analysis.


    Next, Figure [1\(](#page-1-0)b) summarizes the speedtest analysis. With respect
    to the RTT to OOKLA servers (ping), the figure shows a similar trend as Figure
    [1\(](#page-1-0)a). The figure further shows RTT under load, *i.e.,* while measuring
    both download and upload speeds, showing a 3.4x growth (from 200 to 680 ms, at
    the median) thus suggesting large buffer in use (a phenomenon typically called
    bufferbloat [\[36\]](#page-13-2)). Despite these large buffers, users experience
    a median download speed of 2.6 Mbps and an upload speed of 0.69 Mbps—both drastically
    below the global average—at a mobile internet price comparable to developed nations
    (\$1.5 per GB) [\[71\]](#page-15-0). At these speeds, web performance is extremely
    affected, as visualized in Figure [1\(](#page-1-0)c). The median SI is at 9 seconds,
    FCP at 4.5 seconds and LCP at 6.9 seconds, significantly slower than the web vitals
    threshold (LCP < 2.5 seconds) recommended by Google [\[52\]](#page-14-4).


    The combination of high latencies and low download speeds can frustrate users
    and discourage regular internet


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 3: Mobile phones supporting FM receivers for the top four Android brands
    grouped by release year (2017-2024).


    use. Figure [2](#page-1-1) presents responses to two survey questions assessing
    barriers to web access at our deployment site in Cameroon. Survey participants
    are 30 Cameroonians who participated in SONIC deployment. The main plot shows
    the CDF of web use aversion, based on responses to the question: *"To what extent
    do you agree with the following statement: ''I occasionally avoid visiting certain
    websites because my Internet is too slow to load them."*'' (1 = Strongly disagree,
    5 = Strongly agree). Over 75% of the participants indicate either neutrality or
    agreement with avoiding websites due to slow internet speeds. The inset plot shows
    responses to the binary question: *"Is your web browsing experience often restricted
    by data costs?"* where 67% of users answered "Yes."


    These results sheds light on the affordability of web access in low-income regions
    like Cameroon. Beyond slow speeds, high data costs severely limit both how often
    and how effectively users can engage with the internet, restricting access to
    information that many take for granted. Indeed, we also asked participants if
    they had heard of ChatGPT prior to participating in the experiment. 80% reported
    "No", revealing a broader lack of exposure to transformative (and popular) technologies
    simply because the current infrastructure never allowed these tools to reach them.


    FM Radio Availability. FM radio still remains widely used in developing regions.
    In a study covering 39 countries in Africa, about 65% of adults reported listening
    to radio at least a few times per week, with no major difference in the ruralurban
    gap in radio access [\[18\]](#page-12-4). A recent case study in northern Ghana
    found FM radio to be the most reliable and trusted source of developmental information
    in rural communities, providing vital content on agriculture, education, and health
    in local languages [\[20\]](#page-12-5).


    Technical studies confirm that FM signals reliably cover large areas, though terrain
    can influence signal quality. In Nigeria, measurements around a 20 kW FM station
    showed stable reception up to 50 km, beyond which quality declined due to sandy
    or silty soil and obstructed line-of-sight [\[21\]](#page-12-6).


    <span id="page-2-1"></span>![](_page_2_Figure_6.jpeg)


    Figure 4: CDF of prices for android phones equipped with an FM radio receiver.
    Green dotted lines indicate the monthly Gross National Income (GNI) per capita
    for selected low- and middle-income countries.


    A study from Nepal emphasized the importance of antenna height and placement in
    extending coverage, even in hilly regions with shadow zones between elevations
    [\[23\]](#page-12-7). Despite geographic challenges, FM radio continues to provide
    consistent, low-cost coverage in most rural areas, with signal strength sufficient
    for everyday use.


    Building on the widespread availability of FM radio connectivity, we next investigate
    the landscape of FM radio support in mobile phones. Specifically, we analyzed
    the prevalence of FM receivers in Android smartphones currently available on the
    market. To do this, we scraped the database of mobile phone specifications from
    GSMArena [\[41\]](#page-13-3), identifying which models support FM radio and recording
    their release years and market prices. We then cross-referenced this information
    with global economic statistics [\[74\]](#page-15-2) to assess the practical reach
    of FM-based data reception.


    Figure [3](#page-2-0) shows the distribution of FM-capable phones released by
    the top four Android smartphone brands [\[33\]](#page-13-4) over the past eight
    years. Despite a gradual decline in newer models, FM radio functionality remains
    prevalent—especially among Xiaomi and Vivo devices, where approximately 40% of
    current models still include FM support. In total, these top four brands alone
    account for 571 FM-capable models, highlighting a significant and readily accessible
    user base for FM-based data delivery.


    We next evaluate the affordability of these 571 FM-capable phone models in low-income
    countries, using monthly gross national income (GNI) per capita as a benchmark
    [\[74\]](#page-15-2). Figure [4](#page-2-1) shows the price distribution of these
    phones (blue line) alongside GNI thresholds (green dotted lines) for six developing
    countries: Pakistan, Cameroon, India, Bangladesh, Egypt, and Brazil. The figure
    shows that a substantial share of devices are within reach of average consumers,
    from a minimum of 30% in Pakistan up to 99.3% in Brazil. These results highlight
    strong market availability of FM-enabled phones at accessible price points in
    low and middle-income regions.


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Figure 5: SONIC workflow.


    ### 3 SONIC


    Figure [5](#page-3-0) shows how SONIC operates. Users A and B both have the SONIC
    app installed on their smartphones, equipped with chipsets featuring FM receivers.
    However, only User B''s device supports SMS. The SONIC app runs on both smartphones
    with FM receivers tuned to a specific frequency to decode incoming data-over-audio
    streams.


    User B requests a webpage using the SONIC app, which sends an SMS containing the
    desired URL (e.g., bbc.co.uk) to the SONIC server. The server listens for incoming
    SMS messages, retrieves the webpage, captures a screenshot of the rendered page,
    and compresses it into a WebP image. The image is then encoded into sound and
    broadcasted via an FM transmitter. Both User A and User B receive the transmitted
    webpage on their smartphones. In the following, we describe each SONIC component
    which enables the above workflow.


    ### 3.1 SONIC Server


    Figure [6](#page-3-1) shows the architecture of the SONIC server. The server runs
    as a Docker [\[35\]](#page-13-5) container on a computer located at a radio station.
    It consists of several key components that work together to process user requests.
    The SMS Manager handles incoming messages, while the Screenshot Queue processes
    webpage URLs in a First Come, First Serve (FCFS) manner. A Cache stores recently
    requested URLs to avoid redundant processing. The Encoder converts responses into
    SONIC *file format* (see Section [3.1.1\)](#page-3-2) and encodes them to audio.
    Finally, the Player Queue manages the order in which the encoded audio files are
    played, also following the FCFS policy.


    The SMS Manager continuously listens for SMS messages using a USB mobile dongle.
    These messages are received on a phone number assigned to the SIM card inserted
    into the dongle. Messages sent by the SONIC app contain the sender''s information
    in the headers and a payload formatted as: <type> <body> (e.g., url https://nytimes.com).
    When a new message is received, the server classifies it as either an LLM prompt
    or a webpage URL depending on the <type> identifier.


    <span id="page-3-1"></span>![](_page_3_Figure_8.jpeg)


    Figure 6: SONIC server architecture.


    If the message contains a URL, it is added to the Screenshot Queue. The system
    checks whether this URL has been retrieved within the current transmission window
    using the Cache. If not, the Screenshot Queue utilizes Selenium [\[3\]](#page-12-8)
    to load the page in Google Chrome with a mobile resolution of an iPhone SE device
    (375 × 667 pixels). Once the webpage is fully loaded, a full-page screenshot is
    taken and resized to a width of 320 pixels. We empirically selected 320 pixels
    as a sweet spot where both the content layout and text remain comfortably readable
    to the human eye. The screenshot is then encoded into a SONIC file format and
    subsequently converted into audio using the Quiet [\[13\]](#page-12-9) library
    at the Encoder. The resulting audio is then added to the Player Queue. In case
    of cached requests, the cached response directly moves to the Player Queue.


    If the message contains an LLM prompt, the server makes an LLM inference API call
    to either a locally running LLM, or a cloud API (e.g., OpenAI Chat Completions
    API [\[56\]](#page-14-5)). The response from the LLM is encoded into the SONIC
    file format containing structured metadata and payload. Finally, the server generates
    a corresponding audio using the Quiet library, which is then added to the Player
    Queue.


    ### <span id="page-3-2"></span>3.1.1 Encoding


    Encoding takes place in the Encoder, which consists of two steps: 1) encoding
    responses to a new file format, referred to as a *sonic file*; and 2) converting
    this sonic file into a waveform audio file (WAV). The resulting audio file is
    then added to the Player Queue.


    SONIC File Format. SONIC uses a new file format that allows the decoder to easily
    distinguish specific parts of the transmission in absence of a continuous uplink.
    As illustrated in Figure [7,](#page-4-0) this format includes intermediate headers
    (such as "MDTA," "LNKS," "SDTA," etc.) that separate internal sections within
    the metadata and payload. This structure helps reconstruct content in cases when
    metadata is fully received but only parts of the payload are received properly.
    Additionally, each payload frame is prefixed with "C137," inspired by *Rick and
    Morty*''s C-137 dimension [\[60\]](#page-14-6), which helps distinguish the start
    point of each frame. Furthermore, "C137" serves as a keepalive message, allowing
    the app to notify users that the server is online when transmissions are in progress.


    Webpage Compression. Unlike encoding LLM interactions,


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Figure 7: SONIC file format.


    where the payload is simply appended to metadata, encoding webpages presents a
    significant challenge due to the limited data rates achievable over audio transmission—typically
    only tens of kbps (see Section [7\)](#page-11-1). According to [\[16\]](#page-12-10),
    the average mobile webpage size is approximately 2 MB. Broadcasting such a page
    via SONIC could take tens of minutes. Moreover, devices experiencing poor RSSI
    may struggle to reconstruct the page, as critical web components, such as JavaScript,
    may fail in the presence of unrecoverable errors.


    To address these challenges, we must: 1) significantly compress webpages, and
    2) ensure resilience against noise. Various methods exist for reducing webpage
    sizes, such as compression proxies [\[11,](#page-12-11) [38\]](#page-13-6), reader
    modes [\[14,](#page-12-12) [37\]](#page-13-7), JavaScript cleaners [\[29](#page-13-8)[–31\]](#page-13-9),
    and redundant code removal [\[46,](#page-13-10)[51\]](#page-14-7). These approaches
    remain vulnerable to noise and require extensive forward error correction (FEC),
    necessitating a system design that accounts for the worst-case receiver conditions.


    Instead, we utilize a solution where performance degrades gracefully as a function
    of the receiver''s RSSI, analogous to how audio quality deteriorates under poor
    reception. Inspired by [\[25,](#page-12-13) [26\]](#page-12-14), which demonstrate
    how image quality over RDS degrades with RSSI, we opt to transmit images of rendered
    webpages rather than raw web files (HTML/JavaScript/CSS). This approach provides
    both compression and resilience: a 2 MB webpage can be compressed into a few hundred
    KB, and images remain interpretable even if some pixels are lost.


    Interactivity. Modern webpages enable user interaction via hyperlinks, menus,
    and search boxes, whereas images are inherently static. To introduce interactivity,
    [\[24\]](#page-12-15) proposes *click maps*, which store <x,y> coordinates of
    interactive elements. We adopt this approach, allowing SONIC to notify the server
    (via SMS, if available) when a user clicks on a coordinate, retrieving the corresponding
    page if it is not already cached. Given SONIC''s potentially slow network conditions–seconds
    for uplink and minutes for downlink–we limit interactivity to hyperlinks.


    Image Format. Unlike [\[24\]](#page-12-15), which requires lossless PNG for crowdsourced
    screenshot merging, SONIC utilizes *WebP* [\[5\]](#page-12-16), a modern format
    offering superior compression. Webpages are captured as WebP images at 10% quality,
    significantly reducing file size while maintaining readability. Images are 320
    pixels wide and up to 10,000 pixels tall, enabling users to *scroll* with minimal
    data overhead. To accommodate different screen sizes, images are resized using
    a scaling factor (screen width / 320), ensuring accurate click map coordinates.


    Modulation. We use the Quiet library to modulate SONIC files as waveform audio
    files (WAV). Inspired by "audible-7k-channel", we created a new modulation profile
    that uses Orthogonal Frequency-Division Multiplexing (OFDM)—a multi-carrier modulation
    technique that divides the available spectrum into multiple orthogonal narrow-band
    signals called sub-carriers. Our profile uses 92 sub-carriers, with a center frequency
    of 9.2 KHz, achieving a rate of 10 kbps.


    #### 3.1.2 Pushing


    During low-usage periods, typically at night, SONIC *pushes* [\[69\]](#page-15-3)
    popular webpages to its clients to ensure faster response times during peak hours.
    For each webpage, SONIC also pushes some internal links allowing users to seamlessly
    interact with a webpage without delay. However, pushing every hyperlink on a webpage
    would overwhelm the system''s limited transmission bandwidth. To address this,
    we use a prioritization metric to rank internal links based on their importance:


    $$score = 0.68 \cdot w \cdot h - 0.32 \cdot y$$


    where w, h, and y denote the width, height, and vertical distance from the top
    of the page, respectively. We derive this metric from a Prolific user study detailed
    in Section [4.](#page-6-0)


    When a webpage enters the Screenshot Queue, SONIC computes this score for each
    hyperlink and selects the top three ranked links. These are added to a separate
    idle queue that is only activated when the server has no active transmissions.
    As SONIC primarily targets informative sites like


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Figure 8: SONIC client architecture.


    news and blogs, this metric prioritizes pages with top headlines, larger images,
    and prominent font sizes, ensuring that key content is already available when
    users attempt to follow links. Additionally, it penalizes links that are vertically
    farther from the top of the page.


    ### <span id="page-5-2"></span>3.2 SONIC Client


    At a minimum, SONIC users require a smartphone with a built-in FM radio receiver,
    serving as the *downlink*, along with a wired earphone to act as an antenna. Additionally,
    users who wish to send requests, such as retrieving a webpage or interacting with
    the LLM, need access to an SMS service for the *uplink*. On the software side,
    SONIC operates as a user-space application on a modified version of Android (see
    Figure [8\)](#page-5-0). We detail the SONIC client in the following.


    OS Integration. The SONIC client relies on FM radio hardware to receive data transmissions,
    but modern Android devices do not expose FM chip access to third-party applications.
    Default FM radio apps are shipped as system apps, integrated into the ROM and
    signed with privileged keys that allow hardware-level access. Apps like SONIC
    are unable to access FM audio without rooting the device and allowing superuser
    access, which is unrealistic for adoption and raises significant security concerns.
    As such, enabling FM-based decoding requires changes at the operating system level.


    We build a proof-of-concept implementation based on LineageOS [\[50\]](#page-14-8),
    a widely supported open-source Android distribution. We modify its default FM
    radio app to: 1) allow tuning the FM chip to a specific frequency, and 2) forward
    decoded audio streams to other apps like SONIC without needing root or elevated
    permissions. This is achieved by implementing a BroadcastReceiver [\[19\]](#page-12-17)
    in the SONIC app and a matching sender in the default FM radio app that transmits
    raw audio buffers in real time. This change allows SONIC to passively listen to
    FM broadcasts and decode data as it arrives. We verified this approach by flashing
    our customized LineageOS build onto devices that use Qualcomm Snapdragon chipsets,
    including Xiaomi Redmi Go. We observe that as long as FM radio drivers are available,
    this approach can be extended to any other chipset. Manufacturers would implement
    this


    <span id="page-5-1"></span>![](_page_5_Figure_7.jpeg)


    Figure 9: SONIC''s user interface.


    change when building the stock operating system for devices they ship with FM
    radio support (see Section [3.3\)](#page-6-1).


    User Interface. Figure [9](#page-5-1) shows the SONIC app''s user interface (UI),
    which consists of three sections: Browser, ChatGPT and Knowledge Hub. The Browser
    section mimics modern web browsers. It features a search bar at the top, allowing
    users to request URLs. A list of requested URLs is also displayed, and once received,
    they appear as "ready to view." The ChatGPT section provides a chat-like interface
    where users can interact with ChatGPT by asking questions and receiving responses.
    Since FM radio operates as a broadcast system, all devices tuned to the same frequency
    receive the same content, even if it wasn''t specifically requested by those devices.
    This broadcast nature of FM radio is leveraged by SONIC with its Knowledge Hub
    section. In this section, users can access a list of webpages and ChatGPT responses
    that are popular within their region, allowing them to discover trending content
    shared by others nearby.


    Background Service. In addition to the UI, the SONIC app runs a background service
    that performs two key functions: 1) continuously listening to FM radio audio streams
    at a specific frequency, and 2) decoding transmissions when a SONIC-encoded signal
    is detected. The background service listens to bytes broadcasted by the default
    FM radio app using a BroadcastReceiver. To decode SONIC-encoded transmissions,
    it uses a modified version of Quiet''s Android library [\[6\]](#page-12-18), which
    by default decodes audio from the device''s microphone. We modified the library
    to accept bytes retrieved from the phone''s FM radio app and decode them using
    the same modulation profile described in Section [3.1.1.](#page-4-0)


    This background service operates independently of the user interface and automatically
    stores all incoming transmissions in an SQLite database. To optimize storage,
    any unaccessed content is automatically deleted after one day.


    Error Correction. We utilize crc32 checksums per frame to detect errors. Furthermore,
    an inner FEC scheme (v29) and an outer FEC scheme (rs8) are used to correct transmission
    errors. For completely lost frames, the SONIC receiver applies nearest-neighbor
    pixel interpolation [\[62\]](#page-14-9), replacing missing pixels with the value
    of their adjacent left pixel given that webpage consists mostly of text read from
    left to right.


    In the literature, better-performing techniques exist to recover missing pixels
    in images [\[28,](#page-13-11) [59,](#page-14-10) [68\]](#page-15-4), leveraging
    deep neural networks to learn patterns and structures of the image or utilizing
    sparsity and gradients in the data to fill in the missing regions. These techniques
    are both memory- and CPU-intensive, far beyond what a low-end mobile device can
    support today. Thus, we adopt a lightweight approach proposed and benchmarked
    by [\[57\]](#page-14-1) that provides consistently high content readability scores
    even at a 20% pixel loss rate.


    #### <span id="page-6-1"></span>3.3 Discussion


    Rollout. Governments, NGOs, and other organizations can roll out SONIC across
    a wide range of Android phones by preinstalling a modified version of the LineageOS
    ROM (detailed in Section [3.2\)](#page-5-2). Moreover, smartphone manufacturers
    could adopt this approach natively when designing their operating systems. Manufacturers
    can enable support for data-over-FM use cases like SONIC without compromising
    system security or requiring root access by bundling a modified FM radio app that
    exposes decoded audio streams to other applications. This would allow future devices
    to support FM-based services out of the box with minimal engineering overhead.


    Incentives and Monetization. SONIC users benefit by gaining access to a streamlined
    version of the Web in areas where such access is typically unavailable. For providers,
    one approach is to charge users directly. However, this can be difficult since
    users receiving content via downlink are passive, making it hard to know when
    or if content is being accessed. As an alternative, providers could link the service
    to SMS, allowing paying users to request content on demand, while keeping access
    free for others. Notably, FM broadcasting costs remain constant, regardless of
    the number of listeners.


    A more promising revenue model mirrors how traditional radio stations function:
    expanding the audience to increase advertising revenue. SONIC adds a unique offering
    that could draw more users, potentially enhancing ad-based profits. Furthermore,
    ads are no longer limited to audio—they can now include visuals embedded in the
    web pages.


    Privacy Concerns. At a high level, SONIC resembles acceleration platforms like
    Google AMP [\[38\]](#page-13-6) and WebLight [\[11\]](#page-12-11), which modify
    webpage content before sending it to users. These services typically rely on access
    to both the URLs users request and the content they consume, which raises potential
    privacy concerns. While a SONIC server could, in principle, gather enough information
    to build user profiles, it avoids this issue by using FM radio as a broadcast
    channel. This mode of delivery makes it impossible to identify who is receiving
    the content. As a result, users on the downlink side remain fully anonymous, passively
    receiving data initiated by others nearby, without any associated privacy risk.


    <span id="page-6-2"></span>![](_page_6_Figure_7.jpeg)


    Figure 10: CDF of loss percentage under ideal conditions. Inset shows loss % vs
    RSSI.


    Limitations. SONIC does not enable access to loginrestricted content, such as
    online banking or social media accounts. This is unfeasible for downlink-only
    users (*i.e.,* no SMS support); for uplink users, it would involve sharing login
    credentials with the SONIC server which is a significant privacy risk. Moreover,
    because content is broadcasted, any personalized information (like account details)
    would be exposed to anyone within range, further compromising privacy.


    Next, SONIC lacks support for video which is a major part of modern web usage,
    e.g., streaming, news, and social media. SONIC''s limited bandwidth makes video
    streaming infeasible. Instead, video content is replaced with static, non-interactive
    thumbnails. Likewise, advanced features driven by JavaScript or CSS are not supported,
    as SONIC only transmits simplified, pre-rendered versions of webpages.


    #### <span id="page-6-0"></span>4 Benchmarking


    This section benchmarks SONIC under controlled lab settings. We begin by analyzing
    the relationship between RSSI (Received Signal Strength Indicator) and packet
    loss. Next, we benchmark the impact of such losses on the user experience. We
    conclude evaluating the effectiveness of SONIC pixel interpolation and pushing
    techniques.


    Signal Strength and SONIC Performance. We place five Xiaomi Redmi Go phones, each
    with the SONIC app installed, at varying distances from a 0.5 W FM transmitter
    to artificially create diversity in RSSI. All devices are kept fully powered meanwhile
    5,000 randomly-selected webpages from the Tranco [\[48\]](#page-13-12) list are
    broadcasted over FM at a frequency of 91.5 MHz, so that they are concurrently
    received by the testing devices while emulating varying RSSI. Figure [10](#page-6-2)
    shows the CDF of loss rates across all transmissions and its inset plot presents
    loss percentage as a function of RSSI range. The main CDF shows that over 95%
    of transmissions experienced loss rates below 10%. Furthermore, high loss percentages
    (for the remaining 5% of transmissions) are largely confined to relatively poor
    signal conditions, particularly in the RSSI range


    <span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)


    Figure 11: Likelihood of clicking on a hyperlink in a webpage given its area and
    y position. Data from 1,000 page interactions from 100 users across 100 pages.


    of –70 to –60 dBm. As RSSI improves, the loss percentage rapidly declines and
    stabilizes near zero.


    Pixel Interpolation. [\[57\]](#page-14-1) evaluates the impact of visual loss
    and pixel interpolation on perceived content clarity and text readability using
    feedback from 151 Pakistani university students across 50 test webpages. Their
    results show that even at a 20% pixel loss rate – which is rare in SONIC as shown
    in Figure [10](#page-6-2) – users reported a median content clarity score of 7
    out of 10, indicating a generally clear understanding of the page. While text
    readability was more affected, it remained acceptable at a loss rate of 20%. Rating
    distributions from this study are provided in Appendix [A.](#page-15-5)


    Pushing Metric. We conduct a user study on Prolific [\[2\]](#page-12-19) to evaluate
    the likelihood of hyperlink clicks based on visual features of each link. Specifically,
    we examine the area covered by the link in a webpage screenshot (width · height)
    and its vertical position on the page (y-position). We randomly sample 100 webpages
    from the Tranco list. For each page, we generate mobile screenshots along with
    the bounding-box coordinates (x, y, w, h) of every hyperlink. We then create an
    interactive webpage where participants are asked to click on the link they would
    "naturally" choose to visit next.


    A total of 100 participants take part in the study, each interacting with 10 different
    pages. The pages are distributed so that each webpage is evaluated by ten users,
    resulting in 1,000 total page interactions. Using the data collected from our
    study, we train a logistic regression model to learn the relative importance of
    a hyperlink''s area and vertical position in predicting click likelihood. The
    model fits a weighted linear combination of these features to estimate the probability
    of a link being clicked. The resulting scoring function is:


    $$score = 0.68 \cdot w \cdot h - 0.32 \cdot y$$


    where w, h, and y denote the width, height, and vertical distance from the top
    of the page. The negative weight on y-position reflects that links appearing closer
    to the top (*i.e.,* with lower y-values) are more likely to be clicked. We use
    this scoring function as our prioritization metric for pushing.


    Figure [11](#page-7-0) shows the likelihood of clicking a hyperlink based on its
    area and vertical position on the page. The heatmap reveals that links with larger
    areas and located closer to the top (*i.e.,* lower y-values) are more likely to
    be clicked. This trend is visible in the gradient transition from red (low click
    probability) to green (high click probability), moving from the bottom-left to
    the top-right of the plot. Although the maximum observed click probability is
    only 0.53—indicating that clicks are far from guaranteed—the relatively higher
    likelihood still offers a useful signal for prioritization. Since the server remains
    underutilized during idle periods, pushing these links—even at moderate click
    probabilities—can improve user experience with minimal additional cost.


    #### 5 Deployment


    This section outlines SONIC deployment at a live FM radio station in Cameroon.
    We selected this location given its low internet penetration rate comparable to
    low-income regions (58.1% of Cameroon''s population is offline as of 2025 [\[34\]](#page-13-13)).


    Methodology. We start by signing an agreement with an FM radio station *[name
    and exact location redacted for anonymity]* in Cameroon to allow the SONIC server
    to transmit content from 10PM to 5AM daily for six weeks. This overnight window
    was the only available airtime, as the station''s daytime schedule was reserved
    for regular programming. Such opportunistic use of off-peak radio hours represents
    the most feasible adoption path for SONIC in the near term. In the future, we
    envision dedicated FM channels operating full-time for data broadcasting.


    We recruited 30 Cameroonians to experiment with SONIC during this period, *i.e.,*
    request webpages and ask questions to ChatGPT. Study participants were given a
    Xiaomi Redmi Go phone (featuring Qualcomm Snapdragon 425 processor and 1 GB RAM)
    flashed with the modified version of LineageOS, and SONIC app pre-installed. To
    send requests, each phone had a SIM card with an unlimited SMS bundle.


    As shown in Figure [12,](#page-8-1) the SONIC server was set up at the FM radio
    station using a MacBook Air with 8 GB of RAM, running the SONIC Docker container.
    We used Huawei''s E8372h-320 LTE/4G USB Mobile WiFi Dongle [\[43\]](#page-13-14)
    to interface with the SIM card via huawei-lte-api [\[64\]](#page-14-11) and receive
    incoming SMS messages. The SONIC app was programmed to send SMS messages to the
    number associated with the SIM card used by the dongle. For internet access, we
    used a mobile data subscription from MTN Cameroon (see Section [2](#page-1-2)
    for details on plan and connection quality).


    Data Collection. Table [1](#page-8-0) outlines our deployment. A total of 30 participants
    were recruited, divided into two sequential batches of 15 participants each. The
    study spanned 6 weeks in total, with each batch participating for 3 weeks. As
    an


    <span id="page-8-1"></span>![](_page_8_Picture_0.jpeg)


    Figure 12: FM radio station in Cameroon, and SONIC server located inside the station.


    <span id="page-8-2"></span>![](_page_8_Figure_2.jpeg)


    Figure 13: Location map of the 30 SONIC users with their interpolated RSSI values.
    Each cell in the grid is 100 meters in scale. Note that some users are overlapping.


    incentive to the participants, we offered USD 2 per person per day. Before the
    study, participants signed a consent form and were allowed to withdraw from the
    study at any time. The participants were then given an introduction on how to
    use the SONIC app; further, an institutional review board (IRB) approval was granted
    to conduct the study. The authors who conducted the study are CITI [\[4\]](#page-12-20)
    certified. No sensitive or personal information of the participants was collected,
    except for their name and phone number to contact them and disburse the incentive
    money at the end of the experiment.


    Study participants were allowed to make up to 10 requests per day–this included
    both webpage URL requests and GPT queries. Participants were allowed to make requests
    at anytime during the day; however, responses were transmitted during the transmission
    window of 7 hours (10 PM to 5 AM). Over 3 weeks, participants made 1,737 URL requests
    and 2,936 GPT queries in total. The median number of requests per user was 160,
    with 96 GPT queries and 64 URL requests.


    Deployment Challenges. We encountered several challenges during SONIC deployment.
    Initially, airport security confis-


    <span id="page-8-0"></span>


    | Property                 | Description                  |

    |--------------------------|------------------------------|

    | Location                 | Cameroon                     |

    | Participants             | 30                           |

    | Number of batches        | 2                            |

    | Participants per batch   | 15                           |

    | Duration of study        | 6 weeks                      |

    | Duration per batch       | 3 weeks                      |

    | Transmission window      | 10 PM to 5 AM daily          |

    | Daily request quota      | 10 (GPT + URL)               |

    | Total URL requests       | 1,737                        |

    | Total GPT requests       | 2,936                        |

    | Median requests per user | Total: 160, GPT: 96, URL: 64 |

    |                          | (in 3 weeks)                 |


    Table 1: Summary of SONIC deployment.


    cated 10 mobile phones intended for participants, significantly reducing the number
    of devices available for deployment. Only 15 phones ultimately reached Cameroon,
    forcing us to conduct the experiment in two separate batches. Securing reliable
    internet connectivity for the SONIC server also proved challenging: even the best
    available mobile internet plan from MTN was unstable (as discussed in Section
    [2\)](#page-1-2), occasionally unavailable for entire days, and affected by significant
    latency. Compounding these issues, the village where we deployed frequently experienced
    electricity outages—lasting up to 8 hours and often overlapping with the transmission
    window—completely disrupting FM radio transmissions. During these outages, although
    users continued to request content via SMS, they were unable to receive any responses.


    Operational issues further complicated the deployment. In the first batch, the
    SONIC app sent acknowledgment messages (ACKs) for all received transmissions,
    enabled by unlimited SMS bundles purchased for each participant. However, the
    resulting high volume of SMS traffic quickly raised suspicion with the mobile
    operator, leading to the blocking of all deployed SIM cards. Consequently, we
    had to disable the ACK mechanism, leaving us without real-time operational feedback
    or heartbeat signals from participants'' phones. Additionally, the success of
    FM radio transmissions critically depended on the radio station staff accurately
    switching to SONIC broadcasts at exactly 10 PM each night. This switch-over, however,
    was inconsistent, resulting in multiple days without any transmissions. We also
    discovered that the transmitter''s output volume needed to be set to 100% to achieve
    better range and improve transmission quality, but maintaining this setting consistently
    proved challenging for the radio staff as well. Finally, the effectiveness of
    the SONIC system relied heavily on participants regularly charging their phones
    and keeping earphones connected at all times, as the earphones served as antennas
    for receiving broadcasts. Collectively, these logistical and operational hurdles
    made it difficult to maintain ideal conditions for SONIC deployment.


    <span id="page-9-0"></span>![](_page_9_Figure_0.jpeg)


    Figure 14: Loss % as a function of RSSI range. Violin''s width is proportional
    to the percentage of total transmissions occurred in the RSSI range.


    <span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)


    Figure 15: CDF of loss % for GPT and URL transmissions.


    ### 6 Results


    In this section, we present our analysis from SONIC deployment at a live FM radio
    station in Cameroon.


    RSSI and Loss Analysis. Figure [13](#page-8-2) shows the spatial distribution
    of RSSI measurements across a 100m-resolution grid around the FM radio station.
    Using ordinary kriging [\[72\]](#page-15-6), we interpolate user-collected GPS-tagged
    signal data to generate a continuous RSSI map. While the radio station is centrally
    located, the strongest signal regions are notably offset to the northeast, with
    two additional users registering high RSSI values at distances of approximately
    900–950 m (one directly to the south and another to the southwest). Contrary to
    the expected radial decay in signal strength with distance, these observations
    demonstrate the influence of antenna height and placement similar to what was
    observed by [\[23\]](#page-12-7). The northeastward bias in signal strength likely
    results from the antenna''s physical orientation or directional configuration,
    while the isolated strong-signal detections at longer distances suggest favorable
    line-of-sight conditions.


    Figure [14](#page-9-0) shows transmission loss percentages across RSSI ranges,
    with each violin''s width representing the proportion


    <span id="page-9-2"></span>![](_page_9_Figure_8.jpeg)


    Figure 16: Queue size for *actual* (15 users, 1 frequency), and *simulated* (30-300
    users, 1-10 frequencies) loads assuming the busiest day of SONIC deployment.


    of total transmissions in that range (e.g., 22.1% of all transmissions occurred
    between -70 and -60 dBm). We observe a clear trend: better signal strength (*i.e.,*
    higher RSSI) is associated with reduced transmission loss. In poor signal conditions
    below -90 dBm, the loss rate is frequently near 100%. In contrast, for stronger
    signals in the -80 to -50 dBm range (accounting for above 60% of total transmissions),
    data points are more concentrated below 20%, and instances of 100% loss are rare.
    This confirms, in the wild, the observations from our benchmarking under controlled
    settings (see Figure [10\)](#page-6-2).


    To investigate per user performance, Figure [17](#page-10-0) shows boxplots of
    RSSI values per participant alongside their completion rates for both GPT and
    URL requests. Users with stronger median RSSI values (e.g., above –70 dBm) consistently
    achieve high completion rates across both content types. For instance, users 15–17,
    21–28, and 30 report more than 80% completion rates, indicating that strong and
    stable signal conditions are sufficient for reliable content delivery. In contrast,
    users with lower and unstable signal quality, particularly those with median RSSI
    fluctuating below –90 dBm (e.g., users 4, 12, and 19), show drastically reduced
    completion rates, often below 30%.


    Due to their smaller sizes, GPT responses have shorter broadcast duration than
    webpages. However, they are more susceptible to failures from partial frame loss.
    A single 500 byte frame drop can disrupt an entire GPT message, whereas similar
    loss in a webpage screenshot has limited impact (see Figure [20\)](#page-15-7).
    As a result, webpages achieve higher completion rates. This can be observed for
    users 7, 8, 10, and 11, where URL completions are consistently higher than GPT
    under similar signal conditions. This trend is also reflected in Figure [15,](#page-9-1)
    which shows the CDF of loss rates observed for the two types of content—GPT responses
    and URL transmissions. URL transmissions (orange line) appear more loss-resilient
    than GPT (blue line) due to their shorter duration and stricter tolerance to partial
    frame loss.


    Scalability. Figure [16](#page-9-2) shows the evolution of the transmission


    <span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)


    Figure 17: RSSI distribution per user with their request completion rates. Users
    13, 20, and 23 were removed as their devices appeared faulty; they neither recorded
    any RSSI measurements nor received any transmissions.


    queue over the course of a day. We select the busiest day to understand transmission
    trends within the transmission window (marked by the vertical dashed lines at
    22:00 and 05:00). The blue trace represents the real deployment of SONIC, with
    15 users receiving content on a single frequency. Although each user requested
    10 pages, on average, the queue size peaked at 103 items, indicating that roughly
    30% of transmissions were served from the cache. The sharp spike at 09:30 corresponds
    to a scheduled "push" of pre-selected news pages during an otherwise idle period.
    All other curves represent FCFS (First Come First Serve) queue simulations, scaling
    this baseline traffic to heavier loads. For example, with 30 users on a single
    FM frequency (orange), the queue peaks at around 200 items but is still fully
    transmitted within the 7-hour transmission window. This suggests that one frequency
    can support up to 30 active users with similar queuing patterns. When capacity
    increases to two frequencies and 105 users (green), the peak backlog rises to
    800 items, half of which could not be transmitted before 05:00. Similar patterns
    are observed for 150 users with three and four available frequencies, respectively.
    However, a configuration of 300 users with 10 available frequencies appears sufficient
    to fully transmit the queue within the transmission window.


    Content Analysis. Figure [18](#page-11-2) presents a treemap of the top 10 content
    categories for both GPT queries and URL requests made by SONIC users. GPT queries
    are classified using Meta''s Llama 3 model [\[39\]](#page-13-15), while domains
    in URL requests are categorized using Cloudflare''s Domain Intelligence API [\[32\]](#page-13-16)
    following the methodology in [\[63\]](#page-14-12). Among GPT queries, Geography
    dominates with 22% of all requests, followed by Politics (9%), Sports (6%), History
    (5%), Technology (4%), and Medicine (4%). Smaller but still notable portions of
    queries are related to Philosophy, Science, Business, and Chemistry. For URL requests,
    News & Media (12%) and Business (10%) are the most common, followed by Technology
    and Education. Users also accessed sites in categories such as Search Engines,
    Ecommerce, Travel, and Video Streaming. However, content from these latter categories
    is less likely to be useful given SONIC''s current limitation of only supporting
    page screenshots with limited (and slow) hyperlink interactivity.


    User Experience. At the end of the experiment, we asked participants to complete
    an exit survey assessing their experience with SONIC across three dimensions.
    Each question was rated on a 5-point likert scale, with varying response ranges
    depending on the aspect evaluated. For system reliability, users responded to:
    *"How reliable was the content received through the SONIC app?"* (1 = Not reliable
    at all, 5 = Very reliable). For UI intuitiveness, they answered: *"How intuitive
    is the user interface of the SONIC app?"* (1 = Not intuitive at all, 5 = Very
    intuitive). Lastly, for content relevance, users rated: *"How useful was the content
    you received from the system?"* (1 = Not useful at all, 5 = Very useful).


    Figure [19](#page-11-3) shows the CDF of responses to the survey questions. Overall,
    user feedback was positive: 72% of participants rated the SONIC app as "intuitive"
    and the content as "useful"


    <span id="page-11-2"></span>![](_page_11_Figure_0.jpeg)


    Figure 18: Top 10 categories of URL and GPT content requested by SONIC users.


    <span id="page-11-3"></span>![](_page_11_Figure_2.jpeg)


    Figure 19: User experience of SONIC on system reliability, UI intuitiveness, and
    content usefulness.


    (with scores of 4 or 5). In contrast, system reliability received relatively lower
    ratings, with only 62% of users selecting 4 or 5. This is expected, as some participants
    experienced fluctuating or low RSSI values, leading to reduced request completion
    rates, as discussed earlier in Figure [17.](#page-10-0)


    #### <span id="page-11-1"></span>7 Related Work


    Improving internet accessibility in developing regions is crucial for delivering
    essential services such as education and healthcare. However, challenges like
    unreliable hardware, limited cellular coverage, high data costs, and increasingly
    complex webpages hinder connectivity. Existing efforts focus on web simplification
    and new access technologies tailored for these regions. SONIC combines *web simplification*
    with *data-over-sound* transmission to enable connectivity in rural and remote
    areas. In the following, we discuss related works in both research areas.


    Web Simplification. Prior work has explored reducing webpage complexity to improve
    performance under limited connectivity. Habib et al. [\[42\]](#page-13-17) proposed
    a framework that dynamically adapts webpage complexity based on network conditions.
    Muzeel [\[47\]](#page-13-18) removes unused JavaScript, while


    MAML [\[58\]](#page-14-13) introduces a minimalist specification language omitting
    JavaScript and CSS. Klotski [\[27\]](#page-12-21) prioritizes userrelevant content,
    Shandian [\[73\]](#page-15-8) restructures loading via splitbrowser design, and
    Polaris [\[54\]](#page-14-14) accelerates rendering using dependency graphs. Though
    effective under constrained bandwidth, these methods require basic internet access,
    which is unavailable or unaffordable in many rural areas.


    Data over FM. To our knowledge, SONIC is the first system to leverage FM radio
    as a means to broadcast internet connectivity in rural regions. The process of
    encoding webpages as sound is inspired by several research papers [\[22,](#page-12-22)[44,](#page-13-19)[49,](#page-14-15)[61,](#page-14-16)[65\]](#page-14-17)
    and open source tools [\[7](#page-12-23)[,9,](#page-12-24)[13\]](#page-12-9) that
    have explored how to transmit data over sound at inaudible audio frequencies,
    *i.e.,* above 18kHz. The usage of FM radio for novel applications has also been
    explored by in previous works including RevCast [\[66\]](#page-14-18) which leverages
    the broadcast nature of FM radio for certificate revocation. [\[25,](#page-12-13)[26\]](#page-12-14)
    use FM radio broadcasting to disseminate warning information to drivers. In 2003,
    Microsoft used FM subcarrier signals to turn ordinary gadgets into smart gadgets.
    MSN Direct [\[12\]](#page-12-25) was a subscription network which sent short text
    updates over DirectBand, a 67.65 kHz subcarrier leased by Microsoft from commercial
    radio broadcasters.


    #### 8 Conclusion


    Despite decades of progress in global connectivity, billions of people around
    the world remain offline—not due to a lack of infrastructure, but because of persistent
    affordability barriers. Access to the internet continues to be out of reach for
    many, especially in resource-constrained regions where even low-cost mobile data
    can be prohibitively expensive. In this context, SONIC introduces a novel, ultra-low-cost
    approach to narrowing the digital divide by leveraging FM radio—a ubiquitous,
    inexpensive, and underutilized medium—to deliver essential web content and large
    language model (LLM)-based interactions without requiring an internet connection.
    By combining a full-system design, seamless integration with Android FM tuners,
    and deployment in real-world settings such as Cameroon, we demonstrate that SONIC
    can reliably transmit simplified web content and AI-generated responses in a way
    that is accessible, scalable, and resilient. Our work showcases the untapped potential
    of repurposing existing broadcast infrastructure to extend digital access to underserved
    populations, offering a practical path forward for connecting the unconnected
    and promoting more equitable access to knowledge and services across the globe.


    ### References


    <span id="page-11-0"></span>[1] Lighthouse. [https://developer](https://developer.chrome.com/docs/lighthouse).chrome.com/docs/
    [lighthouse](https://developer.chrome.com/docs/lighthouse).


    - <span id="page-12-19"></span>[2] Prolific | quickly find research participants
    you can trust. [https://www](https://www.prolific.com/).prolific.com/. Accessed:
    2025-04-12.

    - <span id="page-12-8"></span>[3] Seleniumhq browser automation, 2017. URL: [http:](http://www.seleniumhq.org/about/)
    //www.[seleniumhq](http://www.seleniumhq.org/about/).org/about/.

    - <span id="page-12-20"></span>[4] Citi program - collaborative institutional
    training initiative. www.[citiprogram](www.citiprogram.org).org, 2019. Accessed:
    2022-05- 18.

    - <span id="page-12-16"></span>[5] An image format for the Web., 2023. [https://](https://developers.google.com/speed/webp)
    developers.google.[com/speed/webp](https://developers.google.com/speed/webp).

    - <span id="page-12-18"></span>[6] Android app built with Quiet which allows o
    pass data through the speakers of an Android device., 2023. [https://github](https://github.com/quiet/org.quietmodem.Quiet).com/quiet/
    org.[quietmodem](https://github.com/quiet/org.quietmodem.Quiet).Quiet.

    - <span id="page-12-23"></span>[7] AudioQR, 2023. [https://github](https://github.com/jamesonrader/AudioQR).com/
    [jamesonrader/AudioQR](https://github.com/jamesonrader/AudioQR).

    - <span id="page-12-1"></span>[8] Expanding internet connectivity with stratospheric
    balloons, 2023. https://x.[company/projects/loon/](https://x.company/projects/loon/).

    - <span id="page-12-24"></span>[9] GGwave, 2023. [https://github](https://github.com/ggerganov/ggwave).com/ggerganov/
    [ggwave](https://github.com/ggerganov/ggwave).

    - <span id="page-12-2"></span>[10] High altitude connectivity: The next chapter,
    2023. [https://engineering](https://engineering.fb.com/2018/06/27/connectivity/high-altitude-connectivity-the-next-chapter/).fb.com/2018/06/
    [27/connectivity/high-altitude-connectivity](https://engineering.fb.com/2018/06/27/connectivity/high-altitude-connectivity-the-next-chapter/)[the-next-chapter/](https://engineering.fb.com/2018/06/27/connectivity/high-altitude-connectivity-the-next-chapter/).

    - <span id="page-12-11"></span>[11] Load mobile pages faster with Web Light, 2023.
    https://support.google.[com/websearch/answer/](https://support.google.com/websearch/answer/9836344?hl=en)
    [9836344?hl=en](https://support.google.com/websearch/answer/9836344?hl=en).

    - <span id="page-12-25"></span>[12] MSN Direct, 2023. [https://en](https://en.wikipedia.org/wiki/MSN_Direct).wikipedia.org/
    [wiki/MSN\\_Direct](https://en.wikipedia.org/wiki/MSN_Direct).

    - <span id="page-12-9"></span>[13] Quiet modem project., 2023. [https://github](https://github.com/quiet/quiet).com/
    [quiet/quiet](https://github.com/quiet/quiet).

    - <span id="page-12-12"></span>[14] Safari Reader Mode, 2023. [https://](https://support.apple.com/guide/safari/hide-ads-when-reading-sfri32632/mac)
    support.apple.[com/guide/safari/hide-ads](https://support.apple.com/guide/safari/hide-ads-when-reading-sfri32632/mac)[when-reading-sfri32632/mac](https://support.apple.com/guide/safari/hide-ads-when-reading-sfri32632/mac).

    - <span id="page-12-0"></span>[15] Starlink, 2023. [https://starlink](https://starlink.com/).com/.

    - <span id="page-12-10"></span>[16] Web Almanac, 2023. [https://](https://almanac.httparchive.org/en/2022/)
    almanac.httparchive.[org/en/2022/](https://almanac.httparchive.org/en/2022/).

    - <span id="page-12-3"></span>[17] Taara: A google x moonshot, 2025. Accessed:
    2025-03- 19. URL: https://x.[company/projects/taara/](https://x.company/projects/taara/).

    - <span id="page-12-4"></span>[18] Afrobarometer. Africa''s shifting media landscapes:
    Digital media use grows, but so do demographic divides, 2024. URL: https://www.[afrobarometer](https://www.afrobarometer.org/wp-content/uploads/2024/04/AD800-PAP14-Digital-media-use-grows-across-Africa%5ELJ-but-so-do-demographic-divides-Afrobarometer-29april24.pdf).org/
    [wp-content/uploads/2024/04/AD800-PAP14-](https://www.afrobarometer.org/wp-content/uploads/2024/04/AD800-PAP14-Digital-media-use-grows-across-Africa%5ELJ-but-so-do-demographic-divides-Afrobarometer-29april24.pdf)
    [Digital-media-use-grows-across-Africa%](https://www.afrobarometer.org/wp-content/uploads/2024/04/AD800-PAP14-Digital-media-use-grows-across-Africa%5ELJ-but-so-do-demographic-divides-Afrobarometer-29april24.pdf)
    [5ELJ-but-so-do-demographic-divides-](https://www.afrobarometer.org/wp-content/uploads/2024/04/AD800-PAP14-Digital-media-use-grows-across-Africa%5ELJ-but-so-do-demographic-divides-Afrobarometer-29april24.pdf)[Afrobarometer-29april24](https://www.afrobarometer.org/wp-content/uploads/2024/04/AD800-PAP14-Digital-media-use-grows-across-Africa%5ELJ-but-so-do-demographic-divides-Afrobarometer-29april24.pdf).pdf.

    - <span id="page-12-17"></span>[19] Android Developers. Broadcastreceiver, 2025.
    Accessed: 2025-04-10. URL: [https:](https://developer.android.com/develop/background-work/background-tasks/broadcasts)
    //developer.android.[com/develop/background](https://developer.android.com/develop/background-work/background-tasks/broadcasts)[work/background-tasks/broadcasts](https://developer.android.com/develop/background-work/background-tasks/broadcasts).

    - <span id="page-12-5"></span>[20] Osman Antwi-Boateng, Muhammed Danladi Musa,
    and Mu-Azu Iddirisu Andani and. Audience listenership of fm radio: A case study
    of rural development in northern ghana. *Cogent Arts & Humanities*, 10(1):2184750,
    2023. [arXiv:https:](https://arxiv.org/abs/https://doi.org/10.1080/23311983.2023.2184750)
    //doi.org/10.[1080/23311983](https://arxiv.org/abs/https://doi.org/10.1080/23311983.2023.2184750).2023.2184750,
    doi:10.[1080/23311983](https://doi.org/10.1080/23311983.2023.2184750).2023.2184750.

    - <span id="page-12-6"></span>[21] Evans C. Ashigwuike, Ale Felix, and Farouq
    E. Shaibu. The impact of soil texture on path loss modelling of an fm signal using
    diffraction technique: A case study of prime fm radio nigeria. *European Journal
    of Engineering Research and Science*, 4(4):56–63, April 2019. URL: [https://www](https://www.ej-eng.org/index.php/ejeng/article/view/1231).ejeng.org/index.[php/ejeng/article/view/1231](https://www.ej-eng.org/index.php/ejeng/article/view/1231),
    doi:10.[24018/ejers](https://doi.org/10.24018/ejers.2019.4.4.1231).2019.4.4.1231.

    - <span id="page-12-22"></span>[22] Yang Bai, Jian Liu, Li Lu, Yilin Yang, Yingying
    Chen, and Jiadi Yu. Batcomm: enabling inaudible acoustic communication with high-throughput
    for mobile devices. In *Proceedings of the 18th Conference on Embedded Networked
    Sensor Systems*, pages 205–217, 2020.

    - <span id="page-12-7"></span>[23] Tina Baidar. Signal coverage mapping of local
    radios. *Journal on Geoinformatics, Nepal*, 2017. Accessed via Nepal Journals
    Online.

    - <span id="page-12-15"></span>[24] Ketan Bhardwaj, Ada Gavrilovska, Moritz Steiner,
    Martin Flack, and Stephen Ludin. Driveshaft: Improving perceived mobile web performance.
    *arXiv preprint arXiv:1809.09292*, 2018.

    - <span id="page-12-13"></span>[25] Radu Gabriel Bozomitu and Florin Doru Hutu.
    Drivers'' warning application through personalized dsss-cdma data transmission
    by using the fm radio broadcasting infrastructure. *IEEE Access*, 11:11711–11731,
    2023.

    - <span id="page-12-14"></span>[26] Radu Gabriel Bozomitu and ¸Stefan Corneliu
    Stoica. A robust radiocommunication system for fm transmission based on software
    defined radio technology. In *2022 IEEE 28th International Symposium for Design
    and Technology in Electronic Packaging (SIITME)*, pages 78–81. IEEE, 2022.

    - <span id="page-12-21"></span>[27] Michael Butkiewicz, Daimeng Wang, Zhe Wu,
    Harsha V. Madhyastha, and Vyas Sekar. Klotski: Reprioritizing web content to improve
    user experience on mobile devices. In *12th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI ''15)*, Oakland, CA, USA, 2015. USENIX Association.
    Open Access. URL: https://www.usenix.[org/conference/nsdi15/](https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/butkiewicz)
    [technical-sessions/presentation/butkiewicz](https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/butkiewicz).

    - <span id="page-13-11"></span>[28] Xintao Chai, Hanming Gu, Feng Li, Hongyou
    Duan, Xiaobo Hu, and Kai Lin. Deep learning for irregularly and regularly missing
    data reconstruction. *Scientific Reports*, 10(1):3302, Feb 2020. doi:10.[1038/s41598-](https://doi.org/10.1038/s41598-020-59801-x)
    [020-59801-x](https://doi.org/10.1038/s41598-020-59801-x).

    - <span id="page-13-8"></span>[29] Moumena Chaqfeh, Russell Coke, Jacinta Hu,
    Waleed Hashmi, Lakshmi Subramanian, Talal Rahwan, and Yasir Zaki. Jsanalyzer:
    A web developer tool for simplifying mobile web pages through non-critical javascript
    elimination. *ACM Transactions on the Web*, 16(4):1–31, 2022.

    - [30] Moumena Chaqfeh, Muhammad Haseeb, Waleed Hashmi, Patrick Inshuti, Manesha
    Ramesh, Matteo Varvello, Lakshmi Subramanian, Fareed Zaffar, and Yasir Zaki. To
    block or not to block: Accelerating mobile web pages on-the-fly through javascript
    classification. In *Proceedings of the 12th International Conference on Information
    and Communication Technologies and Development, ICTD 2022, Seattle, USA, June
    27-29, 2022 (accepted)*, 2022.

    - <span id="page-13-9"></span>[31] Moumena Chaqfeh, Yasir Zaki, Jacinta Hu, and
    Lakshmi Subramanian. Jscleaner: De-cluttering mobile webpages through javascript
    cleanup. In *Proceedings of The Web Conference 2020*, pages 763–773, 2020.

    - <span id="page-13-16"></span>[32] Cloudflare, Inc. *Threat Intelligence APIs
    - Security Center*, 2025. Accessed: 2025-03-22. URL: [https://developers](https://developers.cloudflare.com/security-center/intel-apis/).cloudflare.com/security[center/intel-apis/](https://developers.cloudflare.com/security-center/intel-apis/).

    - <span id="page-13-4"></span>[33] Counterpoint Research. Global smartphone market
    share: Quarterly. 2025. URL: https://www.[counterpointresearch](https://www.counterpointresearch.com/insights/global-smartphone-share/).com/
    [insights/global-smartphone-share/](https://www.counterpointresearch.com/insights/global-smartphone-share/).

    - <span id="page-13-13"></span>[34] DataReportal. Digital 2025: Cameroon, 2025.
    Accessed: 2025-03-26. URL: [https://datareportal](https://datareportal.com/reports/digital-2025-cameroon).com/
    [reports/digital-2025-cameroon](https://datareportal.com/reports/digital-2025-cameroon).

    - <span id="page-13-5"></span>[35] Inc. Docker. Docker: Open platform for developing,
    shipping, and running applications, 2024. URL: [https:](https://www.docker.com/)
    //www.[docker](https://www.docker.com/).com/.

    - <span id="page-13-2"></span>[36] Jim Gettys and Kathleen Nichols. Bufferbloat:
    dark buffers in the internet. *Communications of the ACM*, 55(1):57–65, 2012.

    - <span id="page-13-7"></span>[37] Mohammad Ghasemisharif, Peter Snyder, Andrius
    Aucinas, and Benjamin Livshits. Speedreader: Reader mode made fast and private.
    In *The World Wide Web Conference*, pages 526–537, 2019.

    - <span id="page-13-6"></span>[38] Google. Amp is a web component framework to
    easily create user-first web experiences - amp.dev. [https:](https://amp.dev)
    [//amp](https://amp.dev).dev, 2019. Accessed: 2019-05-05.

    - <span id="page-13-15"></span>[39] Aaron Grattafiori, Laurens Van Der Maaten,
    et al. The llama 3 herd of models, 2024. [https://arxiv](https://arxiv.org/abs/2407.21783).org/
    [abs/2407](https://arxiv.org/abs/2407.21783).21783. [arXiv:2407](https://arxiv.org/abs/2407.21783).21783.

    - <span id="page-13-0"></span>[40] GSMA. The state of mobile internet connectivity
    report 2024, 2024. Accessed: 2025-02-05. URL: https://www.gsma.[com/r/wp-content/uploads/](https://www.gsma.com/r/wp-content/uploads/2024/10/The-State-of-Mobile-Internet-Connectivity-Report-2024.pdf)
    [2024/10/The-State-of-Mobile-Internet-](https://www.gsma.com/r/wp-content/uploads/2024/10/The-State-of-Mobile-Internet-Connectivity-Report-2024.pdf)[Connectivity-Report-2024](https://www.gsma.com/r/wp-content/uploads/2024/10/The-State-of-Mobile-Internet-Connectivity-Report-2024.pdf).pdf.

    - <span id="page-13-3"></span>[41] GSMArena. GSMArena - Mobile Phone Specifications,
    News, Reviews, 2025. Accessed: 21 Mar. 2025. URL: [https://www](https://www.gsmarena.com/).gsmarena.com/.

    - <span id="page-13-17"></span>[42] Rumaisa Habib, Sarah Tanveer, Aimen Inam,
    Haseeb Ahmed, Ayesha Ali, Zartash Afzal Uzmi, Zafar Ayyub Qazi, and Ihsan Ayyub
    Qazi. A framework for improving web affordability and inclusiveness. In *Proceedings
    of the ACM SIGCOMM 2023 Conference*, ACM SIGCOMM ''23, page 592–607, New York,
    NY, USA, 2023. Association for Computing Machinery. doi:10.[1145/3603269](https://doi.org/10.1145/3603269.3604872).3604872.

    - <span id="page-13-14"></span>[43] Huawei Technologies Co., Ltd. Mobile wifi
    4g router huawei e8372h-517 wifi modem. Online. Accessed: 25 March 2025. URL:
    [https://sincereonetech](https://sincereonetech.com/products/4g-router-huawei-e8372h-517-wifi-modem/).com/products/4g[router-huawei-e8372h-517-wifi-modem/](https://sincereonetech.com/products/4g-router-huawei-e8372h-517-wifi-modem/).

    - <span id="page-13-19"></span>[44] Soonwon Ka, Tae Hyun Kim, Jae Yeol Ha, Sun
    Hong Lim, Su Cheol Shin, Jun Won Choi, Chulyoung Kwak, and Sunghyun Choi. Near-ultrasound
    communication for tv''s 2nd screen services. In *Proceedings of the 22nd Annual
    International Conference on Mobile Computing and Networking*, pages 42–54, 2016.

    - <span id="page-13-1"></span>[45] Matt Kimball. My traceroute (MTR). [https://](https://www.bitwizard.nl/mtr/)
    www.[bitwizard](https://www.bitwizard.nl/mtr/).nl/mtr/.

    - <span id="page-13-10"></span>[46] Jesutofunmi Kupoluyi, Moumena Chaqfeh, Matteo
    Varvello, Russell Coke, Waleed Hashmi, Lakshmi Subramanian, and Yasir Zaki. Muzeel:
    assessing the impact of javascript dead code elimination on mobile web performance.
    In *Proceedings of the 22nd ACM Internet Measurement Conference*, pages 335–348,
    2022.

    - <span id="page-13-18"></span>[47] Tofunmi Kupoluyi, Moumena Chaqfeh, Matteo
    Varvello, Waleed Hashmi, Lakshmi Subramanian, and Yasir Zaki. Muzeel: A dynamic
    javascript analyzer for dead code elimination in today''s web. *arXiv preprint
    arXiv:2106.08948*, 2021.

    - <span id="page-13-12"></span>[48] Victor Le Pochat, Tom Van Goethem, Samaneh
    Tajalizadehkhoob, Maciej Korczynski, and Wouter Joosen. ´ Tranco: A research-oriented
    top sites ranking hardened


    against manipulation. In *Proceedings of the 26th Annual Network and Distributed
    System Security Symposium*, NDSS 2019, February 2019. [doi:10](https://doi.org/10.14722/ndss.2019.23386).14722/
    ndss.2019.[23386](https://doi.org/10.14722/ndss.2019.23386).


    - <span id="page-14-15"></span>[49] Hyewon Lee, Tae Hyun Kim, Jun Won Choi, and
    Sunghyun Choi. Chirp signal-based aerial acoustic communication for smart devices.
    In *2015 IEEE Conference on Computer Communications (INFOCOM)*, pages 2407–2415.
    IEEE, 2015.

    - <span id="page-14-8"></span>[50] LineageOS. Lineageos android distribution,
    2024. A free and open-source operating system for various devices, based on the
    Android mobile platform. URL: [https://lineageos](https://lineageos.org/).org/.

    - <span id="page-14-7"></span>[51] Ivano Malavolta, Kishan Nirghin, Gian Luca
    Scoccia, Simone Romano, Salvatore Lombardi, Giuseppe Scanniello, and Patricia
    Lago. Javascript dead code identification, elimination, and empirical assessment.
    *IEEE Transactions on Software Engineering*, 2023.

    - <span id="page-14-4"></span>[52] Bryan McQuade and Barry Pollard. How the core
    web vitals metrics thresholds were defined, 2020. Accessed: 2025-04-16. URL: https://web.[dev/articles/](https://web.dev/articles/defining-core-web-vitals-thresholds)
    [defining-core-web-vitals-thresholds](https://web.dev/articles/defining-core-web-vitals-thresholds).

    - <span id="page-14-2"></span>[53] MTN Cameroon. Mtn cameroon official website,
    2025. Accessed: 2025-03-20. URL: [https://mtn](https://mtn.cm/).cm/.

    - <span id="page-14-14"></span>[54] Ravi Netravali, Ameesh Goyal, James Mickens,
    and Hari Balakrishnan. Polaris: Faster page loads using fine-grained dependency
    tracking. In *13th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 16)*, Santa Clara, CA, 2016. USENIX Association. URL: https://www.usenix.[org/conference/nsdi16/](https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/netravali)
    [technical-sessions/presentation/netravali](https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/netravali).

    - <span id="page-14-3"></span>[55] OOKLA. Speedtest® CLI, Internet connection
    measurement for developers. https://www.speedtest.net/apps/cli.

    - <span id="page-14-5"></span>[56] OpenAI. Openai api reference, 2024. Accessed:
    2025- 03-24. URL: [https://platform](https://platform.openai.com/docs/api-reference/chat).openai.com/docs/
    [api-reference/chat](https://platform.openai.com/docs/api-reference/chat).

    - <span id="page-14-1"></span>[57] Ayush Pandey, Rohail Asim, Khalid Mengal, Matteo
    Varvello, and Yasir Zaki. Sonic: Connect the unconnected via fm radio & sms. In
    *Proceedings of the 20th International Conference on Emerging Networking EXperiments
    and Technologies*, CoNEXT ''24, page 41–47, New York, NY, USA, 2024. Association
    for Computing Machinery. doi:10.[1145/3680121](https://doi.org/10.1145/3680121.3697812).3697812.

    - <span id="page-14-13"></span>[58] Ayush Pandey, Matteo Varvello, Syed Ishtiaque
    Ahmed, Shurui Zhou, Lakshmi Subramanian, and Yasir Zaki. Maml: Towards a faster
    web in developing regions. In *Proceedings of the ACM on Web Conference 2025*,


    WWW ''25, page 727–739, New York, NY, USA, 2025. Association for Computing Machinery.
    [doi:10](https://doi.org/10.1145/3696410.3714584).1145/ [3696410](https://doi.org/10.1145/3696410.3714584).3714584.


    - <span id="page-14-10"></span>[59] Zhen Qin, Qingliang Zeng, Yixin Zong, and
    Fan Xu. Image inpainting based on deep learning: A review. *Displays*, 69:102028,
    2021. URL: https://www.[sciencedirect](https://www.sciencedirect.com/science/article/pii/S0141938221000391).com/
    [science/article/pii/S0141938221000391](https://www.sciencedirect.com/science/article/pii/S0141938221000391),
    [doi:10](https://doi.org/10.1016/j.displa.2021.102028).1016/j.displa.2021.102028.

    - <span id="page-14-6"></span>[60] Rick and Morty Wiki Contributors. Dimension
    c-137, 2025. Accessed: 24 Mar. 2025. URL: [https://rickandmorty](https://rickandmorty.fandom.com/wiki/Dimension_C-137).fandom.com/wiki/
    [Dimension\\_C-137](https://rickandmorty.fandom.com/wiki/Dimension_C-137).

    - <span id="page-14-16"></span>[61] Nirupam Roy, Haitham Hassanieh, and Romit
    Roy Choudhury. Backdoor: Making microphones hear inaudible sounds. In *Proceedings
    of the 15th Annual International Conference on Mobile Systems, Applications, and
    Services*, pages 2–14, 2017.

    - <span id="page-14-9"></span>[62] Olivier Rukundo and Hanqiang Cao. Nearest neighbor
    value interpolation. *International Journal of Advanced Computer Science and Applications*,
    3(4), 2012. URL: http://dx.doi.org/10.[14569/IJACSA](http://dx.doi.org/10.14569/IJACSA.2012.030405).2012.030405,
    doi:10.[14569/IJACSA](https://doi.org/10.14569/IJACSA.2012.030405).2012.030405.

    - <span id="page-14-12"></span>[63] Kimberly Ruth, Aurore Fass, Jonathan Azose,
    Mark Pearson, Emma Thomas, Caitlin Sadowski, and Zakir Durumeric. A world wide
    view of browsing the world wide web. In *Proceedings of the 22nd ACM Internet
    Measurement Conference*, IMC ''22, page 317–336, New York, NY, USA, 2022. Association
    for Computing Machinery. doi:10.[1145/3517745](https://doi.org/10.1145/3517745.3561418).3561418.

    - <span id="page-14-11"></span>[64] Salamek. Huawei lte api. GitHub repository,
    2025. Accessed: 25 March 2025. URL: [https://github](https://github.com/Salamek/huawei-lte-api).com/
    [Salamek/huawei-lte-api](https://github.com/Salamek/huawei-lte-api).

    - <span id="page-14-17"></span>[65] G Enrico Santagati and Tommaso Melodia. A
    softwaredefined ultrasonic networking framework for wearable devices. *IEEE/ACM
    Transactions on Networking*, 25(2):960–973, 2016.

    - <span id="page-14-18"></span>[66] Aaron Schulman, Dave Levin, and Neil Spring.
    Revcast: Fast, private certificate revocation over fm radio. In *Proceedings of
    the 2014 ACM SIGSAC Conference on Computer and Communications Security*, pages
    799– 810, 2014.

    - <span id="page-14-0"></span>[67] We Are Social and Meltwater. Digital 2025 global
    overview report, 2025. Retrieved on 18 March 2025. URL: [https://datareportal](https://datareportal.com/reports/digital-2025-global-overview-report).com/reports/
    [digital-2025-global-overview-report](https://datareportal.com/reports/digital-2025-global-overview-report).

    - <span id="page-15-4"></span>[68] Isidora Stankovic. Recovery of images with
    miss- ´ ing pixels using a gradient compressive sensing algorithm. *ArXiv*, abs/1407.3695,
    2014. URL: [https:](https://api.semanticscholar.org/CorpusID:15997634) //api.semanticscholar.[org/CorpusID:15997634](https://api.semanticscholar.org/CorpusID:15997634).

    - <span id="page-15-3"></span>[69] Kyoungwon Suh, Christophe Diot, Jim Kurose,
    Laurent Massoulie, Christoph Neumann, Don Towsley, and Matteo Varvello. Push-to-peer
    video-on-demand system: Design and evaluation. *IEEE Journal on Selected Areas
    in Communications*, 25(9):1706–1716, 2007.

    - <span id="page-15-1"></span>[70] Matteo Varvello, Hyunseok Chang, and Yasir
    Zaki. Performance characterization of videoconferencing in the wild. In *Proceedings
    of the 22nd ACM Internet Measurement Conference*, pages 261–273, 2022.

    - <span id="page-15-0"></span>[71] Matteo Varvello and Yasir Zaki. A worldwide
    look into mobile access networks through the eyes of amigos. In *TMA 2023 - Proceedings
    of the 7th Network Traffic Measurement and Analysis Conference*. Institute of
    Electrical and Electronics Engineers Inc. doi:10.[23919/TMA58422](https://doi.org/10.23919/TMA58422.2023.10198920).2023.10198920.

    - <span id="page-15-6"></span>[72] Hans Wackernagel. *Ordinary Kriging*, pages
    74–81. Springer Berlin Heidelberg, Berlin, Heidelberg, 1995. doi:10.[1007/978-3-662-03098-1\\_11](https://doi.org/10.1007/978-3-662-03098-1_11).

    - <span id="page-15-8"></span>[73] Xiao Sophia Wang, Arvind Krishnamurthy, and
    David Wetherall. Speeding up web page loads with shandian. In *13th USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 16)*, pages 109–122, Santa
    Clara, CA, 2016. USENIX Association. URL: https://www.usenix.[org/conference/nsdi16/](https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/wang)
    [technical-sessions/presentation/wang](https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/wang).

    - <span id="page-15-2"></span>[74] WorldData.info. Worlddata.info: The world in
    numbers, 2025. Accessed: 2025-04-08. URL: [https:](https://www.worlddata.info/)
    //www.[worlddata](https://www.worlddata.info/).info/.


    ## <span id="page-15-5"></span>A Pixel Interpolation


    Figure [20](#page-15-7) contains the results from a user study conducted by [\[57\]](#page-14-1)
    to benchmark the effectiveness of their pixel interpolation approach. The study
    simulates varying levels of visual loss on popular Pakistani webpages and evaluates
    their impact on perceived readability and content clarity. Screenshots of the
    top 50 webpages from the Tranco list were captured and processed under four levels
    of synthetic visual loss (5%, 10%, 20%, and 50%). Each screenshot was rendered
    in two variants—one with missing pixels left dark, and another corrected using
    nearest-neighbor pixel interpolation (detailed in Section [3.2\)](#page-5-1)—yielding
    400 total images.


    A total of 151 university students in Pakistan participated in the study, each
    rating 20 randomly assigned screenshots such that each image received at least
    seven responses. Ratings were collected on a 0–10 Likert scale across two dimensions:


    <span id="page-15-7"></span>![](_page_15_Figure_10.jpeg)


    Figure 20: Distribution of user ratings (0–10) for top 50 Pakistani webpages,
    with/without interpolation.


    (a) content clarity (*How well can you understand the content in this image?*),
    and (b) text readability (*How readable is the text in the image given the noise?*).
    Figure [20](#page-15-7) presents median boxplots per webpage, with hatched bars
    representing content clarity and plain bars representing text readability. Even
    with a 20% pixel loss rate, participants reported a median content clarity score
    of 7 out of 10, suggesting that the overall understanding of the page remained
    largely intact. Although text readability was more impacted, it continued to be
    within an acceptable range at 20% loss.'
- title: "A Hierarchical Optimization Framework Using Deep Reinforcement Learning\n\
    \  for Task-Driven Bandwidth Allocation in 5G Teleoperation"
  abstract: 'The evolution of 5G wireless technology has revolutionized connectivity,

    enabling a diverse range of applications. Among these are critical use cases

    such as real time teleoperation, which demands ultra reliable low latency

    communications (URLLC) to ensure precise and uninterrupted control, and

    enhanced mobile broadband (eMBB) services, which cater to data-intensive

    applications requiring high throughput and bandwidth. In our scenario, there

    are two queues, one for eMBB users and one for URLLC users. In teleoperation

    tasks, control commands are received in the URLLC queue, where communication

    delays occur. The dynamic index (DI) controls the service rate, affecting the

    telerobotic (URLLC) queue. A separate queue models eMBB data traffic. Both

    queues are managed through network slicing and application delay constraints,

    leading to a unified Lagrangian-based Lyapunov optimization for efficient

    resource allocation. We propose a DRL based hierarchical optimization framework

    that consists of two levels. At the first level, network optimization

    dynamically allocates resources for eMBB and URLLC users using a Lagrangian

    functional and an actor critic network to balance competing objectives. At the

    second level, control optimization finetunes the best gains for robots,

    ensuring stability and responsiveness in network conditions. This hierarchical

    approach enhances both communication and control processes, ensuring efficient

    resource utilization and optimized performance across the network.'
  url: http://arxiv.org/abs/2505.15977v1
  keywords: Tactile Internet, Telerobotics, 5G, Network Slicing, Deep Reinforcement
    Learning, Hierarchical Optimization
  document: "## I. INTRODUCTION\n\nThe rapid advancement in sensing, processing and\
    \ control technologies have enabled exponential growth in robotic applications\
    \ [1], [2], [3] in recent times. Although autonomous operations of robots are\
    \ prevalent in many mission-critical applications, safety-critical applications\
    \ involving robots often need teleoperation with human supervision. For example,\
    \ robots used in medical surgery are usually teleoperated under the supervision\
    \ of a trained physician [4]. As typically the users and/or the robots are mobile\
    \ in these applications, the teleoperation is enabled by 5G wireless communications.\n\
    \nIn the context of 5G, telerobotic applications are categorized as Tactile Internet\
    \ applications, which encompass both Ultra-Reliable Low-Latency Communications\
    \ (URLLC) and enhanced Mobile Broadband (eMBB) [5], [6]. These applications require\
    \ stringent latency and reliability standards, with URLLC ensuring low-latency\
    \ control commands from operators to robots, while eMBB is responsible for transmitting\
    \ the latest system status and video feedback, enabling operators to provide accurate\
    \ and effective commands. Given their distinct Quality of Service (QoS) requirements,\
    \ 5G networks employ network slicing. This allows for dedicated slices—one optimized\
    \ for URLLC to handle control commands and another slice designed for eMBB to\
    \ manage video feedback and system status updates—ensuring optimal performance\
    \ for both applications. The dynamic nature of wireless environments and the varying\
    \ demands of telerobotic operations introduce challenges in maintaining stable\
    \ performance, control, and overall system reliability. Through network slicing,\
    \ 5G networks can effectively address these diverse needs, guaranteeing robust\
    \ and efficient service for all users.\n\nThis work aims to address a hierarchical\
    \ 2-level optimization, where the upper-level joint Lyapunov optimization ensures\
    \ performance objectives at both the network and application levels while maintaining\
    \ queue stability. We consider two queues – one for enhanced Mobile Broadband\
    \ (eMBB) users and another for Ultra-Reliable Low-Latency Communications (URLLC)\
    \ users. Both queues have arrival rates modeled by Poisson processes. For URLLC\
    \ users, the departure rate is determined by the Dexterity Index (DI) combined\
    \ with throughput, ensuring low-latency, reliable communication. For eMBB users,\
    \ the departure rate is governed by the overall system throughput. At the lower\
    \ level, a robust closed-loop control is maintained using Razumikhin-based linear\
    \ quadratic regulator (LQR) [7] to ensure stable control gains that meet delay\
    \ constraints in dynamic environments. Given the stochastic nature of the upper-level\
    \ optimization, occasional violations of delay constraints may occur, particularly\
    \ when reliability is low. To handle these cases, we adapt nominal control gains\
    \ to ensure stable performance. Through numerical simulations, we demonstrate\
    \ the effectiveness of our framework, which includes efficient resource allocation\
    \ through slicing, stable network performance, and robust control gains for telerobotic\
    \ users, meeting the specified objectives.\n\nIn summary, our contributions are\
    \ as follows:\n\n- A hierarchical optimization framework is developed that employs\
    \ a deep reinforcement learning model to allocate resources across slices, and\
    \ a Razumikhin-based LQR controller to derive robust optimal control gains based\
    \ on network delay.\n- We present a Lyapunov-based joint optimization framework\
    \ tailored for tactile Internet applications, which uses a dual-queue model to\
    \ address the distinct demands of URLLC-driven telerobotic control and eMBB data\
    \ traffic. It leverages 5G network slicing and subcarrier resource\n\n<sup>© 2025</sup>\
    \ IEEE. Personal use of this material is permitted. Permission from IEEE must\
    \ be obtained for all other uses, in any current or future media, including reprinting/republishing\
    \ this material for advertising or promotional purposes, creating new collective\
    \ works, for resale or redistribution to servers or lists, or reuse of any copyrighted\
    \ component of this work in other works.\n\nallocation in the radio access network\
    \ (RAN) for adapting to fluctuations in network conditions and user demands.\n\
    \n• Unlike standard DRL methods, which are typically designed for unconstrained\
    \ Lyapunov optimization, we integrate a Lagrangian function to account for network\
    \ constraints directly within the optimization process. The reward function we\
    \ design effectively balances multiple competing goals, including quality of service,\
    \ adherence to constraints, and queue stability.\n\n## II. RELATED WORK\n\nDynamic\
    \ resource allocation is crucial to balance competing needs of eMBB and URLLC\
    \ in 5G networks. [8] and [9] highlight techniques that optimize resource use\
    \ to achieve low latency and high reliability for URLLC and high bandwidth for\
    \ eMBB. Similarly, Korrai et al. Chen et al. [10] underscore the critical role\
    \ of ultra-low latency in supporting missioncritical applications. For diverse\
    \ 5G service demands, network slicing is essential for isolating and guaranteeing\
    \ performance. Popovski et al. [11] focused to ensure URLLC's stringent latency\
    \ targets do not impact eMBB's high data needs. Santos et al. [12] explore max-matching\
    \ diversity channel allocation within network slicing, demonstrating effective\
    \ load balancing.\n\nIn addition to resource allocation, queuing models for URLLC\
    \ and eMBB traffic help maintain QoS. Makeeva [13] examines the impact of URLLC\
    \ on eMBB quality, highlighting the delicate balance needed in shared networks,\
    \ while Zhang et al. [14] propose a stochastic optimization framework to jointly\
    \ schedule both traffic types effectively. In [15], [16], the authors investigate\
    \ power allocation to different 5G slices using Lyapunov optimization, but do\
    \ not consider the capacity constraint for URLLC users and the control dynamics.\
    \ In [17], the authors formulate a Lyapunov optimization of eMBB and URLLC slices\
    \ to minimize the control drift error.\n\nDeep reinforcement learning (DRL) shows\
    \ promise in optimizing resource allocation under changing conditions. Filali\
    \ et al. [18] showcase DRL's potential to adapt resource allocations dynamically,\
    \ enhancing performance for both URLLC and eMBB, a sentiment echoed by Zhang et\
    \ al. [19] in machine learning-based scheduling approaches.\n\nHowever, these\
    \ works do not consider the control performance and the network queue dynamics\
    \ of both types of users and their proposed solution is not suitable for realtime\
    \ implementation. Our proposed research lays a robust foundation for understanding\
    \ and addressing these challenges in dynamic environments, with advanced resource\
    \ allocation techniques, network slicing, and intelligent scheduling.\n\n## III.\
    \ SYSTEM MODEL\n\n## *A. Application Scenario*\n\nWe consider a scenario of a\
    \ 5G network which is privately deployed for telerobotic applications, e.g, multiple\
    \ users teleoperating multiple robots for some critical tasks in healthcare or\
    \ industrial applications. For safety, we assume that one user can teleoperate\
    \ exactly one robot at a time. We assume a locally deployed 5G network connects\
    \ operators and robots, where operators send control commands, and robots transmit\
    \ real-time video feedback to enable precise decision-making.\n\n# *B. Wireless\
    \ Communication Channel Model*\n\nWe model the 5G wireless network in terms of\
    \ multiple subcarriers as per the 3GPP standard. The subcarrier resources can\
    \ be allocated to multiple users in terms of physical resource blocks (PRB). Suppose,\
    \ there are k PRBs available in the 5G network. We also assume that the eMBB slice\
    \ has ne(t) users and URLLC slice has nu(t) users at decision time slot t, when\
    \ the upper-level optimizer will make decision for resource allocation. Suppose\
    \ pij (t) is the transmit power between the base station gNodeB and user i over\
    \ PRB j at decision time t. We define ρij (t) as an indicator function such that\
    \ ρij (t) = 1, indicates that P RB<sup>j</sup> is allocated to user i at decision\
    \ time slot t; otherwise ρij (t) = 0. Now for wireless channel model, we assume\
    \ there will be multipath fading as the robots are expected to operate in the\
    \ indoor or outdoor environments surrounding other structures. The data rate for\
    \ each user i is then can be derived as:\n\n$$r\\_i(t) = B \\ast \\sum\\_{j=1}^{k}\
    \ \\rho\\_{ij} \\log\\_2(1 + \\frac{p\\_{ij}(t)h\\_{ij}}{\\sigma^2}) \\tag{l}$$\n\
    \nwhere σ 2 is noise variance, hij (t) is the time-varying Rayleigh fading channel\
    \ gain [20], and B is the bandwidth of each PRB which is composed of 12 subcarriers\
    \ [21].\n\n# *C. Network Queues*\n\nThe queue dynamics for the teleop users is\
    \ influenced by two main factors: 1) The arrival rate into the network depends\
    \ on the user behavior; 2) The serving (departure) rate is dependent on the robots\
    \ ability to complete a particular task.\n\n# *Modified Arrival and Departure\
    \ Rates*\n\nIn teleoperation, the user generates a sequence of setpoint commands\
    \ (e.g., velocity commands) at varying rates depending on user skill, leading\
    \ to some application delay. Hence, the arrival rates are modeled using a modified\
    \ Poisson distribution that incorporates delay and bandwidth sensitivity. Let\
    \ λ<sup>0</sup> denote the base arrival rate, r u i the data rate for URLLC users,\
    \ α a sensitivity coefficient (0 < α < 1), and D<sup>i</sup> the application delay.\
    \ The adjusted arrival rate λ(u) is expressed as: λ(u) = λ0−αr<sup>u</sup> i .\
    \ Using this, the probability mass function (PMF) for k arrivals during a time\
    \ interval τ + D<sup>i</sup> is:\n\n$$P\\_i(k, \\tau + D\\_i) = \\frac{((\\lambda\\\
    _0 - \\alpha r\\_i^u)(\\tau + D\\_i))^k e^{-(\\lambda\\_0 - \\alpha r\\_i^u)(\\\
    tau + D\\_i)}}{k!},\\tag{2}$$\n\nwhere D<sup>i</sup> represents the delay associated\
    \ with user i. For departures, we model the serving rate as a function of task\
    \ efficiency, represented by the Dynamic Task Execution Index (DI) defined as:\n\
    \n$$\\begin{aligned} \\text{DI} &= \\, 0.4\\* \\max\\left(0, 1 - \\frac{\\text{tracking\\\
    \\_error}}{10}\\right) + \\\\ &0.3\\* \\left(\\max\\left(0, 1 - \\frac{\\text{orientation\\\
    \\_error}}{180}\\right) + \\max\\left(0, 1 - \\frac{\\text{curv}}{1}\\right)\\\
    right) \\end{aligned} (3)$$\n\nwhere tracking error, orientation error, and curv\
    \ quantify the robot's performance in tracking set-points, orienting correctly,\n\
    \n<sup>© 2025</sup> IEEE. Personal use of this material is permitted. Permission\
    \ from IEEE must be obtained for all other uses, in any current or future media,\
    \ including reprinting/republishing this material for advertising or promotional\
    \ purposes, creating new collective works, for resale or redistribution to servers\
    \ or lists, or reuse of any copyrighted component of this work in other works.\n\
    \nand minimizing trajectory curvature, respectively. The serving rate µi(DI),\
    \ DI ∈ (0, 1) for user i is modeled as:\n\n$$\n\\mu\\_i(DI) = r\\_i^u - \\beta\
    \ \\,\\, DI \\tag{4}\n$$\n\nwhere r u i is the data rate for URLLC robot users\
    \ and β > 0 is a serving coefficient that scales the impact of DI on the serving\
    \ rate. Overall, the URLLC queue, describing the queue of set-point commands,\
    \ is modeled as:\n\n$$F(t+1) = \\left[F(t) + A\\_1(t) - D(t)\\right]^+\\tag{5}$$\n\
    \nwhere, F(t): Queue length at time t, A1(t) = P(t, τ +Dmax), and D(t) = µ(DI).\
    \ For eMBB users, the queue is defined as:\n\n$$G(t+1) = [G(t) + A\\_2(t) - r\\\
    _i^e]^+\\tag{6}$$\n\nwhere G(t) represents the queue length at time t, A2(t) is\
    \ the arrival rate following a standard Poisson distribution, and r e i is the\
    \ departure rate for eMBB users.\n\n## IV. PROPOSED HIERARCHICAL FRAMEWORK\n\n\
    We propose a dual level optimization framework (see fig. 1) that integrates upper-level\
    \ network resource allocation with lower-level robot control to address challenges\
    \ in teleoperation over a shared 5G network. This dual-layered approach ensures\
    \ efficient resource utilization, network stability, and reliable control for\
    \ both URLLC and eMBB applications. The upper level uses a Deep Reinforcement\
    \ Learning (DRL) agent to dynamically allocate resources across network slices,\
    \ optimizing performance based on real-time conditions and task demands. The DRL\
    \ agent observes system states, such as virtual queue lengths, and adjusts its\
    \ policy through rewards that prioritize network efficiency and stability.\n\n\
    At the lower level, robot control optimization leverages the Razumikhin stability\
    \ criterion to ensure robust performance despite network delays. A matrix inequality\
    \ computes optimal control gains, minimizing tracking errors and control efforts\
    \ while maintaining stability under varying network conditions. The upper level\
    \ resource allocation decisions influence the network delays, which affect the\
    \ lower level robot control performance. The task execution performance of the\
    \ robots, in the form of a dexterity index (DI) influence the departure rate and\
    \ network queue, which in turn updates the DRL agent's state, enabling adaptive\
    \ policy refinement that aligns network decisions with control outcomes. This\
    \ interconnected dual level closed-loop design ensures seamless coordination between\
    \ the network and control systems.\n\n# *A. Upper Level Optimization Problem*\n\
    \nWe optimize the network data rate for telerobotics operations involving n<sup>e</sup>\
    \ eMBB users and n<sup>u</sup> URLLC users. Let Dti represent the end-to-end delay\
    \ for the i-th operatorrobot pair, with Dmax as the URLLC delay deadline. Define\
    \ a reliability index χ<sup>s</sup> ∈ (0, 1] representing the percentage of time\
    \ Dti < Dmax. This Optimization problem can be formulated as a minimization problem\
    \ as follows:\n\n$$\\arg\\min\\_{\\rho\\_{ij}} \\lim\\_{t \\to \\infty} \\left(\
    \ \\sum\\_{\\tau=1}^{t-1} \\sum\\_{i=1}^{n\\_u} \\frac{1}{(r\\_i^u)^2(\\tau) +\
    \ \\epsilon} + \\sum\\_{\\tau=1}^{t-1} \\sum\\_{i=1}^{n\\_e} \\frac{1}{(r\\_i^e)^2(\\\
    tau) + \\epsilon} \\right) \\quad (7)$$\n\n![](_page_2_Figure_14.jpeg)\n\nFig.\
    \ 1: Proposed Hierarchical Optimization Framework showing the components and dataflow\n\
    \nsubject to constraints:\n\n$$\\text{Reliability:} P(D\\_{t\\land} > D\\_{\\\
    max}) \\le 1 - \\chi\\_{\\text{s}}, \\ \\forall i = 1, \\ldots, n\\_{\\text{u}},\
    \ \\qquad (8a)$$\n\n$$\\text{Resoure Allocation Allocation:} \\sum\\_{j=1}^{k}\
    \ \\sum\\_{i=1}^{n\\_u + n\\_e} \\rho\\_{ij} = K,\\tag{8b}$$\n\n$$\\sum\\_{\\\
    substack{i=1\\\\k\\end{r}}^{n\\_u+n\\_e} \\rho\\_{ij} = 1 \\quad \\forall \\ j\
    \ = 1,2,\\ldots,k,\\tag{8c}$$\n\n$$\\sum\\_{j=1}^{K} \\rho\\_{ij} \\ge 1 \\quad\
    \ \\forall \\, i = 1, 2, \\dots, n\\_u + n\\_c \\tag{8d}$$\n\nwhere r u i and\
    \ r e i are the URLLC and eMBB data rates respectively, both of which are function\
    \ of ρij as mentioned in equation (1). The parameter ϵ is a small positive scalar,\
    \ which is introduced to keep the minimization problem computationally feasible\
    \ in an unwanted case of very low data rate.\n\nTo satisfy Lyapunov stability,\
    \ define virtual queues for URLLC (F(k)) and eMBB (G(k)):\n\n$$\\begin{array}{l}\
    \ F(k+1) = F(k) + A\\_1(k) - D(k) \\\\ G(k+1) = G(k) + A\\_2(k) - r\\_i^e \\end{array}\
    \ \\tag{9a}$$\n\nUsing a candidate network Lyapunov function:\n\n$$L(F(k), G(k))\
    \ = \\begin{bmatrix} 1 \\\\ 2 \\\\ \\vdots \\end{bmatrix} F(k)^2 + G(k)^2 \\Big|\\\
    ,,\\tag{10}$$\n\nthe Lyapunov drift-plus-penalty equation becomes:\n\n$$\n\\Delta\
    \ L(k) + Vh(r\\_i), \\tag{11}\n$$\n\nwhere h(ri) is the original cost function,\
    \ and V > 0 is a penalty weight. The delay constraint can be reformulated as:\
    \ u\n\n$$y(\\tau) = e^{-\\alpha\\_1 r\\_i^u(\\tau)D\\_{\\text{max}}} - (1 - \\\
    chi\\_s) \\ge 0,\\qquad(12)$$\n\nwhere α<sup>1</sup> > 0 determines the impact\
    \ of data rate on delay.\n\n# *Lagrangian Formulation*\n\nWe combine the cost\
    \ function and the constraint through a Lagrangian function. The Lagrangian for\
    \ the unconstrained problem is chosen as:\n\n$$L\\_a(r\\_i, y(\\tau), \\lambda\\\
    _l) = \\Delta L(\\tau) + Vh(r\\_i) - \\lambda\\_l y(\\tau), \\qquad (13)$$\n\n\
    where λ<sup>l</sup> ≥ 0 is the Lagrange multiplier. The optimization becomes:\
    \ arg min\n\n$$\\arg\\min\\_{\\rho\\_{ij}} \\arg\\max\\_{\\lambda\\_l>0} L\\_a(r\\\
    _i, y(\\tau), \\lambda\\_l). \\tag{14}$$\n\nThis unified framework incorporates\
    \ reliability, resource allocation, and delay constraints into a single optimization\
    \ problem.\n\n<sup>© 2025</sup> IEEE. Personal use of this material is permitted.\
    \ Permission from IEEE must be obtained for all other uses, in any current or\
    \ future media, including reprinting/republishing this material for advertising\
    \ or promotional purposes, creating new collective works, for resale or redistribution\
    \ to servers or lists, or reuse of any copyrighted component of this work in other\
    \ works.\n\n## *B. Lower Level Optimization*\n\nWe model the robot dynamics to\
    \ be a discrete-time linear system with delay T<sup>d</sup> as:\n\n$$x(k+1) =\
    \ Ax(k) + A\\_d x(k - T\\_d) + Bu(k)\\qquad(15)$$\n\nwhere the states are represented\
    \ as x(k) with system matrices A, A<sup>d</sup> and B. With a state feedback control\
    \ law u(k) = −Kx(k), the closed loop dynamics becomes:\n\n$$x(k+1) = (A - BK)x(k)\
    \ + A\\_dx(k - T\\_d).$$\n\nFor a candidate Lyapunov function V (x(k)) = x(k)\
    \ <sup>T</sup> P x(k), we can exploit the Razumikhin's stability criterion:\n\n\
    $$V(x(k - T\\_d)) \\le \\gamma V(x(k)), \\quad \\Delta V(x(k)) \\le -\\alpha V(x(k)),$$\n\
    \nwhere α > 0 and γ = 1 + αTd, for ensuring stability.\n\nUsing the augmented\
    \ state vector <sup>ζ</sup>(k) = x(k) x(k − Td) , the\n\nstability condition can\
    \ be derived as a matrix inequality:\n\n$$\n\\begin{bmatrix}\nA^T PA - P + \\\
    alpha P & A^T PA\\_d & A^T PBY \\\\\nA\\_d^T PA & \\gamma A\\_d^T PA\\_d & A\\\
    _d^T PBY \\\\\nY^T B^T PA & Y^T B^T PA\\_d & Y^T B^T PBY - P\n\\end{bmatrix} <\
    \ 0, \\quad (16)\n$$\n\nwhere Y = KP, γ = 1 + αT max d , and T max d is the maximum\
    \ anticipated delay. This ensures stability under nominal scenario provided the\
    \ sampling period exceeds the network delay. For dealing with the stochastic condition\
    \ for delay Td, a convex optimization problem minimizing ∥Kˆ − K∥ <sup>2</sup>\
    \ need to be solved along with (16) as a constraint, where Yˆ = KPˆ points to\
    \ the adjusted gain Kˆ .\n\n## *C. Deep Reinforcement Learning (DRL) Framework*\n\
    \nThe goal is to minimize the average cost while satisfying the constraints 8b\
    \ and 8c. This problem is framed as a Markov Decision Process (MDP), where decisions\
    \ are made under uncertainty. To solve this, the Advantage Actor-Critic (A2C)\
    \ algorithm, a Deep Reinforcement Learning (DRL) method, is applied. A2C integrates\
    \ policy optimization with value estimation, stabilizing the learning process.\
    \ In this framework, the Actor learns a policy that outputs action probabilities,\
    \ while the Critic estimates the expected future rewards. The advantage function\
    \ refines the Actor's decisions by providing feedback on whether actions lead\
    \ to better or worse outcomes than expected, thus improving the learning process.\n\
    \nIn our setup, two independent agents handle resource allocation: one for eMBB\
    \ users and another for URLLC users. These agents aim to optimally allocate K\
    \ physical resource blocks (PRBs) while satisfying system constraints. The parameters\
    \ used in the pseudocode (Algorithm 1) include the Actor and Critic networks for\
    \ both agents, represented by θa1, θc<sup>1</sup> for Agent 1 (eMBB) and θa2,\
    \ θc<sup>2</sup> for Agent 2 (URLLC). Each agent uses a replay buffer (Replay\
    \ buffer1, Replay buffer2) to store experiences and update their networks. The\
    \ learning rate and discount factor γ control the learning process, with each\
    \ agent updating their Actor and Critic networks at each time step. The reward\
    \ function r<sup>1</sup> and r<sup>2</sup> guide the agents to minimize penalties,\
    \ while the Lagrangian multiplier λ is used to adjust the constraints, ensuring\
    \ the optimal allocation strategy is both effective and compliant with system\
    \ requirements.\n\n## Algorithm 1 Dual-Agent DDPG for Resource Allocation\n\n\
    ```\n1: Initialize: Actor and Critic networks for each agent with parameters\n\
    \   θa1, θc1, θa2, θc2; Replay buffers for each agent; Set hyperparameters\n2:\
    \ for each episode do\n```\n2\n\n3: Reset environment and observe initial states\
    \ s 1 , s\n\n4: for each time step do\n\n5: Agent 1: Select action a <sup>1</sup>\
    \ with Actor1, execute it, receive r 1 , s 1 ′ 6: Agent 2: Select action a<sup>2</sup>\
    \ with Actor2, execute it, receive r 2 , s 2 7: Store transitions (s 1 , a<sup>1</sup>\
    \ , r<sup>1</sup> , s<sup>1</sup> ′ ) in Replay buffer1, (s 2 , a<sup>2</sup>\
    \ , r<sup>2</sup> , s<sup>2</sup> ′ ) in Replay buffer2\n\n8: Update Agent1: Sample\
    \ batch from Replay buffer1, calculate target y 1 and advantage, update Critic1\
    \ and Actor1\n\n9: Update Agent2: Sample batch from Replay buffer2, calculate\
    \ target y 2 and advantage, update Critic2, λ, and Actor2 <sup>1</sup> ← s 1 ′\
    \ <sup>2</sup> ← s 2 ′\n\n, s\n\n10: Update states: s\n\n11: end for\n\n12: end\
    \ for\n\n![](_page_3_Figure_26.jpeg)\n\nFig. 2: Reward for eMBB and URLLC Slices\n\
    \nAs outlined in the pseudocode, at each time step, the agents interact with the\
    \ environment, select actions a<sup>1</sup> and a<sup>2</sup> based on their policies\
    \ π(s1|θa1) and π(s2|θa2), respectively, and update their parameters using the\
    \ calculated advantages.\n\n## V. EXPERIMENT AND RESULTS\n\n## *A. Simulation\
    \ Setup*\n\nWe implemented our DRL framework in Python and simulated it using\
    \ realistic 5G parameters (Table I). The 5G network has a 400 m radius with two\
    \ slices: eMBB and URLLC. Network conditions follow a defined channel model, and\
    \ packet arrivals follow a Poisson distribution. Practical values for control\
    \ dexterity, URLLC delay, and reliability constraints were considered. We assume\
    \ user counts do not exceed available PRBs, with a one-to-one mapping between\
    \ telerobotic users and robots, and users of each type (URLLC or eMBB) are independently\
    \ and identically distributed.\n\n## *B. Results*\n\n*1) DRL Performance:* We\
    \ measure the performance of DRL that solves the upper-level optimization. The\
    \ total reward is converged, and this reward is the sum of the eMBB and URLLC\
    \ rewards as shown in Figs. 2a and 2b. Furthermore, the queue length for the users\
    \ of those slices is displayed in Figs. 3a and 3b. There has been a reduction\
    \ in the queue\n\n| Parameter                           | Symbols | Values   |\n\
    |-------------------------------------|---------|----------|\n| Bandwidth    \
    \                       | B       | 10 MHz   |\n| Noise Variance             \
    \         | 2<br>σ  | -110 dBm |\n| Reliability                         | χs \
    \     | 95%      |\n| Deadline for URLLC Delay            | Dmax    | 20 ms  \
    \  |\n| Transmit power                      | pij     | 20 dBm   |\n| Number of\
    \ PRBs                      | K       | 25       |\n| modified Poisson distribution\
    \ alpha | α       | 0.1      |\n\nTABLE I: Network Simulation Parameters\n\n©\
    \ 2025 IEEE. Personal use of this material is permitted. Permission from IEEE\
    \ must be obtained for all other uses, in any current or future media, including\
    \ reprinting/republishing this material for advertising or promotional purposes,\
    \ creating new collective works, for resale or redistribution to servers or lists,\
    \ or reuse of any copyrighted component of this work in other works.\n\n![](_page_4_Figure_1.jpeg)\n\
    \n(a) CDF of eMBB datarate (b) CDF of URLLC datarate Fig. 4: CDF of eMBB and URLLC\
    \ datarates with Varying σ (Rayleigh Fading Scale) in the Wireless Network.\n\n\
    length during training in the final episodes of the series for eMBB users.\n\n\
    *2) Varying Network Conditions:* We measure the data rate over the time varying\
    \ wireless channel conditions. Figs. 4a and 4b show the cumulative distribution\
    \ function (CDF) of the data rate for the eMBB and URLLC users respectively. As\
    \ we allocated more bandwidth to the eMBB slice, eMBB users get higher bandwidth\
    \ as expected. For this experiment we chose the scaling factor of the Rayleigh\
    \ fading as 1.0. The CDF shows the throughput are more or less evenly distributed\
    \ in the lower and upper region with a sharper slope in the middle, indicating\
    \ the mid-datarate value achieved frequently.\n\nWe also vary the Rayleigh fading\
    \ distribution scaling factor which can potentially indicate varying multipath\
    \ conditions in indoor, outdoor and cluttered environments. Fig. 5a shows the\
    \ avg. data rate of the eMBB and URLLC users for increasing scaling factor which\
    \ reducing peaks of the Rayleigh distribution indicating better channel conditions\
    \ along X-axis. Both eMBB and URLLC data rate shows improvement due to better\
    \ channel conditions, without any change in the bandwidth allocation policy. Fig.\
    \ 5b shows the avg. queue length of the eMBB and URLLC users for varying Rayleigh\
    \ fading scaling factor. It shows that with improved network conditions, i.e.,\
    \ higher scaling factor, as the network can carry more data, the queue length\
    \ also slowly increases for the eMBB users, but still the queue length is finite\
    \ and not very high. For URLLC users, the queue length is much smaller due to\
    \ smaller control data packets and, also better optimized control gain by the\
    \ lower-level optimizer. It remains almost unaffected with the network variations.\n\
    \n*3) Varying Sampling Interval:* We wanted to check the effects of varying sampling\
    \ intervals in the lower level controllers. The optimal data rate resulted from\
    \ these variations are shown in Fig. 6. The minor variations in the eMBB and URLLC\
    \ data rate observed in Fig. 6 with changes in the\n\n![](_page_4_Figure_7.jpeg)\n\
    \n![](_page_4_Figure_8.jpeg)\n\n![](_page_4_Figure_9.jpeg)\n\nFig. 6: Average\
    \ Data rate for eMBB and URLLC users with different sampling interval control\n\
    \nsampling interval demonstrate the robustness of our proposed resource allocation\
    \ scheme. The upper level DRL agent effectively adapts the resource allocation\
    \ to maintain a consistent data rate for both users, even when the sampling period\
    \ of the robot control system changes. The changes in sampling period affect the\
    \ robustness of the nominal gains and may potentially degrade task performance.\
    \ However, the upper level adapt to the lower level task execution by dynamically\
    \ changing the data rate according to the situation.\n\n*4) Varying DI:* To examine\
    \ the effects of task execution variation on the optimal data rate, we choose\
    \ 3 different set of reference signals which gave us low, moderate and high DI\
    \ (through trial and error). As shown in the Fig. 7, lower DI results in a higher\
    \ data rate and higher DI produces lower data rate. As the dexterity index increases,\
    \ indicating more complex and demanding tasks, the DRL agent intelligently adjusts\
    \ the resource allocation, potentially lowering the data rate to provide the robots\
    \ with more time for task execution, effectively balancing performance and efficiency.\
    \ This adaptability is crucial for supporting a wide range of teleoperation tasks\
    \ with varying dexterity requirements. The dynamic variations in the URLLC and\
    \ eMBB data rate with changes in the DI (Fig. 7) highlight the adaptability of\
    \ our DRL-based resource allocation to the varying demands of teleoperation task\
    \ execution.\n\n*5) Adjusted Gain:* We wanted to compare the nominal gains and\
    \ adjusted gains when the delay constraint is violated. The results in table II\
    \ compares the tracking error of the robot control system for two different control\
    \ gain settings. The table shows that the adjusted gain generally leads to a lower\
    \ tracking error compared to the nominal gain when the delay constraint is violated.\
    \ This highlights the effectiveness of the proposed\n\n<sup>© 2025</sup> IEEE.\
    \ Personal use of this material is permitted. Permission from IEEE must be obtained\
    \ for all other uses, in any current or future media, including reprinting/republishing\
    \ this material for advertising or promotional purposes, creating new collective\
    \ works, for resale or redistribution to servers or lists, or reuse of any copyrighted\
    \ component of this work in other works.\n\n![](_page_5_Figure_1.jpeg)\n\n(a)\
    \ eMBB datarate vs DI (b) URLLC datarate vs DI\n\nFig. 7: Average Data rate for\
    \ eMBB and URLLC users with different Dynamic Task Execution Index values\n\n\
    ![](_page_5_Figure_4.jpeg)\n\nFig. 8: Average of Delay Violation Across all URLLC\
    \ users for our method (DRL) and Proportional Fair (PF) allocation\n\n| Delay\
    \ Violation | Tracking Error |               |  |\n|-----------------|----------------|---------------|--|\n\
    | Time Slot       | Nominal Gain   | Adjusted Gain |  |\n| 5               | 0.8962\
    \         | 0.7077        |  |\n| 10              | 0.7877         | 0.5609  \
    \      |  |\n| 20              | 0.8726         | 0.5660        |  |\n\nTABLE\
    \ II: Comparison of Tracking Error for Nominal and Adjusted Gain in Delay Violation\
    \ Time Slots\n\n|          |              | Datarate (Mbps) |         |\n|----------|--------------|-----------------|---------|\n\
    | Timeslot | Channel gain | DRL             | PF      |\n| 5        | 1.0481 \
    \      | 17.9188         | 19.4697 |\n| 10       | 1.0645       | 19.6458    \
    \     | 19.2875 |\n| 15       | 1.8141       | 19.7586         | 19.9794 |\n\n\
    TABLE III: Comparison of Data rate for DRL and Proportional fair allocation, Average\
    \ across all URLLC users\n\ngain adjustment in maintaining satisfactory task execution\
    \ even when the network quality degrades.\n\n*6) Comparison:* Proportional Fair\
    \ (PF) resource allocation is a widely used method in wireless networks that balances\
    \ throughput and fairness by maximizing the sum of the logarithms of users' data\
    \ rates. We compared PF based resource allocation with our proposed method. While\
    \ the datarate is comparable in both (table III), the delay violations are significantly\
    \ lower with the proposed approach as shown in in figure 8. This highlights its\
    \ effectiveness in meeting stringent latency requirements, particularly for applications\
    \ such as telerobotics, where minimizing delay violations is critical for reliable\
    \ and responsive operation.\n\n## VI. CONCLUSIONS\n\nThis paper presents a novel\
    \ hierarchical optimization framework for resource allocation in 5G networks serving\
    \ a mix of URLLC and eMBB applications, with a particular focus on telerobotics.\
    \ We address the challenges of varying robotic task execution requirements in\
    \ teleoperation by integrating the upper level DRL with a lower-level Razumikhin-based\
    \ controller that adapts to network delays, ensuring stability and performance.\
    \ Simulation results demonstrate the efficacy of our approach in achieving robust\
    \ performance and efficient resource utilization across diverse scenarios, highlighting\
    \ its potential for enabling reliable and responsive teleoperation in the evolving\
    \ 5G landscape. Future work will address generalization to unknown robot dynamics\
    \ and uncertain network conditions.\n\n## REFERENCES\n\n- [1] J. Leng, W. Sha,\
    \ B. Wang, P. Zheng, C. Zhuang, Q. Liu, T. Wuest, D. Mourtzis, and L. Wang, \"\
    Industry 5.0: Prospect and retrospect,\" *Journal of Manufacturing Systems*, vol.\
    \ 65, pp. 279–295, 2022.\n- [2] K. A. Demir, G. Doven, and B. Sezen, \"Industry\
    \ 5.0 and human-robot ¨ co-working,\" *Procedia computer science*, vol. 158, pp.\
    \ 688–695, 2019.\n- [3] A. Khalid, P. Kirisci, Z. Ghrairi, K.-D. Thoben, and J.\
    \ Pannek, \"A methodology to develop collaborative robotic cyber physical systems\
    \ for production environments,\" *Logistics Research*, vol. 9, pp. 1–15, 2016.\n\
    - [4] P. J. Choi, R. J. Oskouian, and R. S. Tubbs, \"Telesurgery: past, present,\
    \ and future,\" *Cureus*, vol. 10, no. 5, 2018.\n- [5] B. S. Khan, S. Jangsher,\
    \ A. Ahmed, and A. Al-Dweik, \"Urllc and embb in 5g industrial iot: A survey,\"\
    \ *IEEE Open Journal of the Communications Society*, vol. 3, pp. 1134–1163, 2022.\n\
    - [6] R. Ali, Y. B. Zikria, A. K. Bashir, S. Garg, and H. S. Kim, \"Urllc for\
    \ 5g and beyond: Requirements, enabling incumbent technologies and network intelligence,\"\
    \ *IEEE Access*, vol. 9, pp. 67 064–67 095, 2021.\n- [7] M. M. Rayguru, M. R.\
    \ Elara, B. F. Gomez, and B. Ramalingam, \"A ´ time delay estimation based adaptive\
    \ sliding mode strategy for hybrid impedance control,\" *IEEE Access*, vol. 8,\
    \ pp. 155 352–155 361, 2020.\n- [8] X. Han, K. Xiao, R. Liu, X. Liu, and G. Alexandropoulos,\
    \ \"Dynamic resource allocation schemes for embb and urllc services in 5g wireless\
    \ networks,\" *Intelligent and Converged Networks*, 2022.\n- [9] P. Korrai, E.\
    \ Lagunas, S. Sharma, S. Chatzinotas, and B. Ottersten, \"Slicing based resource\
    \ allocation for multiplexing of embb and urllc services in 5g wireless networks,\"\
    \ 2019.\n- [10] H. Chen, R. Abbas, P. Cheng, M. Shirvanimoghaddam, W. Hardjawana,\
    \ W. Bao, Y. Li, and B. Vucetic, \"Ultra-reliable low latency cellular networks:\
    \ use cases, challenges and approaches,\" *Ieee Communications Magazine*, vol.\
    \ 56, pp. 119–125, 2018.\n- [11] P. Popovski, K. F. Trillingsgaard, O. Simeone,\
    \ and G. Durisi, \"5g wireless network slicing for embb, urllc, and mmtc: a communicationtheoretic\
    \ view,\" *IEEE Access*, vol. 6, pp. 55 765–55 779, 2018.\n- [12] E. Santos, R.\
    \ Souza, J. Rebelatto, and H. Alves, \"Network slicing for urllc and embb with\
    \ max-matching diversity channel allocation,\" *Ieee Communications Letters*,\
    \ vol. 24, pp. 658–661, 2020.\n- [13] E. Makeeva, I. Kochetkova, and R. Alkanhel,\
    \ \"Retrial queueing system for analyzing impact of priority ultra-reliable low-latency\
    \ communication transmission on enhanced mobile broadband quality of service degradation\
    \ in 5g networks,\" *Mathematics*, vol. 11, p. 3925, 2023.\n- [14] W. Zhang, M.\
    \ Derakhshani, and S. Lambotharan, \"Stochastic optimization of urllc-embb joint\
    \ scheduling with queuing mechanism,\" *IEEE Wireless Communications Letters*,\
    \ vol. 10, pp. 844–848, 2021.\n- [15] A. T. Z. Kasgari and W. Saad, \"Stochastic\
    \ optimization and control framework for 5g network slicing with effective isolation,\"\
    \ in *2018 52nd Annual Conference on Information Sciences and Systems (CISS)*.\
    \ IEEE, 2018, pp. 1–6.\n- [16] P. Samanipour and H. A. Poonawala, \"Stability\
    \ analysis and controller synthesis using single-hidden-layer relu neural networks,\"\
    \ *IEEE Transactions on Automatic Control*, 2023.\n- [17] N. Golmohammadi, M.\
    \ M. Rayguru, and S. Baidya, \"Lyapunovoptimized 5g-sliced communications for\
    \ telerobotic applications,\" in *IEEE INFOCOM 2024-IEEE Conference on Computer\
    \ Communications Workshops (INFOCOM WKSHPS)*, 2024.\n- [18] A. Filali, Z. Mlika,\
    \ S. Cherkaoui, and A. Kobbane, \"Dynamic sdn-based radio access network slicing\
    \ with deep reinforcement learning for urllc and embb services,\" 2022.\n- [19]\
    \ J. Zhang, X. Xu, K. Zhang, B. Zhang, X. Tao, and P. Zhang, \"Machine learning\
    \ based flexible transmission time interval scheduling for embb and urllc coexistence\
    \ scenario,\" *IEEE Access*, 2019.\n- [20] M. H. Mahmud, M. M. Hossain, A. A.\
    \ Khan, S. Ahmed, M. A. Mahmud, and M. H. Islam, \"Performance analysis of ofdm,\
    \ w-ofdm and f-ofdm under rayleigh fading channel for 5g wireless communication,\"\
    \ in *2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)*.\
    \ IEEE, 2020, pp. 1172–1177.\n- [21] D. 3GPP, \"Study on new radio access technology\
    \ physical layer aspects,\" *Technical Report (TR) 38.802, V14. 2.0*, 2017."
- title: 'Integration of TinyML and LargeML: A Survey of 6G and Beyond'
  abstract: "The transition from 5G networks to 6G highlights a significant demand\
    \ for\nmachine learning (ML). Deep learning models, in particular, have seen wide\n\
    application in mobile networking and communications to support advanced\nservices\
    \ in emerging wireless environments, such as smart healthcare, smart\ngrids, autonomous\
    \ vehicles, aerial platforms, digital twins, and the metaverse.\nThe rapid expansion\
    \ of Internet-of-Things (IoT) devices, many with limited\ncomputational capabilities,\
    \ has accelerated the development of tiny machine\nlearning (TinyML) and resource-efficient\
    \ ML approaches for cost-effective\nservices. However, the deployment of large-scale\
    \ machine learning (LargeML)\nsolutions require major computing resources and\
    \ complex management strategies\nto support extensive IoT services and ML-generated\
    \ content applications.\nConsequently, the integration of TinyML and LargeML is\
    \ projected as a promising\napproach for future seamless connectivity and efficient\
    \ resource management.\n  Although the integration of TinyML and LargeML shows\
    \ abundant potential,\nseveral challenges persist, including performance optimization,\
    \ practical\ndeployment strategies, effective resource management, and security\n\
    considerations. In this survey, we review and analyze the latest research aimed\n\
    at enabling the integration of TinyML and LargeML models for the realization of\n\
    smart services and applications in future 6G networks and beyond. The paper\n\
    concludes by outlining critical challenges and identifying future research\ndirections\
    \ for the holistic integration of TinyML and LargeML in\nnext-generation wireless\
    \ networks."
  url: http://arxiv.org/abs/2505.15854v1
  keywords: 6G, artificial intelligence (AI), deep learning (DL), distributed learning,
    federated learning (FL), Internetof-Things (IoT), large machine learning (LargeML),
    machine learning (ML), tiny machine learning (TinyML).
  document: "#### I. INTRODUCTION\n\nThe evolution of communication technologies has\
    \ been characterized by extraordinary advancements every decade. From the first-generation\
    \ (1G) analog systems, which introduced mobile voice communications, to fifth-generation\
    \ (5G) networks currently transforming industries with ultra-fast speeds and low\
    \ latency, each generation has delivered major transformative impacts [ [1\\]](#page-26-0).\
    \ Looking ahead, sixth-generation\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_9.jpeg)\n\
    \nFig. 1: Transition from 5G to 6G and beyond (B6G): From target throughputs to\
    \ use cases, core technologies, and emerging applications.\n\n(6G) networks are\
    \ expected to push technological boundaries further, offering unprecedented capabilities\
    \ and driving global innovation. The core features of 6G will clearly differentiate\
    \ it from previous generations (Fig. [1\\)](#page-0-0), positioning the paradigm\
    \ as a transformative technology for diverse applications. These features address\
    \ the escalating demands for higher data rates, superior reliability, efficient\
    \ spectrum management, and superintelligence capabilities. Furthermore, 6G aims\
    \ to integrate advanced artificial intelligence (AI) and ML technologies seamlessly,\
    \ expanding the possibilities of wireless networks and significantly improving\
    \ the delivery of high-end applications and services to users.\n\nThe integration\
    \ of AI and ML into 6G will enable new levels of automation and intelligence,\
    \ unlocking numerous new applications and high-end services [ [2\\]](#page-26-1).\
    \ AI already operates on multiple network layers to improve dynamic resource management,\
    \ predict maintenance needs, and personalize user experiences. Advances in ML,\
    \ particularly deep learning (DL) architectures that execute computer vision and\
    \ natural language processing (NLP) tasks [\\[3\\]](#page-26-2), are expected\
    \ to support high-fidelity holographic communications. This capability will allow\
    \ users to interact in immersive three-dimensional environments and could revolutionize\
    \ remote collaboration. In a hyper-connected future, 6G will unlock the full potential\
    \ of Internet-of-Things (IoT) and mobile big data to drive innovation and lead\
    \ the world into smarter, deeper connectivity.\n\n#### *A. Context and Motivation*\n\
    \nIn the last decade, AI has converged around two contrasting approaches: TinyML\
    \ and Large ML (LargeML). TinyML focuses on creating efficient models for resource-constrained\
    \ edge devices, allowing real-time analytics close to data sources (i.e., bringing\
    \ analytics closer to data sources) [ [4\\]](#page-26-3). In contrast,\n\nThai-Hoc\
    \ Vu and Sunghwan Kim (corresponding author) are with the School of Electronic\
    \ Engineering, Kyonggi University, Republic of Korea (email: vuthaihoc1995@gmail.com,\
    \ skim@kyonggi.ac.kr). Ngo Hoang Tu is with the Department of Electrical and Information\
    \ Engineering, Seoul National University of Science and Technology, Seoul 01811,\
    \ South Korea (email: ngohoangtu@seoultech.ac.kr). Thien Huynh-The is with the\
    \ Department of Computer and Communications Engineering, Ho Chi Minh City University\
    \ of Technology and Education, Vietnam (e-mail: thienht@hcmute.edu.vn). Kyungchun\
    \ Lee is with the Department of Electrical and Information Engineering and the\
    \ Research Center for Electrical and Information Technology, Seoul National University\
    \ of Science and Technology, Seoul 01811, South Korea (e-mail: kclee@seoultech.ac.kr).\
    \ Miroslav Voznak is with the Faculty of Electrical Engineering and Computer Science,\
    \ VSB-Technical University of Ostrava, 17. Listopadu 2172/15, 708 00, Ostrava,\
    \ Czechia (e-mail: miroslav.voznak@vsb.cz). Quoc-Viet Pham is with the School\
    \ of Computer Science and Statistics, Trinity College Dublin, Dublin 2, D02 PN40,\
    \ Ireland (e-mail: viet.pham@tcd.ie).\n\nTABLE I. Comparison of TinyML and LargeML\
    \ Features.\n\n<span id=\"page-1-0\"></span>\n\n| Aspects                    \
    \                                                                            \
    \                                  | TinyML                                  \
    \                     | LargeML                                              \
    \                                           |  |  |  |\n|------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------------------|--|--|--|\n\
    | Definition                                                                 \
    \                                                              | ML on resource-constrained\
    \ devices                           | Large-scale AI models with vast numbers\
    \ of parameters and<br>large data processing capacity     |  |  |  |\n| Computational\
    \ requirements                                                               \
    \                                                | Limited power, memory, and\
    \ processing capability             | Substantial power, memory, and computational\
    \ resources                                          |  |  |  |\n| Deployment\
    \ environment                                                                \
    \                                                   | Smart sensors, edge devices,\
    \ and microcontrollers            | Cloud servers and specialized hardware (e.g.,\
    \ GPUs, TPUs)                                       |  |  |  |\n| Latency    \
    \                                                                            \
    \                                                  | Ultra-low latency for real-time\
    \ processing                   | Higher latency, acceptable for batch processing\
    \                                                 |  |  |  |\n| Power consumption\
    \                                                                            \
    \                                            | Very low, suitable for battery-powered\
    \ devices               | High, requiring a robust power supply              \
    \                                             |  |  |  |\n| Data processing  \
    \                                                                            \
    \                                            | Local (on-device) processing  \
    \                               | Centralized (large-scale) data processing  \
    \                                                     |  |  |  |\n| Real-time\
    \ analytics and sensor data processing in IoT<br>Typical applications<br>environments\
    \ (e.g., smart homes, industrial monitoring) |                               \
    \                               | Complex data analysis and decision-making (e.g.,\
    \ NLP,<br>image recognition, autonomous systems) |  |  |  |\n| Advantages    \
    \                                                                            \
    \                                               | Low latency, reduced data transmission,\
    \ enhanced pri<br>vacy | High accuracy, with the ability to process and analyze\
    \ large<br>datasets                        |  |  |  |\n| Challenges          \
    \                                                                            \
    \                                         | Constrained by hardware limitations\
    \ and task complexity      | Requires extensive computational infrastructure and\
    \ incurs<br>high costs                        |  |  |  |\n| Complementary roles\
    \                                                                            \
    \                                          | Supports real-time, lightweight,\
    \ and edge-based tasks        | Enables deep, comprehensive analysis and cloud-based<br>decision-making\
    \                         |  |  |  |\n| Examples                             \
    \                                                                            \
    \                        | Keyword spotting, anomaly detection in sensor data\
    \           | GPT-4, BERT, large-scale image classifiers                     \
    \                                 |  |  |  |\n\nLargeML leverages powerful cloud\
    \ servers to handle complex, data-intensive tasks, exemplified by models like\
    \ GPT-4 and BERT, which excel in applications such as sentiment analysis and creative\
    \ text generation [\\[5\\]](#page-26-4) (e.g., Large Language Models (LLMs), generative\
    \ AI (GenAI), and Agentic AI).\n\nHowever, LargeML's reliance on significant computational\
    \ resources limits its real-time processing abilities on constrained devices and\
    \ raises privacy concerns. TinyML mitigates these issues by pre-processing data\
    \ at the edge, while LargeML can deploy compressed models on edge devices through\
    \ knowledge distillation [\\[6\\]](#page-26-5). This collaborative learning strategy\
    \ permits more flexible deployment and operation of IoT applications and services.\
    \ TinyML benefits from the sophisticated learning capabilities of LargeML to improve\
    \ accuracy, whereas LargeML gains from the real-time processing potential and\
    \ privacy-preserving features of TinyML. Table [I](#page-1-0) presents a concise\
    \ comparison of TinyML and LargeML.\n\nThe integration of TinyML and LargeML addresses\
    \ these individual limitations by combining their strengths, enhancing AI capabilities\
    \ in 6G and future networks. For instance, wearable health monitors can use TinyML\
    \ for real-time vital sign analysis [\\[7\\]](#page-26-6), while LargeML can detect\
    \ medical risks using historical data. Similarly, in virtual environments, TinyML\
    \ can process wearable Electroencephalography (EEG) data to gauge emotional states\
    \ [\\[8\\]](#page-26-7), with LargeML translating this into actions within the\
    \ metaverse [\\[9\\]](#page-26-8).\n\nDespite these advantages, the integration\
    \ faces challenges, including increased security risks from numerous connected\
    \ devices and ethical concerns regarding AI explainability and data privacy. In\
    \ summary, leveraging the unique strengths of TinyML and LargeML can lead to seamless\
    \ device-to-server interactions and drive advancements in data-driven innovation\
    \ and intelligent living environments.\n\n#### *B. State-of-the-Art and Contributions*\n\
    \nThe AI ecosystem is evolving rapidly, driven in part by two distinct yet complementary\
    \ paradigms: TinyML and LargeML. TinyML enables resource-limited edge devices\
    \ to perform ondevice ML tasks with local data, whereas LargeML employs powerful\
    \ server infrastructures to train complex models on vast datasets. As the IoT\
    \ continues to evolve and the advent of 6G approaches, the collaborative potential\
    \ of these two different learning strategies presents a viable path on which to\
    \ increase the performance of IoT applications and 6G network operations, especially\
    \ under challenging conditions. This section reviews the recent survey literature\
    \ on TinyML and LargeML systems, highlighting their contributions and limitations\
    \ in the context of 6G and its envisioned applications.\n\nOver the past decade,\
    \ numerous survey works have examined the role of TinyML in edge computing. Key\
    \ contributions [\\[10\\]](#page-26-9)–[\\[15\\]](#page-26-10) provide comprehensive\
    \ overviews of TinyML, covering its fundamentals, development tools, applications,\
    \ and future directions. These surveys highlight both the transformative potential\
    \ and widespread use of TinyML in domains such as industrial IoT, smart healthcare,\
    \ autonomous driving, environmental monitoring, public safety, human-machine interaction,\
    \ agriculture, and emergency response. A significant contribution noted in [\\\
    [10\\]](#page-26-9) is the concept of hardware-software cooperative design for\
    \ efficient TinyML implementation. This survey highlights the necessity of optimizing\
    \ both hardware and software components concurrently for efficient TinyML execution\
    \ on resource-constrained devices. Coordinated design enhances model efficiency\
    \ while minimizing power consumption and computational overhead. Similarly, the\
    \ survey in [\\[12\\]](#page-26-11) offers a systematic overview of existing TinyML\
    \ development tools, including hardware platforms, software frameworks, and supporting\
    \ libraries. These resources assist AI developers and specialists in designing\
    \ and deploying TinyML models at the network edge. Other surveys [\\[11\\]](#page-26-12),\
    \ [\\[14\\]](#page-26-13), [\\[15\\]](#page-26-10) explore a range of TinyML applications\
    \ across domains such as anomaly detection, predictive maintenance, and intelligent\
    \ sensor networks. For example, anomaly detection allows TinyML models to continuously\
    \ monitor sensor data and identify deviations from normal operating behavior,\
    \ supporting timely maintenance and minimizing unplanned downtime. In predictive\
    \ maintenance, TinyML models forecast potential equipment failures, improving\
    \ resource allocation and operational efficiency through proactive maintenance\
    \ strategies. Intelligent sensor networks use TinyML to process data locally at\
    \ the edge, enabling quicker decision-making and near real-time responsiveness.\
    \ These applications illustrate the broad potential of TinyMl to transform sectors\
    \ such as industrial automation, environmental monitoring, and smart infrastructure.\n\
    \nDespite these advances, the existing survey literature on TinyML reveals some\
    \ limitations when considered in the\n\nTABLE II. Summary of related surveys on\
    \ TinyML and LargeML.\n\n|              | Review Topics |         |          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                               |                                            \
    \                                                                            \
    \                     |\n|--------------|---------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | Refs.        | TinyML        | LargeML | Key Contributions                 \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \      | Limitations                                                         \
    \                                                                        |\n|\
    \ [10]         | ✓             |         | Highlights hardware-software co-design\
    \ for optimizing TinyML<br>frameworks on resource-limited devices and reviews\
    \ ML algorithms<br>optimized for low-power deployment.                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \  | Concentrates on TinyML and 5G, without dis<br>cussion of integration with\
    \ LargeML in 6G<br>systems.                                         |\n| [11]\
    \         | ✓             |         | Introduces a classification system for TinyML\
    \ applications based on<br>an extensive literature review, exploring benefits\
    \ and use cases.                                                             \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                       | Does\
    \ not address TinyML–LargeML integration<br>or the specific challenges of 6G network\
    \ inte<br>gration.                                    |\n| [12]         | ✓  \
    \           |         | Surveys TinyML frameworks, development tools, enablers,\
    \ and<br>applications and outlines current research challenges.              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                             | Lacks discussion\
    \ of TinyML–LargeML integra<br>tion in the context of 6G networks.           \
    \                                                |\n| [13]         | ✓       \
    \      |         | Proposes a taxonomy for reformable TinyML solutions and evaluates<br>the\
    \ suitability of TinyML layers for reformability.                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                            | Does not explore TinyML–LargeML\
    \ integration<br>in the context of 6G networks.                              \
    \                                 |\n| [14]         | ✓             |        \
    \ | Reviews neural network (NN) compression techniques and catego<br>rizes TinyML\
    \ hardware and software resources.                                           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                       | Examines the foundational aspects of\
    \ TinyML,<br>with limited discussion on integration with<br>LargeML or implications\
    \ for 6G environments. |\n| [15]         | ✓             |         | Reviews TinyML\
    \ research, workflows, algorithms, and development<br>tools, with an emphasis\
    \ on recent ML advancements.                                                 \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                          | Lacks analysis of integration with LargeML in<br>the\
    \ context of 6G networks.                                                    \
    \            |\n| [16]         |               | ✓       | Reviews transformer\
    \ architectures in LLMs, discussing training<br>methods, applications, societal\
    \ impact, and deployment challenges.                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                   | Does<br>not<br>address<br>6G-related<br>challenges<br>or<br>TinyML–LargeML\
    \ integration for enhancing 6G<br>capabilities.                    |\n| [17] \
    \        |               | ✓       | Examines LLM evaluation methods, benchmarks,\
    \ and performance<br>across domains.                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                        | Omits\
    \ discussion of TinyML–LargeML integra<br>tion and analysis of challenges in enhancing\
    \ 6G<br>applications.                              |\n| [18]         |       \
    \        | ✓       | Analyzes fine-tuning algorithms for LargeML, detailing performance<br>metrics\
    \ and deployment scenarios.                                                  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                       | Does not investigate the integration\
    \ of fine<br>tuning algorithms with TinyML for 6G appli<br>cations.          \
    \                            |\n| [19]         |               | ✓       | Proposes\
    \ a taxonomy of explainability techniques for LLMs, with<br>evaluation metrics\
    \ and implementation challenges.                                             \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                               | Concentrates on LLM explainability without ad<br>dressing\
    \ its integration with TinyML in collab<br>orative 6G systems.               \
    \       |\n| [20]         |               | ✓       | Outlines architectures,\
    \ techniques, and challenges in pre-trained<br>language models (PLMs).       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                 | Concentrates on natural language processing<br>(NLP) applications\
    \ of PLMs; lacks discussion<br>of TinyML integration in 6G systems.         |\n\
    | This<br>work | ✓             | ✓       | Offers the first survey of the integration\
    \ of TinyML and LargeML<br>for 6G systems. Specifically:<br>• Introduces fundamentals\
    \ of TinyML and LargeML.<br>• Discusses<br>motivations<br>and<br>requirements<br>for<br>integrating<br>TinyML\
    \ and LargeML in 6G.<br>• Proposes bidirectional integration frameworks for enhancing<br>future\
    \ network performance and capabilities.<br>• Explores<br>potential<br>applications<br>of<br>integrated<br>TinyML–<br>LargeML\
    \ systems.<br>• Identifies<br>challenges<br>and<br>future<br>research<br>directions<br>for<br>TinyML–LargeML\
    \ integration. |                                                             \
    \                                                                            \
    \    |\n\ncontext of 6G integration. Although these surveys examine TinyML applications\
    \ within general IoT frameworks, they overlook the specific challenges and opportunities\
    \ presented by integrating TinyML with LargeML in 6G networks [\\[11\\]](#page-26-12).\
    \ Some surveys [\\[14\\]](#page-26-13), [\\[15\\]](#page-26-10) provide limited\
    \ clarification on finetuning TinyML models to the specific resource constraints\
    \ of 6G environments. For example, 6G may require the development of lightweight\
    \ communication protocols tailored to TinyML models for efficient data exchange\
    \ with server-side LargeML systems. These surveys also fall short of exploring\
    \ optimization strategies for minimizing power consumption in TinyML models while\
    \ preserving the accuracy required for effective collaboration with LargeML [\\\
    [15\\]](#page-26-10).\n\nIn contrast, many studies on LargeML outline distinct\
    \ contributions and limitations. Surveys such as [\\[16\\]](#page-26-15)–[\\[20\\\
    ]](#page-26-19), explore the architectures, applications, evaluation methods,\
    \ and research challenges associated with LargeML. These works offer a clear view\
    \ of the current state of the LargeML field. For example, the work in [\\[16\\\
    ]](#page-26-15) presents a detailed analysis of transformer-based architectures,\
    \ from foundational models to advanced, with a focus on LLMs. It traces the development\
    \ of of LLMs, discussing training methods, application domains, and real-world\
    \ deployments. Similarly, the survey in [\\[20\\]](#page-26-19) explores the impact\
    \ of large pre-trained language models (PLMs) on a range of NLP tasks. These surveys\
    \ highlight the strong performance of LargeML in complex tasks such as text generation,\
    \ machine translation, and sentiment analysis. Several reviews [\\[17\\]](#page-26-16)–[\\\
    [19\\]](#page-26-18) examine key challenges associated with LargeML. These include\
    \ explainability, where understanding model decision-making processes remains\
    \ critical for real-world deployment. Notably, [\\[18\\]](#page-26-17) emphasizes\
    \ the need for parameter-efficient fine-tuning (PEFT) techniques to reduce computational\
    \ overhead, facilitating deployment in resourceconstrained environments. Addressing\
    \ these challenges is essential for the responsible and effective use of LargeML\
    \ across diverse applications. However, as with the surveys on TinyML, current\
    \ LargeML reviews exhibit limitations when considered in the context of 6G applications.\
    \ Although the computational demands of LargeML are acknowledged, the specific\
    \ challenges of deployment and and coordination with LargeML within 6G networks\
    \ are not examined. Given 6G's focus on real-time processing and edge intelligence,\
    \ it becomes necessary to investigate how LargeML models can be compressed or\
    \ adapted for efficient operation at the network edge, potentially in conjunction\
    \ with TinyML systems. Furthermore, the surveys overlook the impact of 6G-specific\
    \ features such as novel communication protocols and strict latency constraints\
    \ on the training and deployment of LargeML models.\n\nThe preceding discussion\
    \ highlights a significant gap in comprehensive surveys that examine the integration\
    \ of TinyML and LargeML in the context of 6G and beyond. Although recent progress\
    \ in LargeML integration [\\[21\\]](#page-26-20), [\\[22\\]](#page-26-21) is critical\
    \ for developing efficient and intelligent 6G networks, these advancements have\
    \ not been thoroughly examined. Existing studies focus on isolated aspects of\
    \ either TinyML or LargeML [\\[23\\]](#page-26-22), failing to provide a cohesive\
    \ analysis of their combined potential to address emerging network challenges.\
    \ The present survey aims to fill this gap by presenting a comprehensive survey\
    \ on the integration of TinyML and LargeML for 6G and future network paradigms.\
    \ A key contribution is a detailed examination of the motivations and requirements\
    \ behind such integration. We begin by reviewing the foundations and recent advancements\
    \ in both TinyML and LargeML, highlighting their respective strengths. We then\
    \ propose efficient bidirectional integration strategies and asses their relevance\
    \ to future network architectures. In addition, we examine the realworld applications\
    \ of integrated TinyML–LargeML systems in areas such as smart healthcare, autonomous\
    \ vehicles, and industrial IoT. These case studies illustrate the potential to\
    \ improve communication efficiency, optimize resource use, and scale systems systems\
    \ effectively. Finally, we identify major unresolved challenges in the literature\
    \ and propose research directions to guide future work. The features and contributions\
    \ of this work are summarized below:\n\n- Background: This work first provides\
    \ the background of TinyML and LargeML, accordingly highlighting their individual\
    \ advancements and potential cooperation.\n- Integration requirements: We outline\
    \ the motivations and requirements for integrating TinyML and LargeML to emphasize\
    \ their importance for 6G networks.\n- Efficient solutions: We propose and discuss\
    \ efficient bidirectional integration solutions, aiming to enhance the performance\
    \ and capabilities of future networks.\n- Applications of integrated systems:\
    \ We conduct an extensive review of the applications of integrated TinyML and\
    \ LargeML systems across various domains to demonstrate their practical benefits\
    \ and potential impacts.\n- Discussion of challenges and future research: From\
    \ our extensive review, we identify critical challenges and propose several potential\
    \ directions for future research, including resource allocation, communication\
    \ efficiency, interplay with LargeML, and standardizations.\n\n#### *C. Paper\
    \ Organization*\n\nThe remainder of the paper is structured as follows. Section\
    \ [II](#page-3-0) provides an overview of TinyML and LargeML, outlining their\
    \ key concepts and recent developments Section [III](#page-8-0) examines the motivations\
    \ and requirements for integrating TinyML and LargeML in the context of 6G networks.\
    \ Section [IV](#page-12-0) presents a comprehensive review of efficient, bidirectional\
    \ integration approaches for 6G and beyond, such as transfer learning (TL), federated\
    \ transfer learning (FTL), split learning (SL), and federated split learning (FSL).\
    \ Section [V](#page-21-0) explores the applications of integrated TinyML and LargeML\
    \ systems in domains such as data privacy and network security, network management,\
    \ zero-touch networks, and the brainlevel metaverse. In Section [VI,](#page-24-0)\
    \ we summarize insights from earlier sections and discuss existing challenges\
    \ and potential directions for future research before ending with Section [VII.](#page-26-23)\n\
    \n#### II. OVERVIEW OF TINYML AND LARGEML\n\n#### <span id=\"page-3-0\"></span>*A.\
    \ TinyML*\n\nTinyML refers to a lightweight, resource-constrained class of ML\
    \ designed for environments with limited memory and processing capabilities [\\\
    [4\\]](#page-26-3). Despite its compact form, TinyML performs effectively in a\
    \ wide range of communication tasks; Table [III](#page-4-0) details its characteristics.\
    \ According to [\\[25\\]](#page-26-24), the TinyML life cycle consists of three\
    \ stages:\n\n- Stage 1: Data collection from peripheral devices, such as sensors\
    \ or production environments (real or simulated).\n- Stage 2: Model deployment\
    \ using quantization and compression techniques to address hardware heterogeneity\
    \ and resource constraints.\n- Stage 3: Model operationalization by converting\
    \ the trained model into a generic, interpretable form suitable for target devices.\n\
    \nTinyML relies on application-specific data formats, prioritizing model customization\
    \ for limited device capacity and efficient data transmission. The next sections\
    \ detail these design strategies and operational considerations (Fig. [2](#page-4-1)\
    \ for short overview).\n\n<span id=\"page-3-1\"></span>*1) TinyML Design:* Although\
    \ no standard guidelines for TinyML design currently exist, several best practices\
    \ are commonly adopted to guide development.\n\nQuantization: Quantization reduces\
    \ the model's memory footprint and accelerates computation by representing parameters\
    \ with lower precision numerical formats (e.g., converting 32-bit floating point\
    \ values into 8-bit integers) [\\[26\\]](#page-26-25). Quantization can be implemented\
    \ during training (quantization-aware training) or after training (post-training\
    \ quantization). Variants include uniform and non-uniform quantization for optimizing\
    \ accuracy, symmetric or asymmetric schemes to reduce quantization error, and\
    \ Bayesian inference for minimizing bit encoding requirements.\n\nPruning: Pruning\
    \ reduces NN size by removing unimportant parameters, such as weights, neurons,\
    \ or layers, that contribute little to model performance [\\[27\\]](#page-26-26).\
    \ Pruned models typically require fine-tuning to restore accuracy. Common strategies\
    \ include threshold-based pruning, in which weights below a specified magnitude\
    \ are discarded. Depending on the differences in the model weight matrix, pruning\
    \ is classified as unstructured (removal of individual weights) or structured\
    \ (removal of neurons, filters, rows, or columns), guided by redundancy or regularization\
    \ metrics.\n\nLow-rank matrix decomposition: This technique applies linear algebra\
    \ methods, such as singular value decomposition or its extensions to tensors,\
    \ to approximate large weight\n\n<span id=\"page-4-1\"></span>![](_page_4_Picture_0.jpeg)\n\
    \nFig. 2: Overview of TinyML and LargeML: life cycle, design, operation, and performance\
    \ evaluation.\n\n|  |  |  |  |  | TABLE III. TinyML classifications, hardware\
    \ devices, environmental deployment, and software tools. |  |  |  |  |\n|--|--|--|--|--|----------------------------------------------------------------------------------------------------|--|--|--|--|\n\
    |--|--|--|--|--|----------------------------------------------------------------------------------------------------|--|--|--|--|\n\
    \n<span id=\"page-4-0\"></span>\n\n| Classification                          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                 | Hardware Device\
    \ [15]                                                                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                     | Environmental Deployment [11]                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                | Software Tools [13]                                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                        |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\
    | • Model size: (1) Compression techniques<br>(pruning, quantization, knowledge\
    \ dis<br>tillation), (2) Memory and storage de<br>sign (kilobytes to megabytes),\
    \ (3) Oper<br>ational modes (centralized, distributed,<br>decentralized).<br>•\
    \ Architecture: (1) Convolutional neural<br>network (CNN) for image processing,<br>(2)\
    \ Recurrent neural network (RNN) for<br>sequential data, (3) Fully connected net<br>works\
    \ for certain tasks, (4) Transformer<br>[24] for data generation, context aware<br>ness,\
    \ translation, and summarization.<br>• Training data: (1) Quality assurance<br>and\
    \ data labeling, (2) Data augmenta<br>tion techniques (rotation, scaling, flip<br>ping),\
    \ (3) Fine-tuning pre-trained mod<br>els on large datasets.<br>• Algorithms: Ten\
    \ types [11]: super<br>vised, weakly supervised, unsupervised,<br>self-supervised,\
    \ meta-learning, contin<br>ual learning, deep reinforcement learn<br>ing (DRL),\
    \ TL, SL, and federated learn<br>ing (FL). | • Central processing unit (CPU):<br>Arm\
    \ Cortex M family offers ultra<br>low-power operation but limited<br>support for\
    \ parallel processing.<br>• Graphics processing unit (GPU):<br>Supports parallel\
    \ computation and<br>optimized sequential processing of<br>large datasets (e.g.\
    \ NVIDIA Jetson<br>family, AMD, Intel, Arm), but is<br>not cost-effective.<br>•\
    \ Field-programmable<br>gate<br>ar<br>ray (FPGA): Enables model cus<br>tomization\
    \ and hardware-level ac<br>celeration for complex operations<br>and large dataset\
    \ access, but lacks<br>broad library support.<br>• Tensor processing unit (TPU):<br>Accelerates\
    \ ML workloads, partic<br>ularly those involving NNs, with<br>two representations\
    \ of Edge TPUs<br>and Cloud TPUs. | • Atmospheric:<br>Low-power<br>sen<br>sors\
    \ to monitor air quality in ur<br>ban, forest, and industrial areas,<br>detecting\
    \ pollutants by measuring<br>particulate matter, carbon dioxide<br>levels, and\
    \ others.<br>• Hydrosphere: Sensors in aquatic<br>environments to detect pollutants,<br>measure\
    \ pH, and monitor temper<br>ature changes.<br>• Biosphere:<br>Device<br>used<br>for<br>wildlife\
    \ conservation, monitoring<br>biological activity, or classifying<br>sounds (e.g.,\
    \ animal habitats and<br>anti-poaching surveillance).<br>• Lithosphere: Applied\
    \ in agricul<br>ture for environmental monitoring<br>(e.g., soil moisture, nutrient\
    \ lev<br>els, pests) or earthquake detection.<br>• Human-related:<br>Wearables<br>for<br>health<br>monitoring<br>(e.g.,<br>blood<br>pressure)\
    \ and assistive technolo<br>gies for humans (e.g., mobility). | • Mobile<br>device:<br>TensorFlow<br>Lite\
    \ for model optimization<br>and PyTorch for programming.<br>• Embedded<br>devices:\
    \ Tensor<br>Flow Lite for model optimiza<br>tion,<br>PyTorch<br>for<br>program<br>ming,\
    \ and Qeexo AutoML for<br>end-to-end customization.<br>• Edge<br>devices:<br>PyTorch<br>for<br>programming,\
    \ NanoEdgeAIS<br>tudio for low/no-code solution,<br>and Imagimob/Edge Impulse<br>for\
    \ end-to-end customization.<br>• Microcontroller:<br>Arm<br>NN<br>for<br>network<br>architecture<br>optimization,<br>MinUn<br>for<br>bandwidth/memory<br>management,<br>STM32CubeA<br>and<br>uTensor<br>for<br>pre-trained<br>model<br>conversion,<br>and<br>NXP\
    \ eIQ as a development<br>environment. |\n\nmatrices with lower-rank components.\
    \ The resulting factorized forms contain fewer parameters, reduce computational\
    \ complexity, and simplify matrix operations [\\[28\\]](#page-26-28). Despite\
    \ its benefits, this technique can be computationally intensive and more difficult\
    \ to implement, as the decomposed network requires additional fine-tuning to recover\
    \ accuracy loss, similar to model pruning.\n\nWeight sharing: Weight sharing reduces\
    \ the number of unique parameters for storage and computation by reusing weights\
    \ across different parts of the model [\\[29\\]](#page-26-29). For example, shared\
    \ weights can be arranged by row or column within a weight matrix. However, designing\
    \ effective sharing configurations manually is difficult and can lead to unpredictable\
    \ performance, despite the inherent redundancy in DL models.\n\nAlgorithm-architecture\
    \ co-design: Integrating algorithm development with specialized hardware accelerators,\
    \ such as TPUs or custom-designed chips (e.g., Cortex-M7, Cortex-M4, Cortex-M0+,\
    \ and eDMPv1), can significantly improve the inference efficiency in TinyML models\
    \ [\\[30\\]](#page-26-30). This joint optimization of algorithms and hardware\
    \ also enhances computational performance and energy efficiency.\n\nHuffman coding:\
    \ Huffman coding assigns variable-length codes to symbols based on their frequencies,\
    \ with more frequent symbols receiving shorter codes [\\[31\\]](#page-26-31).\
    \ When applied to model weights and parameters, this technique can significantly\
    \ reduce overall model size through efficient encoding.\n\nHyperparameter optimization:\
    \ This refers to the process of selecting the optimal configuration of model parameters,\
    \ such as learning rate, batch size, activation functions, number of neurons,\
    \ and network depth [\\[32\\]](#page-26-32). Common optimization methods include\
    \ grid search, random search, Bayesian optimization, Hyperband (which dynamically\
    \ allocates resources to promising configurations while terminating underperforming\
    \ configurations early), genetic algorithms, tree-structured Parzen estimators,\
    \ and hyperdimensional computing. These techniques aim to improve model performance\
    \ by minimizing memory and computational overhead while ensuring userdefined accuracy.\n\
    \nKnowledge distillation: Knowledge distillation involves transferring learned\
    \ representations from a larger model (the *teacher*) to a smaller model (the\
    \ *student*) to produce a compact model with comparable performance. Three common\
    \ types of distillation techniques are (1) response-based, where the student is\
    \ trained to mimic the teacher's final output layer, (2) feature-based, where\
    \ the student replicates intermediate feature representations from the teacher,\
    \ and (3) relation-based, where the student learns the structural relationships\
    \ between different components of the teacher model [\\[33\\]](#page-26-33).\n\
    \n*2) TinyML Operation:* From an operational standpoint, TinyML can be categorized\
    \ into three main levels: firmware updates, model operation, and communication\
    \ protocols [\\[34\\]](#page-26-34).\n\nFirmware updates: This involves replacing\
    \ the entire software stack on a device, including the TinyML model and underlying\
    \ system components. Although comprehensive, this process is resource-intensive\
    \ and time-consuming. Firmware updates are typically delivered over-the-air (OTA)\
    \ using lightweight machine-to-machine (M2M) protocols, which support both push\
    \ and pull modes for firmware transfer [\\[35\\]](#page-26-35). Tools such as\
    \ RIOT-ML facilitate secure OTA updates, enabling remote, unattended deployment.\n\
    \nModel operations: LargeML deployment involves multiple strategies to optimize\
    \ model updates and execution on resource-constrained devices [\\[34\\]](#page-26-34).\
    \ Full model updates replace the entire ML model via OTA delivery, ideal for significant\
    \ architecture or parameter changes, ensuring devices run the latest version.\
    \ Partial model updates target specific layers or parameters, minimizing network\
    \ traffic and power use, making them suitable for fine-tuning on constrained devices\
    \ [\\[36\\]](#page-26-36). Model transpilers convert models between frameworks,\
    \ like TensorFlow to TensorFlow Lite Micro, enhancing compatibility and resource\
    \ efficiency across diverse hardware [\\[37\\]](#page-26-37). Model compilers\
    \ like uTensor and TinyTS translate models into optimized binary code for microcontrollers,\
    \ using quantization and pruning to reduce size and boost inference efficiency,\
    \ fitting within tight memory and computational limits.\n\nCommunication protocols:\
    \ TinyML systems leverage specialized communication protocols to optimize data\
    \ transfer under power, bandwidth, and range constraints in IoT ecosystems [\\\
    [15\\]](#page-26-10). For intra-device communication, protocols like interintegrated\
    \ circuit and serial peripheral interface enable efficient data exchange between\
    \ microcontrollers and peripherals during local collection. Device-to-cloud communication\
    \ often employs message queuing telemetry transport, a lightweight protocol ideal\
    \ for low-bandwidth, high-latency environments. Short-range wireless needs are\
    \ met by low-power wide-area network (LoWPAN) protocols such as Bluetooth, Zigbee,\
    \ or 6LoWPAN for wearables and smart home sensors, while Wi-Fi supports higher\
    \ data volumes in smart cameras or environmental monitors. For long-range, low-power\
    \ communication, long-range wide area network excels in remote applications like\
    \ agriculture and smart cities, offering kilometerscale coverage with minimal\
    \ energy use. Sigfox, designed for ultra-narrowband IoT and M2M tasks, transmits\
    \ small, infrequent data packets on unlicensed bands, ensuring lowcost, secure\
    \ deployments. Similarly, Narrowband IoT, a 3GPP standard, supports low-data-rate,\
    \ energy-efficient communication in licensed/unlicensed spectra, enabling scalable,\
    \ reliable connectivity for massive IoT deployments in challenging environments\
    \ like underground or indoor settings.\n\n*3) Performance Evaluation:* Given the\
    \ diversity of application contexts and strict resource limitations, performance\
    \ evaluation of TinyML models requires the consideration of both device constraints\
    \ and learning model characteristics.\n\nMemory Footprint: TFLite Micro targets\
    \ microcontrollers\n\nwith limited ROM and RAM, using quantization and pruning\
    \ to reduce model size with minimal accuracy loss [\\[38\\]](#page-26-38). Evaluating\
    \ ROM and RAM consumption is crucial for assessing TinyML deployments on IoT devices.\n\
    \nOn/Off characteristics: TinyML components (sensors, protocols, inference engines)\
    \ switch between sleep and active power states to save energy [\\[37\\]](#page-26-37).\
    \ Optimizing the duration and frequency of these transitions minimizes active\
    \ time, enhancing energy efficiency while preserving functionality.\n\nComplexity\
    \ profiling: This refers to the analysis of the computational demands of TinyML\
    \ models. Floating-point operations are more computationally intensive than quantized\
    \ int8 operations [\\[15\\]](#page-26-10), which reduce weight and activation\
    \ precision to shrink model size and accelerate inference. Complexity profiling\
    \ helps quantify the trade-offs between model size, accuracy, and inference performance\
    \ [\\[4\\]](#page-26-3).\n\nEnergy profiling: Energy profiling measures CPU cycles,\
    \ latency, and total energy consumption during inference [\\[39\\]](#page-26-39).\
    \ Floating-point models typically consume more energy than quantized int8 models\
    \ due to the greater computational overhead. Thus, it helps identify energy-efficient\
    \ TinyML models.\n\nModel Evaluation Metrics: Several metrics are used to evaluate\
    \ TinyML model performance: Accuracy, mean absolute error (MAE), root mean square\
    \ error (RMSE), R-Squared, precision, recall, F1-score, and cross-entropy loss.\n\
    \nDeployment robustness: Deployment robustness ensures TinyML model reliability\
    \ across environmental conditions. Two evaluation approaches are used [\\[34\\\
    ]](#page-26-34): *per-model evaluation* assesses performance in controlled pre-deployment\
    \ tests, while *per-operator evaluation* monitors adaptability in realworld settings.\
    \ Robustness testing involves diverse datasets, hardware, and environments to\
    \ ensure consistent performance.\n\n## *B. LargeML*\n\nIntegrating LargeML into\
    \ wireless networks is a key trend for enabling autonomous operations, using cognitive\
    \ modules for resource allocation, channel estimation, and mobility management\
    \ [\\[5\\]](#page-26-4). The LargeML life cycle consists of three stages:\n\n\
    - Stage 1: Collect and pre-process datasets from multiple domains to construct\
    \ a high-quality training corpus.\n- Stage 2: Design model architectures by selecting\
    \ appropriate layer types and arrangements, configuring parameters, and determining\
    \ suitable training algorithms.\n- Stage 3: Train the model in two phases: (1)\
    \ Pre-train on broad datasets to build generalized representations; (2) Fine-tune\
    \ on task-specific data for applications such as question answering, summarization,\
    \ and code generation.\n- Stage 4: Evaluate performance using established benchmarks.\
    \ Refine model architecture, hyperparameters, and datasets based on results.\n\
    - Stage 5: Apply model compression techniques to reduce the memory footprint and\
    \ inference latency without sacrificing performance.\n- Stage 6: Test and optimize\
    \ the model in controlled environments to maximize inference efficiency without\
    \ degrading output quality.\n- Stage 7: Deploy in production settings. Continuously\
    \ monitor, fine-tune, and adapt the model to accommodate evolving data patterns\
    \ and user requirements.\n- Stage 8: Mitigate risks and enhance transparency by\
    \ filtering outputs through techniques such as adversarial training, fairness\
    \ constraints, and curated datasets. These steps address bias, privacy, security,\
    \ and misuse concerns.\n\n*1) LargeML Design:* The design process for LargeML\
    \ covers six key areas: data processing, learning architecture, network configuration,\
    \ pre-training tasks, fine-tuning strategies, and prompt engineering. Each area\
    \ is outlined below.\n\n*a) Data Processing:* Training LargeML relies on extensive\
    \ IoT filme data, where data quality significantly impacts prediction and decision\
    \ accuracy. Effective data processing involves key actions to ensure high-quality\
    \ outcomes.\n\nCollection and labeling: Data is gathered from heterogeneous sources\
    \ (i.e., real-time sensors, network logs, user feedback, and environmental inputs).\
    \ These data points are annotated with application-relevant features (e.g., signal\
    \ strength, noise levels, and user mobility patterns [\\[40\\]](#page-26-40)).\
    \ Consistent and accurate labeling is critical to prevent bias in state or event\
    \ label representations [\\[41\\]](#page-26-41). Well-structured datasets enable\
    \ the model to generalize across a range of operational scenarios.\n\nCleaning,\
    \ transformation, and augmentation: This phase prepares the raw data for modeling.\
    \ Cleaning involves removing duplicates, correcting errors, handling missing values,\
    \ and filtering noise. Transformation includes normalization, encoding categorical\
    \ variables (e.g., one-hot or label encoding), and feature engineering. These\
    \ transformations convert raw data into model-ready formats for analysis, such\
    \ as token-based input (for textual and code sequences), tree or graph-based structures\
    \ (to capture syntactic or hierarchical relationships), pixel-based representations\
    \ (for visual encoding of data), and hybrid formats that integrate multiple representations\
    \ [\\[17\\]](#page-26-16). Augmentation techniques, including rotation, flipping,\
    \ or noise injection, generate additional samples to enhance model robustness.\
    \ The overall process ensures a clean, consistent, and well-structured dataset\
    \ for downstream tasks.\n\nFeaturing and splitting: This phase involves feature\
    \ selection, feature extraction, and data partitioning [\\[42\\]](#page-26-42).\
    \ Feature selection identifies the variables with the greatest impact on model\
    \ performance, a process often referred to as sensitivity analysis. Feature extraction\
    \ derives new variables, for example the rate of change in wireless signal strength,\
    \ to reveal deeper patterns within the data. After the features are defined, the\
    \ dataset is partitioned into training, validation, and testing sets to evaluate\
    \ the model's performance and prevent bias. Crossvalidation, involving training\
    \ the model on different subsets of the data, is used to assess robustness and\
    \ generalizability.\n\n*b) Learning Architecture:* Future 6G networks aim to support\
    \ integrated sensing and communication (ISAC), task-oriented communications, and\
    \ digital twins, requiring LargeML models for efficient real-time data processing\
    \ [\\[5\\]](#page-26-4). Selecting suitable learning architectures is challenging\
    \ due to application-specific constraints. Transformers, leveraging attention\
    \ mechanisms, outperform RNNs in sequence data processing [\\[24\\]](#page-26-27).\
    \ They use an encoder-decoder structure with stacked sub-layers: encoders create\
    \ context-aware representations, positional embeddings preserve token order, and\
    \ decoders apply masked or multi-head self-attention to assess token relevance.\
    \ Aside from three typical architectures (Encoder only [\\[43\\]](#page-26-43),\
    \ Encoder-Decoder [\\[24\\]](#page-26-27), and Decoder only with types of causal\
    \ decoders [\\[44\\]](#page-26-44) and decoders [\\[45\\]](#page-26-45)), below\
    \ are new variants of transformer architectures.\n\nMixture of experts (MoE):\
    \ This technique extends all three architectural types by scaling a subset of\
    \ NN weights, enabling sparse activation. Models such as Switch Transformer [\\\
    [46\\]](#page-26-46) and GLaM [\\[47\\]](#page-26-47) demonstrate MoE's ability\
    \ to increase model parameter counts without proportional increases in computational\
    \ cost. Performance improves with more experts or larger total parameter size.\
    \ However, training can become unstable due to complex routing operations. To\
    \ address this, techniques such as incorporating high-precision tensors in the\
    \ routing module, narrowing initialization ranges, or introducing resource-conditional\
    \ computation alongside traditional dataconditional computation have shown promise\
    \ [\\[48\\]](#page-26-48).\n\nEmergence: Conventional transformer architectures\
    \ may struggle with quadratic computational complexity with respect to sequence\
    \ length, resulting in high processing costs for long inputs. This limitation\
    \ has prompted research into parameterized state space architectures [\\[49\\\
    ]](#page-26-49), which integrate features of RNNs and CNNs. These models generate\
    \ outputs recursively in the same manner as RNNs, referencing only the previous\
    \ state during decoding while avoiding the need to revisit all previous states,\
    \ which is typical in conventional transformers. These models also encode entire\
    \ sequences in parallel using convolution operations, exploiting GPU efficiency\
    \ through techniques such as parallel scan, fast Fourier transform, and chunkwise\
    \ recurrent processing.\n\n*c) Network Configuration:* Network configuration directly\
    \ impacts model training and performance. Selecting the appropriate operational\
    \ strategy for the model is a critical early step. The following approaches highlight\
    \ important considerations for efficient network design [\\[50\\]](#page-26-50).\n\
    \nParallel training: This technique accelerates training through parallelism strategies.\
    \ Data parallelism synchronizes GPUs using a parameter server/all-reduce gradients.\
    \ Pipeline parallelism divides the model across devices to overlap the execution\
    \ of data processing. Model parallelism distributes layers or neurons across devices,\
    \ with each device processing parts of the model and exchanging intermediate outputs.\n\
    \nMixed precision: During model training, parameter values typically remain within\
    \ the range of 16-bit floating-point numbers. However, the product of gradients\
    \ and the learning rate during updates can exceed this range, leading to underflow\
    \ and ineffective parameter updates. To prevent this problem, optimizers store\
    \ an auxiliary set of parameters in singleprecision (32-bit). While training uses\
    \ half-precision (16-bit) for parameters and gradients, the optimizer computes\
    \ and stores updates in 32-bit before conversion into 16-bit.\n\nOffloading: Optimizer\
    \ parameters often consume more memory than model parameters, requiring offloading\
    \ from GPU to CPU. However, offloading to a single CPU reduces the training throughput.\
    \ This bottleneck is overcome by distributing the load across multiple CPUs linked\
    \ to the GPU.\n\nOverlapping: Memory operations are generally asynchronous. To\
    \ improve computational efficiency during the model's forward propagation, a two-phase\
    \ operation is applied. Memory requests (parameter retrieval) are issued first,\
    \ followed by computations (parameter processing), allowing the memory operation\
    \ to complete (new parameter collection).\n\nCheckpointing: In transformer models,\
    \ backward propagation uses a checkpoint mechanism to optimize memory by saving\
    \ only selected inputs (checkpoints) at each major layer (attention and feed-forward)\
    \ during forward propagation, rather than storing all intermediate results. During\
    \ backpropagation, gradients are computed by re-executing the forward pass for\
    \ each layer using these checkpoints, after which both checkpoints and recomputed\
    \ results are discarded, balancing memory efficiency with computational cost.\n\
    \n*d) Pre-training Tasks:* Pre-training establishes the groundwork for the model\
    \ by equipping it with essential skills and knowledge that require minimal task-specific\
    \ training, involving extensive data collection and preparation, selfsupervised\
    \ learning to acquire foundational knowledge, and fine-tuning for specific tasks.\
    \ Before deployment, LargeML undergoes pre-training on extensive corpora to learn\
    \ universal representations that encode both syntactic and semantic information.\
    \ The models are then fine-tuned for downstream tasks, facilitating knowledge\
    \ transfer to a broad range of applications. Below are common pre-training workflows\
    \ [\\[51\\]](#page-27-0).\n\nSingle modal: This method trains models on a single\
    \ data modality (e.g., text, images, audio) to build domain-specific representations.\
    \ In NLP, for example, pre-training on large text corpora enables models to capture\
    \ linguistic structures and semantics, excelling in tasks such as sentiment analysis,\
    \ speech signal classification, and machine translation.\n\nMultiple modal: This\
    \ method combines different data types (e.g., text, images) to build richer representations.\
    \ For example, training on both image captions and corresponding images enables\
    \ models to link visual and textual information, improving performance in tasks\
    \ such as image captioning, visual question answering, and cross-modal retrieval.\n\
    \nToken prediction: Token prediction tasks train models to predict the next token\
    \ in a sequence, strengthening contextual interpretation and language processing.\
    \ Masked language modeling (e.g., BERT) hides random tokens for prediction, while\
    \ autoregressive modeling (e.g., GPT) predicts the next token based on prior content,\
    \ supporting coherent and contextually accurate outputs.\n\n*e) Fine-tuning:*\
    \ After pre-training, a model is fine-tuned on a smaller, task-specific dataset\
    \ to adjusts its parameters for the target application. This process builds on\
    \ the knowledge acquired during pre-training. In addition to the TinyML strategies\
    \ discussed in Section [II-A1,](#page-3-1) this section reviews the fine-tuning\
    \ techniques relevant to LargeML systems [\\[52\\]](#page-27-1).\n\nPrompt tuning:\
    \ Prompt tuning introduces trainable tokens (prompts) to guide the model's output\
    \ without modifying its architecture or parameters.\n\nPrefix tuning: This technique\
    \ prepends a set of trainable tokens (prefixes) to the model's input and intermediate\
    \ layers. These prefixes steer the model's attention and interpretative processes\
    \ without updating the core parameters, enabling task adaptation with low computational\
    \ overhead.\n\nAdapter tuning: Adapter tuning inserts small NN modules (adapters)\
    \ into the model's architecture. During fine-tuning, only the adapters are trained\
    \ to capture task-specific information [\\[53\\]](#page-27-2), while the original\
    \ model remains unchanged.\n\nLow-rank adaptation (LoRA): LoRA enhances transformer\
    \ models by introducing trainable low-rank matrices to selected layers, keeping\
    \ original weights frozen [\\[54\\]](#page-27-3). Only these lightweight matrices\
    \ are trained to approximate weight updates, enabling efficient fine-tuning with\
    \ minimal memory and computational costs.\n\nKnowledge preservation: This technique\
    \ adapts pre-trained models to new tasks while retaining their valuable pre-existing\
    \ knowledge. It addresses *catastrophic forgetting*, using techniques such as\
    \ causal graph framing to preserve important causal effects from pre-trained data,\
    \ and PEFT techniques such as LoRA and Half Fine-Tuning. This balance ensures\
    \ that the model remains effective across diverse tasks without overfitting to\
    \ new data.\n\nTask orientation: This technique adapts pre-trained models to specific\
    \ tasks (e.g., dialogue systems) by fine-tuning the model's parameters on task-specific\
    \ data. The incorporation of techniques such as adapter layers and prefix tuning\
    \ reduces the overall parameter count for training, improving efficiency and lowering\
    \ resource demands.\n\n*f) Prompt Engineering:* After fine-tuning, prompt engineering\
    \ refines how the model interprets and responds to inputs. Prompting techniques\
    \ vary depending on task requirements and application contexts. Common types include\
    \ multi-turn instructions, multiple modeling, task planning, tool augmented alignment,\
    \ and retrieval augmentation. The most widely used prompting techniques in wireless\
    \ networks are described below [\\[5\\]](#page-26-4).\n\nFew-shot prompting: This\
    \ method enables the model to generalize from a small number of examples. It is\
    \ particularly useful when large annotated datasets are unavailable, allowing\
    \ the model to perform new tasks with minimal additional data.\n\nZero-shot prompting:\
    \ This method exploits the model's pre-trained knowledge to perform tasks without\
    \ prior examples or specific training. The model generates outputs based solely\
    \ on instructions, drawing from its existing knowledge. In contrast to few-shot\
    \ prompting, zero-shot prompting provides no examples to guide the model's output.\n\
    \nIn-context learning: This method refers to a model's ability to infer patterns\
    \ and peform tasks based solely on prompts, without modifying its internal parameters.\
    \ The user defines the task and provides examples within the prompt. The model\
    \ uses attention mechanisms to analyze these examples and deduce the task structure.\
    \ The model then applies the inferred pattern to generate outputs for new inputs.\
    \ For example, LargeML can use in-context learning for network intrusion detection,\
    \ adaptive response to threats, or for resource management, optimizing resource\
    \ allocation in real time.\n\nChain of thought: This approach improves reasoning\
    \ by breaking complex tasks into intermediate steps. It guides the model to generate\
    \ rationales for each step, processing information logically and maintaining contextual\
    \ coherence. Consequently, each decision is based on established information.\
    \ A practical application is fault diagnosis and troubleshooting, where LargeML\
    \ analyzes network issues step-bystep, accelerating resolution and improving system\
    \ resilience.\n\nPrompt-based learning: This method uses the model's pre-trained\
    \ knowledge to generate accurate, contextually appropriate responses without additional\
    \ training or parameter adjustments. Prompts focus the model's attention and shape\
    \ its output for specific tasks. For example, LargeML can analyze historical data\
    \ to predict maintenance needs and preempt outages by identifying failure patterns.\n\
    \n*2) LargeML Operation:* LargeML operates through a multistage process, beginning\
    \ with comprehensive data collection and preprocessing to ensure quality. It undergoes\
    \ initial training on advanced algorithms and high-performance hardware to identify\
    \ data patterns, followed by fine-tuning for specific tasks to enhance accuracy.\
    \ Post-deployment, LargeML processes real-time user inputs, delivering precise\
    \ responses while continuously adapting to feedback and new data, maintaining\
    \ its accuracy and relevance over time.\n\nHowever, LargeML operation varies with\
    \ the application domain and character of input data from sensors and devices\
    \ [\\[5\\]](#page-26-4). In wireless communication, LargeML is typically used\
    \ to optimize connectivity and manage network traffic across mobile networks,\
    \ Wi-Fi systems, and satellite links. In IoT contexts, LargeML analyzes data from\
    \ smart home devices, connected vehicles, and healthcare monitors to support intelligent\
    \ automation and real-time decisions. In smart homes, LargeML functions as an\
    \ AI control platform, adjusting lighting, temperature, and security systems based\
    \ on user behaviors. In connected vehicles, LargeML enables real-time navigation,\
    \ vehicle diagnostics, and autonomous driving by processing sensors and wireless\
    \ network data. In healthcare, LargeML supports remote monitoring of vital signs,\
    \ enabling prompt interventions and personalized care.\n\nLargeML can also be\
    \ deployed on edge devices and cloud platforms for efficient system management\
    \ [\\[52\\]](#page-27-1). Edge deployment reduces latency by enabling local data\
    \ processing, while cloud integration offers the scalability required to manage\
    \ large volumes of data and user interactions. Continuous monitoring and updates\
    \ based on user feedback and new data help maintain the model's accuracy and relevance.\
    \ Such integration across wireless and IoT systems enables more responsive and\
    \ efficient solutions in a range of domains. For example, LargeML can optimize\
    \ traffic flow and energy consumption in smart city infrastructure, enable predictive\
    \ maintenance through industrial IoT monitoring, and improve agricultural practices\
    \ based on real-time environmental data.\n\n*3) Performance Evaluation:* Given\
    \ its broad application scope, the performance of LargeML can be evaluated at\
    \ three levels. At the learning level, key indicators include model accuracy,\
    \ output quality, and operational efficiency. Model accuracy is measured using\
    \ metrics [\\[55\\]](#page-27-4) such as precision, recall, F1 score, and area\
    \ under the receiver operating characteristic (AUC) curve, which assesses the\
    \ model's ability to correctly classify inputs. For example, AUC measures the\
    \ model's ability to discriminate between classes. Model output quality is assessed\
    \ using metrics such as perplexity, bilingual evaluation understudy (BLEU) score,\
    \ and recalloriented understudy for gisting evaluation (ROUGE). Perplexity gauges\
    \ the model's confidence in predicting subsequent outputs. BLEU and ROUGE compare\
    \ the model's generated output to reference resources.\n\nAt the operational level,\
    \ performance is measured by efficiency metrics, such as latency, throughput,\
    \ and resource use. Latency refers to the time required for the model to generate\
    \ a response. Throughput indicates the number of requests the model can process\
    \ per unit of time. Resource use tracks the computational resources consumed to\
    \ ensure the model operates efficiently without overloading the system.\n\nAt\
    \ the service level, evaluation focuses on user experience, ethics, and accountability\
    \ [\\[17\\]](#page-26-16), [\\[19\\]](#page-26-18). User experience is assessed\
    \ by: (*i*) *User satisfaction* to capture user perceptions of model performance\
    \ via direct feedback, (*ii*) *Engagement* to measures the frequency and depth\
    \ of user interactions with the model, and (*iii*) *Retention rate* to indicate\
    \ the proportion of users who continue using the model over time. Meanwhile, ethics\
    \ and accountability metrics assess whether LargeML meets critical standards:\
    \ (*i*) *Fairness* to evaluates whether the model's predictions are fair across\
    \ different user groups, helping identify and reduce bias, (*ii*) *Transparency*\
    \ to measure how clearly users and stakeholders can understand the model's decisions,\
    \ often supported by explainability tools, and (*iii*) *Ethical compliance* to\
    \ ensures adherence to privacy, security, and accountability standards, fostering\
    \ trust in the system.\n\n## <span id=\"page-8-0\"></span>III. MOTIVATIONS AND\
    \ REQUIREMENTS FOR TINYML– LARGEML INTEGRATION IN 6G NETWORKS\n\n#### <span id=\"\
    page-8-1\"></span>*A. Challenges in TinyML*\n\n*1) Resource Constraints:* TinyML\
    \ devices, such as edge IoT nodes equipped with sensors or microcontrollers, operate\
    \ under stringent processing power, memory, and energy constraints that complicate\
    \ the deployment of complex ML models, forcing trade-offs between model accuracy\
    \ and device capabilities. The resource constraints for TinyML device vary. For\
    \ example, as noted in [\\[56\\]](#page-27-5), microcontrollers may lack DRAM\
    \ and an operating system and operate with up to 1 MB of read-only flash memory\
    \ and up to 256 KB of SRAM. The study in [\\[11\\]](#page-26-12) compares hardware\
    \ platforms that support TinyML devices, reporting flash memory capacities of\
    \ up to to 128 MB and SRAM capacities up to 64 MB. Consequently, given the absence\
    \ of sufficient CPU, GPU, or TPU resources, the deployment and training of ML\
    \ models on such devices is impractical. Larger models, although more accurate,\
    \ often exceed the memory capacities of TinyML devices, and require aggressive\
    \ model compression or simplification. In the context of 6G, such limitations\
    \ become more pronounced as the demand for real-time, data-intensive applications\
    \ increases.\n\n*2) Model Compression:* The deployment of ML models on TinyML\
    \ devices requires compression techniques such as pruning, quantization, and knowledge\
    \ distillation to reduce model size and complexity [\\[57\\]](#page-27-6). However,\
    \ these methods often degrade model accuracy and performance, introducing the\
    \ need for a careful trade-off between compression efficiency and model integrity.\
    \ For example, quantized graph optimization presents particular challenges due\
    \ to mixed-precision tensors and the absence of batch normalization layers [\\\
    [58\\]](#page-27-7). In addition, TinyML devices lack the computational capacity\
    \ to support full backpropagation. Although pruning reduces model size by eliminating\
    \ less critical parameters, it may also inadvertently remove essential parameters,\
    \ resulting in lower prediction accuracy.\n\n*3) Energy Efficiency:* Many TinyML\
    \ devices operate on battery power, requiring models with high energy efficiency\
    \ to ensure sustained operation. The design of TinyML models that minimize power\
    \ consumption while satisfying performance requirements is a challenging task,\
    \ given the computational overhead of ML inference. Poor energy efficiency can\
    \ quickly deplete power sources, shorten device lifespan, and limit deployment\
    \ in remote or inaccessible locations. For example, continuous data processing\
    \ on a battery-powered IoT sensor can rapidly exhaust the power supply if the\
    \ model lacks an energy optimization mechanism [\\[40\\]](#page-26-40). In real-time\
    \ applications, such as autonomous vehicles or industrial automation, which are\
    \ key areas in the 6G paradigm, low-latency inference is critical. Achieving fast\
    \ inference without sacrificing energy efficiency requires advanced optimization\
    \ strategies.\n\n*4) Data Preprocessing and Limited Memory Resources:* TinyML\
    \ devices face significant constraints in performing ondevice data preprocessing\
    \ due to limited computational and memory resources. Tasks such as normalization,\
    \ augmentation, and feature extraction require substantial processing power and\
    \ memory, which TinyML devices generally lack. For example, image data captured\
    \ by a camera may need extensive preprocessing before it can be used for object\
    \ recognition [\\[59\\]](#page-27-8). Reduction of the dataset to satisfy memory\
    \ constraints can also impair model robustness and reliability.\n\n*5) Mission-Critical\
    \ Reliability:* TinyML's limited numerical precision and potential for error during\
    \ circuit fabrication and wafer-level assembly raise concerns about system reliability.\
    \ These concerns are amplified in mission-critical applications. For example,\
    \ in healthcare, TinyML may be used for remote surgery, vital sign monitoring,\
    \ diagnosis, or treatment management, where any error could lead to severe or\
    \ life-threatening consequences [\\[7\\]](#page-26-6). Reliability and robustness\
    \ are therefore non-negotiable in such contexts, where device failures could directly\
    \ affect patient safety.\n\n*6) Data Privacy and Security:* TinyML frequently\
    \ processes data from sensors such as cameras and microphones, which may inadvertently\
    \ capture sensitive personal information. This raises serious privacy concerns,\
    \ especially as 6G is expected to interconnect billions of IoT devices [\\[11\\\
    ]](#page-26-12). Implementation of privacy-preserving techniques, such as differential\
    \ privacy or secure multiparty computation, is challenging on TinyML platforms\
    \ due to their limited computational capacity. Such limitations also increase\
    \ vulnerability to unauthorized manipulation or access by malicious actors [\\\
    [60\\]](#page-27-9).\n\n#### <span id=\"page-9-0\"></span>*B. Challenges in LargeML*\n\
    \n*1) High Resource Consumption:* LargeML models, which contain millions to trillions\
    \ of parameters [\\[61\\]](#page-27-10), require substantial computational resources\
    \ for both training and inference. These models typically depend on high performance\
    \ GPUs or specialized hardware such as TPUs. Training LargeML can span days or\
    \ weeks, consuming considerable energy and raising environmental concerns. Their\
    \ size, along with huge datasets, necessitates extensive memory and storage capacity.\
    \ High-capacity RAM and storage systems are essential equipment for managing weights\
    \ and activation states during both training and inference.\n\n*2) Latency Sensitivity\
    \ and Real-Time Processing:* The complexity and size of LargeML models result\
    \ in increased inference latency. In addition to execution time, these models\
    \ are often deployed on cloud servers to meet their resource demands, requiring\
    \ frequent data transfers between edge devices and the cloud. This dependence\
    \ introduces network latency, which can be inconsistent and difficult to control.\
    \ Many 6G applications demand real-time or near-real-time responsiveness, requirements\
    \ that LargeML may struggle to meet. For example, 6G real-time applications such\
    \ as augmented reality, virtual reality, and autonomous driving are highly sensitive\
    \ to latency and cannot accommodate the delay caused by both inference processing\
    \ and data transmission. [\\[62\\]](#page-27-11).\n\n*3) Deployment and Accessibility:*\
    \ The deployment of LargeML involves a series of complex tasks, including hardware\
    \ setup, software configuration, and model optimization for specific applications.\
    \ These steps require substantial technical expertise and financial investment.\
    \ Hardware, such as GPUs, TPUs, and high-capacity servers, is often prohibitively\
    \ expensive. For advanced LargeML models designed to achieve high accuracy on\
    \ complex tasks, the costs can further restrict accessibility, reproducibility,\
    \ and broader adoption, particularly among small and medium-sized enterprises\
    \ and independent researchers [\\[63\\]](#page-27-12). In many developing regions,\
    \ infrastructure to support LargeML remains inadequate, hindered by unreliable\
    \ internet access, limited data centers, and a shortage of skilled professionals.\
    \ These disparities widen the digital divide, limiting the reach of AI advances\
    \ as the 6G era approaches.\n\n*4) Scalability:* The scaling of LargeML presents\
    \ significant challenges due to high computational costs, data management complexities,\
    \ and stringent network requirements [\\[64\\]](#page-27-13). Resource demands\
    \ frequently exceed the capabilities of current infrastructure. LargeML requires\
    \ extensive data for training and continuous updates, with consequent resource-intensive,\
    \ complex data management, compounded by heterogeneous datasets sourced from a\
    \ range of domains served by 6G networks. Managing such datasets is resource-intensive,\
    \ and inefficient handling leads to data bottlenecks and degraded model performance.\
    \ Integration of heterogeneous data sources adds further complexity, requiring\
    \ extensive preprocessing and validation. As model scale increases, supporting\
    \ infrastructure must enable automation and orchestration to manage higher data\
    \ throughput and maintain low-latency communication between distributed systems.\
    \ Network constraints can become critical bottlenecks, undermining scalability\
    \ and compromising the real-time performance demanded by 6G applications.\n\n\
    *5) Interpretability and Fine-Tuning:* LargeML models often function as \"closed\
    \ boxes\", with internal decision-making processes that lack transparency. The\
    \ complexity of interactions between millions of parameters and layers obscures\
    \ how specific decisions or predictions are generated. This opacity is unacceptable\
    \ in mission-critical 6G applications such as\n\n<span id=\"page-10-0\"></span>![](_page_10_Figure_0.jpeg)\n\
    \nFig. 3: Envisaged 6G capabilities and corresponding pivotal general requirements.\n\
    \nhealthcare, autonomous vehicles, and finance, where trust and accountability\
    \ require explainable outputs. Regulatory requirements, for example the European\
    \ Union's General Data Protection Regulation [\\[65\\]](#page-27-14), explicitly\
    \ mandate the right to explanation, further reinforcing the need for interpretable\
    \ AI. Effective model adaptation also depends on fine-tuning that is representative\
    \ of the target data domain [\\[66\\]](#page-27-15). Fine-tuning involves adjustment\
    \ of learning rates and batch sizes, regularization of parameters, or retraining\
    \ of model components. This process must balance performance gains against the\
    \ risks of overfitting or underfitting problems. Inconsistencies between training\
    \ and data fine-tuning degrade model accuracy and limit real-world applicability.\n\
    \n*6) Data Privacy and Security:* LargeML models typically aggregate data from\
    \ multiple sources into centralized databases to support server-based training\
    \ and inference. However, such centralization introduces significant privacy risks,\
    \ as large repositories of sensitive information become prime targets for cyberattack\
    \ [\\[67\\]](#page-27-16). Even during inference, LargeML models may requires\
    \ access to confidential data, complicating privacy protection in real time, particularly\
    \ when deployed across distributed networks. In addition to data privacy, securing\
    \ LargeML against threats such as model extraction [\\[68\\]](#page-27-17) and\
    \ adversarial attacks [\\[69\\]](#page-27-18) remains critical. Attacks can compromise\
    \ model integrity and expose sensitive training data. For example, an adversary\
    \ may reverse-engineer a model to recover private information embedded in its\
    \ training data.\n\n## *C. The Vision for 6G: Objective Requirements and the Assistance\
    \ of Bidirectional Integration*\n\nIMT-2030 (6G) will support 15 key capabilities:\
    \ nine enhanced from 5G (e.g., reliability, latency, data rates) and six new ones\
    \ (e.g., coverage, sensing, AI), as per [\\[70\\]](#page-27-19). Some metrics\
    \ are qualitatively described due to complexity [\\[71\\]](#page-27-20). These\
    \ align with six 6G requirements: ubiquitous connectivity, extreme performance,\
    \ high intelligence, strong security, green communication, and efficient ISAC\
    \ (see Fig. [3\\)](#page-10-0).\n\nStandalone technologies cannot meet 6G demands\
    \ (TinyML and LargeML limitations are evident in Sections [III-A,](#page-8-1)\
    \ [III-B\\)](#page-9-0),\n\n ability Spectrum and thus, the focus will shift to\
    \ an open wireless architecture, with integrations of multiple key technologies\
    \ from 4G and beyond-5G [\\[72\\]](#page-27-21). Bidirectional TinyML–LargeML\
    \ integration leverages 6G infrastructure: TinyML processes data at the edge,\
    \ sending critical information to LargeML for advanced analytics and decision-making.\
    \ Specific approaches (TL, FTL, SL, FSL) are detailed in Section [IV.](#page-12-0)\n\
    \n data rate experienced data rate efficiency 6G *1) Ubiquitous Connectivity:*\
    \ 6G will accommodate the exponential growth of interconnected devices, including\
    \ IoT, sensors, wearables, and smart infrastructure, via scalable architecture\
    \ and dynamic resource management. It must support connection densities of 10<sup>6</sup>\
    \ − 10<sup>8</sup> devices/km<sup>2</sup> and data volumes five times greater\
    \ than 5G [\\[73\\]](#page-27-22). Key enablers include network slicing and virtualization\
    \ [\\[74\\]](#page-27-23), which allow efficient resource allocation across diverse\
    \ devices. Deep integration with AI and ML frameworks will enable pervasive, intelligent\
    \ functionality. A promising approach involves the bidirectional integration of\
    \ TinyML and LargeML. TinyML handles local data processing at the edge, reducing\
    \ latency and offloading servers, while LargeML, deployed across a *ubiquitous\
    \ server set* (cloud, edge, mobile devices) [\\[75\\]](#page-27-24), provides\
    \ a global view for system-wide optimization. This approach supports decentralized\
    \ computing and further enhances scalability.\n\n> Beyond scalability, 6G aims\
    \ to deliver ubiquitous global coverage, including terrestrial, aerial, satellite,\
    \ and underwater systems [\\[76\\]](#page-27-25), [\\[77\\]](#page-27-26), ensuring\
    \ connectivity in remote, challenging environments. It must also support high-mobility\
    \ platforms (500 − 1000 km/h), requiring seamless handovers and latency management\
    \ strategies [\\[78\\]](#page-27-27). AI-powered techniques like mobility prediction\
    \ and intelligent handover strategies are expected to ensure seamless and reliable\
    \ connectivity [\\[79\\]](#page-27-28). The integration of TinyML and LargeML\
    \ is central to this approach. TinyML enables real-time monitoring and handover\
    \ prediction at the device level, while LargeML aggregates data across the network\
    \ to refine mobility and handover strategies. Despite the heterogeneity and non-independent\
    \ and identically distributed (non-IID) nature of data generated by edge devices\
    \ [\\[80\\]](#page-27-29), LargeML frameworks can facilitate collaborative learning\
    \ and efficient data aggregation. For example, wearable devices equipped with\
    \ TinyML can monitor patient vitals and detect anomalies locally. Given the non-IID\
    \ nature of health data, from among patients and over time from a single patient,\
    \ LargeML performs server-side aggregation and analysis to support personalized\
    \ healthcare delivery.\n\n> *2) Extreme Performance Networking:* 6G networks require\
    \ extreme performance across multiple dimensions, including ultra-high peak and\
    \ user-experienced data rates, ultra-low latency, and high reliability. Quantitatively,\
    \ reliability targets range from 99.999 to 99.99999 with latency goals between\
    \ 0.1 and 1 ms. As outlined in [\\[81\\]](#page-27-30), other metrics are qualitative.\
    \ Data rate expectations for 6G far exceed those of 5G. Peak data rates are projected\
    \ between 50 to 200 Gbps, potentially reaching 1 Tbps, while user-experienced\
    \ rates are expected to range from 300 to 500 Mbps.\n\n> Integrating TinyML and\
    \ LargeML enhances 6G systems by combining real-time, lightweight local processing\
    \ with advanced server-side computation. TinyML reduces latency and bandwidth\
    \ use for simple tasks, while LargeML handles\n\ncomplex computations. In the\
    \ 6G environment, where servers are ubiquitous, exchanging trained models instead\
    \ of raw data lowers communication latency, and hierarchical model distribution\
    \ improves response times and throughput. Bidirectional integration enables collaborative\
    \ learning through model splitting and transfer, optimizing distributed computing\
    \ for both edge and server training. From a reliability perspective, integrating\
    \ TinyML and LargeML harnesses the powerful capabilities of LargeML for server-based\
    \ decision-making in mission-critical applications. These models can be fine-tuned\
    \ in real time using data from TinyML devices, ensuring consistent performance\
    \ and reliability. Integration allows multiple devices to jointly validate shared\
    \ data without compromising privacy or security. Redundancy in this manner strengthens\
    \ model accuracy and resilience to data loss or corruption.\n\n*3) High Intelligence:*\
    \ The 5G era has evolved alongside rapid developments in AI, propelling progress\
    \ toward autonomous machine intelligence. With the advent of 6G, a new phase of\
    \ distributed intelligence is emerging, marked by seamless integration of distributed\
    \ data processing, learning, computing, and AI inference across large-scale, interconnected\
    \ systems [\\[81\\]](#page-27-30). This network of smart devices is strengthened\
    \ by the complementary roles of TinyML and LargeML. TinyML enables edge devices\
    \ to perform local data processing and training, transmitting only essential outputs\
    \ to servers, thereby enabling real-time decisions. LargeML, operating on powerful\
    \ servers, handles resource-intensive tasks, optimizing service intelligence and\
    \ personalization through a decentralized architecture. Besides, the bandwidth\
    \ demands of 6G use cases are substantial, for instance, high-accuracy ISAC requires\
    \ 0.75 GHz, while holographic communications may need up to 1.1 GHz [\\[73\\]](#page-27-22).\
    \ To meet these needs, AI-driven spectrum optimization will be vital. Real-time\
    \ AI monitoring can guide dynamic spectrum allocation, reducing congestion and\
    \ enhancing frequency utilization. AI-empowered cognitive radio networks will\
    \ enable devices to sense and adapt to their environments, while AI-enabled signal\
    \ processing will improve signal quality and spectrum efficiency. AI will also\
    \ facilitate rapid interference mitigation, spectrum sharing, and reallocation\
    \ of underused bands, ensuring optimal network performance. The convergence of\
    \ AI and 6G is poised to drive innovation across domains such as smart cities,\
    \ autonomous transport, healthcare, and immersive entertainment. AI-integrated\
    \ 6G networks will support real-time urban infrastructure management, enhance\
    \ the safety and responsiveness of autonomous vehicles, expand access to AI-powered\
    \ medical diagnostics and remote care, and deliver deeply interactive, high-fidelity\
    \ entertainment experiences.\n\n*4) High Privacy and Security:* Privacy and security\
    \ are critical in deploying ubiquitous AI tasks over 6G networks [\\[82\\]](#page-27-31).\
    \ As the volume of sensitive data grows, protecting it from unauthorized access\
    \ and addressing ethical concerns, such as transparency in AI decision-making\
    \ and privacy risks, becomes imperative. Increasing interconnectivity and system\
    \ complexity heighten vulnerability to cyber threats, necessitating robust security\
    \ frameworks that comply with evolving data protection regulations without compromising\
    \ performance.\n\nWhile the integration of TinyML and LargeML helps mit-\n\n12\n\
    \nigate some privacy concerns, further technologies are needed to address security\
    \ comprehensively. TinyML enables local preprocessing at the edge, extracting\
    \ essential features and minimizing the transmission of sensitive data, thus preserving\
    \ user privacy. It also supports collaborative model training without sharing\
    \ raw data, reducing breach risks. However, TinyML's limited resources may restrict\
    \ implementation of advanced privacy-preserving techniques. Centralized and distributed\
    \ data repositories remain key targets for attacks in a hyperconnected 6G landscape.\
    \ TinyML further enhances resilience by enabling real-time anomaly detection at\
    \ the edge, allowing for rapid threat identification and response. In tandem,\
    \ LargeML systems leverage high-performance servers to perform in-depth threat\
    \ analysis using aggregated data from multiple devices. These systems can deploy\
    \ adaptive, sophisticated security mechanisms, as discussed in [\\[83\\]](#page-27-32),\
    \ bolstering the overall security posture of 6G networks.\n\n*5) Green Communications:*\
    \ Green communications prioritize energy efficiency and sustainability, core elements\
    \ of the 6G vision aimed at reducing CO<sup>2</sup> emissions and minimizing the\
    \ environmental impact of communication technologies [\\[84\\]](#page-27-33).\
    \ As 6G expands the number of connected devices, energy consumption is expected\
    \ to rise, making the reduction of the information and communication technology\
    \ sector's carbon footprint a critical goal. Energy-efficient systems not only\
    \ cut operational costs but also benefit both providers and users. The combined\
    \ use of TinyML and LargeML advances this goal by optimizing resource allocation\
    \ and energy efficiency across the network. TinyML enables low-power devices to\
    \ process data locally, reducing transmission frequency and reliance on energy-intensive\
    \ servers. LargeML complements this by efficiently managing data at the server\
    \ level, leveraging its computational capacity without unnecessary overhead. Their\
    \ integration distributes computation across edge and cloud environments, alleviating\
    \ the load on central servers and data centers, thereby significant reducing CO<sup>2</sup>\
    \ emissions.\n\nTinyML further supports sustainable operations by adapting to\
    \ real-time energy constraints and incorporating energy harvesting (e.g., solar\
    \ or kinetic sources), minimizing dependence on traditional power. When paired\
    \ with LargeML, these systems maintain performance even under tight energy budgets.\
    \ In environmental monitoring, TinyML enables on-site anomaly detection, forwarding\
    \ only critical data to centralized systems for deeper analysis, facilitating\
    \ timely, informed responses. Additionally, their bidirectional integration supports\
    \ predictive maintenance of network infrastructure, identifying issues early to\
    \ prevent energy inefficiencies and environmental degradation. Such proactive\
    \ measures are vital to reducing the ecological footprint of future communication\
    \ networks.\n\n*6) Efficient ISAC:* ISAC is a key 6G requirement, unifying advanced\
    \ sensing, communication, and precise positioning within a unified framework [\\\
    [85\\]](#page-27-34). ISAC supports critical functions such as range, velocity,\
    \ and angle estimation, as well as object detection, localization, imaging, and\
    \ mapping. This integration reduces redundancy, optimizes resource use, and boosts\
    \ overall network performance. High-precision sensing is essential for latency-sensitive\
    \ applications like autonomous vehicles, smart cities, and AR, all of which demand\
    \ real-\n\n<span id=\"page-12-1\"></span>![](_page_12_Figure_0.jpeg)\n\nFig. 4:\
    \ TinyML–LargeML system with TL.\n\ntime, location-specific data. Positioning\
    \ accuracy in 6G is targeted within 1 − 10 cm [\\[81\\]](#page-27-30). The integration\
    \ of TinyML and LargeML can significantly enhance the efficiency, accuracy, and\
    \ robustness of ISAC systems. For example, TinyML enables on-device processing\
    \ and feature extraction, allowing initial estimation of key parameters and basic\
    \ object detection. This local filtering reduces noise and improves signal fidelity.\
    \ Processed outputs are then sent to LargeML systems for deeper analysis and validation,\
    \ improving detection accuracy and reducing false positives. Additionally, TinyML\
    \ supports real-time mapping by continuously updating environmental models, which\
    \ is crucial for autonomous navigation that relies on high-resolution, up-to-date\
    \ spatial information.\n\n## <span id=\"page-12-0\"></span>IV. EFFICIENT BIDIRECTIONAL\
    \ INTEGRATION SOLUTIONS FOR 6G AND BEYOND NETWORKS\n\n#### <span id=\"page-12-2\"\
    ></span>*A. Transfer Learning*\n\nTL, a subset of ML methods, focuses on reusing\
    \ the knowledge from pre-trained models across related tasks and data domains.\
    \ The technique provides an effective approach to TinyML–LargeML integration in\
    \ 6G and beyond networks by combining the strengths of server-based large-scale\
    \ training and on-device adaptability [\\[86\\]](#page-27-35). Fig. [4](#page-12-1)\
    \ depicts a TL-based twoway integration framework, consisting of three main steps.\n\
    \n1 Server-side training: A large dataset of D<sup>S</sup> samples, denoted D<sup>S</sup>\
    \ = {1, . . . , DS}, is used to train a LargeML model, denoted fS, on a high-performance\
    \ cloud or edge server, referred to as the *source domain*. Let Θ = {w<sup>l</sup>\
    \ , bl} L <sup>l</sup>=1 be the set of parameters comprising all weights w<sup>l</sup>\
    \ and biases b<sup>l</sup> across L layers, which form the model pipeline. The\
    \ training objective is to minimize the loss function, i.e.,\n\n$$\\Theta = \\\
    operatorname\\*{arg\\,min}\\_{\\Theta} \\frac{1}{D\\_S} \\sum\\_{i \\in \\mathcal{D}\\\
    _S} \\mathcal{L}\\left(\\mathbf{y}\\_i, f\\_S\\left(\\mathbf{x}\\_i\\right)\\\
    right), \\qquad \\text{(l)}$$\n\nwhere {x<sup>i</sup> , yi}i∈D<sup>S</sup> represents\
    \ the training samples, and f<sup>S</sup> (xi) denotes the model's activation\
    \ after processing input feature x<sup>i</sup> . The loss function L(·) varies\
    \ depending on the learning task and may include metrics such as cross-entropy,\
    \ mean squared error, MAE, or log-likelihood. For example, the loss function of\
    \ a linear regression problem over dataset (X, y) is given by L<sup>0</sup> (w|X,\
    \ y) = <sup>1</sup> M P<sup>M</sup> <sup>m</sup>=1 w<sup>T</sup> x<sup>m</sup>\
    \ − y<sup>m</sup> 2 , where x<sup>m</sup> and y<sup>m</sup> are the input feature\
    \ vector and label scalar, respectively, of the data sample m among M data points\
    \ of (X, y). The server's high computational resources and storage capabilities\
    \ allow it to process large datasets and train complex models effectively.\n\n\
    Server-side training can adopt either a standalone architecture (e.g., two-tiered\
    \ client-edge, two-tiered client-cloud) or a multi-level collaboration model [\\\
    [87\\]](#page-27-36). Compared to standalone setups, multi-level collaboration\
    \ offers greater flexibility in balancing communication and computational overheads.\
    \ Four representative scenarios include: (*i*) *Hybrid training*, where initial\
    \ training of LargeML is conducted in the cloud, followed by deployment to edge\
    \ servers for fine-tuning and real-time adaptation using local data; (*ii*) *Distributed\
    \ fashion*, where the training workload is distributed between the cloud and edge\
    \ servers, with large-scale, resource-intensive tasks handled in the cloud and\
    \ latency-sensitive, less intensive tasks processed at the edge; (*iii*) *Hierarchical\
    \ processing*, where edge servers perform data preprocessing to reduce data volume\
    \ and complexity before transmitting it to the cloud for deeper analysis, while\
    \ updates from the cloud can subsequently relayed back to edge servers for local\
    \ execution and adaptation; and (*iv*) *Enhanced availability*, wherein hierarchical\
    \ server models improve system resilience by allowing edge servers to operate\
    \ independently during cloud outages.\n\n2 Pre-trained knowledge transfer: After\
    \ training on the source domain, selected components of the LargeML fS, specifically\
    \ a subset of Θ, are transferred to the target domain (i.e., TinyML devices).\
    \ These transferred components encapsulate the general features and patterns learned\
    \ from DS. Effective knowledge transfer in TL hinges on four primary considerations\
    \ as described below [\\[88\\]](#page-27-37)–[\\[90\\]](#page-27-38).\n\n*a) What\
    \ to transfer:* The \"what\" aspect refers to identifying the source model elements\
    \ to be reused in the target domain. These may include the components or knowledge\
    \ that are likely to benefit the target task. We denote t<sup>S</sup> as the transferred\
    \ model, where t<sup>S</sup> is a subset of Θ. This decision is the most critical\
    \ in TL.\n\n*b) When to transfer:* The \"when\" aspect involves determining the\
    \ appropriate timing and frequency of knowledge transfer between the source and\
    \ target domains. It requires the selection of a strategy for updating the target\
    \ domain with knowledge from the source domain. Strategies can be grouped into\
    \ three types [\\[91\\]](#page-27-39): (*i*) *Real-time TL*: Knowledge transfers\
    \ occur immediately after updates in the source domain, which is suitable for\
    \ dynamic environments requiring rapid adaptation; (*ii*) *Non-real-time or semi-real-time\
    \ TL*: Knowledge transfers follow a fixed schedule (e.g., daily, weekly) or occur\
    \ at defined operational checkpoints, which suits stable environments or models\
    \ where immediate updates are not critical; and (*iii*) *Ondemand TL*: Knowledge\
    \ transfers occur only when explicitly requested by the target domain, which benefits\
    \ scenarios in which the target domain operates autonomously and requires updates\
    \ under specific conditions only.\n\n<span id=\"page-12-3\"></span>*c) Where and\
    \ how to transfer:* The \"where\" aspect refers to identifying the target domain\
    \ where the knowledge will be applied (e.g., TinyML devices). The \"how\" aspect\
    \ ensures compatibility and maximizes the effectiveness of knowledge use in the\
    \ target domain. Effective transfer depends on both domain similarity and the\
    \ selection of optimization techniques. Transfer is typically more successful\
    \ between domains with overlapping characteristics. For example, a resource management\
    \ model trained on 5G network infrastructure may be adapted to monitor and optimize\
    \ components in a 6G network [\\[92\\]](#page-27-40). The hardware and software\
    \ constraints of TinyML devices also affect both where and how knowledge transfer\
    \ occurs. The transferred model must be compatible with these constraints to function\
    \ efficiently under limited computational and memory resources.\n\nExamined through\
    \ these guiding questions, TL can be categorized according to several criteria\
    \ [\\[89\\]](#page-27-41). One approach focuses on label availability: (*i*) In\
    \ *transductive TL*, labeled data is available only in the source domain, and\
    \ the source and target tasks are similar; (*ii*) In *inductive TL*, labeled data\
    \ is available only in the target domain, and the source and target tasks differ;\
    \ and (*iii*) In *Unsupervised TL*, neither domain has labeled data, and the target\
    \ task is distinct yet related to the source task. Another categorization approach\
    \ depends on the alignment of feature and label spaces between the source and\
    \ target domains. When both domains share identical features and label spaces,\
    \ the setting is classified as *homogeneous TL*; otherwise, it is considered *non-homogeneous\
    \ TL*. A third, widely used classification is based on the transfer mechanism.\
    \ Specifically, *Instance-based TL* selects and re-weights specific instances\
    \ (data points) from the source domain to improve their relevance in the target\
    \ domain, aiming to identify the most useful source data points for training in\
    \ the target domain [\\[93\\]](#page-27-42). *Feature-based TL* seeks to identify\
    \ shared feature representations for the source and target domains [\\[94\\]](#page-27-43).\
    \ By transforming features from both domains into a shared feature space, using\
    \ techniques such as dimensionality reduction, feature extraction, or transformations,\
    \ this approach facilitates more effective knowledge transfer. *Parameter-based\
    \ TL* reuses model parameters (e.g., weights and biases) learned from the source\
    \ domain and transfers them to the target domain [\\[95\\]](#page-27-44). Typically,\
    \ a model is trained on a large source-domain dataset, and the learned parameters\
    \ serve as an initialization point for training on the target dataset, enabling\
    \ the model to retain useful knowledge while adapting to the new domain. Finally,\
    \ *Relational-based TL* is used when data is structured as relationships or graphs\
    \ (i.e., learned rules) [\\[96\\]](#page-27-45). It transfers structural or relational\
    \ information, such as node embeddings, relational patterns, or graph kernels,\
    \ from the source domain to support learning in the target domain.\n\n3 Model\
    \ personalization and on-device adaptation: Consider a set of N TinyML devices\
    \ in the target domain, denoted N = {1, . . . , N}. Each device n ∈ N , equipped\
    \ with limited computational resources and storage, adapts a pretrained model\
    \ t<sup>S</sup> along with its TinyML model gn, using its individual, smaller,\
    \ scenario-specific dataset D<sup>n</sup> = {1, . . . , Dn}. This adaptation involves\
    \ fine-tuning (i.e., freezing t<sup>S</sup> while updating the parameters associated\
    \ with gn) and additional training to tailor the model for relevance to the device's\
    \ specific tasks. This process is referred to as model personalization. The concept\
    \ of an adaptation layer introduced in [\\[97\\]](#page-27-46) augments this process\
    \ by minimizing the general loss function and the maximum mean discrepancy (MMD)\
    \ loss. MMD measures the divergence between source and target domain distributions\
    \ by evaluating their distance in a reproducing kernel Hilbert space (RKHS). It\
    \ serves as a criterion for comparing distributional discrepancy in transfer learning.\
    \ An empirical estimate of the MMD loss function between D<sup>S</sup> and D<sup>n</sup>\
    \ is given by M<sup>D</sup><sup>S</sup> ,D<sup>n</sup> = 1 D<sup>S</sup> P i∈D<sup>S</sup>\
    \ ϕ (x<sup>i</sup> , yi) − 1 D<sup>n</sup> P j∈D<sup>n</sup> ϕ x n j , y n j H\
    \ [\\[97\\]](#page-27-46), where ϕ (·) is the nonlinear mapping into RKHS H, and\
    \ x n j , y n j j∈D<sup>n</sup> represents the samples from Dn. Consequently,\
    \ the objective of minimizing the loss function for local training on client {n}n∈N\
    \ is expressed as\n\n<span id=\"page-13-0\"></span>\n$$\\Theta\\_n = \\arg\\min\\\
    _{\\Theta\\_n} \\frac{1}{D\\_n} \\sum\\_{j \\in \\mathcal{D}\\_n} \\mathcal{L}\\\
    left(\\mathbf{y}\\_j^n, t\\_S\\left(\\mathbf{x}\\_j^n\\right)\\right) + \\lambda\
    \ \\mathcal{M}^2\\_{\\mathcal{D}\\_S, \\mathcal{D}\\_n}, \\tag{2}$$\n\nwhere λ\
    \ > 0 is the trade-off parameter. However, given the simplicity of this shallow\
    \ model, its performance is limited. To further mitigate domain shift between\
    \ the source and target domains, correlation alignment (CORAL) can be applied\
    \ [\\[98\\]](#page-27-47). The CORAL loss between D<sup>S</sup> and D<sup>n</sup>\
    \ is formally defined as ℓ<sup>D</sup><sup>S</sup> ,D<sup>n</sup> = 1 <sup>4</sup>d<sup>2</sup>\
    \ ∥R<sup>D</sup><sup>S</sup> − R<sup>D</sup><sup>n</sup> ∥ 2 F , where d is the\
    \ dimension of embedding features, ∥·∥<sup>F</sup> denotes the Frobenius norm,\
    \ and R<sup>D</sup> denotes the covariance matrix of dataset D. Consequently,\
    \ [\\(2\\)](#page-13-0) can be reformulated as\n\n<span id=\"page-13-1\"></span>\n\
    $$\\boldsymbol{\\Theta}\\_{n} = \\arg\\min\\_{\\boldsymbol{\\Theta}\\_{n}} \\\
    frac{1}{D\\_{n}} \\sum\\_{j \\in \\mathcal{D}\\_{n}} \\mathcal{L}\\left(\\mathbf{y}\\\
    _{j}^{n}, t\\_{S}\\left(\\mathbf{x}\\_{j}^{n}\\right)\\right) + \\lambda \\ell\\\
    _{\\mathcal{D}\\_{S}, \\mathcal{D}\\_{n}}.\\tag{3}$$\n\nSince TinyML devices can\
    \ perform tasks locally after knowledge transfer,they retain sensitive data on-device,\
    \ eliminating the need for continuous cloud transmission or inter-device sharing,\
    \ thus improving privacy.\n\nRecent studies have explored the integration of TinyML\
    \ and LargeML through TL for various applications [\\[36\\]](#page-26-36), [\\\
    [38\\]](#page-26-38), [\\[99\\]](#page-27-48)–[\\[103\\]](#page-27-49). For example,\
    \ [\\[38\\]](#page-26-38) proposes a TL-based system which combines TinyML and\
    \ LargeML, named TINYTL. The framework employs ProxylessNAS-Mobile as the backbone\
    \ NN for source-domain pre-training, and fine-tunes the model on TinyML devices\
    \ using a pre-trained once-for-all network. The parameter-based TL approach transfers\
    \ feature extractor weights. Target tasks include object and facial attribute\
    \ classifications, such as recognition of cars, aircraft, flowers, birds, pets,\
    \ food, and celebrities. [\\[36\\]](#page-26-36) introduces a bidirectional integration\
    \ framework incorporating parameter-based TL. The system pre-trains a CNN in the\
    \ source domain, then adapts the CNN to TinyML devices using Tensorflow Lite Micro\
    \ with pruning and quantization techniques. Target tasks include presence detection\
    \ and classification inside cars. In [\\[99\\]](#page-27-48), the authors developed\
    \ a bidirectional integration system using feature-based TL aimed at object classification\
    \ and human activity recognition for IoT health tracker applications. The source\
    \ domain backbone is a deep neural network (DNN), and the target model runs on\
    \ CMSIS-NN.\n\nBuilding on prior work, the study in [\\[100\\]](#page-27-50) proposes\
    \ a feature-based TL system that exploits VGG-16, DenseNet, and MobileNet as backbone\
    \ architectures for source-domain pre-training. For deployment in the TinyML domain,\
    \ the system uses the EON Tuner tool, integrated with the Edge\n\n<span id=\"\
    page-14-0\"></span>![](_page_14_Figure_0.jpeg)\n\nFig. 5: TinyML–LargeML system\
    \ with FTL.\n\nImpulse platform, to perform face mask detection and compare performance\
    \ across the three models. Another study [\\[101\\]](#page-27-51) applies a parameter-based\
    \ TL approach for soil humidity prediction in smart agriculture. The model combines\
    \ a DNN and long short-term memory (LSTM) architecture for sourcedomain pre-training.\
    \ In the target domain, fine-tuning with TensorFlow Lite Micro and quantization\
    \ techniques are used for deployment on TinyML devices. In [\\[102\\]](#page-27-52),\
    \ the authors introduce a feature-based TL bidirectional integration system for\
    \ motion classification. The approach uses principal component analysis for dimensionality\
    \ reduction and trains a CNN-LSTM model on large-scale, diverse source-domain\
    \ datasets. Finetuning is performed on TinyML devices, with a Raspberry Pi as\
    \ an edge intermediary platform. A feature-based TL system presented in [\\[103\\\
    ]](#page-27-49) incorporates meta-learning for a TinyML–LargeML framework and\
    \ cross-domain classification tasks. The system uses a DNN pre-trained with MCUNet,\
    \ MobileNetV2, and ProxylessNAS architectures in the source domain and performs\
    \ fine-tuning on TinyML devices using a task-adaptive sparse-update method. The\
    \ design prioritizes adaptability to dynamic user contexts and device constraints,\
    \ using few-shot learning with minimal samples.\n\n## *B. Federated Transfer Learning*\n\
    \n*1) Vanilla Federated Transfer Learning:* The TL strategies described in Section\
    \ [IV-A](#page-12-2) are effective when source and target domains share related\
    \ tasks and datasets. However, their applicability in 6G environments is limited\
    \ due to the prevalence of highly non-IID data across massively connected devices.\
    \ FTL, which combines the strengths of FL and TL [\\[104\\]](#page-27-53), is\
    \ better suited for such heterogeneous settings. In 6G's device-dense landscape,\
    \ effective personalization is crucial for optimizing performance on device-specific\
    \ tasks. Notably, FL enables collaborative training across distributed TinyML\
    \ systems while preserving data locality on resource-constrained devices [\\[105\\\
    ]](#page-27-54). FTL enhances this by distinguishing between source clients (who\
    \ participate in training the initial global model) and target clients (who personalize\
    \ the model to their specific needs). As both FL and TL strategies emphasize privacy,\
    \ FTL effectively addresses the dual challenges of data heterogeneity and privacy\
    \ in integrated TinyML–LargeML systems. Fig. [5](#page-14-0) illustrates the workflow\
    \ of this FTL-based approach, as outlined below.\n\nThe FTL process begins with\
    \ the standard FL protocol, differing primarily in the initialization of the global\
    \ model. Specifically, the FL phase includes the following steps.\n\n- 1 Initial\
    \ global model sharing: In contrast to conventional FL, where the server initializes\
    \ the global model with random weights, FTL initializes the global model using\
    \ a pre-trained LargeML model fS, trained on the server's large dataset DS. As\
    \ with [\\(1\\)](#page-12-3), the objective function for training is given by\
    \ f (0) <sup>S</sup> = arg minf<sup>S</sup> 1 D<sup>S</sup> P i∈D<sup>S</sup>\
    \ L(y<sup>i</sup> , f<sup>S</sup> (xi)). Once trained, f (0) S is distributed\
    \ to a source client set K = {1, . . . , K}.\n- 2 Local training: In the training\
    \ round t, each source client k ∈ K uses its local dataset D<sup>k</sup> = {1,\
    \ . . . , Dk} to train and update a local copy of the model. Local training enables\
    \ the model to learn patterns and insights specific to each client's data. Mathematically,\
    \ the objective for minimizing the loss function during client k's local training\
    \ is expressed as f (t) S,k = arg min<sup>f</sup> (t−1) S 1 D<sup>k</sup> P j∈D<sup>k</sup>\
    \ L y k j , f(t−1) S x k j .\n- 3 Local model update: After local training, each\
    \ source client uploads its updated model parameters f (t) S to the server for\
    \ aggregation.\n- 4 Server-side aggregation: The server obtains global model parameters\
    \ by minimizing the global loss function f (t) <sup>S</sup> = arg min<sup>f</sup>\
    \ (t) S,k P k∈K rk D<sup>k</sup> P j∈D<sup>k</sup> L y k j , f(t) S,k x k j where\
    \ r<sup>k</sup> = Dk/ P<sup>K</sup> <sup>k</sup>=1 D<sup>k</sup> represents the\
    \ ratio of client k's data size to the total data size. This aggregation problem\
    \ can be solved using the FedAvg algorithm [\\[106\\]](#page-27-55), a proportion\
    \ weighting strategy [\\[107\\]](#page-27-56), or more advanced methods that account\
    \ for data heterogeneity and varying client contributions.\n- 5 Global model update:\
    \ The updated global model is then sent back to the source clients.\n\nSteps 2\
    \ to 5 are repeated until the global model reaches satisfactory performance.\n\
    \nStep 6 presents a knowledge transfer phase. Once the global model has been sufficiently\
    \ trained and demonstrates strong generalization, the server transfers the useful\
    \ components of the pre-trained global model fS, denoted tS, to the target TinyML\
    \ devices (target clients). This transferred model, derived form the pre-trained\
    \ global model, encapsulates the knowledge learned from source clients and serves\
    \ as the foundation for the next phase.\n\nStep 7 stands for model personalization\
    \ and on-device adaptation. Target clients typically hold smaller datasets or\
    \ domain-specific datasets that differ from those of source clients. To tailor\
    \ the transferred model t<sup>S</sup> to local tasks, each target client performs\
    \ personalization through TL. We consider the set of target TinyML devices N =\
    \ {1, . . . , N}. Each target client fine-tunes t<sup>S</sup> with its local dataset\
    \ Dn. Depending on the similarity of the global model's training data and the\
    \ target client's data, fine-tuning may involve updating the entire model t<sup>S</sup>\
    \ or only a subset of layers. To support\n\nNumerous recent technical publications\
    \ have explored the use of FTL to enable seamless integration between TinyML and\
    \ LargeML across diverse application domains [\\[108\\]](#page-27-57)–[\\[111\\\
    ]](#page-28-0). For example, the work in [\\[108\\]](#page-27-57) introduces TINYFEDTL,\
    \ an open-sourced, parameter-based FTL framework designed for TinyML–LargeML systems.\
    \ The framework pre-trains MobileNetV2 on a source domain and applies Perf-MobileNet,\
    \ part of the TinyML Perf Benchmark, for personalized model adaptation on IoT\
    \ edge devices. Experimental results validate the effectiveness of TINYFEDTL in\
    \ enhancing the quality of experience (QoE) under resource constraints, without\
    \ compromising data privacy. Proposed in [\\[109\\]](#page-28-1), FEDHEALTH is\
    \ a parameter-based FTL framework for wearable healthcare devices. FEDHEALTH employs\
    \ a CNN backbone network for source-domain pre-training, followed by fine-tuning\
    \ during the TL phase to adapt the model to resource-constrained IoT devices.\
    \ Evaluations of wearable activity recognition and auxiliary Parkinson's disease\
    \ diagnosis tasks demonstrate that FEDHEALTH achieves high accuracy, without compromising\
    \ privacy and security. A parameter-based FTL framework incorporating auxiliary\
    \ classifier generative adversarial networks, termed ACGAN-FTL, is proposed in\
    \ [\\[110\\]](#page-28-2). ACGAN-FTL runs personalized models on Raspberry Pi-based\
    \ TinyML devices and is designed to predict the quality of pre-baked carbon anodes\
    \ in industrial production. Recently, the authors in [\\[111\\]](#page-28-0) demonstrated\
    \ the superior performance of FTL compared to standalone FL, TL, and Tensorflow\
    \ Lite-based fusion approaches in both classification and regression tasks in\
    \ TinyML–LargeML systems. The TL principle applied in this framework centers on\
    \ feature-based TL, using Genann [\\[112\\]](#page-28-3) as the backbone neural\
    \ network for source-domain pretraining, followed by fine-tuning for device-specific\
    \ adaptation across multiple TinyML platforms, including Arduino WiFi Rev2, Arduino\
    \ MKR1010, ESP8266, and ESP32.\n\nFrom a hierarchical FTL perspective, the study\
    \ in [\\[113\\]](#page-28-4) introduces IOTDEFENDER, an intrusion detection framework\
    \ designed for resource-constrained IoT networks. IOTDE-FENDER employs three edge\
    \ servers that run standard FL protocols on distinct IoT networks. Their local\
    \ models are subsequently aggregated at a cloud server. IOTDEFENDER uses a CNN\
    \ backbone for source-domain pre-training, with subsequent fine-tuning for parameter-based\
    \ TL and on-device adaptation in resource-constrained IoT devices. Expanding on\
    \ the topology proposed in [\\[113\\]](#page-28-4), the study in [\\[114\\]](#page-28-5)\
    \ proposes HFTL, a hierarchical FTL framework that incorporates additional fog\
    \ servers. These fog servers aggregate models from associated edge servers before\
    \ forwarding to a cloud server for further aggregation. HFTL uses VGG-16, ResNet50V2,\
    \ and MobileNet as pre-trained models, followed by fine-tuning with an additional\
    \ DNN for secure and efficient fault classification in additive manufacturing.\
    \ The framework adopts parameterbased TL as its core approach.\n\n*2) Advanced\
    \ Federated Transfer Learning:* Building upon the foundations of FTL, below are\
    \ efficient methods for integrating TinyML with LargeML.\n\n*a) Federated Meta-Learning\
    \ (FML):* FML aims to achieve a higher level of generalization and adaptability\
    \ by enabling models to learn new tasks rapidly through metaknowledge acquired\
    \ from diverse clients or domains [\\[115\\]](#page-28-6). Unlike standard FTL,\
    \ which focuses on transferring knowledge across related tasks, FML trains models\
    \ to adapt to previously unseen tasks. This \"learning how to learn\" capability\
    \ improves robustness and broadens applicability, supporting both knowledge transfer\
    \ and efficient task-specific adaptation.\n\nTINYREPTILE, an FML-based TinyML–LargeML\
    \ system introduced in [\\[116\\]](#page-28-7), draws on concepts from FL and\
    \ online learning. The system enables distributed resource-constrained IoT devices\
    \ to collaboratively learn a robust RNN initialization for rapid adaptation to\
    \ new target devices. The study evaluates TINYREPTILE on classification tasks\
    \ using sine-wave, Omniglot, and keyword spotting datasets and demonstrates a\
    \ simple yet efficient implementation on Raspberry Pi 4 and Cortex-M4 MCU edge\
    \ hardware, using parameter-based TL. TINYMETAFED, a comparable framework, is\
    \ introduced in [\\[117\\]](#page-28-8). Evaluated on similar TinyML devices,\
    \ TINYMETAFED incorporates a modified algorithm that reduces energy consumption\
    \ and communication overhead, accelerates convergence, and stabilizes training.\
    \ Building on the capabilities of TINYREPTILE and TINYMETAFED, the study in [\\\
    [118\\]](#page-28-9) proposes an advanced prototype, deployed on similar TinyML\
    \ testbed devices. The framework employs a CNN backbone architecture for pre-trained\
    \ models and applies the MLPerf Tiny Benchmark for on-device, personalized model\
    \ adaptation. The prototype demonstrated its effectiveness in three realworld\
    \ TinyML applications: handwritten character image classification, keyword spotting,\
    \ and presence detection in smart buildings. PMFED, proposed in [\\[119\\]](#page-28-10),\
    \ is an FML framework that employs a pre-trained CNN and fine-tunes the model\
    \ on a Raspberry Pi 3B for personalized adaptation on TinyML devices. The framework\
    \ demonstrated efficient performance in three real-world, IoT-enabled health monitoring\
    \ tasks: prompt defibrillation following detection of ventricular arrhythmia,\
    \ atrial fibrillation detection, and human activity recognition.\n\n*b) Federated\
    \ Foundation Models (FFMs) and Federated Fine-Tuning (FedFT) of Foundation Models\
    \ (FMs):* FFMs integrate the strengths of FMs and FL, enabling decentralized clients\
    \ to collaboratively adapt pre-trained LargeML models without exchanging raw data.\
    \ This approach combines the privacy advantages of FL with the TL benefits of\
    \ TFMs [\\[22\\]](#page-26-21). FedFT of FMs allows each client to locally personalize\
    \ a shared model while maintaining data confidentiality. To reduce computational\
    \ overhead, PEFT techniques such as LoRA, adapter modules, and prompt tuning are\
    \ widely applied [\\[22\\]](#page-26-21).\n\nThe study in [\\[120\\]](#page-28-11)\
    \ introduces FEDPT, a federated proxytuning method for LLMs on resource-constrained\
    \ edge devices. By relying solely on model predictions rather than parameter access,\
    \ FEDPT reconstructs full-sized LoRA modules for server-side aggregation, reducing\
    \ computational and communication overhead while matching FEDPT performance in\
    \ instruction-following tasks. However, its reliance on homogeneous LoRA hampers\
    \ performance under non-IID data distributions. To address this, HETLORA [\\[121\\\
    ]](#page-28-12) uses zeropadding and rank-constrained truncation to standardize\
    \ LoRA modules, enhancing convergence and efficiency in heterogeneous systems\
    \ for tasks like multi-session chat dialogue and text summarization. CAFF [\\\
    [122\\]](#page-28-13) further improves accuracy and fairness in language and vision\
    \ classification tasks by combining tiny transformers with a layer-wise FedFT\
    \ and NN selection strategy. In [\\[123\\]](#page-28-14), the authors present\
    \ FEDD2P, an efficient FedFT framework for FMs targeting heterogeneous, resource-constrained\
    \ IoT devices. FEDD2P distills aggregated knowledge into a prompt generator, allowing\
    \ frozen FMs to adapt efficiently to downstream tasks such as image classification,\
    \ satellite imagery analysis, and texture recognition. The work in [\\[124\\]](#page-28-15)\
    \ proposes FWDLLM, an efficient FedFT method for LLMs on heterogeneous, resource-constrained\
    \ edge devices. Instead of applying traditional BP to compute precise gradients,\
    \ FWDLLM combines BP-free training with PEFT by injecting minor self-generated\
    \ perturbations into model parameters. Compared to existing PEFT benchmarks, this\
    \ approach reduces memory footprint and accelerates convergence.\n\nTo further\
    \ support model heterogeneity and computational efficiency, sparse architectures\
    \ like MoE can be combined with PEFT. For example, FEDFMSL [\\[125\\]](#page-28-16)\
    \ incorporates a two-stage MoE-based FedFT. The method first trains a global expert,\
    \ then fine-tunes a local expert for improved personalization. To adapt to heterogeneous,\
    \ resource-constrained edge environments, FEDFMSL employs a sparsely-activated\
    \ LoRA algorithm that progressively activates LoRA matrices based on historical\
    \ performance tracked in a capability queue. This approach yields performance\
    \ gains and resource savings in various image classification tasks, for example\
    \ food recognition, land use, land cover, and human action detection. In [\\[126\\\
    ]](#page-28-17), the authors introduce FEDMOE, a sparsely-activated MoE operating\
    \ with LoRA-based FedFT for heterogeneous, resource-constrained edge environments.\
    \ FEDMOE dynamically adjusts submodels using global expert recommendations and\
    \ achieves fast convergence and improved performance in text classification, summarization,\
    \ and reading comprehension tasks. Lastly, A<sup>3</sup>SMOE [\\[48\\]](#page-26-48)\
    \ introduces an efficient FedFT method for FMs tailored to heterogeneous, resourcerestricted\
    \ edge settings. A<sup>3</sup>SMOE extends the concept of data-conditional computation\
    \ in sparsely-activated MoEs to resource-conditional computation, enabling clients\
    \ to activate suitable experts based on resource availability. It outperforms\
    \ prior LoRA-based techniques in instruction-tuning and complements existing heterogeneous\
    \ LoRA methods.\n\n#### *C. Split Learning*\n\nSL is an ML technique designed\
    \ for collaborative model training under resource-constrained environments, enabling\
    \ full model training on devices to improve learning efficiency [\\[75\\]](#page-27-24).\
    \ SL is particularly suitable for integrating TinyML and LargeML within 6G and\
    \ future network architectures. By partitioning the model across client devices\
    \ and servers, SL allows TinyML devices to participate in training and inference\
    \ tasks without becoming overwhelmed by the computational demands of LargeML while\
    \ also preserving data privacy. Two main SL variants can be applied to TinyML–LargeML\
    \ systems: vanilla SL (VSL) and parallel SL (PSL).\n\n<span id=\"page-16-0\"></span>*1)\
    \ Vanilla Split Learning:* VSL is the foundational form of SL in which the training\
    \ process proceeds sequentially fro client to client [\\[127\\]](#page-28-18).\
    \ On the server side, either a standalone server architecture (e.g., two-tiered\
    \ client-edge or two-tiered client-cloud) or a multi-level server collaboration\
    \ model can be implemented.\n\nFig. [6\\(](#page-17-0)a) illustrates a VSL-based\
    \ integration of TinyML– LargeML in which a standalone cloud or edge server collaborates\
    \ with multiple TinyML devices during training. Typically, a smaller sub-model\
    \ resides on each TinyML device, and a larger sub-model is hosted on the server\
    \ [\\[127\\]](#page-28-18). In a collaborative setting with N TinyML devices,\
    \ denoted N = {1, . . . , N}, and a coordinating server, each TinyML device n\
    \ ∈ N stores a local dataset D<sup>n</sup> = {1, . . . , Dn} and a local TinyML\
    \ model Tn. The server maintains the LargeML model L. After client n completes\
    \ its training pass, the temporary global model is defined as G<sup>n</sup> =\
    \ {Tn;L}. The VSL training protocol is executed over R rounds (outer iterations),\
    \ with model training executed sequentially on the N participating TinyML devices\
    \ in each round. For each server-client interaction involving client n, T (inner)\
    \ iterations are performed. In the classic setup, clients follow a round-robin\
    \ sequence: client n + 1 begins its interaction only after client n completes\
    \ its T iterations for the current round. During each inner iteration, client\
    \ n updates its model G<sup>n</sup> by minimizing its local loss function L¯(Gn|Dn)\
    \ ∆= 1 D<sup>n</sup> P j∈D<sup>n</sup> L y n j , G<sup>n</sup> x n j , where x\
    \ n j , y n j <sup>D</sup><sup>n</sup> <sup>j</sup>=1 = Dn. The process begins\
    \ with client n training its local model T<sup>n</sup> on dataset Dn. This model\
    \ consists of the initial layers of the full model up to a designated split (or\
    \ cut) layer. This step, referred to as client-side local forward propagation\
    \ (FP) (Step n.1), produces *smashed data*, which are early-stage feature representations\
    \ computed as T<sup>n</sup> (x<sup>n</sup> ), where x<sup>n</sup> = x n j <sup>D</sup><sup>n</sup>\
    \ <sup>j</sup>=1. The smashed data and corresponding labels, denoted {T<sup>n</sup>\
    \ (x<sup>n</sup> ), yn} with y<sup>n</sup> = y n j <sup>D</sup><sup>n</sup> <sup>j</sup>=1,\
    \ are forwarded to the server for additional processing, i.e., serverside FP (Step\
    \ n.2). The input (x<sup>n</sup> , y<sup>n</sup> ) may also represent a mini-batch\
    \ of data samples, with its size determined by optimal hyperparameters or task-specific\
    \ requirements [\\[128\\]](#page-28-19), [\\[129\\]](#page-28-20). The server\
    \ uses T<sup>n</sup> (x<sup>n</sup> ) as input to the LargeML model L, which consists\
    \ of the remaining layers of the full model. This step constitutes the server-side\
    \ local FP (Step n.3). The gradients of the loss function L¯(Gn|Dn) are then computed\
    \ to generate ∇L¯(L) and ∇L¯(Tn(xn)). These gradients are used to update the parameters\
    \ of L locally on the server before return to the currently associated client\
    \ n. The update process L, expressed as L ← L−ηS∇L¯(L), where η<sup>S</sup> is\
    \ the learning rate for the server-side model update, represents server-side local\
    \ backward propagation (BP) (Step n.3). Server-side BP concludes with the server\
    \ returning the gradients of smashed data, ∇L¯(Tn(xn)), to the current client\
    \ n (Step n.4). Upon receiving these gradients, client n performs local BP by\
    \ updating its local model as T<sup>n</sup> ← T<sup>n</sup> − ηn∇L¯(Tn(xn)), where\
    \ η<sup>n</sup> is the learning rate for the client-side model up-\n\n<span id=\"\
    page-17-0\"></span>![](_page_17_Figure_0.jpeg)\n\nFig. 6: TinyML–LargeML systems:\
    \ (a) VSL and (b) PSL.\n\ndate (Step n.5). For consistency, the terms η<sup>S</sup>\
    \ and η<sup>n</sup> are reused with the same definitions in subsequent sections.\
    \ This completes one inner iteration between a client and the server. During the\
    \ outer iterations, the aim is to minimize the global loss function min {G<sup>n</sup>\
    \ }∀<sup>n</sup> P <sup>n</sup>∈N <sup>D</sup>nL¯(Gn|Dn) .P <sup>n</sup>∈N Dn.\
    \ After convergence, TinyML devices can employ the global model G for inference\
    \ on a designated target task.\n\nThe integration of VSL-based TinyML with LargeML\
    \ has enabled a diverse range of applications for tasks in resourceconstrained\
    \ environments. For example, a two-stage wireless channel adaptive split inference\
    \ method [\\[130\\]](#page-28-21) addresses memory and energy constraints on edge\
    \ devices. The experimental results demonstrate enhanced privacy, improved inference\
    \ accuracy, and dynamic adjustment of split points based on real-time variations\
    \ in wireless channel gain. ARES [\\[131\\]](#page-28-22) is an adaptive, resource-aware\
    \ SL scheme designed for efficient model training in resource-constrained IoT\
    \ systems. ARES mitigates the impact of stragglers by employing device-specific\
    \ split layers, balances energy consumption with training time, and adapts to\
    \ fluctuating network throughput and computing capacity. An adaptive SL algorithm\
    \ based on Lyapunov theory is introduced in [\\[132\\]](#page-28-23). This approach\
    \ dynamically selects split layers for resource-constrained devices and allocates\
    \ serverside computing resources in wireless edge networks. Simulations demonstrate\
    \ significant reductions in average training delay and energy consumption.\n\n\
    When multi-level server collaboration is employed, multiple cloud and edge servers\
    \ jointly participate in VSL through a strategy known as multi-hop SL [\\[75\\\
    ]](#page-27-24). In this architecture, multiple hierarchical servers are organized\
    \ in sequence, forming a serial multi-hop SL framework, as described in [\\[133\\\
    ]](#page-28-24). Here, the model is partitioned into multiple segments: (*i*)\
    \ initial layers are deployed on TinyML devices; (*ii*) middle layers are distributed\
    \ across multiple servers, each managing a distinct segment; (*iii*) final layers\
    \ reside on a server that generates the output. This setup follows a similar structure\
    \ to standalone server architectures but involves sequential inference and BP\
    \ across multiple servers [\\[133\\]](#page-28-24)–[\\[135\\]](#page-28-25). Specifically,\
    \ the first client processes raw local data through its local model segment to\
    \ generate smashed data before forwarding it to the first server. The server processes\
    \ this data through its model segment and forwards the output to the next server,\
    \ continuing through the chain until the final output is produced. During BP,\
    \ the final server computes the loss, performs gradient updates, and propagates\
    \ gradients backward through the server chain. Each intermediate server updates\
    \ its own parameters and passes gradients to the preceding server or client. Clients\
    \ then perform local BP and parameter updates in sequence, completing the training\
    \ loop [\\[134\\]](#page-28-26), [\\[135\\]](#page-28-25). In a system with N\
    \ TinyML devices and K collaborating servers, each inner iteration requires 2NK\
    \ inference and BP steps, highlighting the need for efficient algorithms to reduce\
    \ latency.\n\nBeyond serial data flow, centralized smashed data routing [\\[75\\\
    ]](#page-27-24) can also be implemented in 6G networks by integrating ad-hoc data\
    \ routing and centralized model partitioning, enabled through software-defined\
    \ networking. This approach must consider bandwidth, memory, and computing constraints.\
    \ Effective server coordination is crucial for compatibility between model segments,\
    \ especially with non-IID data distribution across TinyML devices, necessitating\
    \ periodic synchronization of model parameters for consistency.\n\n<span id=\"\
    page-17-1\"></span>*2) Parallel Split Learning:* The sequential training process\
    \ of VSL introduces substantial training latency and does not scale as the number\
    \ of participating TinyML devices increases, a problem that will be exacerbated\
    \ by the massive connectivity expected in 6G networks. The highly non-IID nature\
    \ of client data can also degrade model performance in sequential training, since\
    \ the model may become biased toward the data distribution of the previously trained\
    \ client. To address these challenges, PSL enables multiple TinyML devices to\
    \ train their local model segments concurrently [\\[136\\]](#page-28-27), [\\\
    [137\\]](#page-28-28). This parallelization reduces total training time, introduces\
    \ \"semiscalability\" relative to the number of participating devices, and partially\
    \ mitigates the impact of non-IID data in 6G mediums.\n\nFig. [6\\(](#page-17-0)b)\
    \ illustrates the workflow of PSL-based TinyML– LargeML systems under a standalone\
    \ server architecture. Unlike VSL-based bidirectional integration, which requires\
    \ N sequential rounds of forward and backward propagation, PSL completes training\
    \ with a single round, regardless of N. This is the basis of semi-scalability,\
    \ meaning the number of propagation rounds is scalable on the client side. However,\
    \ the server-side training workload (i.e., smashed data gradients and computational\
    \ workload), still scales proportionally with N. The inner-iteration training\
    \ process of a PSL-based TinyML– LargeML system proceeds as follows:\n\n- 1 Simultaneous\
    \ client-side local FP: Each TinyML device N = {1, . . . , N} trains on its local\
    \ datasets D<sup>n</sup> = {1, . . . , Dn} using its model segment Tn, generating\
    \ smashed data Tn(xn) at the designated split layer.\n- 2 FP of smashed data:\
    \ Each device transmits its smashed data and corresponding labels {T<sup>n</sup>\
    \ (x<sup>n</sup> ), yn} to the server. The server receives these inputs concurrently\
    \ from all devices.\n- 3 Server-side local FP and BP: The server trains the concatenated\
    \ smashed data S = [T<sup>1</sup> (x1) ; . . . ; T<sup>N</sup> (x<sup>N</sup>\
    \ )] using its model segment, i.e., LargeML L. Concurrent training accelerates\
    \ the overall local FP and generates the model output yˆ = L(S). Given the predicted\
    \ result yˆ and the concatenated ground-truth labels y = [y1; ...; y<sup>N</sup>\
    \ ], the server computes the loss value L¯(y, yˆ). Gradients for updating L, i.e.,\
    \ ∇L¯(L(S)), are then calculated, and the gradients for each set of smashed data,\
    \ i.e., ∇L¯(Tn(xn)), are determined. Local BP is then performed as L ← L − ηS∇L¯(L(S)).\n\
    - 4 BP of smashed data gradients: The computed gradients ∇L¯(Tn(xn)) are returned\
    \ to each respective TinyML device in parallel.\n- 5 Simultaneous client-side\
    \ local BP: Each TinyML device performs local BP and updates its model parameters,\
    \ i.e., T<sup>n</sup> ← T<sup>n</sup> − ηn∇L¯(Tn(xn)).\n\nThese training steps\
    \ are repeated over consecutive rounds until the model converges, i.e., minimizing\
    \ the global loss function\n\n$$\\min\\_{\\mathbf{L}, \\{\\mathbf{T}\\_n\\}\\\
    _{\\forall n}} \\frac{\\sum\\_{n \\in \\mathcal{N}} \\sum\\_{j \\in \\mathcal{D}\\\
    _n} \\mathcal{L}\\left(\\mathbf{y}\\_j^n, \\mathbf{L}\\left(\\mathbf{T}\\_n\\\
    left(\\mathbf{x}\\_j^n\\right)\\right)\\right)}{\\sum\\_{n \\in \\mathcal{N}}\
    \ D\\_n}. \\qquad (4)$$\n\nAfter convergence, each TinyML device fine-tunes the\
    \ model to its specific task. However, two potential issues arise:\n\n- Although\
    \ PSL helps prevent overfitting caused by aligning the end-to-end output model\
    \ too closely with the distribution of the previous client, the highly non-IID\
    \ nature of data across TinyML devices can reduce the convergence rate. This may\
    \ require server or clientside aggregation strategies. Relevant solutions include\
    \ EDGESPLIT [\\[138\\]](#page-28-29) and FSL.\n- The server assumes the primary\
    \ training workloads from N TinyML devices, which does not scale as N → ∞. Although\
    \ cloud/edge servers are generally powerful, they could still become bottlenecks\
    \ in the massive connectivity expected in 6G systems. An efficient variant, PSL\
    \ (EPSL), addresses this issue [\\[139\\]](#page-28-30).\n\nFurther discussion\
    \ of EDGESPLIT and EPSL is given in Section [IV-C3;](#page-18-0) details of FSL\
    \ are provided in Section [IV-D.](#page-20-0)\n\nRecent publications have applied\
    \ PSL to TinyML–LargeML systems [\\[140\\]](#page-28-31)–[\\[142\\]](#page-28-32).\
    \ For example, the study in [\\[140\\]](#page-28-31) proposes a privacy-sensitive\
    \ PSL framework that supports SL training across multiple resource-constrained\
    \ devices in parallel. The framework addresses overfitting caused by non-IID training\
    \ orders and varying dataset sizes on resourceconstrained devices while maintaining\
    \ data privacy. Introduced in [\\[141\\]](#page-28-33), ADASPLIT represents an\
    \ efficient scaling SL method for heterogeneous, resource-constrained systems.\
    \ ADASPLIT minimizes on-device computation and optimizes server-side processing\
    \ and communication. It eliminates the need for\n\n<span id=\"page-18-1\"></span>![](_page_18_Figure_12.jpeg)\n\
    \nFig. 7: Integrated TinyML–LargeML system with EDGESPLIT.\n\nserver gradients\
    \ at the client side, reduces payload size and interaction frequency to reduce\
    \ communication overhead, and support updates to sparse model partitions on heterogeneous\
    \ resource-constrained devices. The work in [\\[142\\]](#page-28-32) formulates\
    \ and solves a multiplayer bargaining problem to identify optimal split-layer\
    \ strategies in PSL-based bidirectional integration systems. The approach balances\
    \ computation and wireless transmission energy consumption, training time, and\
    \ data privacy. The proposed framework also demonstrates robustness with non-IID\
    \ datasets across resource-constrained devices.\n\n<span id=\"page-18-2\"></span>In\
    \ PSL-based TinyML–LargeML systems involving multilevel server collaboration,\
    \ the architecture extends beyond simple client–edge or client–cloud configurations\
    \ by incorporating multiple servers or hierarchical layers of servers that collaborate\
    \ during the training process. Such setups require server orchestration and aggregation\
    \ strategies, necessitating advanced PSL schemes to coordinate interactions between\
    \ server layers and integrate TinyML with LargeML seamlessly. Detailed in Section\
    \ [IV-C3,](#page-18-0) this class of schemes is referred to as advanced PSL, where\
    \ parallel-server PSL [\\[143\\]](#page-28-34) and multi-hop PSL [\\[144\\]](#page-28-35)\
    \ serve as pioneering frameworks. These frameworks support the development of\
    \ more sophisticated server orchestration and aggregation methods.\n\n<span id=\"\
    page-18-0\"></span>*3) Advanced Parallel Split Learning:* This section introduces\
    \ EDGESPLIT and EPSL, which are advanced PSL schemes designed to address the highly\
    \ non-IID nature of data across TinyML devices. Both schemes consider model aggregation\
    \ at the server side. EPSL further provides full scalability. The details of each\
    \ scheme are described below.\n\nEDGESPLIT [\\[138\\]](#page-28-29) is an advanced\
    \ PSL variant designed to address the challenge of highly non-IID data distributed\
    \ across TinyML devices. Non-IID data causes local models trained on individual\
    \ TinyML devices to diverge, reducing generalization and degrading global model\
    \ performance. EDGESPLIT introduces a coordinated learning approach across diverse\
    \ datasets while preserving the core benefits of PSL. Fig. [7](#page-18-1) illustrates\
    \ the inner-iteration training steps of EDGESPLIT, which begins by a standard\
    \ PSL procedure: 1 Simultaneous client-side local FP, 2 FP1 of smashed data, 3\
    \ server-side local FP and local BP, 4 BP1 of smashed data gradients, and 5 simultaneous\
    \ client-side local BP. The subsequent steps introduce the key advancements of\
    \ EDGESPLIT over conventional PSL.\n\n<span id=\"page-19-0\"></span>![](_page_19_Figure_0.jpeg)\n\
    \nFig. 8: Integrated TinyML–LargeML system with EPSL.\n\n- 6 FP2 with updated\
    \ weights: After client-side local BP, each TinyML device forwards its updated\
    \ model weights T<sup>n</sup> to the server. These weights incorporate the local\
    \ learning adjustments made by each device based on its non-IID data.\n- 7 Server-side\
    \ aggregation: The server aggregates all updated weights received from TinyML\
    \ devices using an algorithm, such as FedAvg [\\[106\\]](#page-27-55):\n\nTFedAvg\
    \ = X n∈N DnT<sup>n</sup> .X n∈N Dn. (5)\n\nThis step harmonizes learning across\
    \ all devices by combining their weights and enables the server to produce a more\
    \ generalized model that reflects the diversity of data distributions.\n\n- 8\
    \ BP2 of aggregated weights: The server broadcasts the aggregated weights TFedAvg\
    \ to all TinyML devices, ensuring that each device updates its model with globally\
    \ informed weights that reduce the impact of non-IID data.\n- 9 Updated client-side\
    \ model: Each TinyML device receives the globally aggregated model TFedAvg and\
    \ updates its weights accordingly, i.e., T<sup>n</sup> ← TFedAvg, ∀n, to synchronize\
    \ all devices with a consistent model.\n\nThis iterative process continues until\
    \ convergence, guided by the global objective function defined in [\\(4\\)](#page-18-2).\
    \ During the inference phase, each TinyML device adapts the converged model to\
    \ its specific target task.\n\nEDGESPLIT is applied to a defined subset of TinyML\
    \ devices. In the context of massive 6G connectivity, device heterogeneity is\
    \ the expected norm, with differences in computing power, data distribution, and\
    \ channel conditions being significant. In such scenarios, cluster-based PSL (CPSL)\
    \ [\\[145\\]](#page-28-36) offers a solution by building on the procedural structure\
    \ of EDGESPLIT. Instead of treating all devices as a uniform group, CPSL first\
    \ clusters them according to shared characteristics (e.g., computing capabilities\
    \ and channel conditions). These clusters contain devices with more manageable\
    \ and stable data distributions, enabling more efficient training and improved\
    \ model performance.\n\nConventional PSL methods, such as EDGESPLIT and CPSL,\
    \ achieve scalability on the client side, whereas the server continues to carry\
    \ a significant computational burden. EPSL [\\[139\\]](#page-28-30) advances PSL\
    \ by addressing this imbalance, targeting the issue of semi-scalability through\
    \ a modified aggregation strategy. This strategy reduces server-side computational\
    \ costs and training latency, enabling full scalability across both client and\
    \ server tiers. The key steps of EPSL are illustrated in Fig. [8](#page-19-0)\
    \ and summarized below.\n\n- 1 Simultaneous client-side local FP: Each TinyML\
    \ device n ∈ N = {1, . . . , N} independently trains on its local dataset D<sup>n</sup>\
    \ = {1, . . . , Dn} using its local model segment, Tn. The device generates smashed\
    \ data Tn(xn) at the designated split layer. This step is executed in parallel\
    \ across all devices.\n- 2 FP of smashed data: Devices transmit their smashed\
    \ data and corresponding labels, {T<sup>n</sup> (x<sup>n</sup> ), yn}, to the\
    \ server. The server processes these multiple smashed data streams concurrently.\n\
    - 3 Server-side local FP: The server concatenates the smashed data S = [T<sup>1</sup>\
    \ (x1) ; . . . ; T<sup>N</sup> (x<sup>N</sup> )], then forwards this data through\
    \ its model segment L to generate the predicted output yˆ = L(S).\n- <span id=\"\
    page-19-1\"></span>4 Gradient aggregation and server-side local BP: The server\
    \ computes the gradients of the last-layer activations, denoted Ga, using the\
    \ loss value L¯(y, yˆ), where y = [y1; ...; y<sup>N</sup> ] represents the concatenated\
    \ groundtruth labels. This aggregation compresses the activation gradients, significantly\
    \ reducing server computating load and training latency and improving scalability\
    \ [\\[139\\]](#page-28-30). The server computes the gradients of its model, denoted\
    \ GS. The parameter update for the server-side model is expressed as L ← L − ηSGS.\n\
    - 5 BP1 (aggregated gradients): The server broadcasts the aggregated activation\
    \ gradients Ga, computed at the split layer by performing BP on the last-layer\
    \ aggregated activation gradients, to all participating TinyML devices.\n- 6 BP2\
    \ (unaggregated activation gradients): To enable more precise updates, the server\
    \ also sends the unaggregated activation gradients, denoted Gu, computed at the\
    \ split layer by performing BP on the last-layer aggregated activation gradients,\
    \ to the respective TinyML devices.\n- 7 Client-side local BP: Each TinyML device\
    \ uses the received gradients G<sup>a</sup> and G<sup>u</sup> to compute its local\
    \ gradient G<sup>n</sup> and update its model segment according to T<sup>n</sup>\
    \ ← T<sup>n</sup> − ηnGn.\n\nThe EPSL training process is repeated until convergence.\
    \ Similarly, [\\(4\\)](#page-18-2) can be used as the global objective function\
    \ for the EPSL training process. During inference, each TinyML device adapts the\
    \ trained model to its specific task, enabling personalized performance on its\
    \ local data. Compared to standard PSL, EPSL reduces BP computation and communication\
    \ complexity from O(N) to O(1), achieving full scalability [\\[75\\]](#page-27-24).\
    \ EPSL also introduces a tunable aggregation ratio, denoted r, in the BP phase,\
    \ enabling a trade-off between computational and communication efficiency while\
    \ maintaining learning accuracy. When r = 0, EPSL reverts to PSL.\n\nBuilding\
    \ on the PSL-based multi-level server collaboration framework introduced in Section\
    \ [IV-C2,](#page-17-1) we now examine parallel-server PSL and multi-hop PSL in\
    \ detail. Parallelserver PSL [\\[143\\]](#page-28-34) enables multiple parallel\
    \ servers to process\n\n<span id=\"page-20-1\"></span>![](_page_20_Figure_0.jpeg)\n\
    \nFig. 9: Integrated TinyML–LargeML system with FSL.\n\nmodel segments concurrently\
    \ for a set of resource-constrained clients. This setup introduces orchestration\
    \ management and aggregation strategies that require the solution of joint optimization\
    \ problems related to client-server assignments and scheduling decisions that\
    \ minimize total training time. To address this NP-hard problem, we propose two\
    \ scalable solutions: (*i*) a decomposition-based approach that exploits structural\
    \ symmetry, and (*ii*) a lightweight alternative with low computational overhead.\
    \ In contrast, multi-hop PSL [\\[144\\]](#page-28-35) implements a pipelined,\
    \ parallel multi-hop SL, extending the multi-hop SL approach discussed in Section\
    \ [IV-C1](#page-16-0) with a parallel execution mechanism. Specifically, all participating\
    \ TinyML devices simultaneously train on their local datasets using their respective\
    \ model segments, generate smashed data at the split layer, and forward this data\
    \ to the first server. The subsequent inference and backward paths between the\
    \ servers proceed sequentially, following the same steps as in multihop SL. Until\
    \ the BP process completes at the first server, all participating TinyML devices\
    \ receive their corresponding smashed data gradients from this server, ensuring\
    \ synchronization during the final aggregation phase. Unlike multi-hop SL, multi-hop\
    \ PSL reduces the number of inference and backward path steps to (2K + 2), regardless\
    \ of the value of N.\n\n## <span id=\"page-20-0\"></span>*D. Federated Split Learning*\n\
    \n*1) Vanilla Federated Split Learning:* FSL parallelizes client-side model training\
    \ (see Section [IV-C2\\)](#page-17-1) while addressing highly non-IID data across\
    \ clients, combining the strengths of SL and FL [\\[146\\]](#page-28-37). FSL\
    \ differs from PSL, EDGESPLIT, and EPSL methods by the division of responsibilities\
    \ between two cooperating servers: the main server (typically a highperformance\
    \ cloud server) and the Fed server (usually an edge server running FedAvg). In\
    \ FSL, the Fed server aggregates updated local models from clients using FL-based\
    \ techniques. This process is illustrated in Fig. [9,](#page-20-1) including the\
    \ key steps:\n\n1 Simultaneous client-side local FP : Each TinyML device N = {1,\
    \ . . . , N} performs local FP on its model segment, Tn, using its local dataset\
    \ D<sup>n</sup> = {1, . . . , Dn} to generate smashed data Tn(xn) at the split\
    \ layer.\n\n- 2 FP of smashed data : Devices concurrently transmit the smashed\
    \ data and corresponding labels {T<sup>n</sup> (x<sup>n</sup> ), yn} to the main\
    \ server for further centralized processing.\n- 3 Main-server processing: The\
    \ main server processes the smashed data in both local FP and BP using either\
    \ strategies of SplitFedV1 (when the server has substantial computing capacity\
    \ and can efficiently execute parallel processing) or SplitFedV2 (personalized\
    \ updates on the main server are essential, and client data is relatively homogeneous)\
    \ [\\[146\\]](#page-28-37), [\\[147\\]](#page-28-38). In SplitFedV1, each client's\
    \ smashed data is separately processed and parallelized on its model segment L,\
    \ where the loss value L¯(yn, ˆyn) and its gradient ∇L¯(yn, ˆyn) are calculated\
    \ based on the predicted output ˆy<sup>n</sup> = L(Tn(xn)) and ground-truth label\
    \ yn. In the subsequent steps, the server aggregates the gradients of the smashed\
    \ data and updates the serverside model using the FedAvg algorithm [\\[106\\]](#page-27-55),\
    \ expressed as L ← L − η<sup>S</sup> P <sup>n</sup>∈N <sup>D</sup>n∇L¯(yn, ˆyn)\
    \ .P <sup>n</sup>∈N Dn. In SplitFedV2, the server processes client smashed data\
    \ sequentially on L, following a randomly determined client order represented\
    \ by the list S and the computation of ∇L¯(yn, ˆyn) akin to SplitFedV1. Instead\
    \ of aggregation, the server sequentially process the data in the order defined\
    \ by S to update the global model after each local FP using the rule L ← L−ηS∇L¯(yn,\
    \ ˆyn) and continuing until all clients in the list S are processed.\n- 4 BP of\
    \ smashed data gradients: After performing local FP and BP on the server-side\
    \ model, the main server returns the evaluation of ∇L¯(yn, ˆyn) to TinyML devices.\n\
    - 5 Client-side local BP: Upon receiving the gradients from the main server, each\
    \ TinyML device n ∈ N calculates ∇L¯(T<sup>n</sup> (xn)) and performs local BP,\
    \ updating its model as T<sup>n</sup> ← T<sup>n</sup> − ηn∇L¯(T<sup>n</sup> (xn)).\n\
    - 6 Client-side local model upload: Devices upload their updated local model to\
    \ the Fed server for aggregation.\n- 7 Fed-server aggregation: The Fed server\
    \ aggregates the client-side local models using [\\(5\\)](#page-19-1) to generate\
    \ a global client-side model TFedAvg that incorporates updates from all participating\
    \ TinyML devices.\n- 8 Client-side global model update: TFedAvg is returned to\
    \ all TinyML devices.\n- 9 Client-side local model synchronization: Device update\
    \ their local model with the rule T<sup>n</sup> ← TFedAvg, ∀n. This step ensures\
    \ that all devices converge toward improved and consistent model performance.\n\
    \nThis FSL cycle repeats iteratively until the overall model converges. During\
    \ inference, each TinyML device adapts the converged model to its specific target\
    \ task.\n\nSeveral recent studies have explored the use of FSL for integrating\
    \ TinyML and LargeML in a range of applications [\\[148\\]](#page-28-39)–[\\[151\\\
    ]](#page-28-40). For example, ADAPTSFL, based on SplitFedV2, dynamically adjusts\
    \ model splitting and aggregation to optimize latency and convergence on resource-constrained\
    \ devices, outperforming benchmarks [\\[148\\]](#page-28-39). FEDSL, also on SplitFedV2,\
    \ enables robust healthcare analytics on wearables, excelling in medical imaging\
    \ across diverse data [\\[149\\]](#page-28-41). Beyond standard FSL architectures,SPLITGP,\
    \ built on SplitFedV1, supports bidirectional training for personalized client\
    \ models and generalized server models, surpassing baselines [\\[150\\]](#page-28-42).\
    \ ESFL, also on SplitFedV1, enhances training stability by incorporating prior\
    \ global models, using an iterative algorithm to solve its NP-hard optimization,\
    \ improving efficiency over standard FL, SL, and FSL [\\[151\\]](#page-28-40).\
    \ Hierarchical FSL frameworks further boost efficiency: one trains multiple edge\
    \ server-client pairs with cloud aggregation for resilience [\\[152\\]](#page-28-43),\
    \ while another uses parallel multi-client setups for enhanced performance on\
    \ constrained IoT devices [\\[153\\]](#page-28-44).\n\n*2) Federated Split Foundation\
    \ Models and Federated Split Fine-Tuning of Foundation Models:* Federated split\
    \ FMs (FSFMs) extend the FL framework by partitioning large FMs across clients\
    \ and servers, enabling distributed training and inference with reduced local\
    \ resource requirements [\\[154\\]](#page-28-45). Clients train or fine-tune distinct\
    \ FM segments collaboratively, preserving privacy by not exposing raw data, and\
    \ lowering computational overhead. Federated split fine-tuning (FedSFT) further\
    \ enables client-side personalization of model components while retaining global\
    \ knowledge through coordinated updates. Compared to standard FL and FFMs, FSFMs\
    \ improve scalability by exchanging only compact smashed data rather than full\
    \ model parameters [\\[154\\]](#page-28-45). These features position FSFMs and\
    \ FedSFT as particularly suitable approaches in 6G scenarios characterized by\
    \ device heterogeneity, limited computational capacity, and stringent privacy\
    \ requirements.\n\nFor instance, SFPROMPT is a prompt-based FedSFT framework designed\
    \ for privacy-sensitive, resource-constrained edge environments [\\[155\\]](#page-28-46).\
    \ By dividing the pre-trained FM between server and client and integrating soft\
    \ prompts with local data pruning, SFPROMPT improves FedFT efficiency and performance\
    \ and significantly reduces computational and communication costs in image classification\
    \ tasks. FEDSPLITX is a finegrained FSFM approach that supports multiple model\
    \ partition points to accommodate diverse client capabilities in heterogeneous,\
    \ resource-constrained edge environments [\\[156\\]](#page-28-47). By embedding\
    \ auxiliary networks at each partition, FEDSPLITX reduces communication overhead\
    \ and latency and improves model performance for image classification tasks. FEDVZ\
    \ provides a defense framework against gradient inversion attacks in FSFMs using\
    \ vision transformers [\\[157\\]](#page-28-48). It replaces BP with a single forward\
    \ pass based on zeroth-order optimization to approximate server gradients, proving\
    \ effective in heterogeneous, resource-limited environments for image classification\
    \ tasks. PRINCE provides an incentive mechanism that encourages high-quality device\
    \ participation from multiple FSL tenants through strategic pricing [\\[158\\\
    ]](#page-28-49). PRINCE improves FedSFT performance across a range oftasks, including\
    \ image classification, sentiment analysis, speed-to-text, and question answering.\n\
    \n#### <span id=\"page-21-0\"></span>V. APPLICATIONS OF TINYML–LARGEML INTEGRATION\n\
    \n# *A. Data Privacy and Network Security*\n\n*1) Toward Data Privacy:* Model\
    \ inversion attacks present significant privacy threats, particularly for systems\
    \ trained on sensitive data. TinyML–LargeML integration enables a robust privacy-preserving\
    \ architecture through on-device inference and decentralized data processing.\
    \ TinyML enhances data privacy by performing local processing on edge devices\
    \ [\\[11\\]](#page-26-12), minimizing dependence on centralized cloud servers\
    \ and reducing the exposure of raw data. In parallel, LargeML complements this\
    \ architecture by aggregating and analyzing processed data without accessing the\
    \ original inputs [\\[159\\]](#page-28-50). By sharing only abstracted features\
    \ or model updates, a collaborative model mitigates the risks associated with\
    \ model inversion attacks. TinyML also facilitates the deployment of conventional\
    \ privacy-preserving techniques such as FL, allowing distributed devices to train\
    \ models collaboratively without sharing raw data [\\[11\\]](#page-26-12). Differential\
    \ privacy can further enhance protection by injecting noise into data or model\
    \ outputs, preventing identification of individual users while maintaining meaningful\
    \ analysis [\\[160\\]](#page-28-51). When combined with LargeML, more advanced\
    \ FL-based techniques, such as FTL [\\[104\\]](#page-27-53), FML [\\[115\\]](#page-28-6),\
    \ and FSL [\\[146\\]](#page-28-37), enable stronger privacy protection in AI ecosystems,\
    \ as detailed in Section [IV.](#page-12-0)\n\nPrivacy-preserving integration is\
    \ particularly crucial in healthcare applications [\\[99\\]](#page-27-48), [\\\
    [109\\]](#page-28-1), [\\[119\\]](#page-28-10), [\\[149\\]](#page-28-41), where\
    \ wearable devices embedded with TinyML algorithms can process physiological data\
    \ locally. Personal health information thus remains private, while LargeML systems,\
    \ hosted on secure servers, draw from aggregated insights to improve diagnostic\
    \ accuracy and predictive performance without compromising raw data. Beyond healthcare,\
    \ TinyML–LargeML integration benefits a range of privacy-sensitive domains. For\
    \ example, in smart agriculture [\\[101\\]](#page-27-51), IoT-enabled agricultural\
    \ sensors equipped with TinyML analyze soil conditions, crop health, and environmental\
    \ metrics on-site, protecting proprietary farming practices, while LargeML refines\
    \ predictive models for climate adaptation and yield forecasting. In smart home\
    \ and city [\\[118\\]](#page-28-9), on-device inference captures user behavior\
    \ and preferences with minimal data exposure, while LargeML enhances service personalization\
    \ through cloud-based analysis.\n\n*2) Toward Network Security:* As reviewed in\
    \ Section [III-A,](#page-8-1) TinyML devices operate under stringent resource\
    \ constraints, with memory and computing capacity typically two to three orders\
    \ of magnitude below that of typical IoT or edge devices [\\[11\\]](#page-26-12),\
    \ [\\[56\\]](#page-27-5), [\\[161\\]](#page-28-52). These limitations make traditional\
    \ security mechanisms inadequate, and integrating TinyML and LargeML is projected\
    \ to introduce a novel secure strategy. In particular, TinyML enables lightweight,\
    \ real-time anomaly detection on edge devices, identifying threats with low computational\
    \ cost by monitoring local activity for deviations [\\[162\\]](#page-28-53). Integrated\
    \ with LargeML, which analyzes large-scale network traffic and correlates anomalies\
    \ across devices [\\[163\\]](#page-28-54), this forms a multi-layered defense\
    \ for 6G networks. TinyML handles local threat detection, while LargeML uncovers\
    \ complex attack patterns. For instance, IOTDEFENDER [\\[113\\]](#page-28-4) uses\
    \ FTL across three edge servers to aggregate local models, refined by a cloud-based\
    \ CNN for pre-training. During TL, models are adapted for IoT devices, enabling\
    \ efficient intrusion detection despite hardware constraints.\n\nTo support efficient\
    \ encryption and secure communication, TinyML enhances data security by performing\
    \ lightweight encryption and decryption on edge devices, enabling secure data\
    \ transmission without relying solely on cloud services [\\[164\\]](#page-28-55).\
    \ However, its limited resources restrict complex encryption pro-\n\n<span id=\"\
    page-22-0\"></span>![](_page_22_Figure_0.jpeg)\n\nFig. 10: Complementary roles\
    \ of TinyML and LargeML in enhancing data privacy and network security in 6G systems.\n\
    \ntocols. LargeML addresses this by managing encryption key distribution and security\
    \ protocol orchestration, dynamically updating keys to strengthen communication\
    \ channels [\\[165\\]](#page-28-56). This hybrid approach balances robust security\
    \ with TinyML's constraints. Fig. [10](#page-22-0) depicts a TinyML–LargeML ecosystem\
    \ enhancing 6G data privacy and network security.\n\n#### *B. Network Management*\n\
    \nWith the rapid growth of connected IoT devices and mobile users, 5G networks\
    \ have been engineered to manage high traffic volumes and support ultra-reliable\
    \ low-latency communication (uRLLC), massive machine type communication (mMTC),\
    \ and enhanced mobile broadband (eMBB) using technologies such as network slicing,\
    \ software-defined networking (SDN), network function virtualization (NFV), and\
    \ multi-access edge computing (MEC) [\\[166\\]](#page-28-57). In which, network\
    \ slicing creates virtual networks on shared infrastructure, SDN and NFV enhance\
    \ flexibility by decoupling control/data planes and virtualizing functions, and\
    \ MEC reduces latency by processing data at the edge. As 6G emerges, user-centric,\
    \ intelligent management increases complexity, rendering traditional approaches\
    \ inadequate. Closed-loop automation, powered by ML and big data analytics, is\
    \ now critical [\\[167\\]](#page-28-58).\n\nAgainst this background, TinyML–LargeML\
    \ integration transforms 6G network management by combining edge-based efficiency\
    \ with large-scale analytics for adaptive, intelligent systems. TinyML enables\
    \ localized optimization, monitoring real-time metrics like latency and traffic\
    \ to ensure low-latency and quality-of-service (QoS) in applications like smart\
    \ factories [\\[168\\]](#page-28-59). LargeML processes data from multiple TinyML\
    \ nodes, identifying complex patterns, predicting failures, and optimizing network-wide\
    \ resources, such as rerouting traffic to maintain uRLLC, mMTC, and eMBB connectivity\
    \ [\\[169\\]](#page-29-0). For instance, a Lyapunov-based adaptive split learning\
    \ framework dynamically assigns split layers to TinyML devices, reducing latency\
    \ and energy use [\\[132\\]](#page-28-23). Fig. [11](#page-22-1) is an illustrative\
    \ combination of TinyML's efficiency at the edge with LargeML's capacity for large-scale\
    \ analytics, and applications of bidirectional integration across domains. In\
    \ network slicing, TinyML\n\n<span id=\"page-22-1\"></span>![](_page_22_Figure_6.jpeg)\n\
    \nFig. 11: Applications of TinyML–LargeML integration in 6G network slicing, SDN,\
    \ NFV, and MEC management.\n\nmonitors metrics for real-time resource reallocation,\
    \ while LargeML predicts congestion and optimizes slice provisioning [\\[170\\\
    ]](#page-29-1), [\\[171\\]](#page-29-2). In SDN, TinyML detects anomalies like\
    \ DDoS attacks at the edge, and LargeML enhances traffic management by predicting\
    \ congestion and optimizing routes [\\[172\\]](#page-29-3), [\\[173\\]](#page-29-4).\
    \ For NFV, TinyML addresses performance degradation, while LargeML automates resource\
    \ scaling [\\[174\\]](#page-29-5). In MEC, TinyML reduces latency through on-device\
    \ processing, complemented by LargeML's orchestration, ensuring efficient, low-latency\
    \ 6G networks [\\[175\\]](#page-29-6). For example, SPLITGP [\\[150\\]](#page-28-42),\
    \ an FSL strategy combining personalization and generalization for traininginference\
    \ optimizations. Another is SPLITGP, which optimizes the TinyML device-side model\
    \ for task-specific personalization and trains the MEC-side model to generalize\
    \ across outof-distribution inputs.\n\n#### *C. Intent-based Networking*\n\nIntent-based\
    \ networking (IBN) is a network management paradigm that translates high-level\
    \ business objectives, or human-readable \"intents\", into automated network configurations\
    \ [\\[176\\]](#page-29-7). Through closed-loop automation, IBN continuously verifies\
    \ and enforces these intents, focusing on three core processes [\\[177\\]](#page-29-8):\
    \ (*i*) *Intent refinement*, which involves the translation of user-defined intents\
    \ into maintainable and adaptable network policies, (*ii*) *Intent activation*,\
    \ which uses network analytics and machine learning to identify and resolve policy\
    \ conflicts, and (*iii*) *Intent assurance*, which integrates with SDN controllers\
    \ and orchestration systems, using telemetry to continuously monitor network functions\
    \ and ensure that intents are met through ongoing adjustments.\n\nThe effectiveness\
    \ of IBN depends heavily on the accurate and consistent interpretation of user\
    \ inputs, such as voice, text, or other data, and the extraction of intents from\
    \ these inputs. As illustrated in Fig. [12,](#page-23-0) TinyML–LargeML integration\
    \ supports this goal. TinyML serves as a compact node for realtime, on-device\
    \ AI processing, enabling accurate reflection of user intents and facilitating\
    \ coordinated network behavior [\\[178\\]](#page-29-9). LargeML functions as a\
    \ core server, aggregating intents from TinyML nodes using techniques such as\
    \ TL, FTL, FFMs, SL, FSL, or FSFMs, and performing subsequent translation into\
    \ actions and policies without human intervention [\\[179\\]](#page-29-10). A\
    \ notable example of TinyML–LargeML synergy in IBN is an intent-based on-device\
    \ AI framework [\\[180\\]](#page-29-11) that facilitates collaborative AI across\
    \ resource-constrained IoT devices by capturing and reflecting user intents. In\
    \ this framework, TinyML\n\n<span id=\"page-23-0\"></span>![](_page_23_Figure_0.jpeg)\n\
    \nFig. 12: IBNs with TinyML–LargeML integration.\n\nmanages local processing of\
    \ multi-modal sensor data, for example speech-to-text conversion and an intent\
    \ translation, while LargeML centrally manages policy coordination. Reducing dependence\
    \ on cloud-based processing improves security, processing speed, and energy efficiency.\
    \ TinyML–LargeML integration for IBN has also been applied to resource-constrained\
    \ autonomous aerial vehicle swarm networks [\\[181\\]](#page-29-12). This framework\
    \ combines TinyML with explainable AI to support the tamper-proof security features\
    \ of blockchain and zero-trust architecture, enabling time-sensitive intent-based\
    \ predictions with interpretable cognitive intelligence. It allows the detection\
    \ of false communications that could distort positional data shared between aerial\
    \ vehicles. The numerical results in [\\[181\\]](#page-29-12) indicate a 2.34%\
    \ increase in prediction accuracy, a 2.97% gain in sensitivity, latency reduction\
    \ to 43.1 ms, and storage reduction to 0.0013 MB.\n\n#### *D. Zero-touch Network*\n\
    \nThe zero-touch network (ZTN) is a management paradigm that uses AI, ML, and\
    \ IBN to automate operations with minimal to no human input [\\[182\\]](#page-29-13).\
    \ ZTN improves network efficiency by automating routine tasks such as configuration,\
    \ monitoring, and troubleshooting, which lowers operational costs and frees administrators\
    \ to focus on strategic planning. ZTN also improves transmission reliability by\
    \ continuously analyzing traffic and user requests, adjusting network parameters\
    \ in real time, resolving faults, and restoring services autonomously. ZTN also\
    \ aligns network behavior with organizational policies while maintaining the flexibility\
    \ to scale with increasing network demands and evolving requirements. Importantly,\
    \ ZTN supports real-time threat detection and protection by integrating advanced\
    \ security mechanisms.\n\nAchieving zero-touch automation with zero net emissions\
    \ in AI-driven wireless networks requires energy-efficient TinyML–LargeML integration,\
    \ low-power transceivers, lightweight AI models, and adaptive resource allocation\
    \ [\\[183\\]](#page-29-14). In response, research on energy-efficient AI has accelerated,\
    \ and Fig. [13](#page-23-1) is a vision of the integrated TinyML– LargeML ecosystem\
    \ for 6G ZTNs. To support autonomous solutions in self-organizing networks, TinyML\
    \ optimizes\n\n<span id=\"page-23-1\"></span>![](_page_23_Figure_6.jpeg)\n\n<span\
    \ id=\"page-23-2\"></span>Fig. 13: ZTNs with TinyML–LargeML integration.\n\n![](_page_23_Figure_8.jpeg)\n\
    \nFig. 14: BrainMeta with TinyML and LargeML.\n\nneural network training on IoT\
    \ devices for sustainable local learning, and LargeML uses FL and DL at base stations\
    \ or clouds to reduce energy use and balance computational load [\\[184\\]](#page-29-15).\
    \ In mobile networking, TinyML enables autonomous IoT node operations, and LargeML\
    \ supports coverage map generation and edge AI-as-a-service (AIaaS), as seen in\
    \ NanoDeploy Automator [\\[185\\]](#page-29-16). In industrial IoT, TinyML extracts\
    \ data at terminal nodes, with LargeML at gateways and clouds enhancing efficiency\
    \ and traffic management, exemplified by Open RAN (O-RAN)-based ZTNs for smart\
    \ factories [\\[186\\]](#page-29-17), [\\[187\\]](#page-29-18). Cross-layer cybersecurity\
    \ frameworks like SH-CASH use AutoML to secure physical and protocol layers, with\
    \ potential for TinyML integration within O-RAN architectures, ensuring service-level\
    \ agreement (SLA) compliance and high performance [\\[188\\]](#page-29-19).\n\n\
    #### *E. Brain-level Metaverse*\n\nThe Brain-Level Metaverse (BrainMeta) integrates\
    \ braincomputer interface (BCI) technology with the metaverse to create immersive\
    \ virtual environments where users control avatars, manipulate objects, and navigate\
    \ using brain signals [\\[189\\]](#page-29-20). Enabled by non-invasive (e.g.,\
    \ EEG headsets) [\\[190\\]](#page-29-21) or invasive (e.g., brain implants) [\\\
    [191\\]](#page-29-22) BCI methods, BrainMeta adapts to users' cognitive states,\
    \ delivering personalized experiences for applications like virtual learning,\
    \ gaming, remote work, and rehabilitation therapies for neurological conditions.\
    \ It also opens employment opportunities for individuals with disabilities by\
    \ supporting metaverse-based workspaces.\n\nDespite its potential, BrainMeta remains\
    \ in an early developmental stage and faces key challenges, particularly in building\
    \ brain-inspired cognitive architectures constrained by size, weight, power, and\
    \ processing limits [\\[192\\]](#page-29-23). BrainMeta also demands more efficient,\
    \ high-performance models to accurately interpret and represent information for\
    \ enhanced user interaction, cognition, content creation, and service quality\
    \ in alignment with neural characteristics. These challenges can be negotiated\
    \ through the integrated deployment of TinyML and LargeML, as illustrated in Fig.\
    \ [14.](#page-23-2) TinyML algorithms can exploit neuromorphic computing chips\
    \ [\\[193\\]](#page-29-24) to support diverse message formats across computing\
    \ processes. This capability enables customization of processing elements in brain-inspired\
    \ communication devices, localizes memory, and increases communication bandwidth,\
    \ enhancements which accelerate image recognition and classification tasks. In\
    \ addition, TinyML can operate as an edge-based platform within the metaverse,\
    \ using efficient TinyML processors to reduce latency and resource demands [\\\
    [194\\]](#page-29-25). One such optimization involves winner-takeall circuits\
    \ implemented with simplified leaky-integrate-andfire neurons on field-programmable\
    \ gate arrays. LargeMLbased approaches [\\[9\\]](#page-26-8), such as cloud–edge–end\
    \ collaborative models, mobility-aware pre-rendering, and diffusion modelbased\
    \ adaptive rendering, can be improve data collection from TinyML devices and enhance\
    \ rendering tasks in the metaverse. The process involves extraction of generalized\
    \ representations from extensive EEG data, segmentation of data into channel arrays,\
    \ and application of vector-quantized neural spectrum prediction to train and\
    \ update neural analyzers [\\[195\\]](#page-29-26). AMFL, an adaptive, resource-efficient\
    \ FL framework designed for metaverse environments, offers another example [\\\
    [196\\]](#page-29-27). By combining FL and DRL strategies within BrainMeta, AMFL\
    \ mitigates the adverse effects of non-IID data, reduces resource demands, and\
    \ improves the QoE for human-centric, resourceconstrained augmented reality devices.\n\
    \n#### <span id=\"page-24-0\"></span>VI. CHALLENGES AND FUTURE RESEARCH DIRECTIONS\n\
    \n#### *A. Standardization*\n\n5G faces challenges with dynamic user demands and\
    \ traffic, while 6G aims to enable sustainable applications like holography and\
    \ the metaverse through TinyML–LargeML integration [\\[197\\]](#page-29-28). This\
    \ integration is designed for user-centric services that adapt to changing behaviors\
    \ and networks. However, standardization issues impede the development of a fully\
    \ integrated ecosystem. The ITU-R identifies RAN slicing as crucial for 6G virtual\
    \ network sharing [\\[198\\]](#page-29-29), requiring a unified O-RAN architecture\
    \ that supports diverse air interfaces and AI functions. This architecture should\
    \ offer customizable service parameters, but shared infrastructure raises concerns\
    \ about transparency, reliability, and privacy. Standardization bodies are advancing\
    \ AI-driven wireless architectures. 3GPP's Releases 17 [\\[199\\]](#page-29-30)\
    \ and 18 [\\[200\\]](#page-29-31) enhance 5G RAN with AI for intelligence and\
    \ efficiency, while the O-RAN Alliance integrates AI for orchestration and control\
    \ [\\[201\\]](#page-29-32), [\\[202\\]](#page-29-33).\n\nHowever, challenges remain\
    \ in integrating TinyML and LargeML across the network lifecycle. Proposals include\
    \ a secure TinyML edge AI architecture [\\[60\\]](#page-27-9), explainable AI\
    \ for decision transparency [\\[203\\]](#page-29-34), and a testbed for trustworthy\
    \ network management [\\[204\\]](#page-29-35). Sustainability efforts are supported\
    \ by an energy cost-of-AI metric [\\[205\\]](#page-29-36). At a higher level,\
    \ a shift to a multi-tier mNode model is proposed [\\[197\\]](#page-29-28), featuring\
    \ distributed nodes and network AI logic. Additionally, a digital twin framework\
    \ for AI-RAN can manage digital twins and enable real-time adaptability in dense\
    \ networks [\\[206\\]](#page-29-37).\n\n#### *B. Resource Management and Orchestration*\n\
    \n6G networks face resource management and orchestration challenges, including\
    \ wireless channel uncertainty, ISAC device proliferation, and carbon emission\
    \ reduction. These drive AI/ML-based frameworks to optimize spectrum, power, and\
    \ time allocation, enhancing interference management, energy efficiency, and reliability\
    \ over traditional methods like channel estimation and beamforming [\\[207\\]](#page-29-38).\
    \ However, integrating AI/ML introduces data privacy risks, computational overhead,\
    \ and scalability issues. Adaptive algorithms like DRL with diffusion models [\\\
    [207\\]](#page-29-38) and distributed learning like FTL with blockchain incentives\
    \ [\\[208\\]](#page-29-39) can mitigate these by preserving privacy and reducing\
    \ latency.\n\nDeploying DL with TinyML faces obstacles [\\[209\\]](#page-29-40):\
    \ (*i*) computational trade-offs, (*ii*) vanishing gradients needing hardware\
    \ acceleration, (*iii*) memory constraints for semantic communication, (*iv*)\
    \ reliance on scarce real-world data, and (*v*) dynamic environmental variability.\
    \ LargeML, using distributed neural networks and collaborative learning, encounters\
    \ issues like: (*i*) data segmentation for collective learning, (*ii*) transmission\
    \ errors in FL affecting accuracy, and (*iii*) stringent privacy and coordination\
    \ demands. Future research into model-agnostic meta-learning [\\[210\\]](#page-29-41)\
    \ or accretionary learning [\\[211\\]](#page-29-42) could address these challenges.\n\
    \nAt a broader level of cross-domain interaction and orchestration, AI will be\
    \ integrated into 6G network slicing, leveraging NFV for resource virtualization\
    \ and SDN for centralized management. A central SDN controller handles planning\
    \ and provisioning, while AI-enhanced local controllers manage operations, ensuring\
    \ cost-effective, QoS-compliant network slices. However, cross-domain AI coordination\
    \ poses challenges, including those outlined in [\\[212\\]](#page-29-43) as follows:\
    \ (*i*) Data/knowledge coordination, (*ii*) model training coordination, (*iii*)\
    \ inference and decision coordination, and (*v*) computing resource coordination.\n\
    \nBeyond technical integration, managing and orchestrating TinyML and LargeML\
    \ in 6G network slices is challenging due to the shift from 5G's centralized learning\
    \ to distributed paradigms like FL, swarm learning, multi-intelligent RL, TL,\
    \ meta-learning, and SL. The choice of learning approach depends on node capabilities,\
    \ dataset traits, computing resources, model complexity, latency, and network\
    \ conditions. Supporting diverse distributed methods increases orchestration complexity.\
    \ To address this, AI nodes should adopt specialized roles, such as horizontal\
    \ FL for decentralized data, hierarchical FL for reduced communication, TL for\
    \ efficient training, vertical FL for cross-domain learning, SL for uneven resources,\
    \ multiintelligent RL for localized collaboration, and swarm learning for secure\
    \ joint training.\n\n#### *C. Advanced Security and Privacy-Preserving Techniques*\n\
    \nAs TinyML–LargeML integration becomes central to nextgeneration wireless network\
    \ developments, the security and privacy landscape will continue to grow in complexity.\
    \ Key concerns include adversarial attacks, data security, model trustworthiness\
    \ and integrity, and user privacy.\n\nTinyML faces security challenges due to\
    \ limited edge device resources, making devices like traffic sensors, smartphones\
    \ in FL, and wearables vulnerable to adversarial attacks. Adversarial training\
    \ [\\[213\\]](#page-29-44) enhances TinyML's resilience to malicious inputs, while\
    \ homomorphic encryption [\\[214\\]](#page-29-45) ensures data confidentiality\
    \ and integrity. Privacy risks from sensitive edge data can be mitigated using\
    \ differential privacy, which adds noise to obscure information [\\[215\\]](#page-29-46),\
    \ and advanced encryption to secure model parameters during transmission [\\[216\\\
    ]](#page-29-47).\n\nLargeML deployment also requires robust privacy and security\
    \ due to its broader scope and multi-stakeholder involvement. Model distillation\
    \ [\\[33\\]](#page-26-33) transfers LargeML knowledge to TinyML, reducing computational\
    \ needs while maintaining performance. Integrating LargeML with GANs [\\[217\\\
    ]](#page-29-48) simulates cyberattacks to improve resilience. Blockchain ensures\
    \ model integrity by hashing and recording versions, maintaining TinyML model\
    \ authenticity [\\[215\\]](#page-29-46). Advanced privacypreserving techniques\
    \ further secure LargeML systems: (*i*) privacy-preserving data shuffling, (*ii*)\
    \ differential private synthetic data generation, *iii*) private aggregation of\
    \ teacher ensembles, (*iv*) Renyi differential privacy for generative mod- ´ eling,\
    \ (*v*) cross-Silo FL with secure data sharding, and (*vi*) decentralized privacy-preserving\
    \ knowledge distillation.\n\n#### *D. Real-time, Lightweight Intelligence*\n\n\
    Beyond 5G and 6G networks aim to automate M2M operations by transforming conventional\
    \ communication systems into AI-centric architectures, ultimately transitioning\
    \ toward human-centric networks through advanced AI integration. To meet the growing\
    \ demands for improved user experience and reduced carbon emissions, the integration\
    \ of TinyML and LargeML into AI-enabled systems creates new opportunities for\
    \ real-time, lightweight intelligence yet also introduces distinct technical challenges.\n\
    \nTinyML applications vary in communication protocols, latency requirements [\\\
    [11\\]](#page-26-12), and device specifications. For example, Arduino Nano (64\
    \ MHz, 1 MB flash memory, 256 KB RAM) and Raspberry Pi (up to 1.8 GHz, 32 GB flash\
    \ memory, 8 GB RAM) reflect a range of processing capabilities [\\[4\\]](#page-26-3).\
    \ This variability highlights the need for compact, unified TinyML frameworks\
    \ that support flexible plug-and-play deployment, provide efficient processing,\
    \ and maintain lightweight runtimes [\\[218\\]](#page-29-49). In contrast to TinyML,\
    \ LargeML typically processes vast input volumes from multiple domains, each requiring\
    \ tailored prompt engineering and model-splitting approaches. To deliver real-time\
    \ responses with accurate and reliable outputs, LargeML often depends on extensive\
    \ hyperparameter tuning and multi-step processing. These operations result in\
    \ prolonged training times, high power consumption, and significant memory requirements\
    \ [\\[16\\]](#page-26-15). Consequently, future LargeML should adopt adaptive\
    \ prompt engineering strategies, such as model-adaptive prompts [\\[219\\]](#page-29-50)\
    \ and lightweight tuning paradigms for pluggable prompting [\\[220\\]](#page-29-51),\
    \ to improve explainability and accelerate processing and inference without disruption\
    \ to the life cycle pipeline.\n\nHowever, the limited availability of unified\
    \ platforms and tools for managing workflows, coordinating input data, and ensuring\
    \ target environment compatibility poses additional challenges in TinyML–LargeML\
    \ integration. For example, LLMs require specialized ML libraries, compilers,\
    \ and runtimes for code generation, text processing, and prompt execution tasks,\
    \ each with varying input formats, volumes, and execution times [\\[25\\]](#page-26-24).\
    \ Iterative error-handling and refinement mechanisms are often required to improve\
    \ output reliability. Thus, in addition to concentrating on optimizing TinyML\
    \ and LargeML design, comprehensive guidelines and evaluation frameworks are essential\
    \ to define task-specific roles and system requirements for each deployment and\
    \ operation phase [\\[221\\]](#page-29-52).\n\n### *E. AI-Native and Collective\
    \ AI for 6G*\n\nThe concept of AI-Native redefines system design by positioning\
    \ AI at the core of operations rather than treating it as supplementary feature,\
    \ as seen in autonomous vehicles where AI manages navigation, obstacle detection,\
    \ and collision avoidance. As defined in [\\[222\\]](#page-29-53), future AI-native\
    \ systems aim to unify five critical capabilities: solving complex problems via\
    \ data-driven AI for seamless operations, adapting to dynamic environments, combining\
    \ foundational knowledge with new insights for continuous learning, ensuring trustworthy\
    \ and transparent AI functions, and enabling resilient, cognitive autonomous networks.\
    \ Nevertheless, integrating TinyML and LargeML into AI-Native systems poses challenges:\
    \ (*i*) coordinating resource-constrained TinyML with computationally robust LargeML\
    \ for zero-touch operations, (*ii*) defining their roles across heterogeneous\
    \ infrastructures to have a unified framework, (*iii*) ensuring interoperable\
    \ learning to support continuous learning across the network, (*iv*) managing\
    \ AI life cycles for reliability and fairness, and (*v*) optimizing joint performance\
    \ while maintaining TinyML accuracy.\n\nCollective AI involves multi-agent collaboration,\
    \ where agents contribute to shared goals through local training without direct\
    \ communication [\\[223\\]](#page-29-54). At the platform level, collective reinforcement\
    \ learning enables agents to share models, thereby achieving short training time,\
    \ redundant learning avoidance, and robust models from limited samples [\\[224\\\
    ]](#page-29-55). At a broader implementation level, collective AI swarm intelligence\
    \ supports IoT applications in static and dynamic environments [\\[225\\]](#page-29-56),\
    \ but it requires designing a unified framework for collective dynamics and reasoning\
    \ across heterogeneous systems.\n\nAs the integration of TinyML and LargeML progresses\
    \ within collective AI systems, new opportunities will emerge alongside specific\
    \ challenges: (*i*) *Crowd retention*: Varying agent contributions require robust\
    \ incentive mechanisms to sustain participation; (*ii*) *Spectrum utilization*:\
    \ Dense IoT networks strain spectrum resources, necessitating optimized bandwidth\
    \ allocation, especially with 6G; (*iii*) *Security, trust, and privacy*: Sharing\
    \ training outputs raises concerns, with blockchain's resource demands limiting\
    \ its use on constrained devices, while human-AI collaboration needs better social\
    \ cue interpretation, requiring advanced cognitive architectures; and (*iv*) *Ethical\
    \ issues*: Balancing TinyML and LargeML benefits with risks involves addressing\
    \ transparency, bias, and regulatory gaps. Errors in LargeML outputs could have\
    \ significant consequences, underscoring the need for legal frameworks to ensure\
    \ accountability and trust.\n\n#### VII. CONCLUSION\n\n<span id=\"page-26-23\"\
    ></span>TinyML–LargeML integration is set to revolutionize the development of\
    \ advanced AI solutions in 6G and future networks. These technologies address\
    \ the critical issues of resource allocation, network management, security, and\
    \ privacy. This survey provides a comprehensive examination of the convergence\
    \ of TinyML and LargeML technologies, beginning with a detailed review of design\
    \ considerations, operational mechanisms, and performance evaluation methods.\
    \ The survey then investigates the motivations and technical requirements driving\
    \ the convergence of these systems toward 6G and beyond, presenting efficient\
    \ bidirectional integration strategies and highlighting application scenarios.\
    \ This extensive review identifies the challenges persistent throughout the literature\
    \ and outlines numerous promising research directions, including standardization,\
    \ resource orchestration, advanced security and privacy-preserving techniques,\
    \ real-time and lightweight AI processing, and the adoption of AI-native and collective\
    \ AI paradigms. Sustained research in this domain is essential. This survey aims\
    \ to support and inform the ongoing work in providing a foundation for future\
    \ innovations and advancements in 6G systems.\n\n#### REFERENCES\n\n- <span id=\"\
    page-26-0\"></span>[1] N.-N. Dao *et al.*, \"A review on new technologies in 3GPP\
    \ standards for 5G access and beyond,\" *Comput. Netw.*, p. 110370, 2024.\n- <span\
    \ id=\"page-26-1\"></span>[2] S. Zhang *et al.*, \"Towards artificial intelligence\
    \ enabled 6G: State of the art, challenges, and opportunities,\" *Comput. Netw.*,\
    \ vol. 183, p. 107556, 2020.\n- <span id=\"page-26-2\"></span>[3] Y. Geng *et\
    \ al.*, \"Computer vision and natural language processing,\" in *Practical Mach.\
    \ Learn. Illustrated KNIME*. Springer, 2024, pp. 275–304.\n- <span id=\"page-26-3\"\
    ></span>[4] J. Lin *et al.*, \"Tiny machine learning: Progress and futures [feature],\"\
    \ *IEEE Circuits Syst. Mag.*, vol. 23, no. 3, pp. 8–34, 2023.\n- <span id=\"page-26-4\"\
    ></span>[5] M. Xu *et al.*, \"When large language model agents meet 6G networks:\
    \ Perception, grounding, and alignment,\" *IEEE Wireless Commun.*, 2024.\n- <span\
    \ id=\"page-26-5\"></span>[6] Z. Zhu *et al.*, \"Data-free knowledge distillation\
    \ for heterogeneous federated learning,\" in *Proc. PMLR Int. Conf. Mac. Learn.*,\
    \ 2021, pp. 12 878–12 889.\n- <span id=\"page-26-6\"></span>[7] V. Tsoukas *et\
    \ al.*, \"A review of machine learning and TinyML in healthcare,\" in *Proc. ACM\
    \ Pan-Hellenic Conf. Inform.*, 2021, pp. 69– 73.\n- <span id=\"page-26-7\"></span>[8]\
    \ H.-T. Nguyen *et al.*, \"Behind-the-ear EEG-based wearable driver drowsiness\
    \ detection system using embedded tiny neural networks,\" *IEEE Sensors J.*, vol.\
    \ 23, no. 19, pp. 23 875–23 892, 2023.\n- <span id=\"page-26-8\"></span>[9] Y.\
    \ Wang *et al.*, \"Large model empowered metaverse: State-of-the-art, challenges\
    \ and opportunities,\" *arXiv:2502.10397*, 2025.\n- <span id=\"page-26-9\"></span>[10]\
    \ L. Dutta *et al.*, \"TinyML meets IoT: A comprehensive survey,\" *Internet of\
    \ Things*, vol. 16, p. 100461, 2021.\n- <span id=\"page-26-12\"></span>[11] Y.\
    \ Abadade *et al.*, \"A comprehensive survey on TinyML,\" *IEEE Access*, vol.\
    \ 11, pp. 96 892–96 922, 2023.\n- <span id=\"page-26-11\"></span>[12] P. P. Ray,\
    \ \"A review on TinyML: State-of-the-art and prospects,\" *J. King Saud Univ.-Comput.\
    \ Inf. Sci.*, vol. 34, no. 4, pp. 1595–1623, 2022.\n- <span id=\"page-26-14\"\
    ></span>[13] V. Rajapakse *et al.*, \"Intelligence at the extreme edge: A survey\
    \ on reformable TinyML,\" *ACM Comput. Surv.*, vol. 55, no. 13s, pp. 1–30, 2023.\n\
    - <span id=\"page-26-13\"></span>[14] V. Tsoukas *et al.*, \"A review on the emerging\
    \ technology of TinyML,\" *ACM Comput. Surv.*, 2024.\n- <span id=\"page-26-10\"\
    ></span>[15] L. Capogrosso *et al.*, \"A machine learning-oriented survey on tiny\
    \ machine learning,\" *IEEE Access*, pp. 23 406–23 426, 2024.\n- <span id=\"page-26-15\"\
    ></span>[16] M. A. K. Raiaan *et al.*, \"A review on large language models: Architectures,\
    \ applications, taxonomies, open issues and challenges,\" *IEEE Access*, vol.\
    \ 12, pp. 26 839–26 874, 2024.\n- <span id=\"page-26-16\"></span>[17] Y. Chang\
    \ *et al.*, \"A survey on evaluation of large language models,\" *ACM Trans. Intell.\
    \ Syst. Technol.*, vol. 15, no. 3, pp. 1–45, 2024.\n- <span id=\"page-26-17\"\
    ></span>[18] Z. Han *et al.*, \"Parameter-efficient fine-tuning for large models:\
    \ A comprehensive survey,\" *arXiv:2403.14608*, 2024.\n- <span id=\"page-26-18\"\
    ></span>[19] H. Zhao *et al.*, \"Explainability for large language models: A survey,\"\
    \ *ACM Trans. Intell. Syst. Technol.*, vol. 15, no. 2, pp. 1–38, 2024.\n- <span\
    \ id=\"page-26-19\"></span>[20] B. Min *et al.*, \"Recent advances in natural\
    \ language processing via large pre-trained language models: A survey,\" *ACM\
    \ Comput. Surv.*, vol. 56, no. 2, pp. 1–40, 2023.\n- <span id=\"page-26-20\"></span>[21]\
    \ W. Zhuang *et al.*, \"When foundation model meets federated learning: Motivations,\
    \ challenges, and future directions,\" *arXiv:2306.15546*, 2023.\n- <span id=\"\
    page-26-21\"></span>[22] C. Ren *et al.*, \"Advances and open challenges in federated\
    \ learning with foundation models,\" *IEEE Commun. Surv. Tutor.*, early access,\
    \ 2025.\n- <span id=\"page-26-22\"></span>[23] M. Wang *et al.*, \"A survey on\
    \ large-scale machine learning,\" *IEEE Trans. Knowl. Data Eng.*, vol. 34, no.\
    \ 6, pp. 2574–2594, 2022.\n- <span id=\"page-26-27\"></span>[24] A. Vaswani, \"\
    Attention is all you need,\" *Adv. Neural Inf. Process. Syst. (NeurIPS)*, 2017.\n\
    - <span id=\"page-26-24\"></span>[25] G. Wu *et al.*, \"Consolidating TinyML lifecycle\
    \ with large language models: Reality, illusion, or opportunity?\" *arXiv:2501.12420*,\
    \ 2025.\n- <span id=\"page-26-25\"></span>[26] B. Rokh *et al.*, \"A comprehensive\
    \ survey on model quantization for deep neural networks in image classification,\"\
    \ *ACM Trans. Intell. Syst. Technol.*, vol. 14, no. 6, pp. 1–50, 2023.\n- <span\
    \ id=\"page-26-26\"></span>[27] M. Zhu *et al.*, \"To prune, or not to prune:\
    \ Exploring the efficacy of pruning for model compression,\" *arXiv:1710.01878*,\
    \ 2017.\n- <span id=\"page-26-28\"></span>[28] M. Udell *et al.*, \"Generalized\
    \ low rank models,\" *Found. Trends Mach. Learn.*, vol. 9, no. 1, pp. 1–118, 2016.\n\
    - <span id=\"page-26-29\"></span>[29] J. Ott *et al.*, \"Learning in the machine:\
    \ To share or not to share?\" *Neural Netw.*, vol. 126, pp. 235–249, 2020.\n-\
    \ <span id=\"page-26-30\"></span>[30] M. T. Leˆ *et al.*, \"Efficient neural networks\
    \ for tiny machine learning: A comprehensive review,\" *arXiv:2311.11883*, 2023.\n\
    - <span id=\"page-26-31\"></span>[31] A. Moffat, \"Huffman coding,\" *ACM Comput.\
    \ Surv. (CSUR)*, vol. 52, no. 4, pp. 1–35, 2019.\n- <span id=\"page-26-32\"></span>[32]\
    \ M. Feurer *et al.*, *Hyperparameter Optim.* Springer, 2019.\n- <span id=\"page-26-33\"\
    ></span>[33] C. Yang *et al.*, \"Categories of response-based, feature-based,\
    \ and relation-based knowledge distillation,\" in *Adv. Knowl. Distillation: Towards\
    \ New Horizons Intell. Syst.* Springer, 2023, pp. 1–32.\n- <span id=\"page-26-34\"\
    ></span>[34] Z. Huang *et al.*, \"RIOT-ML: Toolkit for over-the-air secure updates\
    \ and performance evaluation of TinyML models,\" *Ann. Telecommun.*, pp. 1–15,\
    \ 2024.\n- <span id=\"page-26-35\"></span>[35] B. Sudharsan *et al.*, \"OTA-TinyML:\
    \ Over the air deployment of TinyML models and execution on IoT devices,\" *IEEE\
    \ Internet Comput.*, vol. 26, no. 3, pp. 69–78, 2022.\n- <span id=\"page-26-36\"\
    ></span>[36] E. Ostrovan, *TinyML on-device neural network training*. Milan, Italy:\
    \ Master's Thesis, Politecnico di Milano, 2022.\n- <span id=\"page-26-37\"></span>[37]\
    \ R. David *et al.*, \"Tensorflow lite micro: Embedded machine learning for TinyML\
    \ systems,\" *Proc. Mach. Learn. Syst.*, vol. 3, pp. 800–811, 2021.\n- <span id=\"\
    page-26-38\"></span>[38] H. Cai *et al.*, \"TinyTL: Reduce memory, not parameters\
    \ for efficient on-device learning,\" *Adv. Neural Inf. Process. Syst.*, vol.\
    \ 33, pp. 11 285– 11 297, 2020.\n- <span id=\"page-26-39\"></span>[39] A. Sabovic\
    \ *et al.*, \"Towards energy-aware TinyML on battery-less IoT devices,\" *Internet\
    \ of Things*, vol. 22, p. 100736, 2023.\n- <span id=\"page-26-40\"></span>[40]\
    \ M. Le *et al.*, \"Applications of distributed machine learning for the internet-of-things:\
    \ A comprehensive survey,\" *IEEE Commun. Surv. Tutor.*, early access, Jul. 12,\
    \ 2024, doi: 10.1109/COMST.2024.3427324.\n- <span id=\"page-26-41\"></span>[41]\
    \ Q. Huang *et al.*, \"Data collection and labeling techniques for machine learning,\"\
    \ *arXiv:2407.12793*, 2024.\n- <span id=\"page-26-42\"></span>[42] I. Muraina,\
    \ \"Ideal dataset splitting ratios in machine learning algorithms: General concerns\
    \ for data scientists and data analysts,\" in *7th Int. Mardin Artuklu Sci. Res.\
    \ Conf.*, 2022, pp. 496–504.\n- <span id=\"page-26-43\"></span>[43] J. Lee *et\
    \ al.*, \"Pre-training of deep bidirectional transformers for language understanding,\"\
    \ *arXiv:1810.04805*, vol. 3, no. 8, 2018.\n- <span id=\"page-26-44\"></span>[44]\
    \ A. Radford *et al.*, \"Language models are unsupervised multitask learners,\"\
    \ *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.\n- <span id=\"page-26-45\"></span>[45]\
    \ B. Zhang *et al.*, \"Examining scaling and transfer of language model architectures\
    \ for machine translation,\" in *Proc. PMLR Int. Conf. Mac. Learn.*, 2022, pp.\
    \ 26 176–26 192.\n- <span id=\"page-26-46\"></span>[46] W. Fedus *et al.*, \"\
    Switch transformers: Scaling to trillion parameter models with simple and efficient\
    \ sparsity,\" *J. Mach. Learn. Res.*, vol. 23, no. 120, pp. 1–39, 2022.\n- <span\
    \ id=\"page-26-47\"></span>[47] N. Du *et al.*, \"Glam: Efficient scaling of language\
    \ models with mixtureof-experts,\" in *Proc. PMLR Int. Conf. Mac. Learn.*, 2022,\
    \ pp. 5547– 5569.\n- <span id=\"page-26-48\"></span>[48] V.-T. Tran *et al.*,\
    \ \"Revisiting sparse mixture of experts for resourceadaptive federated fine-tuning\
    \ foundation models,\" in *Proc. ICLR 2025 Mod. Collab. Decent. Cont. Deep Learn.\
    \ Workshop.*, Mar. 2025.\n- <span id=\"page-26-49\"></span>[49] A. Gu *et al.*,\
    \ \"Efficiently modeling long sequences with structured state spaces,\" *arXiv:2111.00396*,\
    \ 2021.\n- <span id=\"page-26-50\"></span>[50] Y. Liu *et al.*, \"Understanding\
    \ LLMs: A comprehensive overview from training to inference,\" *Neurocomputing*,\
    \ p. 129190, 2024.\n- <span id=\"page-27-0\"></span>[51] C. Wei *et al.*, \"An\
    \ overview of language models: Recent developments and outlook,\" *APSIPA Trans.\
    \ Signal Inf. Process.*, vol. 13, no. 2, 2024.\n- <span id=\"page-27-1\"></span>[52]\
    \ X. Hou *et al.*, \"Large language models for software engineering: A systematic\
    \ literature review,\" *ACM Trans. Softw. Eng. Methodology*, vol. 33, no. 8, pp.\
    \ 1–79, 2024.\n- <span id=\"page-27-2\"></span>[53] N. Houlsby *et al.*, \"Parameter-efficient\
    \ transfer learning for NLP,\" in *Proc. PMLR Int. Conf. Mac. Learn.*, 2019, pp.\
    \ 2790–2799.\n- <span id=\"page-27-3\"></span>[54] E. J. Hu *et al.*, \"LoRA:\
    \ Low-rank adaptation of large language models,\" in *Int. Conf. Learn. Representations*,\
    \ 2022.\n- <span id=\"page-27-4\"></span>[55] \"AI model performance metrics:\
    \ in-depth guide.\" [Online]. Available: [https://nebius.com/blog/posts/ai-model-performance-metrics?form=](https://nebius.com/blog/posts/ai-model-performance-metrics?form=MG0AV3)\
    \ [MG0AV3](https://nebius.com/blog/posts/ai-model-performance-metrics?form=MG0AV3)\n\
    - <span id=\"page-27-5\"></span>[56] J. Lin *et al.*, \"On-device training under\
    \ 256KB memory,\" *Adv. Neural Inf. Process. Syst.*, vol. 35, pp. 22 941–22 954,\
    \ 2022.\n- <span id=\"page-27-6\"></span>[57] \"State of the TinyAutoML Market\
    \ 2022,\" TinyML Foundation, Altos, CA, USA, Jun. 2020.\n- <span id=\"page-27-7\"\
    ></span>[58] S. Ioffe *et al.*, \"Batch normalization: Accelerating deep network\
    \ training by reducing internal covariate shift,\" in *Proc. PMLR Int. Conf. Mac.\
    \ Learn.*, Lille, France, 2015, pp. 448–456.\n- <span id=\"page-27-8\"></span>[59]\
    \ A. Tuama *et al.*, \"Camera model identification with the use of deep convolutional\
    \ neural networks,\" in *Proc. IEEE Int. Workshop Inf. Forensics Secur. (WIFS)*,\
    \ Abu Dhabi, United Arab Emirates, 2016, pp. 1–6.\n- <span id=\"page-27-9\"></span>[60]\
    \ M. Y. Shabir *et al.*, \"Toward secure TinyML on a standardized AI architecture,\"\
    \ in *Device-Edge-Cloud Continuum: Paradigms, Architectures Appl.* Springer, 2023,\
    \ pp. 121–139.\n- <span id=\"page-27-10\"></span>[61] A. Koubaa, \"GPT-4 vs. GPT-3.5:\
    \ A concise showdown,\" *preprints.org*, 2023.\n- <span id=\"page-27-11\"></span>[62]\
    \ S. A. Mohammed *et al.*, \"Artificial intelligence-based distributed network\
    \ latency measurement,\" in *Proc. IEEE Int. Instrum. Meas. Technol. Conf. (I2MTC)*,\
    \ Auckland, New Zealand, 2019, pp. 1–6.\n- <span id=\"page-27-12\"></span>[63]\
    \ K. Grace *et al.*, \"Thousands of AI authors on the future of AI,\" *arXiv:2401.02843*,\
    \ 2024.\n- <span id=\"page-27-13\"></span>[64] H. Barmer *et al.*, \"Scalable\
    \ AI,\" *Nat. AI Eng. Initiative*, 2021.\n- <span id=\"page-27-14\"></span>[65]\
    \ P. Regulation, \"General data protection regulation,\" *Intouch*, vol. 25, pp.\
    \ 1–5, 2018.\n- <span id=\"page-27-15\"></span>[66] N. Ding *et al.*, \"Parameter-efficient\
    \ fine-tuning of large-scale pretrained language models,\" *Nature Mach. Intell.*,\
    \ vol. 5, no. 3, pp. 220– 235, 2023.\n- <span id=\"page-27-16\"></span>[67] S.\
    \ Dilmaghani *et al.*, \"Privacy and security of big data in AI systems: A research\
    \ and standards perspective,\" in *Proc. IEEE Int. Conf. Big Data (BigData)*,\
    \ Los Angeles, CA, USA, 2019, pp. 5737–5743.\n- <span id=\"page-27-17\"></span>[68]\
    \ R. Shokri *et al.*, \"Membership inference attacks against machine learning\
    \ models,\" in *Proc. IEEE Symp. Secur. Privacy (SP)*, San Jose, CA, USA, 2017,\
    \ pp. 3–18.\n- <span id=\"page-27-18\"></span>[69] S. G. Finlayson *et al.*, \"\
    Adversarial attacks on medical machine learning,\" *Science*, vol. 363, no. 6433,\
    \ pp. 1287–1289, 2019.\n- <span id=\"page-27-19\"></span>[70] R. Liu *et al.*,\
    \ \"Beginning of the journey toward 6G: Vision and framework,\" *IEEE Commun.\
    \ Mag.*, vol. 61, no. 10, pp. 8–9, 2023.\n- <span id=\"page-27-20\"></span>[71]\
    \ ITU-R WP5D, \"Future technology trends of terrestrial International Mobile Telecommunications\
    \ systems towards 2030 and beyond,\" 2022. [Online]. Available: <https://www.itu.int/pub/R-REP-M.2516>\n\
    - <span id=\"page-27-21\"></span>[72] E. Mihret *et al.*, \"4G, 5G, 6G, 7G and\
    \ future mobile technologies,\" *J. Comp. Sci. Info. Technol.*, vol. 9, no. 2,\
    \ p. 75, 2021.\n- <span id=\"page-27-22\"></span>[73] R. Liu *et al.*, \"A vision\
    \ and an evolutionary framework for 6G: Scenarios, capabilities and enablers,\"\
    \ *arXiv:2305.13887*, 2023.\n- <span id=\"page-27-23\"></span>[74] S. Ebrahimi\
    \ *et al.*, \"Resource management from single-domain 5G to end-to-end 6G network\
    \ slicing: A survey,\" *IEEE Commun. Surv. Tutor.*, early access, Apr. 17, 2024,\
    \ doi: 10.1109/COMST.2024.3390613.\n- <span id=\"page-27-24\"></span>[75] Z. Lin\
    \ *et al.*, \"Split learning in 6G edge networks,\" *IEEE Wireless Commun.*, early\
    \ access, May 13, 2024, doi: 10.1109/MWC.014.2300319.\n- <span id=\"page-27-25\"\
    ></span>[76] N.-N. Dao *et al.*, \"Survey on aerial radio access networks: Toward\
    \ a comprehensive 6G access infrastructure,\" *IEEE Commun. Surv. Tutor.*, vol.\
    \ 23, no. 2, pp. 1193–1225, 2021.\n- <span id=\"page-27-26\"></span>[77] ——, \"\
    Neglected infrastructures for 6G–underwater communications: How mature are they?\"\
    \ *J. Netw. Comput. Appl.*, vol. 213, p. 103595, 2023.\n- <span id=\"page-27-27\"\
    ></span>[78] S. Chen *et al.*, \"Vision, requirements, and technology trend of\
    \ 6G: How to tackle the challenges of system coverage, capacity, user datarate\
    \ and movement speed,\" *IEEE Wireless Commun.*, vol. 27, no. 2, pp. 218–228,\
    \ 2020.\n- <span id=\"page-27-28\"></span>[79] H. Yang *et al.*, \"Artificial-intelligence-enabled\
    \ intelligent 6G networks,\" *IEEE Netw.*, vol. 34, no. 6, pp. 272–280, 2020.\n\
    - <span id=\"page-27-29\"></span>[80] P. S. Bouzinis *et al.*, \"Wireless federated\
    \ learning (WFL) for 6G networks–Part I: Research challenges and future trends,\"\
    \ *IEEE Commun. Lett.*, vol. 26, no. 1, pp. 3–7, 2021.\n- <span id=\"page-27-30\"\
    ></span>[81] ITU-R, \"M.2160: Framework and overall objectives of the future development\
    \ of IMT for 2030 and beyond,\" 2023. [Online]. Available: <https://www.itu.int/rec/R-REC-M.2160/en>\n\
    - <span id=\"page-27-31\"></span>[82] L. Huawei *et al.*, \"6G: The next horizon\
    \ from connected people and things to connected intelligence,\" *Huawei, White\
    \ Paper*, 2022.\n- <span id=\"page-27-32\"></span>[83] V.-L. Nguyen *et al.*,\
    \ \"Security and privacy for 6G: A survey on prospective technologies and challenges,\"\
    \ *IEEE Commun. Surv. Tutor.*, vol. 23, no. 4, pp. 2384–2428, 2021.\n- <span id=\"\
    page-27-33\"></span>[84] T. Huang *et al.*, \"A survey on green 6G network: Architecture\
    \ and technologies,\" *IEEE Access*, vol. 7, pp. 175 758–175 768, 2019.\n- <span\
    \ id=\"page-27-34\"></span>[85] F. Liu *et al.*, \"Integrated sensing and communications:\
    \ Toward dualfunctional wireless networks for 6G and beyond,\" *IEEE J. Sel. Areas\
    \ Commun.*, vol. 40, no. 6, pp. 1728–1767, 2022.\n- <span id=\"page-27-35\"></span>[86]\
    \ M. Wang *et al.*, \"Transfer learning promotes 6G wireless communications: Recent\
    \ advances and future challenges,\" *IEEE Trans. Rel.*, vol. 70, no. 2, pp. 790–807,\
    \ 2021.\n- <span id=\"page-27-36\"></span>[87] Y. Tao *et al.*, \"A hybrid cloud\
    \ and edge control strategy for demand responses using deep reinforcement learning\
    \ and transfer learning,\" *IEEE Trans. Cloud Comput.*, vol. 10, no. 1, pp. 56–71,\
    \ 2021.\n- <span id=\"page-27-37\"></span>[88] Y. Jang *et al.*, \"Learning what\
    \ and where to transfer,\" in *Proc. PMLR Int. Conf. Mac. Learn.*, 2019, pp. 3030–3039.\n\
    - <span id=\"page-27-41\"></span>[89] S. J. Pan and Q. Yang, \"A survey on transfer\
    \ learning,\" *IEEE Trans. Knowl. Data Eng.*, vol. 22, no. 10, pp. 1345–1359,\
    \ 2010.\n- <span id=\"page-27-38\"></span>[90] C. T. Nguyen *et al.*, \"Transfer\
    \ learning for wireless networks: A comprehensive survey,\" *Proc. IEEE.*, vol.\
    \ 110, no. 8, pp. 1073–1115, 2022.\n- <span id=\"page-27-39\"></span>[91] S. Parsaeefard\
    \ *et al.*, \"Efficient transfer learning in 6G,\" in *Proc. IEEE Future Netw.\
    \ World Forum*, Montreal, QC, Canada, 2022, pp. 314–319.\n- <span id=\"page-27-40\"\
    ></span>[92] A. Thantharate *et al.*, \"ADAPTIVE6G: Adaptive resource management\
    \ for network slicing architectures in current 5G and future 6G systems,\" *J.\
    \ Netw. Syst. Manage.*, vol. 31, no. 1, p. 9, 2023.\n- <span id=\"page-27-42\"\
    ></span>[93] T. Wang *et al.*, \"Instance-based deep transfer learning,\" in *Proc.\
    \ IEEE Winter Conf. Appl. Comput. Vision*, Waikoloa, HI, USA, 2019, pp. 367–375.\n\
    - <span id=\"page-27-43\"></span>[94] X. Zhong *et al.*, \"Feature-based transfer\
    \ learning based on distribution similarity,\" *IEEE Access*, vol. 6, pp. 35 551–35\
    \ 557, 2018.\n- <span id=\"page-27-44\"></span>[95] W. Zheng *et al.*, \"Disruption\
    \ prediction for future tokamaks using parameter-based transfer learning,\" *Commun.\
    \ Phys.*, vol. 6, no. 1, p. 181, 2023.\n- <span id=\"page-27-45\"></span>[96]\
    \ E. I. V. Salgado *et al.*, \"Relational-based transfer learning for automatic\
    \ optical inspection based on domain discrepancy,\" in *Optoelectronic Imag. Multimedia\
    \ Technol. IX*, vol. 12317. SPIE, 2023, pp. 239–250.\n- <span id=\"page-27-46\"\
    ></span>[97] M. Ghifary *et al.*, \"Domain adaptive neural networks for object\
    \ recognition,\" in *Proc. Pacific Rim Int. Conf. Artif. Intell.*, Gold Coast,\
    \ Dec. 2014, pp. 898–904.\n- <span id=\"page-27-47\"></span>[98] B. Sun *et al.*,\
    \ \"Deep CORAL: Correlation alignment for deep domain adaptation,\" in *Proc.\
    \ European Conf. Comput. Vision Worksh.*, Amsterdam, Oct. 2016, pp. 443–450.\n\
    - <span id=\"page-27-48\"></span>[99] C. Profentzas *et al.*, \"MicroTL: Transfer\
    \ learning on low-power IoT devices,\" in *Proc. IEEE 47th Conf. Local Comput.\
    \ Netw.*, Edmonton, AB, Canada, 2022, pp. 1–8.\n- <span id=\"page-27-50\"></span>[100]\
    \ M. B. Azevedo *et al.*, \"Detecting face masks through embedded machine learning\
    \ algorithms: A transfer learning approach for affordable microcontrollers,\"\
    \ *Mach. Learn. Appl.*, vol. 14, p. 100498, 2023.\n- <span id=\"page-27-51\"></span>[101]\
    \ A. M. Hayajneh *et al.*, \"Tiny machine learning on the edge: A framework for\
    \ transfer learning empowered unmanned aerial vehicle assisted smart farming,\"\
    \ *IET Smart Cities*, vol. 6, no. 1, pp. 10–26, 2023.\n- <span id=\"page-27-52\"\
    ></span>[102] ——, \"TinyML empowered transfer learning on the edge,\" *IEEE Open\
    \ J. Commun. Soc.*, 2024.\n- <span id=\"page-27-49\"></span>[103] Y. D. Kwon *et\
    \ al.*, \"TinyTrain: Resource-aware task-adaptive sparse training of DNNs at the\
    \ data-scarce edge,\" in *Proc. PMLR 41st Int. Conf. Mach. Learn.*, Vienna, Austria,\
    \ 2024.\n- <span id=\"page-27-53\"></span>[104] H. Yang *et al.*, \"FedSteg: A\
    \ federated transfer learning framework for secure image steganalysis,\" *IEEE\
    \ Trans. Netw. Sci. Eng.*, vol. 8, no. 2, pp. 1084–1094, 2021.\n- <span id=\"\
    page-27-54\"></span>[105] T.-B. Nguyen *et al.*, \"Federated domain generalization\
    \ with data-free on-server gradient matching,\" in *Int. Conf. Learn. Representations\
    \ (ICLR)*, 2025.\n- <span id=\"page-27-55\"></span>[106] B. McMahan *et al.*,\
    \ \"Communication-efficient learning of deep networks from decentralized data,\"\
    \ in *Proc. PMLR Artif. Intell. Statist.*, 2017, pp. 1273–1282.\n- <span id=\"\
    page-27-56\"></span>[107] Z. Zhao *et al.*, \"Federated learning with non-IID\
    \ data in wireless networks,\" *IEEE Trans. Wireless Commun.*, vol. 21, no. 3,\
    \ pp. 1927– 1942, 2022.\n- <span id=\"page-27-57\"></span>[108] K. Kopparapu *et\
    \ al.*, \"TinyFedTL: Federated transfer learning on ubiquitous tiny IoT devices,\"\
    \ in *Proc. IEEE Int. Conf. Pervasive Comput. Commun. Worksh.*, Pisa, Italy, 2022,\
    \ pp. 79–81.\n- <span id=\"page-28-1\"></span>[109] Y. Chen *et al.*, \"FedHealth:\
    \ A federated transfer learning framework for wearable healthcare,\" *IEEE Intell.\
    \ Syst.*, vol. 35, no. 4, pp. 83–93, 2020.\n- <span id=\"page-28-2\"></span>[110]\
    \ W. Guo *et al.*, \"Federated transfer learning for auxiliary classifier generative\
    \ adversarial networks: Framework and industrial application,\" *J. Intell. Manuf.*,\
    \ vol. 35, no. 4, pp. 1439–1454, 2024.\n- <span id=\"page-28-0\"></span>[111]\
    \ M. Ficco *et al.*, \"Federated learning for IoT devices: Enhancing TinyML with\
    \ on-board training,\" *Inf. Fusion*, vol. 104, p. 102189, 2024.\n- <span id=\"\
    page-28-3\"></span>[112] Lewis Van Winkle, \"C neural network library: Genann,\"\
    \ Accessed: Aug. 18, 2024. [Online]. Available: <https://codeplea.com/genann>\n\
    - <span id=\"page-28-4\"></span>[113] Y. Fan *et al.*, \"IoTDefender: A federated\
    \ transfer learning intrusion detection framework for 5G IoT,\" in *Proc. IEEE\
    \ 14th Int. Conf. Big Data Sci. Eng. (BigDataSE)*, Guangzhou, China, 2020, pp.\
    \ 88–95.\n- <span id=\"page-28-5\"></span>[114] M. A. P. Putra *et al.*, \"HFTL:\
    \ Hierarchical federated transfer learning for secure and efficient fault classification\
    \ in additive manufacturing,\" *IEEE Access*, vol. 11, pp. 54 795–54 807, 2023.\n\
    - <span id=\"page-28-6\"></span>[115] X. Liu *et al.*, \"Federated learning and\
    \ meta learning: Approaches, applications, and directions,\" *IEEE Commun. Surv.\
    \ Tutor.*, vol. 26, no. 1, pp. 571–618, 2023.\n- <span id=\"page-28-7\"></span>[116]\
    \ H. Ren *et al.*, \"TinyReptile: TinyML with federated meta-learning,\" in *Proc.\
    \ IEEE Int. Joint Conf. Neural Netw. (IJCNN)*, Gold Coast, Australia, 2023, pp.\
    \ 1–9.\n- <span id=\"page-28-8\"></span>[117] ——, \"TinyMetaFed: Efficient federated\
    \ meta-learning for TinyML,\" *arXiv:2307.06822*, 2023.\n- <span id=\"page-28-9\"\
    ></span>[118] ——, \"On-device online learning and semantic management of TinyML\
    \ systems,\" *ACM Trans. Embedded Comput. Syst.*, vol. 23, no. 4, pp. 1–32, 2024.\n\
    - <span id=\"page-28-10\"></span>[119] Z. Jia *et al.*, \"Personalized meta-federated\
    \ learning for IoT-enabled health monitoring,\" *IEEE Trans. Computer-Aided Design\
    \ Integr. Circuits Syst.*, early access, Apr. 15, 2024.\n- <span id=\"page-28-11\"\
    ></span>[120] Z. Gao *et al.*, \"FedPT: Federated proxy-tuning of large language\
    \ models on resource-constrained edge devices,\" *arXiv:2410.00362*, 2024.\n-\
    \ <span id=\"page-28-12\"></span>[121] Y. J. Cho *et al.*, \"Heterogeneous LoRA\
    \ for federated fine-tuning of on-device foundation models,\" *arXiv:2401.06432*,\
    \ 2024.\n- <span id=\"page-28-13\"></span>[122] K. Pfeiffer *et al.*, \"Efficient\
    \ federated finetuning of tiny transformers with resource-constrained devices,\"\
    \ *arXiv:2411.07826*, 2024.\n- <span id=\"page-28-14\"></span>[123] S. K. Atapour\
    \ *et al.*, \"Leveraging foundation models for efficient federated learning in\
    \ resource-restricted edge networks,\" *arXiv:2409.09273*, 2024.\n- <span id=\"\
    page-28-15\"></span>[124] M. Xu *et al.*, \"FwdLLM: Efficient federated finetuning\
    \ of large language models with perturbed inferences,\" in *2024 USENIX Annu.\
    \ Tech. Conf.*, 2024, pp. 579–596.\n- <span id=\"page-28-16\"></span>[125] P.\
    \ Wu *et al.*, \"FedFMSL: Federated learning of foundations models with sparsely\
    \ activated LoRA,\" *IEEE Trans. Mobile Comput.*, vol. 23, no. 12, pp. 15 167–15\
    \ 181, Dec. 2024.\n- <span id=\"page-28-17\"></span>[126] H. Mei *et al.*, \"\
    FedMoE: Personalized federated learning via heterogeneous mixture of experts,\"\
    \ *arXiv:2408.11304*, 2024.\n- <span id=\"page-28-18\"></span>[127] P. Vepakomma\
    \ *et al.*, \"Split learning for health: Distributed deep learning without sharing\
    \ raw patient data,\" *arXiv:1812.00564*, 2018.\n- <span id=\"page-28-19\"></span>[128]\
    \ M. P. Perrone *et al.*, \"Optimal mini-batch size selection for fast gradient\
    \ descent,\" *arXiv:1911.06459*, 2019.\n- <span id=\"page-28-20\"></span>[129]\
    \ M. Li *et al.*, \"Efficient mini-batch training for stochastic optimization,\"\
    \ in *Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining*, 2014, pp.\
    \ 661–670.\n- <span id=\"page-28-21\"></span>[130] J. Lee *et al.*, \"Wireless\
    \ channel adaptive DNN split inference for resource-constrained edge devices,\"\
    \ *IEEE Commun. Lett.*, vol. 27, no. 6, pp. 1520–1524, 2023.\n- <span id=\"page-28-22\"\
    ></span>[131] E. Samikwa *et al.*, \"ARES: Adaptive resource-aware split learning\
    \ for internet of things,\" *Comput. Netw.*, vol. 218, p. 109380, 2022.\n- <span\
    \ id=\"page-28-23\"></span>[132] Z. Li *et al.*, \"Adaptive split learning over\
    \ energy-constrained wireless edge networks,\" *arXiv:2403.05158*, 2024.\n- <span\
    \ id=\"page-28-24\"></span>[133] S. Wang *et al.*, \"HiveMind: Towards cellular\
    \ native machine learning model splitting,\" *IEEE J. Sel. Areas Commun.*, vol.\
    \ 40, no. 2, pp. 626– 640, 2022.\n- <span id=\"page-28-26\"></span>[134] J. Tirana\
    \ *et al.*, \"The role of compute nodes in privacy-aware decentralized AI,\" in\
    \ *Proc. ACM 6th Int. Workshop Embedded Mobile Deep Learn.*, Portland, Oregon,\
    \ 2022, pp. 19–24.\n- <span id=\"page-28-25\"></span>[135] Y. Cao *et al.*, \"\
    Learning-based multi-tier split computing for efficient convergence of communication\
    \ and computation,\" *IEEE Internet Things J.*, 2024, early access, Jun. 25, 2024.\n\
    - <span id=\"page-28-27\"></span>[136] S. Lyu *et al.*, \"Optimal resource allocation\
    \ for u-shaped parallel split learning,\" in *Proc. IEEE Globecom Worksh.*, Kuala\
    \ Lumpur, Malaysia, 2023, pp. 197–202.\n- <span id=\"page-28-28\"></span>[137]\
    \ P. Joshi *et al.*, \"Splitfed learning without client-side synchronization:\
    \ Analyzing client-side split network portion size to overall performance,\" *arXiv:2109.09246*,\
    \ 2021.\n- <span id=\"page-28-29\"></span>[138] M. Zhang *et al.*, \"Resource-efficient\
    \ parallel split learning in heterogeneous edge computing,\" *arXiv:2403.15815*,\
    \ 2024.\n- <span id=\"page-28-30\"></span>[139] Z. Lin *et al.*, \"Efficient parallel\
    \ split learning over resource-constrained wireless edge networks,\" *IEEE Trans.\
    \ Mobile Comput.*, early access, Jan. 26, 2024.\n- <span id=\"page-28-31\"></span>[140]\
    \ J. Jeon *et al.*, \"Privacy-sensitive parallel split learning,\" in *Proc. IEEE\
    \ Int. Conf. Inf. Netw.*, Barcelona, Spain, 2020, pp. 7–9.\n- <span id=\"page-28-33\"\
    ></span>[141] A. Chopra *et al.*, \"Adaptive split learning,\" in *Federated Learning\
    \ Systems (FLSys) Workshop@ MLSys 2023*, 2023.\n- <span id=\"page-28-32\"></span>[142]\
    \ M. Kim *et al.*, \"A bargaining game for personalized, energy efficient split\
    \ learning over wireless networks,\" in *Proc. IEEE Wireless Commun. Netw. Conf.*,\
    \ Glasgow, United Kingdom, 2023, pp. 1–6.\n- <span id=\"page-28-34\"></span>[143]\
    \ J. Tirana *et al.*, \"Workflow optimization for parallel split learning,\" *arXiv:2402.10092*,\
    \ 2024.\n- <span id=\"page-28-35\"></span>[144] ——, \"MP-SL: Multihop parallel\
    \ split learning,\" *arXiv:2402.00208*, 2024.\n- <span id=\"page-28-36\"></span>[145]\
    \ W. Wu *et al.*, \"Split learning over wireless networks: Parallel design and\
    \ resource management,\" *IEEE J. Sel. Areas Commun.*, vol. 41, no. 4, pp. 1051–1066,\
    \ 2023.\n- <span id=\"page-28-37\"></span>[146] C. Thapa *et al.*, \"SplitFed:\
    \ When federated learning meets split learning,\" in *Proc. AAAI Conf. Artif.\
    \ Intell.*, vol. 36, no. 8, 2022, pp. 8485–8493.\n- <span id=\"page-28-38\"></span>[147]\
    \ ——, \"Advancements of federated learning towards privacy preservation: From\
    \ federated learning to split learning,\" *Federated Learning Systems: Towards\
    \ Next-Generation AI*, pp. 79–109, 2021.\n- <span id=\"page-28-39\"></span>[148]\
    \ Z. Lin *et al.*, \"AdaptSFL: Adaptive split federated learning in resourceconstrained\
    \ edge networks,\" *arXiv:2403.13101*, 2024.\n- <span id=\"page-28-41\"></span>[149]\
    \ W. Ni *et al.*, \"FedSL: Federated split learning for collaborative healthcare\
    \ analytics on resource-constrained wearable IoMT devices,\" *IEEE Internet Things\
    \ J.*, 2024.\n- <span id=\"page-28-42\"></span>[150] D.-J. Han *et al.*, \"Federated\
    \ split learning with joint personalizationgeneralization for inference-stage\
    \ optimization in wireless edge networks,\" *IEEE Trans. Mobile Comput.*, 2024.\n\
    - <span id=\"page-28-40\"></span>[151] G. Zhu *et al.*, \"ESFL: Efficient split\
    \ federated learning over resourceconstrained heterogeneous wireless devices,\"\
    \ *IEEE Internet Things J.*, 2024.\n- <span id=\"page-28-43\"></span>[152] Z.\
    \ Zhang *et al.*, \"Privacy and efficiency of communications in federated split\
    \ learning,\" *IEEE Trans. Big Data*, vol. 9, no. 5, pp. 1380–1391, 2023.\n- <span\
    \ id=\"page-28-44\"></span>[153] L. U. Khan *et al.*, \"A joint communication\
    \ and learning framework for hierarchical split federated learning,\" *IEEE Internet\
    \ Things J.*, 2024.\n- <span id=\"page-28-45\"></span>[154] S. Li *et al.*, \"\
    Synergizing foundation models and federated learning: A survey,\" *arXiv:2406.12844*,\
    \ 2024.\n- <span id=\"page-28-46\"></span>[155] L. Cao *et al.*, \"SFPrompt: Communication-efficient\
    \ split federated finetuning for large pre-trained models over resource-limited\
    \ devices,\" *arXiv:2407.17533*, 2024.\n- <span id=\"page-28-47\"></span>[156]\
    \ J. Shin *et al.*, \"FedSplitX: Federated split learning for computationallyconstrained\
    \ heterogeneous clients,\" *arXiv:2310.14579*, 2023.\n- <span id=\"page-28-48\"\
    ></span>[157] Y. Shi *et al.*, \"Heterogeneous federated learning with splited\
    \ language model,\" *arXiv:2403.16050*, 2024.\n- <span id=\"page-28-49\"></span>[158]\
    \ S. Li *et al.*, \"Incentivizing multi-tenant split federated learning for foundation\
    \ models at the network edge,\" *arXiv:2503.04971*, 2025.\n- <span id=\"page-28-50\"\
    ></span>[159] Z. Yang *et al.*, \"On privacy, security, and trustworthiness in\
    \ distributed wireless large AI models (WLAM),\" *arXiv e-prints*, pp. arXiv–2412,\
    \ 2024.\n- <span id=\"page-28-51\"></span>[160] W. Villegas-Ch *et al.*, \"Optimizing\
    \ federated learning on TinyML devices for privacy protection and energy efficiency\
    \ in IoT networks,\" *IEEE Access*, 2024.\n- <span id=\"page-28-52\"></span>[161]\
    \ J. Lin, \"Efficient deep learning computing: From TinyML to LargeLM,\" Ph.D.\
    \ dissertation, Massachusetts Institute of Technology, 2024.\n- <span id=\"page-28-53\"\
    ></span>[162] S. Arcot *et al.*, \"TinyML for cybersecurity: Deploying optimized\
    \ deep learning models for on-device threat detection on resource-constrained\
    \ devices,\" in *Proc. IEEE Int. Conf. Big Data (BigData)*, 2024, pp. 5542– 5550.\n\
    - <span id=\"page-28-54\"></span>[163] S. Yılmaz *et al.*, \"A transfer learning\
    \ approach for securing resourceconstrained IoT devices,\" *IEEE Trans. Inf. Forensics\
    \ Secur.*, vol. 16, pp. 4405–4418, 2021.\n- <span id=\"page-28-55\"></span>[164]\
    \ R. Y. Patil *et al.*, \"Securing TinyML in a connected world,\" in *TinyML Edge\
    \ Intell. IoT LPWAN Netw.* Elsevier, 2024, pp. 311–330.\n- <span id=\"page-28-56\"\
    ></span>[165] J. Buban *et al.*, \"Encrypted large model inference: The equivariant\
    \ encryption paradigm,\" *arXiv:2502.01013*, 2025.\n- <span id=\"page-28-57\"\
    ></span>[166] C. Benzaid *et al.*, \"AI-driven zero touch network and service\
    \ management in 5G and beyond: Challenges and research directions,\" *IEEE Netw.*,\
    \ vol. 34, no. 2, pp. 186–194, 2020.\n- <span id=\"page-28-58\"></span>[167] Z.\
    \ Chen *et al.*, \"Big AI models for 6G wireless networks: Opportunities, challenges,\
    \ and research directions,\" *IEEE Wireless Commun.*, 2024.\n- <span id=\"page-28-59\"\
    ></span>[168] S. Soro, \"TinyML for ubiquitous edge AI,\" *arXiv:2102.01255*,\
    \ 2021.\n- <span id=\"page-29-0\"></span>[169] O. Aouedi *et al.*, \"Deep learning\
    \ on network traffic prediction: Recent advances, analysis, and future directions,\"\
    \ *ACM Comput. Surv.*, 2024.\n- <span id=\"page-29-1\"></span>[170] I. Khan, \"\
    Edge enhanced network monitoring using TinyML,\" Master's thesis, University of\
    \ Oulu, 2024.\n- <span id=\"page-29-2\"></span>[171] D. Bega *et al.*, \"Network\
    \ slicing meets artificial intelligence: An AIbased framework for slice management,\"\
    \ *IEEE Commun. Mag.*, vol. 58, no. 6, pp. 32–38, 2020.\n- <span id=\"page-29-3\"\
    ></span>[172] M. Latah *et al.*, \"Artificial intelligence enabled software-defined\
    \ networking: A comprehensive overview,\" *IET Netw.*, vol. 8, no. 2, pp. 79–99,\
    \ 2019.\n- <span id=\"page-29-4\"></span>[173] J. Ali *et al.*, \"DDoS intrusions\
    \ detection in low power SD-IoT devices leveraging effective machine learning,\"\
    \ *IEEE Trans. Consum. Electron.*, 2024.\n- <span id=\"page-29-5\"></span>[174]\
    \ M. Nekovee *et al.*, \"Towards AI-enabled microservice architecture for network\
    \ function virtualization,\" in *Proc. IEEE 8th Int. Conf. Commun. Netw.*, 2020,\
    \ pp. 1–8.\n- <span id=\"page-29-6\"></span>[175] B. Cao *et al.*, \"Intelligent\
    \ offloading in multi-access edge computing: A state-of-the-art review and framework,\"\
    \ *IEEE Commun. Mag.*, vol. 57, no. 3, pp. 56–62, 2019.\n- <span id=\"page-29-7\"\
    ></span>[176] A. Clemm, L. Ciavaglia, L. Z. Granville, and J. Tantsura, \"RFC\
    \ 9315: Intent-based networking-concepts and definitions,\" 2022.\n- <span id=\"\
    page-29-8\"></span>[177] Y. Njah *et al.*, \"An AI-driven intent-based network\
    \ architecture,\" *IEEE Commun. Mag.*, 2024.\n- <span id=\"page-29-9\"></span>[178]\
    \ L. Velasco *et al.*, \"End-to-end intent-based networking,\" *IEEE Commun. Mag.*,\
    \ vol. 59, no. 10, pp. 106–112, 2021.\n- <span id=\"page-29-10\"></span>[179]\
    \ D. M. Manias, A. Chouman, and A. Shami, \"Towards intent-based network management:\
    \ Large language models for intent extraction in 5G core networks,\" in *Proc.\
    \ IEEE 20th Int. Conf. Design Reliable Commun. Netw. (DRCN)*, 2024, pp. 1–6.\n\
    - <span id=\"page-29-11\"></span>[180] Y. Ahn *et al.*, \"An intent-based management\
    \ framework for on-device artificial intelligence in smart factory,\" in *KICS\
    \ Winter Conf.*, Gangwon, South Korea, 2025, pp. 1–3.\n- <span id=\"page-29-12\"\
    ></span>[181] S. O. Ajakwe *et al.*, \"Time sensitive anti-infoswarm agnostic\
    \ intelligence for safe UAV communication,\" in *Proc. IEEE 15th Int. Conf. Inf.\
    \ Commun. Technol. Convergence*, 2024, pp. 1614–1619.\n- <span id=\"page-29-13\"\
    ></span>[182] M. Liyanage *et al.*, \"A survey on zero touch network and service\
    \ management (ZSM) for 5G and beyond networks,\" *J. Netw. Comput. Appl.*, vol.\
    \ 203, p. 103362, 2022.\n- <span id=\"page-29-14\"></span>[183] W. B. Abbas *et\
    \ al.*, \"Designing future wireless networks (FWN)s with net zero (NZ) and zero\
    \ touch (ZT) perspective,\" *IEEE Access*, vol. 11, pp. 83 301–83 321, 2023.\n\
    - <span id=\"page-29-15\"></span>[184] J. Shodamola, H. Qureshi, U. Masood, and\
    \ A. Imran, \"Towards addressing the spatial sparsity of MDT reports to enable\
    \ zero touch network automation,\" in *Proc. IEEE Global Commun. Conf.*, 2021,\
    \ pp. 1–6.\n- <span id=\"page-29-16\"></span>[185] G. Samaras *et al.*, \"Unlocking\
    \ the path towards automation of tiny machine learning for edge computing,\" in\
    \ *Proc. IEEE Int. Conf. Smart Appl. Commun. Netw. (SmartNets)*, 2024, pp. 1–6.\n\
    - <span id=\"page-29-17\"></span>[186] T. Wang *et al.*, \"Deep-learning-based\
    \ weak electromagnetic intrusion detection method for zero touch networks on industrial\
    \ IoT,\" *IEEE Netw.*, vol. 36, no. 6, pp. 236–242, 2022.\n- <span id=\"page-29-18\"\
    ></span>[187] S.-C. Lin *et al.*, \"Zero-touch network on industrial IoT: An end-to-end\
    \ machine learning approach,\" *arXiv:2204.12605*, 2022.\n- <span id=\"page-29-19\"\
    ></span>[188] L. Yang *et al.*, \"Towards zero touch networks: Cross-layer automated\
    \ security solutions for 6G wireless networks,\" *IEEE Trans. Commun.*, 2025.\n\
    - <span id=\"page-29-20\"></span>[189] H. Y. Zhu *et al.*, \"A human-centric metaverse\
    \ enabled by braincomputer interface: A survey,\" *IEEE Commun. Surv. Tutor.*,\
    \ vol. 26, no. 3, pp. 2120–2145, 2024.\n- <span id=\"page-29-21\"></span>[190]\
    \ A. Kawala-Sterniuk *et al.*, \"Summary of over fifty years with braincomputer\
    \ interfaces–A review,\" *Brain Sci.*, vol. 11, no. 1, p. 43, 2021.\n- <span id=\"\
    page-29-22\"></span>[191] R. Das *et al.*, \"Biointegrated and wirelessly powered\
    \ implantable brain devices: A review,\" *IEEE Trans. Biomed. Circuits Syst.*,\
    \ vol. 14, no. 2, pp. 343–358, 2020.\n- <span id=\"page-29-23\"></span>[192] J.\
    \ Yoo *et al.*, \"Neural interface systems with on-device computing: machine learning\
    \ and neuromorphic architectures,\" *Curr. Opin. Biotechnol.*, vol. 72, pp. 95–101,\
    \ 2021.\n- <span id=\"page-29-24\"></span>[193] D. Kudithipudi *et al.*, \"Neuromorphic\
    \ computing at scale,\" *Nature*, vol. 637, pp. 801–812, 2025.\n- <span id=\"\
    page-29-25\"></span>[194] A. Khajooei *et al.*, \"A super-efficient TinyML processor\
    \ for the edge metaverse,\" *Inf.*, vol. 14, no. 4, p. 235, 2023.\n- <span id=\"\
    page-29-26\"></span>[195] W.-B. Jiang *et al.*, \"Large brain model for learning\
    \ generic representations with tremendous EEG data in BCI,\" *arXiv:2405.18765*,\
    \ 2024.\n- <span id=\"page-29-27\"></span>[196] D. Qiao *et al.*, \"AMFL: Resource-efficient\
    \ adaptive metaverse-based federated learning for the human-centric augmented\
    \ reality applications,\" *IEEE Trans. Neural Netw. Learn. Syst.*, 2024.\n- <span\
    \ id=\"page-29-28\"></span>[197] Y. Yang *et al.*, \"6G network AI architecture\
    \ for everyone-centric customized services,\" *IEEE Netw.*, vol. 37, no. 5, pp.\
    \ 71–80, 2022.\n- <span id=\"page-29-29\"></span>[198] I. WP5D, \"Future technology\
    \ trends of terrestrial international mobile telecommunications systems towards\
    \ 2030 and beyond,\" *International Telecommunication Union, Report M*, pp. 2516–0,\
    \ 2022.\n- <span id=\"page-29-30\"></span>[199] TR-37.817, \"Study on enhancement\
    \ for data collection for NR and ENDC,\" 2022. [Online]. Available: [https://www.3gpp.org/ftp/Specs/a](https://www.3gpp.org/ftp/Specs/archive/37_series/37.817/)\
    \ rchive/37 [series/37.817/](https://www.3gpp.org/ftp/Specs/archive/37_series/37.817/)\n\
    - <span id=\"page-29-31\"></span>[200] TR-38.743, \"Study on enhancements for\
    \ artificial intelligence (AI)/machine learning (ML) for NG-RAN,\" 2024. [Online].\
    \ Available: [https://www.3gpp.org/ftp/Specs/archive/38](https://www.3gpp.org/ftp/Specs/archive/38_series/38.743/)\
    \ series/38.743/\n- <span id=\"page-29-32\"></span>[201] H. Yu *et al.*, \"A comprehensive\
    \ framework for intent-based networking, standards-based and open-source,\" in\
    \ *Proc. IEEE/IFIP Netw. Oper. Manage. Symp.*, Miami, FL, USA, 2023, pp. 1–6.\n\
    - <span id=\"page-29-33\"></span>[202] X. Lin *et al.*, \"Embracing AI in 5G-advanced\
    \ toward 6G: A joint 3GPP and O-RAN perspective,\" *IEEE Commun. Standards Mag.*,\
    \ vol. 7, no. 4, pp. 76–83, 2023.\n- <span id=\"page-29-34\"></span>[203] S. Wang\
    \ *et al.*, \"Applications of explainable AI for 6G: Technical aspects, use cases,\
    \ and research challenges,\" *arXiv:2112.04698*, 2021.\n- <span id=\"page-29-35\"\
    ></span>[204] F. Rezazadeh *et al.*, \"Toward explainable reasoning in 6G: A proof\
    \ of concept study on radio resource allocation,\" *IEEE Open J. Commun. Soc.*,\
    \ 2024.\n- <span id=\"page-29-36\"></span>[205] S.-K. Chou *et al.*, \"Towards\
    \ the standardization of energy efficiency metrics of the AI lifecycle in 6G and\
    \ beyond,\" in *Proc. IEEE Conf. Standards Commun. Netw.*, 2024, pp. 187–190.\n\
    - <span id=\"page-29-37\"></span>[206] S. Faye *et al.*, \"Integrating network\
    \ digital twinning into future AIbased 6G systems: The 6G-twin vision,\" in *Proc.\
    \ IEEE Joint Eur. Conf. Netw. Commun. 6G Summit*, Antwerp, Belgium, 2024, pp.\
    \ 883–888.\n- <span id=\"page-29-38\"></span>[207] H. Du *et al.*, \"Enhancing\
    \ deep reinforcement learning: A tutorial on generative diffusion models in network\
    \ optimization,\" *IEEE Commun. Surv. Tutor.*, 2024.\n- <span id=\"page-29-39\"\
    ></span>[208] Z. Wang *et al.*, \"Incentive mechanism design for joint resource\
    \ allocation in blockchain-based federated learning,\" *IEEE Trans. Parallel Distrib.\
    \ Syst.*, vol. 34, no. 5, pp. 1536–1547, 2023.\n- <span id=\"page-29-40\"></span>[209]\
    \ W. Tong *et al.*, \"Nine challenges in artificial intelligence and wireless\
    \ communications for 6G,\" *IEEE Wireless Commun.*, vol. 29, no. 4, pp. 140–145,\
    \ 2022.\n- <span id=\"page-29-41\"></span>[210] C. Finn *et al.*, \"Model-agnostic\
    \ meta-learning for fast adaptation of deep networks,\" in *Proc. PMLR Int. Conf.\
    \ Mach. Learn.*, 2017, pp. 1126–1135.\n- <span id=\"page-29-42\"></span>[211]\
    \ X. Wei *et al.*, \"Accretionary learning with deep neural networks with applications,\"\
    \ *IEEE Trans. Cogn. Commun. Netw.*, vol. 10, no. 2, pp. 660–673, 2023.\n- <span\
    \ id=\"page-29-43\"></span>[212] P. Li *et al.*, \"Distributed AI-native architecture\
    \ for 6G networks,\" in *Proc. IEEE Int. Conf. Inf. Process. Netw. Provisioning*,\
    \ 2022, pp. 57– 62.\n- <span id=\"page-29-44\"></span>[213] J. Kuhnel ¨ *et al.*,\
    \ \"Semi-supervised anomaly detection in the TinyML domain through multi-target\
    \ few-shot domain adaptation,\" in *Proc. IEEE 29th Int. Conf. Emerg. Technol.\
    \ Factory Automat.*, 2024, pp. 1–8.\n- <span id=\"page-29-45\"></span>[214] S.\
    \ Ullah *et al.*, \"Homomorphic encryption applications for IoT and light-weighted\
    \ environments: A review,\" *IEEE Internet Things J.*, 2024.\n- <span id=\"page-29-46\"\
    ></span>[215] O. Aouedi *et al.*, \"A survey on intelligent internet of things:\
    \ Applications, security, privacy, and future directions,\" *IEEE Commun. Surv.\
    \ Tutor.*, 2024.\n- <span id=\"page-29-47\"></span>[216] S. A. Ajagbe *et al.*,\
    \ \"Advanced encryption standard (AES)-based text encryption for near field communication\
    \ (NFC) using huffman compression,\" *SN Comput. Sci.*, vol. 5, no. 1, p. 156,\
    \ 2024.\n- <span id=\"page-29-48\"></span>[217] T.-H. Vu *et al.*, \"Applications\
    \ of generative AI (GAI) for mobile and wireless networking: A survey,\" *IEEE\
    \ Internet Things J.*, 2024.\n- <span id=\"page-29-49\"></span>[218] W. Huang\
    \ *et al.*, \"A plug-in tiny AI module for intelligent and selective sensor data\
    \ transmission,\" *arXiv:2402.02043*, 2024.\n- <span id=\"page-29-50\"></span>[219]\
    \ Y. Chen *et al.*, \"MAPO: Boosting large language model performance with model-adaptive\
    \ prompt optimization,\" *arXiv:2407.04118*, 2024.\n- <span id=\"page-29-51\"\
    ></span>[220] X. Chen *et al.*, \"LightNER: A lightweight tuning paradigm for\
    \ lowresource NER via pluggable prompting,\" *arXiv:2109.00720*, 2021.\n- <span\
    \ id=\"page-29-52\"></span>[221] C. He *et al.*, \"Ultraeval: A lightweight platform\
    \ for flexible and comprehensive evaluation for LLMs,\" *arXiv:2404.07584*, 2024.\n\
    - <span id=\"page-29-53\"></span>[222] M. Iovene *et al.*, \"Defining AI native:\
    \ A key enabler for advanced intelligent telecom networks,\" *Ericsson white paper*,\
    \ 2023.\n- <span id=\"page-29-54\"></span>[223] L. Zheng *et al.*, \"Magent: A\
    \ many-agent reinforcement learning platform for artificial collective intelligence,\"\
    \ in *Proc. AAAI Conf. Artif. Intell.*, vol. 32, no. 1, 2018.\n- <span id=\"page-29-55\"\
    ></span>[224] M. Li *et al.*, \"Intelligent resource optimization for blockchain-enabled\
    \ IoT in 6G via collective reinforcement learning,\" *IEEE Netw.*, vol. 36, no.\
    \ 6, pp. 175–182, 2022.\n- <span id=\"page-29-56\"></span>[225] L. Rosenberg *et\
    \ al.*, \"Towards collective superintelligence: Amplifying group IQ using conversational\
    \ swarms,\" *arXiv:2401.15109*, 2024."
- title: 'Sionna Research Kit: A GPU-Accelerated Research Platform for AI-RAN'
  abstract: 'We introduce the NVIDIA Sionna Research Kit, a GPU-accelerated research

    platform for developing and testing AI/ML algorithms in 5G NR cellular

    networks. Powered by the NVIDIA Jetson AGX Orin, the platform leverages

    accelerated computing to deliver high throughput and real-time signal

    processing, while offering the flexibility of a software-defined stack. Built

    on OpenAirInterface (OAI), it unlocks a broad range of research opportunities.

    These include developing 5G NR and ORAN compliant algorithms, collecting

    real-world data for AI/ML training, and rapidly deploying innovative solutions

    in a very affordable testbed. Additionally, AI/ML hardware acceleration

    promotes the exploration of use cases in edge computing and AI radio access

    networks (AI-RAN). To demonstrate the capabilities, we deploy a real-time

    neural receiver - trained with NVIDIA Sionna and using the NVIDIA TensorRT

    library for inference - in a 5G NR cellular network using commercial user

    equipment. The code examples will be made publicly available, enabling

    researchers to adopt and extend the platform for their own projects.'
  url: http://arxiv.org/abs/2505.15848v1
  keywords: ''
  document: '# Sionna Research Kit: A GPU-Accelerated Research Platform for AI-RAN


    Sebastian Cammerer, Guillermo Marcus, Tobias Zirr, Fayçal Aït Aoudia, Lorenzo
    Maggi, Jakob Hoydis, and Alexander Keller NVIDIA, contact: scammerer@nvidia.com


    *Abstract*—We introduce the NVIDIA Sionna Research Kit, a GPU-accelerated research
    platform for developing and testing AI/ML algorithms in 5G NR cellular networks.
    Powered by the NVIDIA Jetson AGX Orin, the platform leverages accelerated computing
    to deliver high throughput and real-time signal processing, while offering the
    flexibility of a software-defined stack. Built on OpenAirInterface (OAI) [1],
    it unlocks a broad range of research opportunities. These include developing 5G
    NR and O-RAN compliant algorithms, collecting real-world data for AI/ML training,
    and rapidly deploying innovative solutions in a very affordable testbed. Additionally,
    AI/ML hardware acceleration promotes the exploration of use cases in edge computing
    and AI radio access networks (AI-RAN). To demonstrate the capabilities, we deploy
    a real-time neural receiver—trained with NVIDIA Sionna and using the NVIDIA TensorRT
    library for inference—in a 5G NR cellular network using commercial user equipment.
    The code examples will be made publicly available, enabling researchers to adopt
    and extend the platform for their own projects.


    #### I. BACKGROUND


    Artificial intelligence (AI) for wireless communications has received significant
    attention from both academia [2] and industry [3]. Besides the potential to deliver
    superior reliability and accuracy as compared to many traditional physical layer
    algorithms, AI facilitates the development of novel concepts such as site-specific
    adaptation [4], end-to-end learning [2], and semantic communications [5]. Moreover,
    the AI-RAN Alliance<sup>1</sup> envisions that generative AI applications will
    drive future 6G system requirements, such as edge AI offloading and low-latency
    communications.


    Though many concepts and algorithms have been proposed in the literature, so far
    little has been demonstrated in hardware testbeds and under real-time conditions.
    Certainly, the stringent latency and throughput requirements of wireless systems
    impose strict constraints on neural network (NN) architectures, effectively limiting
    their size and depth. Thus, deploying and validating AI components in the physical
    layer of a real cellular system under realistic latency conditions remains an
    open and yet exciting research challenge.


    Hence, the development and evaluation of novel AI-RAN algorithms requires: (a)
    affordable hardware platforms, (b) pro-


    <sup>1</sup>https://ai-ran.org/


    Fig. 1. Schematic of the demo setup, consisting of an NVIDIA Jetson AGX Orin,
    an USRP B210 by Ettus Research, and a commercial Quectel RM520N-GL 5G NR modem.


    grammable hardware accelerators for real-time signal processing as well as AI
    offloading, and (c) a low barrier-toentry considering the system complexity of
    modern wireless communication standards. Often, these requirements have kept academic
    researchers from engaging in practical systemlevel experimentation. The software-defined
    open stack of the Sionna Research Kit bridges this gap, enabling researchers to
    test their algorithms in an operational 5G NR system, boosting result credibility
    and unlocking new research opportunities.


    ## II. SIONNA RESEARCH KIT


    The Sionna Research Kit<sup>2</sup> relies on the NVIDIA Jetson AGX Orin platform.<sup>3</sup>
    GPU acceleration using NVIDIA CUDA<sup>4</sup> enables real-time signal processing
    with high throughput while preserving the flexibility of a software-defined stack.


    In principle, approaches to hardware acceleration of physical layer signal processing
    may be classified as either (a) look-aside acceleration or (b) inline acceleration
    [6]. Lookaside acceleration offloads specific tasks from the CPU to an accelerator—such
    as an ASIC, FPGA, or GPU—which then processes them asynchronously. While look-aside
    acceleration may include fixed function hardware for specific function blocks
    (e.g., for low-density parity-check (LDPC) decoding), moving data to and from
    the accelerator may add latency. In contrast, inline acceleration integrates directly
    into the data processing pipeline. An advantage of the NVIDIA Jetson platform
    is that both CPU and GPU share the same unified memory, thus eliminating the data
    copying overhead.


    This work has received financial support from the European Union under Grant Agreement
    101096379 (CENTRIC). Views and opinions expressed are however those of the author(s)
    only and do not necessarily reflect those of the European Union or the European
    Commission (granting authority). Neither the European Union nor the granting authority
    can be held responsible for them.


    <sup>2</sup>https://nvlabs.github.io/sionna/rk/index.html


    <sup>3</sup>https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/
    <sup>4</sup>https://developer.nvidia.com/cuda-toolkit


    ![](_page_1_Figure_0.jpeg)


    Fig. 2. Performance evaluation of the NRX [4], varying its network depth and,
    hence, the inference latency. Figure taken from https://developer.nvidia.com/blog/real-time-neural-receivers-drive-ai-raninnovation/.


    #### *A. Case Study: Neural Receiver Under Real-Time Constraints*


    We have developed a prototype of a 5G NR standardcompliant neural receiver [4]
    that replaces parts of the physical layer signal processing with machine-learned
    components. The architecture has been carefully optimized to ensure real-time
    inference capabilities. For details, see [7] and [4].


    As an example, we showcase the deployment the neural receiver in a 5G cellular
    network. The receiver is trained with NVIDIA Sionna [8] and implemented using
    the NVIDIA TensorRT inference library. Fig. 2 illustrates that the required SNR
    to achieve a target block error rate (BLER) of 0.1 depends on the network depth
    and hence the inference latency. These results demonstrate that real-time considerations
    significantly influence the performance and, thus, the design of AI/ML components
    in wireless systems.


    #### *B. Case Study: CUDA-accelerated LDPC Decoding*


    In a second example, we demonstrate GPU offloading in wireless systems by implementing
    a CUDA-accelerated LDPC decoder which seamlessly integrates into the OAI stack.
    To avoid the latency overhead of data transfer between the CPU and the GPU, we
    carefully optimize the caching behavior of the implemented CUDA kernels. While
    some data movement between CPU and GPU still occurs, the unified memory architecture
    of the Jetson AGX Orin significantly reduces the overhead of copying data.


    #### III. DEMO SETUP & HARDWARE REQUIREMENTS


    Fig. 1 shows the schematic of the demo and Fig. 3 reveals the hardware components
    consisting of:


    - NVIDIA Jetson AGX Orin: A GPU-accelerated embedded platform enabling real-time
    signal processing and AI/ML inference.

    - Ettus Research USRP B210 SDR: A flexible SDR frontend interfacing with the software-defined
    5G NR stack.

    - Quectel RM520N-GL 5G Modem: A commercial 5G NR modem serving as the user equipment
    (UE), highlighting the demo''s standard compliance.


    ![](_page_1_Picture_12.jpeg)


    Fig. 3. Photograph of the hardware components of the demo setup.


    Both experiments will be made publicly available and can serve as blueprints for
    implementing novel research prototypes. Additionally, tutorials on real-world
    data acquisition and a TensorRT accelerated neural demapper are provided. This
    enables researchers to acquire the necessary data for training novel AI-RAN algorithms
    and deploying their trained models using an inline AI/ML accelerated computing
    platform.


    ### IV. CONCLUSION


    The Sionna Research Kit enables researchers to develop, deploy, test, and validate
    novel AI/ML algorithms in a 5G NR cellular network, including software-defined
    user equipment (UE). Its unique combination of the OpenAirInterface software-defined
    stack and the NVIDIA Jetson AGX Orin platform allows for real-time signal processing
    and AI/ML inference. As such, we believe the Sionna Research Kit is a step towards
    practical prototyping of the next-generation AI-RAN.


    #### REFERENCES


    - [1] N. Nikaein, M. K. Marina, S. Manickam, A. Dawson, R. Knopp, and C. Bonnet,
    "OpenAirInterface: A flexible platform for 5G research," *ACM SIGCOMM Computer
    Communication Review*, vol. 44, no. 5, pp. 33–38, 2014.

    - [2] T. O''Shea and J. Hoydis, "An introduction to deep learning for the physical
    layer," *IEEE Trans. Cognitive Commun. Netw.*, vol. 3, no. 4, pp. 563–575, 2017.

    - [3] X. Lin, "An overview of the 3GPP study on artificial intelligence for 5G
    new radio," *arXiv preprint arXiv:2308.05315*, 2023.

    - [4] R. Wiesmayr, S. Cammerer, F. Aït Aoudia, J. Hoydis, J. Zakrzewski, and A.
    Keller, "Design of a standard-compliant real-time neural receiver for 5G NR,"
    *arXiv preprint arXiv:2409.02912*, 2024.

    - [5] Z. Qin, X. Tao, J. Lu, W. Tong, and G. Y. Li, "Semantic communications:
    Principles and challenges," *arXiv preprint arXiv:2201.01389*, 2021.

    - [6] L. Kundu, X. Lin, E. Agostini, V. Ditya, and T. Martin, "Hardware acceleration
    for open radio access networks: A contemporary overview," *IEEE Communications
    Magazine*, 2023.

    - [7] S. Cammerer, F. Aït Aoudia, J. Hoydis, A. Oeldemann, A. Roessler, T. Mayer,
    and A. Keller, "A neural receiver for 5G NR multi-user MIMO," in *Proc. IEEE Globecom
    Workshops*, Mar. 2023, pp. 329–334.

    - [8] J. Hoydis, S. Cammerer, F. Aït Aoudia, A. Vem, N. Binder, G. Marcus, and
    A. Keller, "Sionna: An open-source library for next-generation physical layer
    research," *arXiv:2203.11854*, 2022.'
- title: Graph Neural Networks Based Anomalous RSSI Detection
  abstract: "In today's world, modern infrastructures are being equipped with information\n\
    and communication technologies to create large IoT networks.\n  It is essential\
    \ to monitor these networks to ensure smooth operations by\ndetecting and correcting\
    \ link failures or abnormal network behaviour\nproactively, which can otherwise\
    \ cause interruptions in business operations.\n  This paper presents a novel method\
    \ for detecting anomalies in wireless links\nusing graph neural networks. The\
    \ proposed approach involves converting time\nseries data into graphs and training\
    \ a new graph neural network architecture\nbased on graph attention networks that\
    \ successfully detects anomalies at the\nlevel of individual measurements of the\
    \ time series data. The model provides\ncompetitive results compared to the state\
    \ of the art while being\ncomputationally more efficient with ~171 times fewer\
    \ trainable parameters."
  url: http://arxiv.org/abs/2505.15847v1
  keywords: anomaly detection, wireless, machine learning, graph neural networks,
    graph transformation, time series
  document: '## I. INTRODUCTION


    The Internet of Things (IoT) has become one of the most important concepts in
    modern wireless networks. The availability of low-cost sensors with connectivity
    capabilities allows us to augment existing infrastructures and processes, such
    as transportation or power delivery networks, with new information that enables
    effective monitoring. However, each IoT sensor deployed increases network complexity
    and introduces new challenges to solve. Traditionally, wireless connectivity issues
    were monitored manually using a set of predefined metrics that determined the
    potential presence of anomalies and their nature. Furthermore, with the transition
    to fifth generation mobile networks (5G) and beyond, the capabilities of the technology
    such as higher peak data speeds, ultra-low latency, more reliability, massive
    network capacity, and higher availability, will lead to increasingly complex IoT
    networks, impossible to be manually monitored. To ensure the reliability of such
    complex IoT networks, new automated solutions must be developed [\[1\]](#page-4-0).


    According to [\[2\]](#page-4-1), when a monitoring infrastructure consisting of
    multiple devices is operated over a long period of time, there is a possibility
    of malfunctions and downtime caused by various factors such as software bugs,
    physical damage, transceiver degradation, and line of sight obstacles. The authors
    of [\[2\]](#page-4-1) have identified four different types of anomalies that can
    occur in the link layer of wireless communications, namely Sudden Link Degradation
    (SuddenD), Sudden Link Degradation with Recovery (SuddenR), Instantaneous Link
    Degradation (InstaD), and Slow Link Degradation (SlowD). These anomalies are represented
    in Fig. [1.](#page-0-0)


    ![](_page_0_Figure_8.jpeg)


    <span id="page-0-0"></span>Fig. 1. Time-series representation of the links and
    of the four types of anomalies considered in [\[2\]](#page-4-1).


    Numerous automated solutions have been proposed to facilitate the effective management
    of large IoT networks. These solutions include network monitoring [\[3\]](#page-4-2),
    malfunction detection [\[4\]](#page-4-3), and specific anomaly classification
    [\[2\]](#page-4-1), [\[5\]](#page-4-4). Typically, machine learning algorithms
    are used to develop such automated solutions. But the limitation of these methods
    is that they can only detect and classify an anomaly or malfunction, but then
    demand additional manual inspection of network signals to determine the severity
    of the malfunction and appropriate steps to mitigate the problem. Another problem
    of such systems is that they can only detect problems within a fixed time window
    and are unable to precisely pinpoint the time step at which an anomaly occurred
    within the link.


    Graph neural networks (GNNs) [\[6\]](#page-4-5) have recently gained popularity
    for various applications in non-Euclidean spaces, although they are not limited
    to this type of space [\[7\]](#page-4-6). Researchers from various fields have
    used GNNs to address a wide range of problems, one of which is an adaptation for
    time series (TS) anomaly detection tasks, particularly for multivariate TS [\[8\]](#page-4-7),
    [\[9\]](#page-4-8). GNNs can be used to model the temporal relationships between
    data points in the time series, as well as model complex interactions and relationships
    between the features of each data point. Recently, a novel method on transforming
    univariate TS data into graphs that can be utilised for a more in depth analysis
    of deviations happening within the TS, such as wireless link layer anomalies,
    has been introduced [\[10\]](#page-4-9).


    In this paper, we analyse the performance of graph neural networks, for detecting
    and localising wireless link anomalies. The contributions of this paper are:


    - A new approach for wireless link layer anomaly detection, that works on per
    point basis which enables for not only detecting an anomaly, but also localising
    it within the TS and determine its duration.

    - A novel graph neural network model, based on Graph Attention Networks, for classifying
    TS points that achieves similar performance to the state of the art, while having
    an ≈171 times less weights.


    This paper is organised as follows. We discuss the related work in Section [II](#page-1-0)
    while Section [III](#page-1-1) elaborates on the proposed method, transformation
    and architecture. Section [IV](#page-3-0) presents the methodological aspects
    of the work while Section [V](#page-3-1) provides the evaluation. Finally, Section
    [VI](#page-3-2) concludes the paper.


    ## II. RELATED WORK


    <span id="page-1-0"></span>To support our contributions and put our work in perspective,
    we first analyse related work with respect to graph neural networks for time series
    analysis and then narrow down to works for anomaly detection in wireless communication
    networks.


    ## *A. Graph neural networks on time series data*


    The majority of the current state-of-the-art (SotA) research in the field of time
    series analysis focuses on multivariate time series data forecasting. In contrast,
    research on the classification of time series data is limited.


    Cao *et. al.* [\[11\]](#page-4-10) were among the pioneers in proposing the use
    of graph representation of multivariate time series to solve forecasting problems.
    Their approach involved transforming the time series into a graph by training
    a Gated Recurrent Unit (GRU) layer and using an attention mechanism to construct
    a weight matrix that served as the adjacency matrix for the graph representation.
    This learned graph was then used in a spectral graph neural network for forecasting.
    Similar a graph structure learning layer approach was proposed by [\[12\]](#page-4-11),
    also for the purpose of forecasting multivariate TS.


    Researchers also showed the potential of utilising the GNNs for multivariate TS
    anomaly detection, but their approach was again based on forecasting methods.
    For example, Deng *et. al.* [\[9\]](#page-4-8) proposed GNNs to forecast multivariate
    sensor signals based on GAT and if there is a deviation between the forecast and
    actual behaviour they label that as an anomaly. Similar work was also presented
    by [\[8\]](#page-4-7) to try to detect anomalies in cloud infrastructure monitoring.
    Both studies work on per TS data point analysis, but only utilise a simple thresholding
    method rather than the classification power of GNNs to do so.


    More recently, there have been initial attempts to classify univariate TS data.
    For instance, [\[13\]](#page-4-12) proposed an approach that combined a visibility
    graph trained concurrently with a radio signal modulation classification model.
    Similarly, [\[14\]](#page-4-13) utilized a time-labelled visibility graph as a
    component of their electrocardiogram classification model. Another novel approach
    for transforming TS data into graphs was proposed by [\[10\]](#page-4-9), where
    they created a graph based on an adjacency matrix with Markov Transition Fields,
    which was later used to classify wireless anomalies. But similar to multivariate
    TS analysis with graphs, in univariate analysis the researchers analysed the TS
    graphs as a whole rather than at the granular level of nodes representing a point
    within a TS.


    ## *B. Wireless network anomaly detection*


    Current research on anomaly detection in wireless networks mainly focuses on intrusion,
    fraud, fault detection, IoT event detection, system health monitoring, and natural
    disasters [\[15\]](#page-4-14). There are many works focusing on wireless anomaly
    detection that are based on the classical machine learning algorithms. Salem [\[16\]](#page-4-15)
    proposed the use of SVM, decision trees, logistic regression, Naive Bayes, and
    decision tables to discriminate between anomalies in medical data gathered from
    IoT networks. Wazid [\[17\]](#page-4-16) proposed an unsupervised k-means based
    way of detecting intrusions in wireless traffic data improved by [\[18\]](#page-4-17)
    using the k-medoid algorithm. In our recent work [\[2\]](#page-4-1), we detected
    four link layer anomalies based on unsupervised machine learning approaches (LOF,
    Isolation Forests, and one-class SVM) and supervised learning (Logistic regression,
    random forest, and SVM). This was further improved by us with the use of supervised
    deep learning approaches [\[5\]](#page-4-4), [\[19\]](#page-4-18).


    In general, unsupervised learning using autoencoders (AE) is the most commonly
    used deep learning approach for anomaly detection. Thing [\[20\]](#page-4-19)
    conducted an evaluation of four different AE models for intrusion detection using
    IEEE 802.11 network data . Similarly, Ran et al. [\[21\]](#page-4-20) employed
    a semisupervised AE model to detect four different types of cyber attacks in captured
    IEEE 802.11 network data.


    ## III. THE PROPOSED METHOD


    <span id="page-1-1"></span>Suppose a large IoT network is deployed in a smart
    infrastructure and we expect an uninterrupted data communication to ensure uninterrupted
    business process. To accomplish this, an automatic monitoring system of such IoT
    network is designed to quickly detect and mitigate potential problems and anomalies.
    Devices within the IoT network produce time series which is used for monitoring.
    We formulate the anomaly detection task in the system as a classification problem.
    To analyse the input data in graph form, the input time series S needs to be transformed
    into graphs using a transformation function T. Such transformed TS can then be
    fed to the function Φ, that represents a GNN, which maps it to a vector of target
    classes P1×<sup>N</sup> = ⃗p<sup>N</sup> = p1, p2, ..., p<sup>N</sup> , where
    N represents the length of the time series data, as provided in Eq. [\(1\)](#page-2-0).


    <span id="page-2-0"></span>

    $$P\_{1 \times N} = \Phi(G) = \Phi(T(S))\tag{1}$$


    We consider our problem as a binary classification problem where the set of target
    classes is P = {anomalous, non − anomalous}. Each node, that represents a point
    within a TS, is then classified either as anomalous or non-anomalous, producing
    a binary vector P1×<sup>N</sup> .


    ![](_page_2_Figure_2.jpeg)


    <span id="page-2-1"></span>Fig. 2. A simple example of TS to graph transformation.
    Non-anomalous nodes are coloured in green, while the anomalous are coloured red.


    ## <span id="page-2-4"></span>*A. Time-series to graph transformation*


    We define the transformation T depicted in Fig. [2](#page-2-1) that transforms
    the input TS S to the graph representation G as in Eq. [2.](#page-2-2) The time-series
    to graph transformation is realized in three steps: node determination, adjacency
    matrix computation, and final graph representation. We decided to adopt Markov
    Transition Field (MTF) [\[22\]](#page-4-21) representation of TS as an adjacency
    matrix for the transformation G. The MTF representation is a sparse matrix of
    transition probabilities between the values in the TS and can be interpreted as
    weighted edges of a graph.


    <span id="page-2-2"></span>

    $$G = T(S) \tag{2}$$


    To create the graph G, the first step is to convert each point in a time series
    S1×<sup>N</sup> = ⃗s<sup>N</sup> = s1, s2, ..., s<sup>N</sup> into a node in G,
    with the value at that point becoming the node''s label (feature). The number
    of nodes in G is equal to the length of the time series.


    Next, the adjacency matrix A corresponding to G is determined using the Markov
    Transition Field (MTF) technique. This involves quantizing the values of the time
    series into a finite set of bins, and then computing the probability of transitioning
    from one bin to another. This preserves the temporal information of the time series.
    MTF matrix is defined as follows:


    $$A = \begin{pmatrix} w\_{ij\mid s\_1 \in r\_i, s\_1 \in r\_j} & \cdots & w\_{ij\mid
    s\_1 \in r\_i, s\_N \in r\_j} \\ w\_{ij\mid s\_2 \in r\_i, s\_1 \in r\_j} & \cdots
    & w\_{ij\mid s\_2 \in r\_i, s\_N \in r\_j} \\ \vdots & \ddots & \vdots \\ w\_{ij\mid
    s\_N \in r\_i, s\_1 \in r\_j} & \cdots & w\_{ij\mid s\_N \in r\_i, s\_N \in r\_j}
    \end{pmatrix} \tag{3}$$


    MTF encodes multi-span transition probabilities of the time series by assigning
    probabilities from quantiles at one time step to those at another time step, and
    representing them as wij .


    The third and final step involves constructing the graph G from the transition
    matrix A. The edges of G are created based on A, where any wij > 0 generates an
    edge between nodes i and j with wij serving as the edge weight. Consequently,
    each resulting graph G consists of a set of nodes N that represent each time step
    in the time series with its value, a set of edge indexes, and a set of edge weights.


    An example of this three step process is depicted in Fig. [2,](#page-2-1) where
    a TS S1×<sup>30</sup> is transformed by the T transformation into a graph G with
    30 nodes connected by 493 edges. The nodes representing the anomalous part of
    the wireless link are red, while the non-anomalous nodes are green. As it can
    be observed in Fig. [2,](#page-2-1) nodes that are most similar to each other
    group together and have more connections between themselves.


    # *B. Proposed GNN architecture*


    We define our prediction model as function Φ that transforms the input transformed
    data G to the set of target classes P1×<sup>N</sup> as provided in Eq. [1](#page-2-0)
    and depicted in Figure [3.](#page-2-3)


    ![](_page_2_Figure_16.jpeg)


    <span id="page-2-3"></span>Fig. 3. Proposed GNN architecture.


    We have designed our GNN architecture inspired by the the Graph Attention Network
    (GAT) [\[23\]](#page-4-22). GAT improves upon classical Graph Neural Networks
    by applying self-attention over the node features, significantly improving classification
    performance for node classification tasks across different domains. The final
    model architecture was selected based on empirical testing, where as a metric
    we were considering the best ratio between number of weights and performance of
    the model.


    The proposed architecture is realised with 3 GAT layers that are combined with
    Linear layers. Linear layers work as learnable skip connections [\[24\]](#page-4-23),
    that improve training and convergence of the model. In our model all Linear layers
    have 128 nodes. All GAT layers consists of 32 filters, they only differentiate
    by the number of self-attention heads. First two layers consists of 4 self-attention
    heads, while the third employs 6 heads. The number of layers, filters, and heads
    were selected with grid search process, optimising for the best performance of
    the model. The output of final layer is finally sent to the output layer Y, that
    produces a vector P1×<sup>300</sup> of size 300 representing 300 nodes, or measurements,
    within the TS. For each of the 300 nodes network returns probability of it being
    Anomalous or Non-anomalous. All hidden layers use the ReLU function for activation
    except for the final layer that uses a Sigmoid function.


    TABLE I SYNTHETIC ANOMALY INJECTION METHOD SIMILAR TO [\[2\]](#page-4-1).


    <span id="page-3-3"></span>


    | Type                                  | Links | Affected  | Appearance                                                                           |
    Persistence                                                |  |  |

    |---------------------------------------|-------|-----------|--------------------------------------------------------------------------------------|------------------------------------------------------------|--|--|

    | SuddenD<br>SuddenR<br>InstaD<br>SlowD | 2 123 | 33% (700) | once, [200th, 280th]<br>once,
    [25th, 275th]<br>on ≈1% of a link<br>once, [1st, 20th] | for ∞<br>for [5, 20]<br>for
    1 datapoint<br>for [150, 180]† |  |  |

    |                                       |       |           | † RSSI(x,start)
    ← RSSI(x) + min(0, −rand(0.5, 1.5) · (x − start))                    |                                                            |  |  |


    ## IV. METHODOLOGY


    <span id="page-3-0"></span>This section first elaborates on methodological aspects
    of dataset preparation followed by model training and evaluation.


    ## *A. Dataset*


    Rutgers WiFi dataset [\[25\]](#page-4-24) was utilized as our real-world measurement
    dataset for the proposed GNN training. The dataset is consists of link traces
    obtained from 29 nodes, recorded at 5 distinct noise levels. The dataset comprises
    raw Received Signal Strength Indicator (RSSI) values, sequence numbers, source
    node ID, destination node ID, and artificial noise levels. Each RSSI value represents
    the signal strength of a received packet transmitted every 100 milliseconds over
    a 30-second period, resulting in traces that consist of 300 RSSI samples. To produce
    a suitable dataset for our experiments, we synthetically inject four anomalies,
    that were defined by [\[2\]](#page-4-1), according to the guidelines in Table
    [I.](#page-3-3) We only considered links without packet loss. This gave us a dataset
    consisting of four different types of anomalous and non-anomalous links. The final
    dataset had 8492 samples, with 700 samples for each anomaly while the remaining
    5692 samples were nonanomalous.


    Such produced traces were then transformed into graphs based on the procedure
    in Section [III-A.](#page-2-4) Based on the empirical testing, we determined that
    the best number of bins for the MTF calculation is equal to the length of the
    TS trace, in our case that equalled to 300 bins.


    # *B. Model training and evaluation*


    Model training was done using 10-fold stratified shuffle split technique. This
    involved shuffling and splitting the data into a training set and a test set in
    a ratio of 80:20. To ensure credible results the process was repeated 10 times.
    Due to the class imbalance, where we have significantly more non-anomalous measurements
    compared to anomalous ones, we assigned class weights during the training process
    which were equal to the inverse proportions of number of samples for each class.


    The trained models performance was evaluated using standard classification matrices
    Precision, Recall, and F1-score. The F1-score is expressed as a harmonic mean
    between Precision = TP TP+FP and Recall = TP TP+FN , where TP, FP and FN stand
    for true positives, false positives and false negatives. Due to the 10-fold stratified
    shuffle split used for model evaluation, the final result is an average F1-score
    across all 10 splits.


    # V. RESULTS


    <span id="page-3-1"></span>In this section, we evaluate the performance of the
    proposed method against the state of the art methods from [\[5\]](#page-4-4).
    Table [II](#page-4-25) compares our proposed method with three different deep
    learning models trained on time series transformed to images using recurrence
    plots (RP), Grammian Angular Summation Field (GASF) and Grammian Angular Differential
    Field (GADF). The first column displays the target classes, while the second column
    lists our proposed method using the three selected metrics. The remaining three
    columns show the results of the state of the art models. The best performing models
    for each class are bolded in the corresponding column.


    As it can be seen in the first line of the binary classifier results in Table
    [II](#page-4-25) the RP model achieves near perfect F1 score of 0.99 in detecting
    anomalous links. This is slightly better than the 0.98 F1 score of GASF model,
    while GADF model performs the worst out of the three with the F1 score of 0.91.
    Our proposed method has an F1 score lower by 0.05 compared to the RP model and
    0.04 compared to GASF model. Looking at the results for GADF model, our method
    outperforms it with an F1 score higher by 0.03.


    Looking at the second line of binary classifier results in Table [II](#page-4-25)
    it can be seen that our proposed method outperforms all three state-of-the-art
    methods. Both the RP and GADF models are in terms of an F1 score outperformed
    by 0.01. Compared to GASF model our method achieves an F1 score higher by 0.04.


    Although we compare our proposed method to the work from [\[5\]](#page-4-4), there
    is one main difference between the works. In [\[5\]](#page-4-4) researchers were
    trying to detect whether the whole link is anomalous or not, while in our work
    we classify each measurement separately. The advantage of our approach is that
    not only can we detect whether an anomaly occurred in the link, we can also determine
    its location and duration within the link. Based on the location and duration
    we can then also determine what type of an anomaly occurred in the link. According
    to Table [III,](#page-4-26) another benefit of our method is that our model has
    an ≈171 times less weights for slightly worse performance, compared to the state
    of the art imaging models.


    # VI. CONCLUSION


    <span id="page-3-2"></span>In this paper, we performed a first time analysis of
    graph representation for wireless link layer anomalies detection on per TS point
    granularity. Additionally, we proposed a new graph neural network architecture
    that is able to detect and distinguish between the anomalous and non-anomalous
    points within the TS based on Graph Attention Networks. We compared our model
    to the existing state of the art. Our results show that our model in general performs
    similar to the state of the art deep learning model on image representation of
    time


    TABLE II CLASSIFICATION RESULTS OF PROPOSED MODEL COMPARED TO THE SOTA.


    <span id="page-4-25"></span>


    | Class                      | Proposed method |              | RP [5]       |              |
    GASF [5]     |              |              | GADF [5]     |              |              |              |              |

    |----------------------------|-----------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|

    |                            | Prec.           | Rec.         | F1           |
    Prec.        | Rec.         | F1           | Prec.        | Rec.         | F1           |
    Prec.        | Rec.         | F1           |

    | Anomalous<br>Non-anomalous | 0.93<br>0.98    | 0.95<br>0.97 | 0.94<br>0.98 |
    0.99<br>0.99 | 1.00<br>0.96 | 0.99<br>0.97 | 0.97<br>0.94 | 0.98<br>0.93 | 0.98<br>0.94
    | 0.91<br>0.97 | 0.92<br>0.97 | 0.91<br>0.97 |


    <span id="page-4-26"></span>TABLE III MODEL TRAINABLE WEIGHTS COMPARISON.


    | Model                  | Weights |

    |------------------------|---------|

    | Our model              | ≈0.035M |

    | RP/GASF/GADF model [5] | ≈6.000M |


    series, the main difference being that our method can localise which points within
    the time series are anomalous, while the state of can only detect whether an anomaly
    is present within the link, but can not provide an information on its location.


    ## ACKNOWLEDGMENTS


    This work was supported by the Slovenian Research Agency under grant P2-0016 and
    from the European Union''s Horizon Europe Framework Programme under grant agreement
    No 101096456.


    ## REFERENCES


    - <span id="page-4-0"></span>[1] A. M. Annaswamy, A. R. Malekpour, and S. Baros,
    "Emerging research topics in control for smart infrastructures," *Annual Reviews
    in Control*, vol. 42, pp. 259–270, 2016.

    - <span id="page-4-1"></span>[2] G. Cerar, H. Yetgin, B. Bertalanic, and C. Fortuna,
    "Learning to detect anomalous wireless links in iot networks," *IEEE Access*,
    vol. 8, pp. 212 130–212 155, 2020.

    - <span id="page-4-2"></span>[3] J. D. C. Silva, J. J. P. Rodrigues, K. Saleem,
    S. A. Kozlov, and R. A. Rabelo, "M4DN. IoT-A Networks and Devices Management Platform
    ˆ for Internet of Things," *IEEE Access*, vol. 7, pp. 53 305–53 313, April 2019.

    - <span id="page-4-3"></span>[4] A. Sheth, C. Doerr, D. Grunwald, R. Han, and
    D. Sicker, "Mojo: A distributed physical layer anomaly detection system for 802.11
    wlans," in *Proceedings of the 4th international conference on Mobile systems,
    applications and services*. ACM, 2006, pp. 191–204.

    - <span id="page-4-4"></span>[5] B. Bertalanic, M. Me ˇ za, and C. Fortuna, "Resource-aware
    time series ˇ imaging classification for wireless link layer anomalies," *IEEE
    Transactions on Neural Networks and Learning Systems*, 2022.

    - <span id="page-4-5"></span>[6] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and
    S. Y. Philip, "A comprehensive survey on graph neural networks," *IEEE transactions
    on neural networks and learning systems*, vol. 32, no. 1, pp. 4–24, 2020.

    - <span id="page-4-6"></span>[7] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z.
    Liu, L. Wang, C. Li, and M. Sun, "Graph neural networks: A review of methods and
    applications," *AI Open*, vol. 1, pp. 57–81, 2020.

    - <span id="page-4-7"></span>[8] D. Scheinert and A. Acker, "Telesto: A graph
    neural network model for anomaly classification in cloud services," in *Service-Oriented
    Computing – ICSOC 2020 Workshops*, H. Hacid, F. Outay, H.-y. Paik, A. Alloum,
    M. Petrocchi, M. R. Bouadjenek, A. Beheshti, X. Liu, and A. Maaradji, Eds. Cham:
    Springer International Publishing, 2021, pp. 214–227.

    - <span id="page-4-8"></span>[9] A. Deng and B. Hooi, "Graph neural network-based
    anomaly detection in multivariate time series," in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 35, no. 5, 2021, pp. 4027–4035.

    - <span id="page-4-9"></span>[10] B. Bertalanic and C. Fortuna, "Graph isomorphism
    networks for wireless link layer anomaly classification," in *2023 Wireless Communications
    and Networking Conference)*, 2023.

    - <span id="page-4-10"></span>[11] D. Cao, Y. Wang, J. Duan, C. Zhang, X. Zhu,
    C. Huang, Y. Tong, B. Xu, J. Bai, J. Tong *et al.*, "Spectral temporal graph neural
    network for multivariate time-series forecasting," *Advances in neural information
    processing systems*, vol. 33, pp. 17 766–17 778, 2020.

    - <span id="page-4-11"></span>[12] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang,
    and C. Zhang, "Connecting the dots: Multivariate time series forecasting with
    graph neural networks," in *Proceedings of the 26th ACM SIGKDD international conference
    on knowledge discovery & data mining*, 2020, pp. 753–763.

    - <span id="page-4-12"></span>[13] Q. Xuan, J. Zhou, K. Qiu, Z. Chen, D. Xu, S.
    Zheng, and X. Yang, "Avgnet: Adaptive visibility graph neural network and its
    application in modulation classification," *IEEE Transactions on Network Science
    and Engineering*, vol. 9, no. 3, pp. 1516–1526, 2022.

    - <span id="page-4-13"></span>[14] Y. Xiu, X. Ren, T. Zhang, Y. Chen, L. Jiang,
    D. Li, X. Wang, L. Zhao, and W. K. Chan, "Time labeled visibility graph for privacy-preserved
    physiological time series classification," in *2022 7th International Conference
    on Cloud Computing and Big Data Analytics (ICCCBDA)*, 2022, pp. 280–284.

    - <span id="page-4-14"></span>[15] V. Chandola, A. Banerjee, and V. Kumar, "Anomaly
    detection: A survey," *ACM computing surveys (CSUR)*, vol. 41, no. 3, July 2009.

    - <span id="page-4-15"></span>[16] O. Salem, A. Guerassimov, A. Mehaoua, A. Marcus,
    and B. Furht, "Anomaly detection in medical wireless sensor networks using svm
    and linear regression models," *International Journal of E-Health and Medical
    Communications (IJEHMC)*, vol. 5, no. 1, pp. 20–45, 2014.

    - <span id="page-4-16"></span>[17] M. Wazid and A. K. Das, "An efficient hybrid
    anomaly detection scheme using k-means clustering for wireless sensor networks,"
    *Wireless Personal Communications*, vol. 90, no. 4, pp. 1971–2000, 2016.

    - <span id="page-4-17"></span>[18] B. Ahmad, W. Jian, Z. A. Ali, S. Tanvir, and
    M. S. A. Khan, "Hybrid anomaly detection by using clustering for wireless sensor
    network," *Wireless Personal Communications*, vol. 106, no. 4, pp. 1841–1853,
    2019.

    - <span id="page-4-18"></span>[19] B. Bertalanic, H. Yetgin, G. Cerar, and C.
    Fortuna, "A deep learning model for anomalous wireless link detection," in *2021
    17th International Conference on Wireless and Mobile Computing, Networking and
    Communications (WiMob)*, 2021, pp. 265–270.

    - <span id="page-4-19"></span>[20] V. L. Thing, "IEEE 802.11 network anomaly detection
    and attack classification: A deep learning approach," in *IEEE Wireless Communications
    and Networking Conference (WCNC)*, San Francisco, CA, USA, March 2017.

    - <span id="page-4-20"></span>[21] J. Ran, Y. Ji, and B. Tang, "A semi-supervised
    learning approach to ieee 802.11 network anomaly detection," in *2019 IEEE 89th
    Vehicular Technology Conference (VTC2019-Spring)*. IEEE, 2019, pp. 1–5.

    - <span id="page-4-21"></span>[22] Z. Wang and T. Oates, "Encoding time series
    as images for visual inspection and classification using tiled convolutional neural
    networks," in *Workshops at the twenty-ninth AAAI conference on artificial intelligence*,
    2015.

    - <span id="page-4-22"></span>[23] P. Velickovi ˇ c, G. Cucurull, A. Casanova,
    A. Romero, P. Li ´ o, and ` Y. Bengio, "Graph Attention Networks," *International
    Conference on Learning Representations*, 2018. [Online]. Available: [https:](https://openreview.net/forum?id=rJXMpikCZ)
    [//openreview.net/forum?id=rJXMpikCZ](https://openreview.net/forum?id=rJXMpikCZ)

    - <span id="page-4-23"></span>[24] S. A. Taghanaki, A. Bentaieb, A. Sharma, S.
    K. Zhou, Y. Zheng, B. Georgescu, P. Sharma, Z. Xu, D. Comaniciu, and G. Hamarneh,
    "Select, attend, and transfer: Light, learnable skip connections," in *Machine
    Learning in Medical Imaging*, H.-I. Suk, M. Liu, P. Yan, and C. Lian, Eds. Cham:
    Springer International Publishing, 2019, pp. 417– 425.

    - <span id="page-4-24"></span>[25] S. K. Kaul, I. Seskar, and M. Gruteser, "CRAWDAD
    dataset rutgers/noise (v. 2007-04-20)," Downloaded from [https://crawdad.org/](https://crawdad.org/rutgers/noise/20070420/RSSI)
    [rutgers/noise/20070420/RSSI,](https://crawdad.org/rutgers/noise/20070420/RSSI)
    Apr. 2007, traceset: RSSI.'
- title: "Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA\n\
    \  Networks via MARL"
  abstract: 'Visible Light Communication (VLC) combined with Non-Orthogonal Multiple

    Access (NOMA) offers a promising solution for dense indoor wireless networks.

    Yet, managing resources effectively is challenged by VLC network dynamic

    conditions involving user mobility and light dimming. In addition to satisfying

    Quality of Service (QoS) and network stability requirements. Traditional

    resource allocation methods and simpler RL approaches struggle to jointly

    optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA

    networks. This paper presents MARL frameworks tailored to perform complex joint

    optimization of resource allocation (NOMA power, user scheduling) and network

    stability (interference, handovers), considering heterogeneous QoS, user

    mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic

    channel conditions and diverse user QoS , enabling effective joint

    optimization. In these frameworks, VLC access points (APs) act as intelligent

    agents, learning to allocate power and schedule users to satisfy diverse

    requirements while maintaining network stability by managing interference and

    minimizing disruptive handovers. We conduct a comparative analysis of two key

    MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and

    2) Centralized Training with Centralized Execution (CTCE). Comprehensive

    simulations validate the effectiveness of both tailored MARL frameworks and

    demonstrate an ability to handle complex optimization. The results show key

    trade-offs, as the CTDE approach achieved approximately 16\% higher for High

    priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7

    dB higher average SINR and 12\% lower ping-pong handover ratio, offering

    valuable insights into the performance differences between these paradigms in

    complex VLC-NOMA network scenarios.'
  url: http://arxiv.org/abs/2505.15841v1
  keywords: Visible Light Communication (VLC), Non-Orthogonal Multiple Access (NOMA),
    Multi-Agent Reinforcement Learning (MARL), Resource Allocation, Quality of Service
    (QoS), Network Stability, MAPPO, User Mobility, Centralized Training Decentralized
    Execution (CTDE), Centralized Training Centralized Execution (CTCE).
  document: "#### I. INTRODUCTION\n\nT HE relentless demand for increased data rates\
    \ and broad connectivity, especially in indoor environments, is propelling research\
    \ beyond traditional Radio Frequency (RF) wireless technologies. Visible Light\
    \ Communication (VLC) has arisen as an innovative complementary technology, utilizing\
    \ current lighting infrastructure to deliver high-bandwidth, license-free, and\
    \ intrinsically secure wireless connections [2] [52]. By employing Light Emitting\
    \ Diodes (LEDs) for illumination as well as data transmission, Visible Light Communication\
    \ (VLC) provides a substantial unregulated spectrum and significant spatial reuse\
    \ potential, making it appropriate for densely populated indoor environments [3].\
    \ Non-Orthogonal Multiple Access (NOMA) has been recognized as a pivotal technology\
    \ for augmenting spectral efficiency in the VLC systems [4] [53]. NOMA enables\
    \ multiple users to simultaneously utilize identical time-frequency resources\
    \ through power-domain multiplexing, considerably enhancing user capacity relative\
    \ to conventional Orthogonal Multiple Access (OMA) techniques, and is particularly\
    \ compatible with the characteristics of VLC channels [5].\n\nNonetheless, achieving\
    \ the entire capacity of VLC-NOMA in dynamic indoor environments poses major challenges.\
    \ These environments are defined by multiple light sources and users in close\
    \ proximity, resulting in extensive interference patterns. User mobility and light\
    \ attenuation lead to channel fluctuations, requiring effective handover strategies\
    \ between VLC access points to ensure uninterrupted connectivity [6]. In addition,\
    \ within an indoor network, users often require diverse Quality of Service (QoS)\
    \ requirements, with applications such as video conferencing and virtual reality\
    \ demanding elevated data rates and low latency. Alongside traditional data users\
    \ requiring a minimum assured throughput, this addresses the many requirements\
    \ expected in forthcoming 6G applications [7]. Simultaneously guaranteeing higher\
    \ Quality of Service (QoS) for priority users, managing inter-cell interference\
    \ (ICI), and facilitating seamless handovers constitutes a highly intricate and\
    \ multi-faceted resource allocation challenge [8]. The inherent dependency of\
    \ these aims, such as maximizing throughput, can cause interference, consequently\
    \ complicating optimization significantly.\n\nTraditional resource allocation\
    \ approaches to VLC-NOMA, typically reliant on static rules [14]. Also, the mathematical\
    \ optimization designed for simplified scenarios without with the high dimensions\
    \ and unpredictability inherent to dynamic VLC-NOMA networks [9]. Optimization\
    \ methods may become computationally unfeasible, while heuristics frequently yield\
    \ sub optimal results, struggling to adapt to real-time fluctuations in network\
    \ conditions, user requirements, or fluctuations in channel quality due to mobility\
    \ and dimming [10]. Single-agent Reinforcement Learning (RL) approaches have shown\
    \ potential in adapting to dynamic conditions in wireless networks [11] [55] [56].\
    \ Early study utilizing reinforcement\n\nAubida A. Al-Hameed, Safwan Hafeedh Younus,\
    \ and Mohamad A. Ahmed are with the Department of Communication Engineering, College\
    \ of Electronics Engineering, Ninevah University, Mosul, Iraq\n\nAbdullah Baz\
    \ is with the Department of Computer and Network Engineering, College of Computing,\
    \ Umm Al-Qura University, Makkah, Saudi Arabia\n\nCorresponding author: Aubida\
    \ A. Al-Hameed (email:aubida.alhameed@uoninevah.edu.iq)\n\nlearning for VLC-NOMA\
    \ power allocation underlines its potential, however often focuses on relatively\
    \ basic reinforcement learning models [15]. Consequently, a distinct need emerges\
    \ for an intelligent and coordinated resource allocation mechanism that can develop\
    \ near-optimal strategies for these sophisticated network requirements.\n\nIn\
    \ this paper, we present a Multi-Agent Reinforcement Learning (MARL) framework\
    \ to overcome these limitations in resource allocation for dynamic VLC-NOMA networks.\
    \ MARL is effective at handling challenges involving numerous interacting decision-makers\
    \ within a such an environment [12]. In addition, in [16], the MARL has proven\
    \ effective in complicated resource management tasks, including those in VLC network\
    \ setups. In our proposed framework, individual VLC access points act as autonomous\
    \ agents, acquiring cooperative strategies to enhance resource allocation (NOMA\
    \ power coefficients, user scheduling decisions) based on local observations during\
    \ training. Our aim is to empower agents to develop policies that collectively\
    \ enhance network performance across various dimensions: 1) fulfilling diverse\
    \ QoS needs, 2) maintaining network stability, 3) effectively managing interference,\
    \ and 4) assuring handover stability. We examine and compare two key MARL paradigms:\
    \ Centralized Training with Decentralized Execution (CTDE) and Centralized Training\
    \ with Centralized Execution (CTCE). These paradigms present unique trade-offs\
    \ between coordination capabilities and execution complexity [13] [17]. We develop\
    \ tailored state representations, action spaces, and multi-objective reward functions\
    \ that explicitly integrate key components of the VLC-NOMA dynamic environment,\
    \ covering user mobility, light dimming, QoS classes, interference levels, and\
    \ handover. This paper's main contributions are summarized as follows:\n\n- We\
    \ propose and assess tailored MARL frameworks, specifically Centralized Training\
    \ with Decentralized Execution (CTDE) employing Multi-Agent Proximal Policy Optimization\
    \ (MAPPO) and Centralized Training with Centralized Execution (CTCE) utilizing\
    \ Centralized Proximal Policy Optimization (PPO), for the tangled task of simultaneously\
    \ optimizing Quality of Service (QoS) maintaining and network stability, which\
    \ includes resource allocation, interference management, and handover stability,\
    \ within dynamic indoor VLC-NOMA networks characterized by user mobility and light\
    \ dimming.\n- We developed tailored MARL components, which include state representations\
    \ that integrate local and neighboring information, composite action spaces that\
    \ facilitate simultaneous management of NOMA power allocation, users' scheduling\
    \ and handover triggers. In addition to design a multi-objective reward function\
    \ that specifically targets those distinct challenges and dynamics of mobile VLC-NOMA\
    \ systems with diverse user requirements.\n- We establish a systematic method\
    \ employing Bayesian Optimization to determine the critical convergence between\
    \ possibly conflicting QoS and stability objectives within the MARL reward function,\
    \ thereby providing a methodical tuning procedure for the multi-objective problem.\n\
    \n• We illustrated through thorough simulations the ability of the proposed MARL\
    \ frameworks in attaining enhanced performance in QoS satisfaction, network throughput,\
    \ fairness, and handover stability compared to a traditional baseline resource\
    \ allocation strategy.\n\nThe subsequent sections of this paper are structured\
    \ as follows. Section II outlines the system model for the dynamic mobile VLC-NOMA\
    \ network. Section III defines the proposed MARL framework and the particulars\
    \ of the CTDE and CTCE implementations. Section IV defines the simulation configuration\
    \ and settings. Section ?? discusses and analyzes the simulation outcomes. Ultimately,\
    \ Section V concludes the findings and proposes paths for future research.\n\n\
    #### II. SYSTEM MODEL\n\n#### *A. Network Architecture and Layout*\n\nThe VLC\
    \ simulation environment is represented as a singular rectangular room with dimensions\
    \ L × W × H meters, displaying an ordinary indoor space such as an office or conference\
    \ room.\n\nThis room has NAP VLC Access Points (APs), given by the set A = {1,\
    \ 2, ..., NAP}. The VLC APs are assumed to be mounted to the ceiling at height\
    \ H. The VLC APs are organized in a systematic grid layout to ensure consistent\
    \ illumination and communication coverage. The 3D coordinates of the a-th access\
    \ point are given as p AP <sup>a</sup> = (x AP a , yAP a , H), for all a ∈ A.\
    \ Each AP is equipped with an LED array that facilitates simultaneous illumination\
    \ and data transfer using intensity modulation. These VLC APs act as learning\
    \ agents within our proposed MARL frameworks.\n\nThe network accommodates NUE\
    \ mobile users, given by the set U = {1, 2, ..., NUE}. It is assumed that users\
    \ navigate through the room only on a horizontal plane at a standard receiving\
    \ height HUE (e.g., desk level, 0 < HUE < H). The three-dimensional coordinates\
    \ of user u at a particular time t are expressed as p UE u (t) = (x UE u (t),\
    \ yUE u (t), HUE), for all u ∈ U. Each user has a vertically oriented photodiode\
    \ (PD) receiver to capture the downlink VLC signals. The characteristics of user\
    \ mobility are given in Section II-D.\n\n#### *B. VLC Channel Model*\n\nThe communication\
    \ link between each VLC AP a ∈ A and user u ∈ U is established via the VLC channel.\
    \ The received optical power at user u from VLC AP a depends on the channel's\
    \ DC gain Ha,u(t), which is time-varying due to user mobility. We consider both\
    \ Line-of-Sight (LoS) and Non-LoS (NLoS) components.\n\nThe LoS channel DC gain\
    \ HLoS a,u (t) between VLC AP a located at p AP a and user u located at p UE u\
    \ (t) can be modeled using the generalized Lambertian emission pattern [18] [54].\
    \ It is given by:\n\n$$H\\_{a,u}^{\\text{LoS}}(t) = \\begin{cases} \\frac{(m+1)A\\\
    _{\\text{PD}}}{2\\pi d\\_{a,u}^2(t)}\\\\ \\quad \\times \\cos^m(\\phi\\_{a,u}(t))\
    \ & 0 \\le \\psi\\_{a,u}(t) \\le \\Psi\\_c \\\\\\quad \\times T\\_s(\\psi\\_{a,u}(t))\
    \ \\\\\\quad \\times g(\\psi\\_{a,u}(t))\\cos(\\psi\\_{a,u}(t)),\\\\0, & \\psi\\\
    _{a,u}(t) > \\Psi\\_c \\end{cases} \\tag{1}$$\n\nwhere:\n\n- da,u(t) = ∥p UE u\
    \ (t) − p AP <sup>a</sup> ∥<sup>2</sup> is the Euclidean distance between AP a\
    \ and user u at time t.\n- m = − ln(2)/ ln(cos(Φ1/2)) is the Lambertian order\
    \ related to the transmitter's semi-angle at half power Φ1/2.\n- APD is the physical\
    \ detection area of the user's photodiode (PD).\n- ϕa,u(t) is the angle of irradiance\
    \ from VLC AP a relative to its perpendicular axis (normal vector).\n- ψa,u(t)\
    \ is the angle of incidence at user u's receiver relative to its perpendicular\
    \ axis (normal vector).\n- Ts(ψa,u(t)) is the gain of the optical filter at the\
    \ receiver.\n- g(ψa,u(t)) is the gain of the optical concentrator at the receiver.\n\
    - Ψ<sup>c</sup> is the receiver's Field of View (FOV) angle, defining the acceptance\
    \ angle of the PD.\n\nBoth angles ϕa,u(t) and ψa,u(t) are functions of the VLC\
    \ AP and UE positions and their orientations.\n\nThe NLoS components arise primarily\
    \ from reflections off surfaces within the room. In this work, we model the NLoS\
    \ contribution by considering only the first-order reflections, denoted by H (1)\
    \ a,u(t). This component is calculated by integrating the power received from\
    \ all reflecting surface elements dAref within the environment. Assuming Lambertian\
    \ reflection with reflectivity ρ from each surface element, the first-order reflection\
    \ gain is [18], [20]:\n\n$$\\begin{split} H\\_{a,u}^{(1)}(t) &= \\int\\_{\\text{SURFACE}}\
    \ \\frac{(m+1)}{2\\pi^2} \\frac{A\\_{\\text{PD}}\\rho}{d\\_1^2(t)} \\frac{\\rho}{d\\\
    _2^2(t)} \\cos^m(\\phi\\_1(t)) \\\\ &\\quad \\times \\cos(\\psi\\_1(t)) \\cos(\\\
    phi\\_2(t)) \\cos(\\psi\\_2(t)) \\, T\\_S(\\psi\\_2(t)) \\\\ &\\quad \\times g(\\\
    psi\\_2(t)) \\, \\mathbb{I}(\\psi\\_2(t) \\le \\Psi\\_c) \\, dA\\_{\\text{REF}}\
    \ \\end{split} \\tag{2}$$\n\nwhere d1(t) and d2(t) are the distances from the\
    \ VLC AP to dAref and from dAref to the user, respectively; ϕ1(t) and ψ1(t) are\
    \ the angles of irradiance and incidence for the path from VLC AP to dAref; ϕ2(t)\
    \ and ψ2(t) are the angles for the path from dAref to the user; I(·) is the indicator\
    \ function; and the integral is over all reflecting surfaces. While higherorder\
    \ reflections also contribute to the total received power, however, modelling\
    \ the first-order reflections captures the most significant portion of the NLoS\
    \ power while managing computational complexity [19], [21].\n\nThe total channel\
    \ DC gain used in this paper is given as:\n\n$$H\\_{a,u}(t) = H\\_{a,u}^{\\text{LoS}}(t)\
    \ + H\\_{a,u}^{(1)}(t) \\tag{3}$$\n\n## *C. NOMA Transmission and Interference\
    \ Model*\n\nIn this work, we employ power-domain Non-Orthogonal Multiple Access\
    \ (NOMA) in the downlink to enhance spectral efficiency [4], [5]. Each VLC AP\
    \ a ∈ A serves a dynamically selected user set Ua(t) ⊆ U by superimposing their\
    \ signals xa,u(t) using allocated electrical powers Pa,u(t). The total allocated\
    \ electrical power per VLC AP is constrained by P elec a,max:\n\n$$\\sum\\_{u\
    \ \\in \\mathcal{U}\\_a(t)} P\\_{a,u}(t) \\le P\\_{a,\\text{max}}^{\\text{elec}},\
    \ \\quad \\forall a \\in \\mathcal{A}. \\tag{4}$$\n\nIntensity Modulation/Direct\
    \ Detection (IM/DD) is assumed for optical transmission [20].\n\nIn this work,\
    \ we assumed that the users' receiver performs a perfect Successive Interference\
    \ Cancellation (SIC) to decode their messages [22]. Users in Ua(t) are decoded\
    \ sequentially, typically based on their channel power gains Ga,u(t) = (RHa,u(t))<sup>2</sup>\
    \ , where R is the PD responsivity and Ha,u(t) is the total channel gain from\
    \ Eq. (3). Assuming perfect SIC, user u<sup>k</sup> (where users are indexed k\
    \ = 1...Ka(t) ) treats signals intended for users j > k (users with weaker channels)\
    \ as noise and cancels the signals from users j < k (users with stronger channels).\
    \ The resulting Signal-to-Interference-plus-Noise Ratio (SINR) for user u<sup>k</sup>\
    \ served by VLC AP a is:\n\n$$\\text{SINR}\\_{a,u\\_k}(t) = \\frac{G\\_{a,u\\\
    _k}(t)P\\_{a,u\\_k}(t)}{\\sum\\_{j \\neq k}^{K\\_a(t)} G\\_{a,u\\_k}(t)P\\_{a,u\\\
    _j}(t) + I\\_{\\text{ICI},u\\_k}(t) + \\sigma\\_{\\text{noise}}^2} \\tag{5}$$\n\
    \nwhere σ 2 noise is the noise variance, and IICI,u<sup>k</sup> (t) is the Inter-Cell\
    \ Interference (ICI) from neighboring VLC APs, which has a significant impact\
    \ in dense VLC-NOMA network deployments [23] [24]. The ICI is given as:\n\n$$I\\\
    _{\\mathrm{ICI},u\\_k}(t) = \\sum\\_{a' \\in \\mathcal{A}, a' \\neq a} \\sum\\\
    _{u' \\in \\mathcal{U}\\_{a'}(t)} G\\_{a',u\\_k}(t) P\\_{a',u'}(t). \\quad (6)$$\n\
    \nHere, Ga′ ,u<sup>k</sup> (t) = (RHa′ ,u<sup>k</sup> (t))<sup>2</sup> is the\
    \ channel power gain from interfering/neighboring VLC AP a ′ to user uk.\n\nThe\
    \ achievable data rate for user u<sup>k</sup> is calculated using the Shannon\
    \ capacity formula over bandwidth B:\n\n$$\\text{Rate}\\_{a,u\\_k}(t) = B \\log\\\
    _2(1 + \\text{SINR}\\_{a,u\\_k}(t)). \\tag{7}$$\n\nCrucially, the ICI term (6)\
    \ couples the decisions (user selection Ua′ (t) and power allocation Pa′ ,u′ (t))\
    \ made by neighboring VLC APs a ′ . This coupling, combined with the time-varying\
    \ channel gains Ga,u(t) due to mobility and the need to satisfy diverse QoS requirements\
    \ through careful power allocation. The MARL frameworks proposed in Section III\
    \ are designed precisely to enable the distributed VLC AP agents to learn effective\
    \ policies for user selection Ua(t) and power allocation Pa,u(t) that navigate\
    \ these challenges to jointly optimize QoS and network stability objectives.\n\
    \n# *D. User Mobility Model*\n\nIn this study, we deploy the Random Direction\
    \ (RD) model. The RD model is selected for multiple reasons relevant to our examination\
    \ of dynamic indoor VLC-NOMA networks. 1) It allows continuous user mobility,\
    \ essential for modelling the dynamic fluctuations in channel gains and SINR characteristics\
    \ of the indoor VLC environment. 2) In contrast to the Random Waypoint model,\
    \ RD generally yields a more uniform spatial distribution of users over time,\
    \ hence preventing unrealistic clustering in the middle of the room [25] [26].\
    \ In the RD model, each user u ∈ U initiates at a random location p UE u (0) within\
    \ the room at a height of HUE. The user uniformly selects a starting direction\
    \ θ<sup>u</sup> from the interval [0, 2π) and a speed v<sup>u</sup> uniformly\
    \ from the range [vmin, vmax], which denotes normal pedestrian velocities. The\
    \ user thereafter proceeds in a linear trajectory with the velocity vector v<sup>u</sup>\
    \ = (v<sup>u</sup> cos θu, v<sup>u</sup> sin θu, 0) until encountering one of\
    \ the room's boundaries.\n\nOnce the user approach one of the room's boundaries,\
    \ the user's trajectory changes and a new velocity and direction are selected.\
    \ For the sake of simplicity, we assume there is no stop period at the boundaries.\
    \ The user continues in this procedure for the entirety of the simulation period.\n\
    \nUser locations are updated at discrete time periods ∆t. The position at the\
    \ subsequent time step is determined as:\n\n$$\\mathbf{p}\\_{u}^{\\mathrm{UE}}(t+\\\
    Delta t) = \\mathbf{p}\\_{u}^{\\mathrm{UE}}(t) + \\mathbf{v}\\_{u}(t) \\cdot \\\
    Delta t \\tag{8}$$\n\nwhere vu(t) is the user's velocity vector during the interval\
    \ [t, t + ∆t), potentially changing at room's boundaries.\n\n#### *E. Network\
    \ Quality of Service (QoS)*\n\nThis section outlines the service requirements\
    \ of users within the simulated VLC-NOMA network which is reflecting a realistic\
    \ indoor application demand. Achieving these diverse QoS requirements while maintaining\
    \ network stability forms the primary challenge addressed in this study through\
    \ the proposed MARL-based resource allocation strategy.\n\nWe group users into\
    \ two distinct groups depending on their application requirements:\n\n- High-Priority\
    \ (HP) Users: Representing users using highbandwidth/data rate services, for instance,\
    \ real-time video conferencing or interactive gaming. The group of HP users is\
    \ given as UHP ⊆ U.\n- Standard-Priority (SP) Users: Representing users with applications\
    \ such as web browsing, email, or any noncritical data service that generally\
    \ require a low bandwidth/ data rate. The group of SP users is denoted by USP\
    \ ⊆ U.\n\nWe assume these groups form a partition of the total user group U, such\
    \ that UHP ∩ USP = ∅ and UHP ∪ USP = U. In this work we assumed that users are\
    \ assigned to a group based on predefined ratios. The specific and measurable\
    \ QoS requirements are defined for each group as follows:\n\n• HP Users (u ∈ UHP):\n\
    \n- Minimum required data rate: R req HP .\n- SP Users (u ∈ USP):\n\t- Minimum\
    \ required data rate: R req SP where ( R req SP < R req HP).\n- Service Quality\
    \ (Outage):\n\t- QoS also implies providing a quality level of service. This is\
    \ directly related to the *Outage Probability*, defined as the likelihood that\
    \ a SP user's achieved\n\ndata rate falls below a minimum acceptable threshold\
    \ (Routage). Minimizing outage is crucial for the quality of service delivered\
    \ for users in the VLC network.\n\nThese QoS definitions are fundamental to the\
    \ MARL framework design. The MARL agents (VLC APs) must learn resource allocation\
    \ policies (power Pa,u(t), user selection Ua(t)) that strive to meet these objectives.\
    \ The agents' observations include information related to recent QoS performance.\
    \ Critically, the reward function guiding the agents' learning process will be\
    \ directly based on meeting these QoS requirements. For instance, agents receive\
    \ positive rewards for achieving the target data rates (R req HP, Rreq SP ) for\
    \ their associated users and incur penalties for failing to meet minimum rate\
    \ requirements. Eventually, the MARL framework will learn to balance these potentially\
    \ conflicting QoS goals across all users while simultaneously managing network\
    \ stability objectives (throughput, interference, handovers), a common trade-off\
    \ in network resource allocations.\n\n#### *F. Network Stability*\n\nBesides satisfying\
    \ the diverse network QoS needs, maintaining overall network stability is a key\
    \ objective, particularly under the dynamic conditions resulting from user mobility\
    \ and high-density deployments. In this work, we consider the following network\
    \ stability components:\n\n- Handover Stability: Effective mobility management\
    \ includes the reduction of disruptions resulting from handovers. Key indicators\
    \ are [44]:\n\t- *Handover Rate (HOR)*: A measure of the frequency of handovers\
    \ per user. Excessively high handover rates can indicate instability. The goal\
    \ is to achieve necessary handovers efficiently.\n\t- *Ping-Pong Ratio (PPR):*\
    \ The percentage of handovers promptly followed by an additional handover to the\
    \ previous VLC AP within a specified time period. High PPR indicates instability,\
    \ which needs to be avoided.\n- Network Efficiency: The overall network efficiency\
    \ is measured by the *Network Sum-Rate*, which is considered a crucial metric\
    \ for network stability and effective resource use throughout the network.\n\n\
    #### *G. Dimming Model*\n\nVLC APs, light sources, inherently combine illumination\
    \ and communication functions. Dimming control allows adjustment of brightness\
    \ levels for user comfort, which inevitably interacts with communication performance\
    \ [3].\n\nWe model dimming control through a dimming factor, γa, for each VLC\
    \ AP a. This factor represents the target brightness level relative to the maximum\
    \ illumination level. In this work, we assume γ<sup>a</sup> is selected from a\
    \ discrete set Γ = {0.2, 0.3, . . . , 1.0}. The γmin is set to 0.2 to achieve\
    \ the minimum level of illumination. We assume that the average transmitted optical\
    \ power scales proportionally with γa, determining the illumination level in the\
    \ environment. It is worth to mention that, the overall illumination is subject\
    \ to minimum requirements based on standards (office space) [32].\n\nThe communication\
    \ signal is transmitted concurrently with the illumination function. While various\
    \ dimming techniques exist and can interact with the communication signal in complex\
    \ ways [31]. In this work, we assume that the primary impact of operating at a\
    \ specific dimming level γ<sup>a</sup> is implicitly handled within the maximum\
    \ electrical power budget P elec a,max. The MARL agents must learn to perform\
    \ resource allocation effectively within this budget and adapt their strategy\
    \ based on the available power headroom implied by the dimming level. This allows\
    \ the agent's policy to be conditioned on the lighting requirement. In our MARL\
    \ frameworks, the dimming level γ<sup>a</sup> is considered part of the observable\
    \ state for each agent a. It provides crucial context about the VLC AP's operational\
    \ mode (P elec a,max). It is worth mentioning that the MARL agents do not control\
    \ the dimming level. The agents learn, through interaction with the environment,\
    \ how the current dimming level affects the relationship between their actions\
    \ (power allocation Pa,u(t), user selection Ua(t) within the budget P elec a,max)\
    \ and the resulting outcomes (QoS metrics and stability indicators).\n\nBy incorporating\
    \ the dimming factor, the MARL agents can learn context-aware resource allocation\
    \ strategies that optimize communication objectives (QoS, stability) effectively\
    \ under different, pre-defined illumination conditions.\n\n#### III. PROPOSED\
    \ MARL FRAMEWORK\n\n## *A. MARL Problem Formulation*\n\nWe formulate the dynamic\
    \ resource allocation problem in the VLC-NOMA network as an MARL task. The MARL\
    \ key components are defined as follows:\n\n*1) Agents:* The learning agents are\
    \ the VLC APs, indexed by a ∈ A = {1, 2, ..., NAP}. Each agent a learns its own\
    \ policy π<sup>a</sup> to engage with the environment and other agents. The agents\
    \ aim to optimize their objectives, potentially in a collaborative or independent\
    \ way.\n\n*2) State Space (Observation):* At each discrete time step t, each agent\
    \ a receives a local observation oa(t), which forms its current state representation\
    \ sa(t). This observation includes necessary information for resource allocation\
    \ assumed in this work:\n\n- Local Channel Gains: Locally measure channel power\
    \ gains Ga,u(t) for users u currently associated with VLC AP a (Ua(t))\n- Neighbor\
    \ Channel Gains: Locally measure channel power gains Ga′ ,u(t) from relevant neighboring\
    \ VLC APs a ′ to associated users u ∈ Ua(t).\n- Interference Information: Locally\
    \ measure ICI affecting the associated users.\n- QoS Information: Obtain the current\
    \ QoS for associated users u ∈ Ua(t). Such as users achieved data rate in the\
    \ previous step (in the environment) Ratea,u(t − ∆t) and their QoS class (HP or\
    \ SP user).\n- Dimming Level: The operational dimming factor γ<sup>a</sup> of\
    \ light sources (VLC AP a).\n- Handover Status: Handover information for associated\
    \ users include the time since last handover and previous VLC AP to optimize the\
    \ handover ping-pong effect.\n- Own Previous Action: The action aa(t − ∆t) taken\
    \ by agent a in the previous step.\n\nThe state vector sa(t) integrates this information\
    \ to provide agent a with context for joint resource allocation and optimization.\n\
    \n*3) Action Space:* At time step t, agent a selects an action aa(t) ∈ Aa. The\
    \ action space A<sup>a</sup> of VLC-NOMA resource allocations involves:\n\n- User\
    \ Selection: Choosing a group of users Ua(t) ⊆ Ucand,a(t) (candidate users) to\
    \ serve up to a maximum of Kmax users. This is seen as a discrete action.\n- Power\
    \ Allocation: Assigning NOMA power levels Pa,u(t) for each selected user u ∈ Ua(t).\
    \ This is subjected to the total power budget P elec a,max governed by the current\
    \ dimming factor γa. This is seen as a continuous action.\n- Handover Trigger:\
    \ For each associated user u ∈ Ua(t), an action is decided to whether to trigger\
    \ a handover procedure. This is seen as a discrete action.\n\nThe action aa(t)\
    \ is thus a composite action including user selection, power allocation, and handover\
    \ triggers. The dimensionality and nature (discrete/continuous) of the action\
    \ space components have been carefully considered during algorithm implementation\
    \ in this paper.\n\n*4) Reward Function:* The reward ra(t) received by agent a\
    \ (or the global reward r(t) used in centralized training) is designed to guide\
    \ the learning process of resource allocations towards achieving high QoS reliability\
    \ and network stability. We define a weighted-sum reward structure:\n\n$$r\\_a(t)\
    \ = w\\_{\\rm PoS} \\cdot R\\_{\\rm PoS,a}(t) + w\\_{\\rm Stab} \\cdot R\\_{\\\
    rm Stab,a}(t) \\tag{9}$$\n\nwhere wQoS and wStab are primary weighting factors\
    \ balancing the two main objectives. The components are defined as follows:\n\n\
    - QoS Reward/ Penalty (RQoS,a(t)): Focuses on meeting specific data rate requirements\
    \ for HP users and ensuring baseline quality via outage avoidance for SP users.\n\
    \t- *HP Rate Satisfaction:* A positive reward for each HP user meeting its target\
    \ rate, given by +wHP met × (#HP users satisfaction rate).\n\t- *Outage Penalty:*\
    \ A negative reward applied primarily to users who fall below the outage threshold,\
    \ given by −woutage × (#SP users dissatisfaction rate).\n- Stability Reward/Penalty\
    \ (RStab,a(t)): Focuses on handover stability and overall network efficiency.\n\
    \t- *Handover Frequency Penalty:* A negative reward penalizing initiated handovers,\
    \ equal to −wHO × (# HOs initiated by agent a at step t).\n\t- *Ping-Pong Handover\
    \ Penalty:* A negative reward for handovers identified as ping-pong events (based\
    \ on HO history in state), equal to −wpp × (# Ping-Pong HOs involving agent a\
    \ at step t).\n\n– *Sum Throughput Reward:* A positive term encouraging overall\
    \ network efficiency, given by +wthr × (scaled achieved throughput).\n\nThe effective\
    \ performance of the MARL agents are critically depending on the careful tuning\
    \ and scaling of the main weights (wQoS, wStab) and internal sub-weights (wHP\
    \ met, woutage, wHO, wpp, wthr). We employed a shaped reward function to offer\
    \ the MARL agents more granular feedback and improve learning efficacy. This approach\
    \ scales rewards in proportion to the level of QoS satisfaction attained by users\
    \ and network stability requirements. Thus creating denser learning signals. The\
    \ specific design, integration, and tuning of this shaped reward mechanism are\
    \ detailed in Sections IV-C and V-B\n\n*5) Objective:* The goal is to find optimal\
    \ policies (for resource allocations) π <sup>∗</sup> = {π ∗ 1 , ..., π<sup>∗</sup>\
    \ NAP } that maximize the expected long-term discounted cumulative reward for\
    \ the system [49]:\n\n$$\\pi^\\* = \\arg\\max\\_{\\pi} \\mathbb{E}\\_{\\pi} \\\
    left[ \\sum\\_{k=t}^T \\delta^{k-t} r\\_k \\mid s\\_t \\right] \\tag{10}$$\n\n\
    where Eπ[·] denotes the expectation under the joint policy π = {π1, ..., πNAP\
    \ }, δ ∈ [0, 1) is the discount factor controlling the importance of future rewards,\
    \ T is the time horizon and r<sup>k</sup> represents the relevant reward at time\
    \ step t, given the state sa(t).\n\n#### *B. MARL Frameworks*\n\nIn this section,\
    \ we present the selected MARL framework to optimize the resource allocations\
    \ in VLC-NOMA network. We aim to compare the effectiveness of two well-known MARL\
    \ architectures: Centralized Training with Centralized Execution (CTCE) and Centralized\
    \ Training with Decentralized Execution (CTDE). Both architectures are coupled\
    \ with Proximal Policy Optimization (PPO) algorithm.\n\n*1) CTDE architecture\
    \ with Multi-Agent Proximal Policy Optimization (MAPPO):* The CTDE architecture\
    \ is suitable for a dynamic VLC-NOMA network. Where VLC APs agents must promptly\
    \ adapt to the channel fluctuations due to user mobility and light dimming changes\
    \ through CTDE decentralized execution. The CTDE centralized training enables\
    \ agents to develop coordinated tactics essential for tackling network-wide issues,\
    \ including interference and fulfilling diverse QoS requirements [13]. We adopt\
    \ MAPPO [34], an algorithm recognized for its stability and robust performance\
    \ in cooperative MARL implementation. MAPPO's architecture enables decentralized\
    \ agents to acquire policies that implicitly coordinate (directed by a centralized\
    \ critic), making it effective for optimizing resource allocations in VLC-NOMA\
    \ networks. In our MAPPO (CTDE) implementation:\n\n- Each VLC AP agent a employs\
    \ a decentralized actor network π<sup>θ</sup><sup>a</sup> (aa|sa(t)). This network\
    \ takes only the local state sa(t) as input to generate the composite resource\
    \ allocation and handover action aa(t). The agents actors do not share parameters,\
    \ ensuring decentralized execution.\n- A centralized critic network Vϕ(sglobal(t))\
    \ is used during the training phase. The global state sglobal(t) provided to\n\
    \nthe critic is a concatenation of all individual local agent states {s1(t), .\
    \ . . , sNAP (t)}. This allows the critic to learn an accurate value function\
    \ reflecting the overall system state and joint agent performance.\n\n- During\
    \ training, the centralized critic provides a stable and comprehensive learning\
    \ signal for updating the decentralized actors. Each actor πθ<sup>a</sup> is updated\
    \ using the PPO objective function, leveraging advantage estimates calculated\
    \ from the centralized value function Vϕ. MAPPO's effectiveness has been demonstrated\
    \ in complex resource allocation tasks, such as Radio Access Network (RAN) slicing\
    \ [39].\n- During execution, only the trained decentralized actor networks πθ<sup>a</sup>\
    \ are deployed. Each VLC AP makes decisions based solely on its local state sa(t),\
    \ preserving the scalability and low-latency benefits essential for real-time\
    \ operation in dynamic VLC-NOMA networks.\n\nIt is worth mentioning that the centralized\
    \ training phase for MAPPO (CTDE) can still present challenges if the dimensionality\
    \ of the global state sglobal(t) for the critic becomes excessively large in very\
    \ extensive networks.\n\n*2) CTCE architecture with Centralized PPO (CenPPO):*\
    \ The CTCE architecture can be considered as a valuable benchmark for this work.\
    \ As CTCD decisions can be made based on a comprehensive global network view at\
    \ execution time. This is particularly relevant in VLC-NOMA networks where VLC\
    \ APs' actions are tightly coupled through ICI [13]. We implement CTCE using a\
    \ single, centralized PPO agent [38] that holistically controls all VLC APs. In\
    \ our CenPPO (CTCE) implementation:\n\n- A single logical agent acts as the central\
    \ controller for all VLC APs in the VLC-NOMA network. It observes the concatenated\
    \ global state sglobal(t) = {s1(t), . . . , sNAP (t)}, which comprises the local\
    \ states of all VLC APs.\n- The VLC AP agent's actor network is πθ(a|sglobal(t))\
    \ and critic network is Vϕ(sglobal(t)). Both networks receive the full global\
    \ state sglobal(t) as input.\n- The actor network π<sup>θ</sup> generates the\
    \ joint action a(t) = {a1(t), . . . , a<sup>N</sup>AP (t)} for all VLC APs simultaneously.\
    \ This joint action vector encapsulates the individual composite actions. The\
    \ central agent explicitly uses this to make decisions that aim to mitigate interference\
    \ and optimize overall network objectives.\n- Training employs the standard PPO\
    \ algorithm, using the global state, the joint action, and a system-level reward\
    \ which is applied globally.\n- During execution, the central controller gathers\
    \ the global state sglobal(t), uses the trained policy π<sup>θ</sup> to compute\
    \ the joint action a(t), and then distributes the corresponding individual action\
    \ aa(t) to each VLC AP a. This allows for explicit coordination based on the full\
    \ network picture.\n\nCentralized controllers using PPO and similar algorithms\
    \ have been applied to related wireless resource management problems [40]. The\
    \ primary drawback of CTCE is scalability. The global state and joint action spaces\
    \ grow significantly with the number of VLC APs (NAP) and users. This can limit\
    \ its practical applicability in very large-scale VLC networks and impose considerable\
    \ communication overhead. However, it provides a useful performance upper bound\
    \ for comparison in our simulated VLC-NOMA scenario.\n\n*3) Neural Network layout\
    \ (NN):* All actor and critic networks within both MAPPO (CTDE) and CenPPO (CTCE)\
    \ frameworks are implemented using Multi-Layer Perceptrons (MLPs). The MLPs has\
    \ 2 hidden layers (each containing 256 neurons with ReLU activation) were used\
    \ for both actors and critics. Actor outputs employed appropriate activation functions\
    \ Softmax for discrete selections, Gaussian policy heads for normalized power\
    \ allocation, and Sigmoid for handover triggers. While critic outputs used linear\
    \ activation. The final layer(s) of actor networks are structured as separate\
    \ output heads originating from the shared MLP body:\n\n- For *discrete user selection*\
    \ (choosing up to Kmax users from candidates), a head with a softmax activation\
    \ for selecting each candidate user.\n- For *continuous NOMA power allocation*\
    \ (power levels Pa,u(t) to selected users), a head models a Gaussian distribution.\
    \ It has two sub-heads: one outputting the mean values µ (passed through a Tanh\
    \ activation and scaled to ensure power constraints and positivity) and another\
    \ outputting values passed through a softplus activation to ensure positive standard\
    \ deviations σ for each selected user's power.\n- For *discrete handover trigger*\
    \ decisions for each associated user ( keep current association with VLC AP or\
    \ initiate handover), a head with a sigmoid activation function is used, allowing\
    \ for independent binary trigger decisions.\n\nIt is worth mentioning that, no\
    \ parameter sharing is employed between the actor networks of different agents\
    \ in the MAPPO framework, nor between actors and critics beyond the standard PPO\
    \ structure. Fig. 1 illustrates detailed of the shared MLP body and distinct output\
    \ heads for user selection, NOMA power allocation, and handover triggers. Specific\
    \ NN dimensions and hyperparameters are detailed in Section IV-A.\n\n![](_page_6_Figure_6.jpeg)\n\
    \nFig. 1. Structure of the Multi-Headed Actor Network.\n\n![](_page_6_Figure_8.jpeg)\n\
    \nFig. 2. 3D System model illustrating the indoor environment (L × W × H) with\
    \ ceiling-mounted VLC APs, final user positions (HP/SP differentiated), and example\
    \ user paths.\n\n#### IV. SIMULATION SETUP\n\nThis section details the simulation\
    \ environment, network parameters, MARL framework implementations, benchmark and\
    \ baseline methods. Comparisons and performance metrics were employed to evaluate\
    \ the proposed tailored frameworks MAPPO and CenPPO for optimizing resource allocations\
    \ in VLC-NOMA network. Figure 2 shows the VLC network model setup.\n\n## *A. Simulation\
    \ Environment*\n\nSimulations were conducted using Python 3. The core MARL frameworks\
    \ were implemented using the PyTorch deep learning framework [51]. Training was\
    \ accelerated using NVIDIA CUDA-enabled GPUs. The dynamic environment encompassing\
    \ VLC channel models, NOMA protocol , user mobility, QoS classes, and network\
    \ stability, were developed as a custom simulation environment, interacting with\
    \ the PyTorch-based agents via a standard RL interface. The simulation progresses\
    \ in discrete time steps ∆t. Parameters are selected based on typical values and\
    \ standards for indoor VLC, NOMA, and mobility modeling in [3], [4], [18], [27],\
    \ [34], [38]. Key parameters are summarized in Table I.\n\n## *B. Reward Weight\
    \ Optimization*\n\nA key aspect of configuring the proposed MARL frameworks is\
    \ to balance the objectives of QoS and network stability requirements. This primarily\
    \ controlled by the main weights (wQoS and wStab) in the reward function in Eq.\
    \ (9). We utilized BO to systematically search for weights that maximize overall\
    \ system performance governed by a composite score f(w). The\n\nTABLE I SIMULATION\
    \ PARAMETERS\n\n| Parameter                                      | Value     \
    \                       | Unit         |\n|------------------------------------------------|----------------------------------|--------------|\n\
    | Environment                                    |                           \
    \       |              |\n| Room Dimensions (L × W × H)                    | 4x8x3\
    \                            | m            |\n|                             \
    \                   |                                  |              |\n| VLC\
    \ APs                                        |                               \
    \   |              |\n| Number of APs (NAP)<br>AP Arrangement          | 8<br>2x4\
    \ grid x(1,3), y(1,3,5,7) |              |\n| AP Height (H)                  \
    \                | 3                                | m            |\n| elec<br>Max\
    \ Electrical Power (P<br>a,max)      | 15                               | W  \
    \          |\n| LED Semi-Angle (Φ1/2)                          | 60          \
    \                     | ◦            |\n|                                    \
    \            |                                  |              |\n| Users (UEs)\
    \                                    |                                  |    \
    \          |\n| Number of Users (NUE)                          | 20          \
    \                     |              |\n| UE Height (HUE)                    \
    \            | 0.85                             | m            |\n| PD Area (APD)\
    \                                  | 1 cm2                            |      \
    \        |\n| PD FOV (Ψc)                                    | 70            \
    \                   | ◦            |\n| Optical Filter Gain (Ts)             \
    \          | 1                                |              |\n| Optical Concentrator\
    \ Gain (g)                  | 1                                |             \
    \ |\n| PD Responsivity (R)                            | 0.5                  \
    \            | A/W          |\n| Channel & Noise                             \
    \   |                                  |              |\n| Noise Power Spectral\
    \ Density (N0)              | 1 × 10−22                        | A2<br>/Hz   \
    \ |\n| Modulation Bandwidth (B)                       | 20                   \
    \            | MHz          |\n|                                             \
    \   |                                  |              |\n| NOMA              \
    \                             |                                  |           \
    \   |\n| Max Users per AP (Kmax)                        | 10                 \
    \              |              |\n| SIC Assumption                            \
    \     | Perfect                          |              |\n| Mobility (Random\
    \ Direction)                    |                                  |         \
    \     |\n| Min Speed (vmin)                               | 0.5              \
    \                | m/s          |\n| Max Speed (vmax)                        \
    \       | 1.5                              | m/s          |\n| Time Step (∆t)\
    \                                 | 1                                | s     \
    \       |\n|                                                |                \
    \                  |              |\n| QoS                                   \
    \         |                                  |              |\n| Percentage HP\
    \ Users<br>req                     | 25                               | %    \
    \        |\n| HP Rate Req (R<br>HP)<br>req<br>SP Rate Req (R | 12<br>2       \
    \                   | Mbps<br>Mbps |\n| SP )<br>Outage Threshold (Routage)   \
    \          | 0.5                              | Mbps         |\n|            \
    \                                    |                                  |    \
    \          |\n| Dimming                                        |             \
    \                     |              |\n| Minimum Dimming Factor γ           \
    \            | 0.2                              |              |\n| Dimming Factor\
    \ Set (Γ)                         | {0.2, , 1.0}                     |       \
    \       |\n| Handover                                       |                \
    \                  |              |\n| Ping-Pong Time Window (Tpp)           \
    \         | 2                                | s            |\n| Baseline HO Hysteresis\
    \                         | 3 dB                             |              |\n\
    |                                                |                           \
    \       |              |\n| MARL Training                                  | \
    \                                 |              |\n| Agent Type             \
    \                        | CenPPO, MAPPO                    |              |\n\
    | Optimizer                                      | Adam<br>5 × 10−4          \
    \       |              |\n| Learning Rate                                  | \
    \                                 |              |\n| Discount Factor (γ)    \
    \                        | 0.97<br>0.95                     |              |\n\
    | GAE Lambda (λGAE)<br>Actor LR                  | 5 × 10−4                  \
    \       |              |\n| Critic LR                                      | 1\
    \ × 10−4                         |              |\n| PPO Clip Ratio (ϵclip)  \
    \                       | 0.2                              |              |\n\
    | Entropy Coefficient                            | 0.01                      \
    \       |              |\n| Rollout Buffer Size                            | 2048\
    \                             | steps        |\n| NN Hidden Layer Units      \
    \                    | 2                                |              |\n| NN\
    \ layer Neurons                               | 256                          \
    \    |              |\n| Total Training Steps                           | 1.5\
    \ × 106                        | steps        |\n| Evaluation Episodes       \
    \                     | 1000                             |              |\n| Max\
    \ Steps per Episode                          | 100                           \
    \   |              |\n\n8\n\nreward score obtained from a full train-evaluate\
    \ cycle which was calculated using the following formula:\n\n$$\\begin{aligned}\
    \ f(\\mathbf{w}) &= w\\_{\\text{QoS}} (1.0 \\cdot QoSSR\\_{\\text{HP}} - 0.5 \\\
    cdot P\\_{\\text{out}}) \\\\ &+ w\\_{\\text{Stab}} (-0.2 \\cdot HOR - 0.3 \\cdot\
    \ PPR \\quad (11)) \\\\ &+ 0.1 \\cdot \\bar{R}\\_{\\text{sum}}) \\end{aligned}$$\n\
    \nR¯ sum is the normalized value of network rate used in the calculation and scaling\
    \ of the composite reward score given as R¯ sum = Rsum/Rmax where Rmax is the\
    \ maximum network sum rate used for scaling. By scaling Rsum to a consistent range\
    \ (0-1) its contribution to the reward score becomes independent of the raw Mbps\
    \ values achieved. Table II coefficients are set to enable the VLC-NOMA network\
    \ to prioritize reliable high data rate services. Such as real-time video conferencing\
    \ service for HP users. The weights chosen to emphasize HP QoS (QoSSRHP) as critical\
    \ (1.0), followed by network reliability of (Pout) (0.5). Ping-pong (P P R, 0.3),\
    \ handover stability (HOR, 0.2), and total throughput (Rsum, 0.1) are deemed secondary\
    \ to ensure primary application performance.\n\nTABLE II AND SUB-WEIGHTS COEFFICIENTS,\
    \ REWARD TERMS AND PRIORITY LEVELS\n\n| Sub-weight    | Reward Term | Priority\
    \ Level |\n|---------------|-------------|----------------|\n| wHP met = 1   |\
    \ QoSSRHP     | Highest        |\n| woutage = 0.5 | Pout,SP     | High       \
    \    |\n| wP P R = 0.3  | P P R       | Moderate       |\n| wHO = 0.2     | HOR\
    \         | Lower          |\n| wthr = 0.1    | R¯sum       | Lowest         |\n\
    \nThe BO optimization explored the search space W for wQoS ∈ [0.1, 5.0] and wStab\
    \ ∈ [0.1, 5.0], evaluating 81 points/runs in total. The exploration of the parameter\
    \ space and the corresponding performance scores are presented in Figure 3.\n\n\
    ![](_page_7_Figure_9.jpeg)\n\nFig. 3. BO exploration of the reward main weight\
    \ parameter space (wStab vs wQoS).\n\nThe BO results indicate that the composite\
    \ performance score is sensitive to the choice of weights. As shown in Figure\
    \ 3, the highest performance scores, Target ≈ 4.6 − 4.8, were achieved in specific\
    \ regions. The optimization converged to a maximum observed performance score\
    \ of approximately. 4.75 at weights wQoS ≈ 2.5 and wStab ≈ 0.75. The selection\
    \ of the BO-tuned weights reflects an emphasis placed on QoS metrics relative\
    \ to network stability based on the composite score in Eq. (11).\n\nAlgorithm\
    \ 1 Bayesian Optimization for Main Reward Weight Tuning\n\nRequire: Search space\
    \ W for w = [wQoS, wStab] Require: Objective function f(w) (executes MARL\n\n\
    - train+eval, returns composite score)\n- Require: Number of initial samples Ninit\n\
    - Require: Total number of BO iterations Ntrials\n\nRequire: Acquisition function\
    \ α(w)\n\nEnsure: Optimized main weights w<sup>∗</sup>\n\n- 1: Initialization:\
    \ Sample Ninit points {w1, ..., wNinit } from W Evaluate y<sup>i</sup> = f(wi)\
    \ for i = 1, ..., Ninit. Initialize dataset D = {(w<sup>i</sup> , yi)} Ninit <sup>i</sup>=1\
    \ .\n- 2: Optimization Loop:\n- 3: for t = Ninit to Ntrials − 1 do\n- 4: Fit/Update\
    \ Gaussian Process model using data D.\n- 5: Find wt+1 = arg maxw∈W α(w|D).\n\
    - 6: Evaluate yt+1 = f(wt+1). ▷ Requires full MARL train+eval\n- 7: Augment dataset\
    \ D ← D ∪ {(wt+1, yt+1)}.\n- 8: end for\n- 9: Return Best: Find w<sup>∗</sup>\
    \ = arg maxwi|(wi,yi)∈<sup>D</sup> y<sup>i</sup> .\n\n# *C. Training Performance\
    \ (MAPPO vs CenPPO)*\n\n![](_page_8_Figure_18.jpeg)\n\nFig. 4. Normalized Training\
    \ Performance showing Reward versus Timestep for MAPPO and CenPPO.\n\nFigure 4\
    \ shows the learning process of the proposed MAPPO (CTDE) and CenPPO (CTCE) frameworks.\
    \ The figure plots the training normalized cumulative reward against the total\
    \ environment steps for MAPPO and CenPPO. Both MARL frameworks show converging\
    \ towards stable reward levels, learning trends indicating successful policy optimization\
    \ concerning the reward function. MAPPO appears to converge slightly faster as\
    \ compared to CenPPO.\n\n# *D. Comparative Performance Analysis (MAPPO vs CenPPO\
    \ vs Baseline)*\n\nThe core performance comparison between the proposed MAPPO\
    \ (CTDE) and CenPPO (CTCE) and the Baseline resources allocation approaches is\
    \ presented in Figures 5 to 10. The analysis focuses on assessing how effectively\
    \ each resources allocation approach achieves the joint objectives of QoS satisfaction\
    \ and network stability under the challenging dynamic conditions.\n\n![](_page_8_Figure_23.jpeg)\n\
    \nFig. 5. Comparison of Network QoS metrics: Average User Rate, QoSSR HP, and\
    \ SP Outage Probability.\n\n![](_page_8_Figure_25.jpeg)\n\nFig. 6. Comparison\
    \ of HP and SP users average data rates (R¯HP,R¯ SP).\n\n## *1) QoS Performance\
    \ Comparison:*\n\n- Average User Data Rate: Figure 5 shows that both MAPPO and\
    \ CenPPO significantly outperform the Baseline approach. MAPPO achieves the highest\
    \ overall rate of (6.78 Mbit s<sup>−</sup><sup>1</sup> ). Followed by CenPPO with\
    \ (5.73 Mbit s<sup>−</sup><sup>1</sup> ). while Baseline archived (4.24 Mbit s<sup>−</sup><sup>1</sup>\
    \ ). Figure 6 shows MAPPO delivers considerably higher rates specifically to HP\
    \ users (14.85 Mbit s<sup>−</sup><sup>1</sup> ) compared to CenPPO (10.62 Mbit\
    \ s<sup>−</sup><sup>1</sup> ) and the Baseline (6.21 Mbit s<sup>−</sup><sup>1</sup>\
    \ ). For SP users, MAPPO (6.76 Mbit s<sup>−</sup><sup>1</sup> ) and CenPPO (5.76\
    \ Mbit s<sup>−</sup><sup>1</sup> ) perform similarly and better than the Baseline\
    \ with (2.18 Mbit s<sup>−</sup><sup>1</sup> ).\n- QoSSR (HP Users): Figure 5 shows\
    \ the both MAPPO and CenPPO achieve high satisfaction ratios for HP users which\
    \ are significantly better than the Baseline. where MAPPO leads with 0.87, closely\
    \ followed by CenPPO at\n\n0.75. while Basline at (0.02). These results indicate\
    \ that the MAPPO is more effective at prioritizing HP users to meet their target.\n\
    \n• Outage Probability (SP Users): Figure 5 shows the outage probability for SP\
    \ users. The CenPPO achieves the lowest SP outage probability of (0.1. The MAPPO\
    \ performs reasonably well (0.17), while the Baseline exhibits the highest outage\
    \ (0.43). These results indicate that the CenPPO appears better at ensuring baseline\
    \ service for the network users.\n\nThe QoS results (as shown in Figures 5, 6)\
    \ indicate distinct strategies learned by the MARL agents. The MAPPO tend to prioritize\
    \ the HP users, potentially leveraging its decentralized nature for faster local\
    \ optimization, which is leading to higher HP rates and satisfaction. In contrast,\
    \ the CenPPO achieves a more balanced QoS outcome, sacrificing some HP performance\
    \ for significantly minimizing the network users' outage. This is due to that\
    \ CenPPO is benefiting from its global view for resource allocation.\n\n![](_page_9_Figure_3.jpeg)\n\
    \nFig. 7. Comparison of Network Stability Metrics: Network Sum-Rate, Average SINR,\
    \ and JFI.\n\n![](_page_9_Figure_5.jpeg)\n\nFig. 8. CDF of User Interference-to-Noise\
    \ Ratio (INR) for MAPPO, CenPPO, and Baseline.\n\n#### *2) Network Stability Comparison:*\n\
    \n• Network Sum-Rate: Figure 7 shows the achieved network sum rates. The MAPPO\
    \ achieves the highest sum-rate (131.63 Mbit s−<sup>1</sup> ). Followed by CenPPO\
    \ (85.94 Mbit s−<sup>1</sup> ) and the Baseline (45.5 Mbit s−<sup>1</sup> ). The\
    \ results indicates that the MAPPO tend to maximize the overall network throughput.\n\
    \n- Average SINR: Figure 7 shows the user average SNIR in dB. The CenPPO attains\
    \ the highest average SINR (21.3 dB) exceeding the MAPPO (14.35 dB) and the Baseline\
    \ (2.30 dB). These indicate that the CenPPO centralized execution leads to better\
    \ interference coordination in the network.\n- Interference-to-Noise Ratio (INR):\
    \ Figure 8 shows the CDF of user INR. The CenPPO maintains lower INR levels for\
    \ most users compared to the MAPPO and the Baseline. CDF at 50 %, the users under\
    \ CenPPO framework experience INR below 40 dB, whereas for the MAPPO 48 dB and\
    \ Baseline 61 dB. These results corroborates the SINR results showing the lead\
    \ for CenPPO.\n- Jain's Fairness Index (JFI) [42]: Measures the fairness of power\
    \ allocation among all users based on their averaged allocated power P¯ <sup>u</sup>.\
    \ It is calculated as JF I = ( P <sup>u</sup>∈U <sup>P</sup>¯u) 2 NUE· P <sup>u</sup>∈U\
    \ <sup>P</sup>¯<sup>2</sup> u .\n\nThe CenPPO shows a superior SINR and significantly\
    \ lower INR performance. This reflect the advantage of the CenPPO centralized\
    \ execution in managing inter-cell interference across the VLC network. The differing\
    \ fairness indices highlight the inherent trade-off faced by the agents where\
    \ the MAPPO's lower fairness is due to its strong prioritization of HP users.\
    \ While the CenPPO's high fairness indicates a more equitable, globally considered\
    \ resource allocation strategy.\n\n![](_page_9_Figure_14.jpeg)\n\nFig. 9. Comparison\
    \ of handover performance metrics: Handover Rate (HOR) per user per second and\
    \ Ping-Pong Ratio (PPR).\n\n- *3) Handover Performance Comparison:*\n- Handover\
    \ Rate (HOR): Figure 9 shows the handover performance of the VLC network. The\
    \ CenPPO achieves a lower HOR of (3.83 HOs/user/s) compared to the MAPPO with\
    \ (5.25 HOs/user/s).\n- Ping-Pong Ratio (PPR): Figure 9 shows the handover ping-pong\
    \ effect performance of the network. The CenPPO shows a handover stability with\
    \ a lower PPR of (0.15) compared to MAPPO (0.17). These results indicate that\
    \ the CenPPO makes robust decisions by reducing the\n\nwasteful back-and-forth\
    \ user switching between the VLC APs.\n\nThe CenPPO a high performance in maintaining\
    \ handover stability, evidenced by both lower HOR and PPR rates. This is due to\
    \ that the CenPPO global perspective during execution enables more robust handover\
    \ decisions considering network conditions.\n\n![](_page_10_Figure_2.jpeg)\n\n\
    Fig. 10. Comparison of average VLC AP electrical power consumption.\n\n*4) Power\
    \ Consumption Comparison:* Figure 10 shows the average electrical power consumed\
    \ per VLC AP. The Baseline uses an average of (12.62W). The MAPPO uses less (9.57W).\
    \ While the CenPPO is the most power-efficient with (7.65W). The lower power consumption\
    \ of the CenPPO indicates it's excellence in interference coordination and performing\
    \ resource allocation with less overall transmission power compared to the less\
    \ coordinated MAPPO and Baseline approaches.\n\n*5) Computational Complexity Analysis:*\
    \ Table III summarizes the complexity orders for the two proposed MARL frameworks.\
    \ The MAPPO (CTDE) benefits from decentralized execution and offering lower runtime\
    \ complexity that scales well with NAP . However, the MAPPO centralized training\
    \ is more demanding. In contrast, the CenPPO (CTCE) shows high complexity in both\
    \ training and execution due to its reliance on global state processing. This\
    \ limit the CenPPO scalability.\n\n# *E. Discussion and Interpretation*\n\n- Synthesis\
    \ of Findings: The results show that both MARL frameworks significantly outperform\
    \ the non-adaptive Baseline approach. The MAPPO (CTDE) emerges as the leader in\
    \ achieving the highest sum-rate and HP user rates. The CenPPO (CTCE) excels in\
    \ achieving higher SINR, lower interference, better outage performance, better\
    \ handover stability and lower power consumption. The Baseline is generally non-competitive\
    \ due to it basic policy in resource allocations. Except for achieving high fairness\
    \ due to its simple equal allocation, as assumed.\n- MAPPO vs. CenPPO Trade-offs:\
    \ The MAPPO decentralized execution allows it to react faster locally and achieve\
    \ higher throughput for prioritized users. However, the MAPPO suffers from higher\
    \ interference and less stable handovers due to the lack of real-time global coordination\
    \ during execution. The CenPPO leverages its global view for better interference\
    \ management, handover stability, fairness, and power efficiency. But at the cost\
    \ of\n\nlower throughput and potential scalability issues in larger dense VLC\
    \ networks.\n\n• Limitations: In this paper, the results and findings are specific\
    \ to the simulated environment, parameter settings, mobility model, and the chosen\
    \ composite score for BO. The MARL frameworks performance could vary with different\
    \ configurations. It is worth mentioning that the VLC simulation environment was\
    \ included simplifications: 1) perfect SIC is assumed and 2) a perfect global\
    \ state assumption for the CenPPO during execution.\n\n# V. CONCLUSION\n\nThis\
    \ paper tackled a complex challenge of optimizing resource allocation while provisioning\
    \ diverse Quality of Service (QoS) and network stability in a dynamic indoor Visible\
    \ Light Communication-Non-Orthogonal Multiple Access (VLC-NOMA) network. To address\
    \ this complex problem, we proposed, developed, and comparatively evaluated tailored\
    \ Multi-Agent Reinforcement Learning (MARL) frameworks, specifically investigating\
    \ CTDE via MAPPO and CTCE via Centralized PPO. Our approach involved designing\
    \ customized MARL components (state, action, reward) suited for the unique VLC-NOMA\
    \ dynamics and employing Bayesian Optimization to systematically determine the\
    \ critical balance between competing QoS and stability objectives within the multiobjective\
    \ shaped reward function. We adopted a VLC system model incorporating realistic\
    \ channel characteristics, NOMA principles, user mobility, dynamic interference,\
    \ and dimming considerations. The comparison results between MAPPO and CenPPO\
    \ highlighted the inherent trade-offs:\n\n- MAPPO (CTDE) excelled in throughput-related\
    \ metrics achieving approximately 53% higher network sum-rate, 40% higher average\
    \ rates for HP users, and 17% higher rates for SP users as compared to CenPPO.\
    \ Also, the MAPPO provided 16% higher QoS satisfaction for HP users. However,\
    \ the MAPPO results in a higher interference environment and exhibit less network\
    \ stability reflected in a 37% higher handover rate and a 14% higher ping-pong\
    \ ratio as compared to CenPPO.\n- CenPPO (CTCE) shows superior performance in\
    \ network stability. The CenPPO achieved about 7 dB higher average SINR, which\
    \ indicates better interference management. Also, the CenPPO has 37% lower outage\
    \ probability for SP users, better handover stability with 14% lower ping-pong\
    \ ratio, 37% lower handover rate and operated more efficiently with 20% lower\
    \ average AP power consumption as compared to the MAPPO. However, the CenPPO faces\
    \ inherent scalability limitations in 1)training and execution and high implementation\
    \ complexity due to its centralized nature.\n\nThis work underscores the potential\
    \ of MARL approaches to effectively optimize resources in complex VLC-NOMA networks.\
    \ The choice between architectures depends on the specific network scale, QoS,\
    \ stability requirements and the scalability trade-offs of each approach.\n\n\
    TABLE III COMPUTATIONAL COMPLEXITY COMPARISON\n\n| MARL   | Phase     | Complexity\
    \ Order                    | Level        |\n|--------|-----------|-------------------------------------|--------------|\n\
    | MAPPO  | Execution | O(NAP × fNN (Ds))                   | Low/Moderate |\n\
    |        | Training  | O(NAP × fNN (Ds) + fNN (Ds,global)) | High         |\n\
    | CenPPO | Execution | O(fNN (Ds,global))                  | High         |\n\
    |        | Training  | O(fNN (Ds,global))                  | Very High    |\n\n\
    Note: NAP : Number of agents; Ds: Single agent state dim.; Ds,global: Global state\
    \ dim.; fNN (·): NN processing complexity.\n\n#### REFERENCES\n\n- [1] W. Saad,\
    \ M. Bennis, and M. Chen, \"A Vision of 6G Wireless Systems: Applications, Trends,\
    \ Technologies, and Open Research Problems,\" *IEEE Network*, vol. 34, no. 3,\
    \ pp. 134-142, 2020.\n- [2] H. Haas, L. Yin, Y. Wang, and C. Chen, \"What is LiFi?,\"\
    \ *Journal of Lightwave Technology*, vol. 34, no. 6, pp. 1533-1544, 2016.\n- [3]\
    \ Z. Ghassemlooy, W. O. Popoola, and S. Rajbhandari, *Optical Wireless Communications:\
    \ System and Channel Modelling with MATLAB®*. CRC Press, Second ed., 2017.\n-\
    \ [4] Z. Ding, X. Lei, G. K. Karagiannidis, R. Schober, J. Yuan, and V. K. Bhargava,\
    \ \"A Survey on Non-Orthogonal Multiple Access for 5G Networks: Research Challenges\
    \ and Future Trends,\" *IEEE Journal on Selected Areas in Communications*, vol.\
    \ 35, no. 10, pp. 2181-2195, 2017.\n- [5] H. Marshoud, V. M. Kapinas, G. K. Karagiannidis,\
    \ and S. Muhaidat, \"Non-Orthogonal Multiple Access for Visible Light Communications,\"\
    \ *IEEE Wireless Communications Letters*, vol. 5, no. 2, pp. 128-131, 2016.\n\
    - [6] T. Wang, Y. Yang, H. H. Chen, and Z. Han, \"Mobility Management for Visible\
    \ Light Communication Networks: A Survey,\" *IEEE Communications Surveys & Tutorials*,\
    \ vol. 21, no. 1, pp. 416-438, 2019.\n- [7] S. Chen, Y. Liang, S. Sun, S. Kang,\
    \ W. Cheng, and M. Peng, \"Vision, Requirements, and Technology Trend of 6G: How\
    \ to Tackle the Challenges of System Coverage, Capacity, User Data-Rate and Movement\
    \ Speed,\" *IEEE Communications Magazine*, vol. 58, no. 2, pp. 30-37, 2020.\n\
    - [8] P. Yang, Y. Xiao, M. Xiao, and S. Li, \"6G Wireless Communications: Vision\
    \ and Potential Techniques,\" *IEEE Network*, vol. 33, no. 4, pp. 70-75, 2019.\n\
    - [9] I. F. Akyildiz, W. Lee, M. C. Vuran, and S. Mohanty, \"NeXt generation/dynamic\
    \ spectrum access/cognitive radio wireless networks: a survey,\" *IEEE Communications\
    \ Magazine*, vol. 44, no. 6, pp. 126-133, 2006.\n- [10] A. Khisti and B. Hassibi,\
    \ \"Resource allocation problems in wireless networks,\" in *2011 Information\
    \ Theory and Applications Workshop*, 2011, pp. 1-10.\n- [11] N. C. Luong, D. T.\
    \ Hoang, S. Gong, D. Niyato, P. Wang, Y. Liang, and D. I. Kim, \"Applications\
    \ of Deep Reinforcement Learning in Communications and Networking: A Survey,\"\
    \ *IEEE Journal on Selected Areas in Communications*, vol. 37, no. 10, pp. 2195-2224,\
    \ 2019.\n- [12] L. Busoniu, R. Babuska, and B. De Schutter, \"A Comprehensive\
    \ Survey of Multiagent Reinforcement Learning,\" *IEEE Transactions on Systems,\
    \ Man, and Cybernetics, Part C (Applications and Reviews)*, vol. 38, no. 2, pp.\
    \ 156-172, 2008.\n- [13] P. S. Sadeghi, A. Oroojlooyjadid, M. Nazari, D. Hajinezhad,\
    \ and H. R. Rabiee, \"A Survey of Multi-Agent Reinforcement Learning Algorithms\
    \ for Communication Networks,\" *IEEE Communications Surveys & Tutorials*, vol.\
    \ 25, no. 2, pp. 1295-1334, 2023.\n- [14] X. Zhang, Q. Gao, C. Gong, and Z. Xu,\
    \ \"User Grouping and Power Allocation for NOMA Visible Light Communication Multi-Cell\
    \ Networks,\" *IEEE Communications Letters*, vol. 21, no. 4, pp. 777-780, 2017.\n\
    - [15] X. Guo, H. Zhang, and N. Ha, \"Power Allocation based on Q-Learning for\
    \ NOMA Visible Light Communication Networks,\" in *Proceedings of 2020 the 10th\
    \ International Workshop on Computer Science and Engineering (WCSE 2020)*, 2020,\
    \ pp. 278–282.\n- [16] M. Shafi, R. K. Jha, and S. Jain, \"Multi-Agent Reinforcement\
    \ Learning Trajectory Design and Two-Stage Resource Management in CoMP UAV VLC\
    \ Networks,\" *IEEE Transactions on Communications*, vol. 71, no. 1, pp. 338-352,\
    \ 2023.\n- [17] Z. Ma, A. Liu, Y. Huang, and L. Yang, \"AoI-Oriented Resource\
    \ Allocation for NOMA-Based Wireless Powered Cognitive Radio Networks Based on\
    \ Multi-Agent Deep Reinforcement Learning,\" *IEEE Access*, vol. 12, pp. 18506-18518,\
    \ 2024.\n- [18] T. Komine and M. Nakagawa, \"Fundamental analysis for visible-light\
    \ communication system using LED lights,\" *IEEE Transactions on Consumer Electronics*,\
    \ vol. 50, no. 1, pp. 100-107, 2004.\n- [19] F. J. Lopez-Hernandez, R. Perez-Jimenez,\
    \ J. J. Vinagre, V. Guerra, J. Rabadan, and J. Rufo, \"Review of channel models\
    \ for indoor visible light communications,\" *Optics and Laser Technology*, vol.\
    \ 43, no. 2, pp. 282-290, 2011.\n- [20] J. M. Kahn and J. R. Barry, \"Wireless\
    \ infrared communications,\" *Proceedings of the IEEE*, vol. 85, no. 2, pp. 265-298,\
    \ 1997.\n- [21] J. R. Barry, J. M. Kahn, W. J. Krause, E. A. Lee, and D. G. Messerschmitt,\
    \ \"Simulation of multipath impulse response for indoor wireless optical channels,\"\
    \ *IEEE Journal on Selected Areas in Communications*, vol. 11, no. 3, pp. 367-379,\
    \ 1993.\n- [22] L. Dai, B. Wang, Y. Yuan, S. Han, C. Chih-Lin, and Z. Wang, \"\
    Nonorthogonal multiple access for 5G: solutions, challenges, opportunities, and\
    \ future research trends,\" *IEEE Communications Magazine*, vol. 53, no. 9, pp.\
    \ 74-81, 2015.\n- [23] L. Cui, G. Chen, and Z. Xu, \"Interference Management for\
    \ Multiuser Visible Light Communication Systems: A Survey,\" *Journal of Lightwave\
    \ Technology*, vol. 35, no. 21, pp. 4608-4621, 2017.\n- [24] Y. Liu, Z. Qin, M.\
    \ Elkashlan, Z. Ding, A. Nallanathan, and L. Hanzo, \"Nonorthogonal Multiple Access\
    \ for 5G and Beyond,\" *IEEE Transactions on Wireless Communications*, vol. 16,\
    \ no. 3, pp. 1462-1480, 2017.\n- [25] T. Camp, J. Boleng, and V. Davies, \"A survey\
    \ of mobility models for ad hoc network research,\" *Wireless Communications and\
    \ Mobile Computing*, vol. 2, no. 5, pp. 483-502, 2002.\n- [26] C. Bettstetter,\
    \ G. Resta, and P. Santi, \"The node distribution of the random waypoint mobility\
    \ model for wireless ad hoc networks,\" in *Proceedings of the 5th ACM international\
    \ symposium on Mobile ad hoc networking and computing (MobiHoc '04)*, 2004, pp.\
    \ 116–127.\n- [27] ITU-T, \"Recommendation Y.1541: Network performance objectives\
    \ for IP-based services,\" International Telecommunication Union, Recommendation\
    \ Y.1541, 2011.\n- [28] S. Li, L. D. Xu, and S. Zhao, \"The internet of things:\
    \ A survey,\" *IEEE Access*, vol. 6, pp. 78617-78641, 2018.\n- [29] A. Gupta and\
    \ R. K. Jha, \"A Survey of 5G Network: Architecture and Emerging Technologies,\"\
    \ *IEEE Communications Surveys & Tutorials*, vol. 17, no. 4, pp. 1986-2017, 2015.\n\
    - [30] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjoland, and F. ¨ Tufvesson,\
    \ \"6G Wireless Systems: Vision, Requirements, Challenges, Enabling Technologies,\
    \ and Operations,\" *Proceedings of the IEEE*, vol. 109, no. 7, pp. 1166-1199,\
    \ 2021.\n- [31] I. Stefan and H. Haas, \"Analysis of the impact of dimming on\
    \ the data rate of LiFi Attocell networks,\" in *2013 IEEE Globecom Workshops\
    \ (GC Wkshps)*, 2013, pp. 1100-1105.\n- [32] ISO/CIE, \"ISO 8995-1:2002(E)/CIE\
    \ S 008/E:2001: Lighting of work places – Part 1: Indoor,\" International Organization\
    \ for Standardization / International Commission on Illumination, Standard ISO\
    \ 8995-1:2002 / CIE S 008/E:2001, 2002.\n- [33] R. Lowe, Y. I. Wu, A. Tamar, J.\
    \ Harb, P. Abbeel, and I. Mordatch, \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive\
    \ Environments,\" in *Advances in Neural Information Processing Systems 30 (NIPS\
    \ 2017)*, 2017, pp. 6379–6390.\n- [34] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A.\
    \ Bayen, and Y. Wu, \"The Surprising Effectiveness of PPO in Cooperative Multi-Agent\
    \ Games,\" in *Proceedings of the 39th International Conference on Machine Learning\
    \ (ICML 2022)*, 2022, vol. 162, pp. 24611–24624.\n- [35] P. Sunehag et al., \"\
    Value-Decomposition Networks For Cooperative Multi-Agent Learning,\" in *Proceedings\
    \ of the 17th International Conference on Autonomous Agents and MultiAgent Systems\
    \ (AAMAS '18)*, 2018, pp. 2085–2087.\n- [36] T. Rashid et al., \"QMIX: Monotonic\
    \ Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,\"\
    \ in *Proceedings of the 35th*\n\n*International Conference on Machine Learning\
    \ (ICML 2018)*, 2018, vol. 80, pp. 4295–4304.\n\n- [37] F. A. Oliehoek and C.\
    \ Amato, *A Concise Introduction to Decentralized POMDPs*. Springer Publishing\
    \ Company, Incorporated, 2016.\n- [38] J. Schulman, F. Wolski, P. Dhariwal, A.\
    \ Radford, and O. Klimov, \"Proximal Policy Optimization Algorithms,\" in *Advances\
    \ in Neural Information Processing Systems 30 (NIPS 2017)*, 2017, pp. 2946–2956.\n\
    - [39] Y. Hua, R. Li, Z. Zhao, X. Chen, and Z. Zhang, \"GAN-Powered Deep Distributional\
    \ Reinforcement Learning for Resource Management in Network Slicing,\" *IEEE Transactions\
    \ on Vehicular Technology*, vol. 70, no. 10, pp. 10505-10517, 2021.\n- [40] Y.\
    \ Li, J. Zhang, and C. Wang, \"Joint Trajectory and Communication Design for Multi-UAV\
    \ Enabled Wireless Networks: A Federated Deep Reinforcement Learning Approach,\"\
    \ *IEEE Internet of Things Journal* , vol. 9, no. 16, pp. 14747-14759, 2022.\n\
    - [41] E. Liang et al., \"RLlib: Abstractions for Distributed Reinforcement Learning,\"\
    \ in *Proceedings of the 17th Python in Science Conference (SciPy 2018)*, 2018,\
    \ pp. 42–49.\n- [42] R. Jain, D.-M. Chiu, and W. R. Hawe, *A quantitative measure\
    \ of fairness and discrimination for resource allocation in shared computer system*\
    \ . Digital Equipment Corporation, DEC Research Report TR-301, 1984.\n- [43] Q.\
    \ Ye, O. Al-Khazraji, and H. Al-Raweshidy, \"Deep Reinforcement Learning for Adaptive\
    \ Network Slicing in 5G Networks to Minimize End-to-End Latency and Guarantee\
    \ QoS,\" *IEEE Access*, vol. 7, pp. 164857-164870, 2019.\n- [44] D. Lopez-Perez\
    \ et al., \"Enhanced intercell interference coordination challenges in heterogeneous\
    \ networks,\" *IEEE Wireless Communications* , vol. 18, no. 3, pp. 22-30, 2011.\n\
    - [45] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas, \"Taking\
    \ the Human Out of the Loop: A Review of Bayesian Optimization,\" *Proceedings\
    \ of the IEEE*, vol. 104, no. 1, pp. 148–175, 2016.\n- [46] C. E. Rasmussen and\
    \ C. K. I. Williams, *Gaussian Processes for Machine Learning*. MIT Press, 2006.\n\
    - [47] D. R. Jones, M. Schonlau, and W. J. Welch, \"Efficient Global Optimization\
    \ of Expensive Black-Box Functions,\" *Journal of Global Optimization*, vol. 13,\
    \ no. 4, pp. 455–492, 1998.\n- [48] J. Snoek, H. Larochelle, and R. P. Adams,\
    \ \"Practical Bayesian Optimization of Machine Learning Algorithms,\" in *Advances\
    \ in Neural Information Processing Systems 25 (NIPS 2012)*, 2012, pp. 2951–2959.\n\
    - [49] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*\
    \ . The MIT Press, Second ed., 2018.\n- [50] D. P. Kingma and J. L. Ba, \"ADAM:\
    \ A Method for Stochastic Optimization,\" arXiv preprint arXiv:1412.6980, 2017.\n\
    - [51] A. Paszke et al., \"PyTorch: An Imperative Style, High-Performance Deep\
    \ Learning Library,\" arXiv preprint arXiv:1912.01703, 2019.\n- [52] S. H. Younus,\
    \ A. A. Al-Hameed, A. T. Hussein, M. T. Alresheedi, and J. M. H. Elmirghani, \"\
    Parallel Data Transmission in Indoor Visible Light Communication Systems,\" *IEEE\
    \ Access*, vol. 7, pp. 1126-1138, 2019.\n- [53] S. H. Younus, A. A. Al-Hameed,\
    \ A. T. Hussein, M. T. Alresheedi, and J. M. H. Elmirghani, \"Experimental Demonstration\
    \ of Parallel Data Transmission for Indoor Visible Light Communication Systems,\"\
    \ *IET Communications*, vol. 13, no. 1, pp. 1-13, 2019.\n- [54] A. A. J. Al-Hameed,\
    \ \"Detection and Localisation Using Light,\" Ph.D. dissertation, University of\
    \ Leeds, 2019.\n- [55] M. M. Qazzaz, S. A. Zaidi, D. McLernon, A. Salama, and\
    \ A. A. Al-Hameed, \"Low complexity online rl enabled uav trajectory planning\
    \ considering connectivity and obstacle avoidance constraints,\" in *2023 IEEE\
    \ International Black Sea Conference on Communications and Networking (BlackSeaCom)*,\
    \ 2023, pp. 82-89.\n- [56] M. M. Qazzaz, S. A. Zaidi, D. C. McLernon, A. Salama,\
    \ and A. A. Al-Hameed, \"Optimizing Search and Rescue UAV Connectivity in Challenging\
    \ Terrain through Multi Q-Learning,\" in *2024 11th International Conference on\
    \ Wireless Networks and Mobile Communications (WINCOM)*, 2024, pp. 1-8."
- title: "Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based\n\
    \  Indoor Localization"
  abstract: 'Wireless Fidelity (WiFi) based indoor positioning is a widely researched
    area

    for determining the position of devices within a wireless network. Accurate

    indoor location has numerous applications, such as asset tracking and indoor

    navigation. Despite advances in WiFi localization techniques -- in particular

    approaches that leverage WiFi telemetry -- their adoption in practice remains

    limited due to several factors including environmental changes that cause

    signal fading, multipath effects, interference, which, in turn, impact

    positioning accuracy. In addition, telemetry data differs depending on the WiFi

    device vendor, offering distinct features and formats; use case requirements

    can also vary widely. Currently, there is no unified model to handle all these

    variations effectively. In this paper, we present WiFiGPT, a Generative

    Pretrained Transformer (GPT) based system that is able to handle these

    variations while achieving high localization accuracy. Our experiments with

    WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can

    effectively capture subtle spatial patterns in noisy wireless telemetry, making

    them reliable regressors. Compared to existing state-of-the-art methods, our

    method matches and often surpasses conventional approaches for multiple types

    of telemetry. Achieving sub-meter accuracy for RSSI and FTM and

    centimeter-level precision for CSI demonstrates the potential of LLM-based

    localisation to outperform specialized techniques, all without handcrafted

    signal processing or calibration.'
  url: http://arxiv.org/abs/2505.15835v1
  keywords: Indoor localization, WiFi, LLM, LLaMA, CSI, GPT
  document: '## I. INTRODUCTION


    Indoor positioning technologies have various applications, ranging from indoor
    navigation in environments such as airports, hospitals, and shopping malls, accessibility,
    healthand elderly care to improving smart building systems [1]–[3]. However, its
    adoption in the real world has been slow due to practical issues. Many localization
    systems are trained and tested on static environments in a controlled manner.
    However, the real world is dynamic, and even slight environmental variations can
    cause catastrophic loss in accuracy [4]. In order to make real impact, localization
    approaches must be able to adapt to different dynamic scenarios while maintaining
    high accuracy [5]. More recently, localization using WiFi telemetry has been receiving
    considerable attention from researchers and practitioners due to WiFi''s ubiquity,
    cost-effectiveness and scalability [6]. WiFi telemetry offers valuable information
    about the Radio Frequency (RF) environment through metrics such as the Received
    Signal Strength Indicator (RSSI), Channel State Information (CSI) and Fine Timing
    Measurement (FTM) [7], [8]. This telemetry, especially the CSI, characterizes
    how a signal propagates from a transmitter to a receiver, encompassing effects
    like scattering, fading, and power decay [9].


    As AI and ML tools become increasingly prevalent, they have been used for indoor
    localization systems to improve accuracy, adaptability, and real-time performance
    in a variety of environments [10]. Indoor localization has traditionally relied
    on rule-based algorithms that need telemetry features like RSSI, CSI, or FTM to
    be manually customized. While effective in custom settings, these models often
    fail to generalize across environments and require significant engineering effort
    to adapt to different target scenarios. Common neural networks or non-linear regressor
    based approaches often require data preprocessing steps such as filtering and
    data scaling. These methods rely on fixed schemas and struggle with missing or
    incomplete data, limiting flexibility and scalability across heterogeneous environments
    [11].


    As Transformers [12] become more ubiquitous and accessible, we have explored their
    use to address the challenges posed by accurate WiFi-based indoor localization.
    Transformers [12] were originally built for language translation, where the goal
    is to understand textual sequences and generalize across different contexts. Their
    generalization capability makes Transformers a good candidate for modeling WiFi
    telemetry, which is often dynamic, noisy, sparse, and affected by signal propagation
    impairments. In environments that are heavily affected by the effect of multipath,
    like corridors or indoor spaces, even small changes in position can lead to significant
    signal variations, which traditional methods struggle to capture. Transformers,
    and more broadly LLMs, provide a flexible way to learn patterns from spatiotemporal
    data without needing hand-crafted features. Their generalization ability, which
    has already been shown in NLP, computer vision, robotics, and even biomedical
    signals [13]– [16], can be applied here to make sense of complex patterns in WiFi
    environments. In other words, instead of building different models for every setup,
    we can train a single model that learns to work across environments.


    Motivated by the cross-generalization capabilities of GPTs, in particular LLMs,
    we investigate their potential to address indoor localization tasks using WiFi
    signals to accurately estimate positioning in dynamic environments subject to
    noise and multipath effects.


    In this paper, we introduce WiFiGPT, a decoder-only large


    language model adapted and re-tooled to model real-world WiFi telemetry for accurate
    indoor localization. LLMs have been typically trained to model textual sequences
    in natural language. In WiFiGPT, we leverage decoder-only LLMs'' learning capability
    to model the distribution of telemetry signals, treating each signal feature as
    an input and mapping the final answer to the device position. We demonstrate that
    LLMs are not limited to text-based systems and can operate in a noisy environment
    utilizing wireless telemetry data for indoor positioning. The system is designed
    with portability, agility, and adaptability in mind. It can quickly adjust to
    different environments while maintaining high accuracy. This allows fast learning,
    even from just a few samples, and enables strong cross-generalization. Because
    of this, the resulting model can be rapidly trained and deployed in real-world
    scenarios with minimal computational overhead where LLMs already exist, such as
    smart home devices, smartphones. In this paper, we make the following contributions:


    - To the best of our knowledge, WiFiGPT is the first system repurposing decoder-only
    LLMs for indoor localization.

    - We present a generalized framework with deterministic outputs that integrates
    diverse telemetry sources and adapts across radio environments. Extending LLMs
    beyond language, we apply a simple training scheme to map telemetry sequences
    to distance predictions for sensor-based regression.

    - We show that LLMs can handle WiFi telemetry directly without manual feature
    engineering. WiFiGPT performs well on real-world indoor datasets and adapts quickly,
    even with limited training data, getting sub-meter accuracy.

    - We analyze how the accuracy of different LLaMA model sizes varies under changing
    environmental conditions.


    The rest of the paper is organized as follow: In Section II, we provide an overview
    of the background and related work setting the stage for our contributions. Section
    III describes in detail our large WiFi-telemetry model for indoor localization,
    while in Section IV, we present its implementation details. Section V describes
    how we evaluated the performance of our system and Section VI presents and discusses
    our results. Finally, Section VII concludes the paper and discusses directions
    for future work.


    # II. BACKGROUND AND RELATED WORK


    In this section, we start with a brief overview of the background on WiFi telemetry
    and then describe the stateof-the-art approaches including both traditional models
    and more recent methods based on large language models (LLMs). We highlight the
    strengths and limitations of each, setting the stage for our proposed approach.


    ## *A. WiFi Telemetry*


    WiFi provides different telemetry data, such as Received Signal Strength Indicator
    (RSSI), Channel State Information (CSI), Angle of Arrival (AoA) or time-based
    systems like Fine Timing Measurement (FTM), Time of Flight (ToF), Return Time
    of Flight (RToF) each offering trade-offs between accuracy, complexity, and practicality
    for Indoor Positioning. (Table I).


    RSSI: Received Signal Strength Indicator (RSSI) is a metric used to quantify the
    strength of a received signal during wireless communication [17]. RSSI-based techniques
    are commonly used because they are easily accessible and involve minimal overhead
    [18]. RSSI values are often scaled based on the specific hardware and manufacturer
    specifications, meaning there is no standardized scale to compare data. Fingerprinting
    approaches [19] are common. However, it is sensitive to environmental noise and
    device heterogeneity. Hence, even after enhancement of ML and sensor fusion [20],
    RSSI-based approach often shows high localization error. To reduce the error,
    RSSI is used in conjunction with other telemetry data. The process of combining
    telemetry data from different sensors is called Sensor Fusion. In 2021, Microsoft''s
    indoor localization competition by Microsoft [21] attracted participation from
    1,170 global teams exploring various methodologies. The dataset encompasses dense
    signatures of WiFi, GMF, iBeacons, BLE and ground truth (waypoint) locations from
    numerous buildings in Chinese cities, and the top teams achieved high localization
    accuracy using the sensor fusion setup using non-NN models like KNN and LGBM with
    an error margin of 1.3 m.


    FTM: IEEE 802.11-2016 [22] included the first generation of the FTM protocol that
    sends a burst of frames and averages the round-trip-time (RTT). In the Line-Of-Sight
    (LOS) environment, FTM can perform significantly better than RSSI, with errors
    around two meters [23]. However, not all commercial devices support it; accuracy
    is affected by clock skew and drift and needs calibration accordingly. Its performance
    suffers in Non-Line-Of-Sight environments (NLOS), where positioning error exacerbates
    and can be as high as 5-10 meters in NLOS settings [23], [24].


    CSI and AoA: CSI captures environmental conditions by providing the phase and
    amplitude of different subcarriers. Consequently, the system is more robust against
    scattering, fading and power delays. Indoor localization has seen advancements
    in the Non-Line-of-Sight (NLOS) environment through systems like Ubicarse, ArrayTrack,
    and SpotFi [25]– [27], each based on using Angle of Arrival(AoA) derived from
    the Channel State Information(CSI). MUSIC (Multiple Signal Classification) [28]
    and ESPRIT (Estimation of Signal Parameters via Rotational Invariance Techniques)
    [29] signal processing technique that separates signal and noise spaces and can
    be used to estimate angles of arrival (AoA) of signals in array signal processing.
    All AoA methodology are based on some variation of MUSIC and ESPRIT requires multiple
    antennae to get phase differences. Hence, most existing work relies on specialized
    hardware as traditional routers have firmware restrictions and cannot work on
    single-antenna systems (ESP32, Raspberry Pi) [30], [31]. This makes them less
    ubiquitous than RSSI and FTM. System performance is also affected by feedback
    delays, and errors introduced by


    | Feature                  | CSI                               | RSSI            |
    Time-Based       | AoA            |  |

    |--------------------------|-----------------------------------|-----------------|------------------|----------------|--|

    | Accuracy                 | High                              | Low to Moderate
    | Moderate to High | High           |  |

    | Resolution               | Fine-grained                      | Coarse-grained  |
    Fine-grained     | Fine-grained   |  |

    | H/W Requirements         | Specialized<br>(e.g.,             | Standard WiFi   |
    Timing hardware  | Multiple<br>an |  |

    |                          | Intel 5300 NIC)                   |                 |                  |
    tennas         |  |

    | Computational Complexity | Moderate to High                  | Low             |
    Moderate         | High           |  |

    | Multipath Resilience     | Good (can leverage<br>multi-path) | Poor            |
    Moderate         | Good           |  |

    | Ease of Implementation   | Moderate to Com<br>plex           | Simple          |
    Moderate         | Complex        |  |


    TABLE I COMPARISON OF WIRELESS LOCALIZATION TECHNIQUES


    the use of outdated or incorrect CSI. Acquiring accurate CSI introduces requires
    control message exchanges between the transmitter and receiver, restricting valuable
    bandwidth by adding overhead.


    ## *B. LLMs and Their Applications*


    Transformer models have demonstrated remarkable adaptability across diverse domains.
    In particular, as shown in [13], large language models (LLMs), which can be realized
    by generative pre-trained transformers (GPTs), which are trained primarily in
    English, still perform well in other languages. Since then, transformers'' capacity
    for generalization has expanded far beyond natural language processing to include
    applications such as computer vision (vision transformers), robotics, structured
    code generation, modeling vital signs, e.g., eletrocardiogram (ECG) signals, and
    even interpreting dolphin vocalizations [14]–[16].


    Transformers can be broadly classified into two types: Models that can handle
    all tokens at once (encoders) and those that can only handle past tokens (decoders)
    [32]. Encoderbased transformers such as BERT [33] take full input and process
    it simultaneously. This requires access to the whole input sequence before any
    processing can begin (non-causal). However, they cannot be used when data is available
    in real time (such as WiFi packets) and are more suitable for offline tasks such
    as recovering lost network packets [34] or data imputation [35]. Decoder-based
    transformers generate an output sequence that is not dependent on the future element
    (causal). Decoder-only transformers are also called autoregressive, in that each
    output is generated one step at a time based on current and past elements. Decoder-only
    architectures are thus well suited for real-time applications. Contemporary systems,
    such as Openai''s GPT [36] and Meta''s LLaMA [37], adopt a decoder-only architecture.
    Throughout the paper, we use the term Generated Pretrained Transformers (GPT)
    for decoderbased transformers only.


    LLMs are gaining traction in networking applications. NetLLM [38] introduces a
    framework that adapts large language models (LLMs) for networking tasks, including
    viewport prediction, adaptive bitrate streaming, and cluster job scheduling. WirelessLLM
    [39] deals with challenges like power allocation and spectrum sensing in communication
    networks and SigLLM [40] explores the application of LLMs for time series anomaly
    detection. CSI-BERT [41] uses an encoder-based transformer to recover lost CSI
    in wireless sensing applications caused by packet loss. LocGPT [42] proposes a
    custom encoder-decoder transformer architecture for antenna array-based triangulation
    using phase spectra. It requires extensive pre-training data to learn general
    localization features.


    Vacareanu et al. (2024) [43] show that LLMs can perform regression purely through
    in-context learning, achieving competitive results on nonlinear synthetic datasets
    without finetuning. Tang et al. (2024) [44] show that LLM embeddings can be used
    for regression tasks in higher-dimensional settings than traditional feature engineering
    on synthetic datasets. Building on this, we show that exising decoder-only LLMs
    can accurately perform distance calculations. Once fine-tuned on a sufficient
    set of WiFi packets, our system can deliver accurate distance predictions without
    requiring large-scale pretraining or extensive scene-specific data.


    ## III. WIFIGPT


    WiFiGPT (Figure 1) is a deterministic language modelbased system designed for
    indoor distance estimation using Wi-Fi telemetry. It begins by collecting telemetry
    data from transmitter-receiver (Tx/Rx) pairs. This raw data is then aligned with
    ground-truth distance labels to enable promptstyle learning. In the training phase,
    a large language model is fine-tuned in a supervised manner to map telemetry sequences
    to corresponding distances, using MAE and MSE as custom loss metrics. During inference,
    the model applies greedy decoding to output a single numeric distance prediction
    in meters, ensuring deterministic behavior. The system is evaluated by computing
    MAE/MSE directly on the output format, making it a streamlined and interpretable
    solution for localization tasks.


    ## *A. Telemetry Source*


    We use telemetry data captured between transmitters (Tx) and receivers (Rx) of
    various WiFi devices, including ESP32 devices, Google Access Points and Pixel
    smartphones. Specifically, our system shows results for multisource telemetry
    inputs such as CSI, FTM, and RSSI measurements. However, our model is not restricted
    to these particular types of telemetry and can handle other telemetry and sensor
    data. Our method processes these telemetry signals sequentially through an autoregressive
    model, allowing the system to


    ![](_page_3_Figure_0.jpeg)


    Fig. 1. WiFiGPT: System Flow


    generalize distance predictions across different environments and hardware setups.
    Importantly, by learning directly from raw telemetry data, our model remains strong
    and works well with any device. It adapts reliably to different indoor conditions
    without relying on specific hardware or signal types.


    ## *B. Data Preparation Layer*


    A token is a chunk of character sets that the model reads or generates step by
    step. Large Language models are primarily autoregressive models that means they
    do next token prediction based on the previous inputs and is expressed by the
    equation:


    $$P(x\_1, x\_2, \dots, x\_T) = \prod\_{t=1}^T P(x\_t \mid x\_1, \dots, x\_{t-1})
    \qquad (1)$$


    where x<sup>t</sup> is the token at position t, predicted based on previous x1...xt−<sup>1</sup>
    tokens.


    To adapt the LLM for a regression-style task using sensor telemetry, we present
    the problem as a language modelling goal, where the model is familiarized with
    the telemetry input. The model learns to generate the target value as the next
    token in a structured prompt-response format. By framing the positioning task
    as a next-word prediction problem, we avoid modifying the LLM architecture, eliminating
    the need of a encoder for input data. The output token must be a numeric value
    in a consistent and parseable format. It should be deterministic and agnostic
    to the system conditions. The telemetry data needs to be represented effectively
    in a structured format so that LLM can understand the effective relationship and
    predict the assign task. We modify equation (1) as follows:


    $$P(\{\text{Distance}\} \text{m} \mid \text{Instrution}, \{\text{Telementry}\})
    \qquad (2)$$


    We treat the instruction prompt and telemetry data as the input tokens x1, x2,
    . . . , xt−<sup>1</sup> and the position as token xt. Reframes the LLM modeling
    objective into a regression task. The LLM is trained using a JSONL-based instruction
    tuning format [45] where each WiFi packet is structured as a JSON object containing
    a single text field. We use four delimiter tokens for different sections of our
    prompts. <|begin\_of\_text|> and <|end\_of\_text|> indicate the start and end
    of each new packet, respectively. <|start\_header\_id|> and <|end\_header\_id|>
    creates a distinct identity between the user and the assistant. The user agent
    can send available WiFi telemetry or other sensor data (BLE, gyroscope), and the
    assistant returns the estimated distance in the format: {answer}m. The <|end\_of\_text|>
    also acts as end of sequence (EOS) token. eg.


    ```

    <|begin_of_text|><|start_header_id|>

    user <|end_header_id|>

    based on the array predict distance in

    meter and nothing else in this format:

    {answer}m.

    this is array: [WiFi Telemetry]

    <|start_header_id|> assistant

    <|end_header_id|> {Distance}m

    <|end_of_text|>

    ```

    This format is used for alignment [46]. Formatting the model response in this
    way simplifies automated evaluation in later phases. During inference, the text
    until the end header ID is provided to the LLM, prompting it to predict the distance
    as the next token in this format {answer}m.


    Unlike traditional deep learning methods like CNNs, LSTMs, or nonlinear regressors
    like LGBM and KNN, the LLM-based approach does not require data scaling or converting
    categorical features (like vendor information or room numbers) into one-hot vectors.
    A fixed schema is not required, and missing features can be omitted. It can convert
    wireless environment information into high-dimensional vectors, capturing semantic
    meanings and relationships into the latent space. For instance, the user commands,
    "Turn on the nearest light." The system can calculate the user''s precise location,
    understands the contextual intent, and activates the nearest light. The model
    could answer questions like "Where is the nearest restroom?" or "What is the signal
    strength near Room 404?".


    ## *C. Training*


    *1) Select Base LLM Model:* We select the Meta''s LLaMA 3 family of models, including
    variants 1B [47], 3B [48], and 8B [49], due to their open-source availability,
    strong performance characteristics, and established presence in the research literature
    as representative examples of modern decoder-only transformer architectures. LLaMA
    has open-weights, permissive open-source license with strong benchmark scores
    for language tasks and a longer context window size, allowing us to process a
    whole packet in one pass [50].


    *2) Fine Tuning:* Large Language Models (LLMs) can model a task when given examples
    of that task in their context, exhibiting Few-Shot and Zero-Shot Capabilities.
    We conduct supervised fine-tuning (SFT) [51] using LoRA (Low-Rank Adaptation)
    adapters [52] on the LLaMA models. We used Apple''s MLX framework [53] on a 32GB
    M1 Max chip. SFT involves training the LLM model on the positioninglabeled data,
    while LoRA enables efficient fine-tuning of only selected model layers instead
    of the whole network. LoRA is an efficient way to adapt base language models to
    new tasks without changing the entire layer, which can be both time- and computing-intensive.
    LoRA freezes the original weights and updates only a fraction of the parameters.
    After training, it creates a LoRA adapter for the given dataset to be loaded/unloaded
    for different tasks. Reduces the computational overhead of training on consumer-grade
    hardware, allows modularity, and allows us to easily swap or stack adapters for
    different tasks without retraining the entire model. We use batch size 2, number
    of layers 16, and learning rate (lr) 2e-4. We use a smaller batch size and num-layers
    due to limited GPU memory, which can lead to noisier gradients [54]. We use the
    relatively standard learning rate to fine-tune [55]. We train the LLM model separately
    for each dataset and environment to evaluate how it responds to different signal
    distributions and spatial layouts. This allows us to study its behavior in a controlled
    setting and to understand how well it captures environment-specific patterns.
    According to the Chinchilla Law [56], models benefit more from increased training
    data than from an increased parameter count. Therefore, as we accumulate more
    radio environment data across diverse settings, the model should generalize better
    in future deployments, even without retraining, by leveraging its prior exposure
    to similar propagation characteristics.


    ## *D. Deterministic Inference*


    LLMs are generative models that learn statistical patterns in training data to
    predict the next token in a sequence. As a result, they are inherently stochastic,
    meaning that their output can vary depending on the decoding strategy used. We
    used deterministic text generation (greedy decoding) by setting the temperature
    t to 0.0, top-p to 1.0 and do-sample to false. We also set a fixed random seed
    to ensure that the results are fully reproducible every time we run the experiments.
    The temperature parameter controls randomness in the generation process by scaling
    the logits (raw output scores produced by the model) before applying the softmax
    function. Temperature 0.0 ensures that the model consistently selects the most
    probable token. Top-p value of 1.0 enables consideration of the full token distribution.
    Setting do-sample to false forces the model to not do sampling and ensures greedy
    decoding. The random seed establishes a fixed initialization point for any probabilistic
    components, guaranteeing identical outputs given the same input conditions.


    - Temperature T = 0.0 scales the logits (z) such that P(x) = softmax(z/T) → arg
    max as T → 0, forcing deterministic selection.

    - do-sample = F alse restricts candidate tokens to the single most likely token.

    - Top-p = 1.0 includes the entire token distribution (no truncation by cumulative
    probability).

    - Random seed fixes any stochastic elements (e.g., dropout, sampling noise), ensuring
    reproducibility.


    Under these conditions, the output is fully deterministic; the same input prompt
    always allows the same output token.


    ## IV. EXPERIMENTAL METHODOLOGY


    ## *A. Datasets*


    We demonstrate the accuracy of our system using two datasets. The first dataset
    is custom collected, where we capture Channel State Information (CSI) using two
    ESP32- S2 devices positioned 6 to 10 meters apart in a hallway with human presence,
    recording data for around 5 minutes and maintaining LOS. The second data set is
    publicly available [57], combining WiFi fine-time measurement (FTM) and received
    signal strength indicator (RSSI) data collected in three environments: a lecture
    theater (LOS), an office (mixed LOS-NLOS), and a corridor (NLOS) to improve positioning
    accuracy under varying conditions. Using these two datasets underscores our model''s
    adaptability and robustness across telemetry and environmental scenarios.


    *1) ESP-32-CSI-HALLWAY:* Hallways are a multipathprevalent environment [58]. ESP32
    is a single antenna device; therefore, we cannot extract phase difference information
    between antennas. Therefore, we cannot use state-of-the-art algorithms like MUSIC
    because they are inapplicable for similar low-cost devices. This limitation also
    applies to many low-cost Android phones and even the Raspberry Pi, making the
    single-antenna constraint particularly relevant in such scenarios.


    We use two ESP32-S2 devices operating at 20 MHz, one as a transmitter and the
    other as a receiver. The ESP32 chip we are using uses 802.11n WiFi 4. 802.11n
    was the first generation to introduce CSI. The devices are placed 6 to 10 meters
    apart at 1-meter intervals, recording CSI for approximately 5 minutes in a hallway.
    The environment includes multipath noise and people occasionally lingering around,
    but the setup maintains line-of-sight (LOS). The data set is custom collected
    to ensure that Meta''s LLaMA model is not exposed to these data, since the specifics
    of LLaMa''s training data are not publicly available. The data set is structured
    as follows:


    ```

    <|begin_of_text|><|start_header_id|>

    user <|end_header_id|>

    based on the array predict distance in

    meter and nothing else in this format:

    {answer}m.

    this is array: [84 -64 4 0 0 0 0 0 0 0

    0 0 13 1 14 2 14 3 14 4 14 3 13 4 12 3

    13 4 13 4 11 4 11 5 10 5 9 5 9 5 8 5 7

    5 6 4 5 4 4 4 3 3 3 3 2 2 1 2 1 2 0 2

    -1 2 0 0 -3 1 -2 2 -4 2 -5 2 -5 2 -6 3

    -6 4 -7 4 -7 5 -7 3 -9 5 -9 5 -9 6 -10

    7 -11 7 -11 8 -10 8 -10 9 -10 10 -9 11

    -10 12 -9 13 -9 15 -9 16 -11 16 -10 17

    0 0 0 0 0 0 0 0 0 0 ]

    <|start_header_id|> assistant

    <|end_header_id|> 6m <|end_of_text|>

    ```

    TABLE II WIFI RSS & RTT DATASET WITH DIFFERENT LOS CONDITIONS FOR INDOOR POSITIONING
    [59]


    | Data features      | Lecture Theatre | Office    | Corridor  |  |

    |--------------------|-----------------|-----------|-----------|--|

    | Testbed (m2<br>)   | 15 × 14.5       | 18 × 5.5  | 35 × 6    |  |

    | Grid size (m2<br>) | 0.6 × 0.6       | 0.6 × 0.6 | 0.6 × 0.6 |  |

    | Number of RPs      | 120             | 108       | 114       |  |

    | All samples        | 7,200           | 6,480     | 6,840     |  |

    | Training samples   | 5,280           | 4,860     | 5,110     |  |

    | Testing samples    | 1,920           | 1,620     | 1,740     |  |

    | WiFi condition     | LOS             | LOS-NLOS  | NLOS      |  |


    *2) WiFi RSS and RTT dataset with different LOS conditions for indoor positioning:*
    We use a publicly available WiFi dataset that comprises various LOS and NLOS environments
    [57]. It contains an extensive selection of samples from multiple reference points
    (RP) in three different scenarios: lecture theater (LOS), corridor (mixed LOS
    / NLOS) and office (NLOS) (Table II). Data are collected using a Google AC-1304
    WiFi router and Pixel 3 smartphones. The data set contains RTT and RSSI measurements,
    as both the Access Point (AP) and the Station (STA) support the FTM protocol.
    The dataset is structured as follows:


    ```

    <|begin_of_text|><|start_header_id|>

    user <|end_header_id|>

    based on the array predict distance in

    meter and nothing else in this format:

    {answer}m.

    this is array: [WiFi FTM AP 1 (ns):

    ```


    ```

    11351.0, WiFi RSSI AP 1 (dbm): -63.0,

    WiFi FTM AP 2 (ns): 5599.0, WiFi RSSI

    AP 2 (dbm): -60.0, WiFi FTM AP 3 (ns):

    2918.0, WiFi RSSI AP 3 (dbm): -52.0,

    WiFi FTM AP 4 (ns): 13573.0, WiFi RSSI

    AP 4 (dbm): -68.0, WiFi FTM AP 5 (ns):

    10671.0, WiFi RSSI AP 5 (dbm): -64.0]

    <|start_header_id|> assistant

    <|end_header_id|> 18.25m

    <|end_of_text|>

    ```

    # *B. Evaluation Metrics*


    Training large-language models with MSE or MAE instead of cross-entropy loss can
    create fundamental issues. MAE (Mean Absolute Error) and MSE (Mean Squared Error)
    loss functions assume continuous, symmetric error distributions, incompatible
    with the probabilistic nature of next-token prediction that cross-entropy handles.
    We are interested in monitoring the training progress without modifying the transformer
    architecture. Therefore, we insert the loss functions as an auxiliary module to
    observe whether the model converges.


    $$\text{MSE} = \frac{1}{n} \sum\_{i=1}^{n} (y\_i - \hat{y}\_i)^2 \tag{3}$$


    $$\text{MAE} = \frac{1}{n} \sum\_{i=1}^{n} |y\_i - \hat{y}\_i| \tag{4}$$


    where:


    - y<sup>i</sup> represents the actual value

    - yˆ<sup>i</sup> represents the predicted value

    - n is the total number of samples


    These evaluation metrics are auxiliary diagnostic tools; we do not use them for
    gradient updates during training. This allows us to evaluate the regression behaviour
    of the LLM model while preserving the underlying cross-entropy-based language
    modelling intent. We keep a misalignment counter additionally to track outputs
    that are misaligned or cannot be parsed. These are skipped in the loss metrics
    and logged separately for quality checks allowing us to monitor whether the LLM
    is learning to align with the regression task.


    ## V. RESULTS


    TABLE III PERFORMANCE OF LLAMA-3 MODELS ON 51,500 CSI SAMPLES (80% OF THE DATASET)


    | Model        | MSE (m) | MAE (m) | R2     | Misalignments |

    |--------------|---------|---------|--------|---------------|

    | LLaMA-3.2-1B | 0.1809  | 0.0759  | 0.9163 | 0             |

    | LLaMA-3.2-3B | 0.1678  | 0.0688  | 0.9224 | 6             |

    | LLaMA-3.1-8B | 0.3772  | 0.1809  | 0.8255 | 2             |


    | TABLE IV                                                           |  |  |  |  |  |  |

    |--------------------------------------------------------------------|--|--|--|--|--|--|

    | MODEL PERFORMANCE ON TESTING SAMPLES ACROSS DIFFERENT ENVIRONMENTS |  |  |  |  |  |  |


    | Parameter | Environment | MSE     | MAE      | R2      | 25th   | 50th   | 75th   |
    100th   | Iteration |

    |-----------|-------------|---------|----------|---------|--------|--------|--------|---------|-----------|

    | 1b        | Corridor    | 3.0567  | 1.3359   | 0.9891  | 0.9600 | 1.0000 | 2.0000
    | 10.0100 | 2750      |

    | 3b        | Corridor    | 1.8553  | 1.1166   | 0.9934  | 0.9750 | 1.0000 | 1.0000
    | 9.98    | 4000      |

    | 8b        | Corridor    | 2.4009  | 1.2155   | 0.9914  | 0.9800 | 1.0000 | 1.9900
    | 10.0500 | 4350      |

    | 1b        | Theatre     | 1.8281  | 1.0328   | 0.9557  | 0.47   | 0.83   | 1.36   |
    9.34    | 2550      |

    | 3b        | Theatre     | 1.23085 | 0.90263  | 0.97    | 0.4299 | 0.8299 | 1.08   |
    5.37    | 2500      |

    | 8b        | Theatre     | 1.5360  | 0.9601   | 0.9628  | 0.4900 | 0.76   | 1.36   |
    6.71    | 3750      |

    | 1b        | Office      | 2.4462  | 1.1589   | 0.9641  | 0.39   | 0.8    | 1.85   |
    5.98    | 2150      |

    | 3b        | Office      | 2.2595  | 1.0974   | 0.9668  | 0.29   | 0.9    | 1.58   |
    7.22    | 3500      |

    | 8b        | Office      | 1.9816  | 1.107094 | 0.97094 | 0.47   | 0.97   | 1.76   |
    5.37    | 2400      |


    ## *A. Few Shot Learning-CSI*


    To evaluate how the model behaves with a few shots of learning, we split the CSI
    dataset by class, assigning 10% for training, 10% for validation, and the remaining
    80% for testing. We chose this configuration to present a realistic fewshot learning
    scenario. Table III presents the performance of various LLaMA models on 51,500
    CSI samples collected in a hallway environment with strong multipath propagation
    and signal reflections from the surrounding, including human presence. Despite
    these challenging conditions, the CSI-based models overall achieved near-perfect
    accuracy. The LLaMA-3.2-3B model led in performance, reaching the lowest MSE of
    0.1678 m and MAE of 0.0688 m, although it experienced six misalignments. The LLaMA-3.2-1B
    model had slightly higher error metrics (MSE: 0.1809 m, MAE: 0.0759 m, R<sup>2</sup>
    : 0.9163) but confirmed perfectly stable predictions (0 misalignments). LLaMA-3.1-8B
    model performed slightly worse on all metrics, although not significantly. These
    results highlight the effectiveness of LLMs fine-tuned with CSI data in enabling
    accurate indoor localization, even in complex multipath indoor environments.


    ![](_page_6_Figure_4.jpeg)


    Fig. 2. CDF of Localization Error


    # *B. FTM and RSSI*


    The results (Table IV) show a clear trend in which the environmental conditions
    (LOS and NLOS) and the complexity of the model affect the positioning accuracy.
    Across all setups, LLMs exhibit strong regression capabilities, with median localization
    errors typically near or below 1 meter.


    In the fully NLOS Corridor environment, the 3B model has the highest accuracy,
    with an MAE of 1.12 m, and MSE of 1.86. The smaller 1B model also remains effective
    in such scenario with the lowest 25th percentile (0.96) error showing its capability
    in constrained, noisy environment.


    Theater (LOS) has the best performance with the 3 billion model with the lowest
    MSE (1.23) suggesting few outliers and lowest MAE (0.90) suggesting consistent
    performance overall. The LOS condition and relatively square shape (15 × 14.5
    m<sup>2</sup> ) likely contributing to the higher accuracy. The theater has the
    highest number of RPs (120), which also explains better spatial granularity and
    improved results.


    In contrast, in the office environment, which presents mixed LOS-NLOS conditions,
    the 8B model outperforms others with an MSE of 1.98 highlighting its ability to
    generalize better in heterogeneous signal environments. Nevertheless, the 1B model
    notably achieves the lowest MAE (approximately 1.15 m) and the smallest errors
    at the median (0.81 m) making it suitable for resource-constrained deployment
    scenarios.


    Across all environment, LLMs exhibit strong regression capabilities, with median
    localization errors typically near or below 1 meter. These findings demonstrate
    the practical use of decoder-only architectures to integrate diverse WiFi telemetry
    data (RSSI, FTM, CSI) within a unified learning framework, enhancing indoor localization
    without relying on explicit rule based models. This sets the system accuracy suitable
    for indoor navigation, asset tracking, and augmented reality (AR) use cases.


    # *C. Data Imputation*


    Large Language Models (LLMs) often perform implicit data imputation to compensate
    for missing input features. By framing the positioning problem through a languagebased
    interface, we are able to leverage this capability to enhance robustness and generalization.
    In the background and related work, we''ve already mentioned that combining multiple
    telemetry sources (sensor fusion) improves accuracy, especially since FTM, a time-based
    mechanism, offers finer granularity than coarse-grained mechanisms like RSSI.
    To test whether our model has learned to use these modalities dynamically, we
    use a model trained on both FTM and RSSI data and then selectively ablate one
    feature set at a time. Figure 2 shows the cumulative distributed probability of
    model performance based on the choice of telemetry selected.


    *1) FTM+RSSI:* Across all model sizes (1B, 3B, 8B), the FTM+RSSI combinations
    consistently achieve the highest cumulative probability at lower error thresholds.
    This supports the theoretical premise that fusing time-based (FTM) and power-based
    (RSSI) features provides complementary information, improving generalization and
    implying that the model can capture the intricacies.


    *2) FTM-Only Ablation Study:* FTM performs the second best. The FTM-only models
    (especially 3B FTM and 8B FTM) show strong performance, closely following the
    FTM+RSSI curves. This aligns with the fact that time-based telemetry perform better
    than course grained RSSI. It is worth noting that the 1B model exhibited 296 misalignments,
    the 3B model reduced this to 22, and the 8B model achieved perfect alignment with
    zero misalignments out of 1920 samples used for testing.


    *3) RSSI-Only Ablation Study:* RSSI-only models consistently lag behind in performance
    across all model sizes, with slower CDF growth and heavier tails. This is expected
    due to RSSI''s high sensitivity to multipath and environment-specific attenuation,
    which limits its localization precision. It is also worth noting that 1b had 8
    misalignments whereas other models didn''t have any misalignments.


    ## *D. Discussion*


    Our methodology analyzes the accuracy using a single WiFi packet from each AP
    to isolate models'' behaviour. However, a single packet may be corrupted in practical
    deployments due to hardware imperfections, clock drift, or other systemlevel limitations
    beyond the model''s control, implicating tail latency. To mitigate these issues,
    most systems rely on multiple packets. For instance, even with CSI (most precise
    form of WiFi telemetry) SpotFi [60] requires 10 CSI packets to achieve a 0.5 median
    accuracy while worst error reaches up to 10m.


    Practitioners adopting our system can apply a sliding window of 5–10 packets to
    smooth out tail-heavy errors, incurring only a few milliseconds of additional
    delay. Alternatively, the LLM can be adapted to ingest multiple packets jointly
    rather than processing each individually. However, it is important to note that
    increasing the input context window also increases the time complexity in a quadratic
    manner [61]. LLMs can also be given the physical dimensions of the environment
    in the prompt itself to prevent hallucinations beyond the defined boundary conditions.


    Figure 2 showcases that even with incomplete telemetry (RSSI-only), larger models
    (e.g., 8B RSSI) still show improvement over smaller counterparts (e.g., 1B RSSI).
    This indicates that the LLaMA model effectively leverages its internal latent
    space, learning correlations and compensating for missing features during fine-tuning.
    The model''s ability to infer distance from imperfect data (e.g., RSSI-only cases)
    demonstrates that the vector embeddings capture semantically rich structure, allowing
    practical distance estimation even under partial observability. The bigger parameter
    models showcase no misalignments whereas the smaller model tends to misalign with
    ablated data. However, as noted in [56], model performance improves more with
    increased data than just scaling model size. Once the smaller models are trained
    on a bigger corpus of radio data, they can generalize further and achieve better
    accuracy. That''s why we trained the model separately for each environment and
    dataset, so we could closely observe how it behaves under different conditions
    without external data. As long as the model gets relevant data, it can generalize
    effectively.


    Rather than using embeddings or few-shot prompts, we treat indoor positioning
    as a next-token prediction task, aligning the output distribution with predicted
    distance as the final token. This approach uses LLMs'' autoregressive modelling,
    internal attention mechanisms, positional encodings, and emergent sequence forecasting
    behaviours, capturing sequential dependencies that static methods miss. Our simplified
    pipeline consists of a single autoregressive pass that is generalizable across
    sensors and environments.


    ## VI. LIMITATIONS AND FUTURE WORK


    We selected the LLaMA family of models for our initial experiments because they
    are open source, available in different sizes (1B, 3B, 8B), and are widely adopted
    in the research community. Moving forward, we plan to explore other model families
    like Gemma and Phi, and closed-source LLMs such as Claude and OpenAI''s ChatGPT.
    We also aim to study the effects of quantization across different models to understand
    better the trade-offs in performance, efficiency, and deployment feasibility.


    LLMs are known to be sensitive to prompt phrasing due to their autoregressive
    property [62]. Even a slight change of wording can drastically affect the next
    token, potentially hindering its ability to do regression. We plan to study the
    influence of prompts on indoor positioning accuracy. As a temporary workaround,
    we can treat the regression task as a separate model call using the structured
    query described in the training data format (as outlined in the paper).


    ![](_page_7_Figure_13.jpeg)


    Fig. 3. Trilateration geometry: (a) Ideal Scenario (b) Real Scenario [63]


    Our system''s performance may degrade in a new environment without any prior training
    or Dataset of new sensors. If there is no error, the device should meet at a singular
    point in space as shown in Figure 2a [63]. However, errors due to signal fading,
    path loss, create a geometry instead of a centroid, as seen in Figure 2b. Calculating
    the area of the geometry and having multiple access points(more than 3) allows
    us to get the ground truth. Smaller error area (A) implies higher accuracy, so
    we define the feedback as ffeedback = 1 A . Without the ground truth, this feedback
    serves as a reward signal, minimizing localization errors.


    Our approach shows great potential for localization accuracy. Even though our
    model runs on commodity hardware, it requires significant computing power. However,
    that is starting to change. There has been a large-scale effort to reduce computing
    costs while making models more power efficient [64], [65]. For example, Microsoft''s
    BitNet b1.58 [66] reduces memory and computing needs without losing much performance.
    With breakthroughs like this, more models are moving toward low-bit quantization.
    We can expect these models to run directly on consumer Stations (STA) or affordable
    Access Points (AP) that can support multiple user without costing a fortune.


    ## VII. CONCLUSION


    In this paper, we introduce WiFiGPT, a telemetry-agnostic, token-based WiFi positioning
    system built entirely on large language models. Using a custom training template
    for the telemetry dataset and fine-tuning Llama 3 using LoRA, we could adapt a
    purely autoregressive decoder to a regression task. We show it can reliably capture
    complex signal behaviours like multipath without filtering or preprocessing while
    consistently staying under 1m error. The pipeline, which includes an auxiliary
    regression monitor and a fully deterministic inference setup, makes LLMs a reliable
    regressor. This setup shows that token-based regression is a viable alternative
    to traditional methods. It is schema-less by design, requires no preprocessing
    pipeline, and effectively captures semantic relationships for categorical data.
    It also highlights how large language models can implicitly learn nuanced spatial
    patterns in noisy wireless telemetry by attending to structured input sequences
    and understanding the language of wireless.


    ## REFERENCES


    - [1] N. Singh, S. Choe, and R. Punmiya, "Machine learning based indoor localization
    using wi-fi rssi fingerprints: An overview," *IEEE Access*, vol. 9, pp. 127 150–127
    174, 2021.

    - [2] P. S. Farahsari, A. Farahzadi, J. Rezazadeh, and A. Bagheri, "A survey on
    indoor positioning systems for iot-based applications," *IEEE Internet of Things
    Journal*, vol. 9, no. 10, pp. 7680–7699, 2022.

    - [3] A. Correa, M. Barcelo, A. Morell, and J. L. Vicario, "A review of pedestrian
    indoor positioning systems for mass market applications," *Sensors*, vol. 17,
    no. 8, p. 1927, 2017.

    - [4] F. Dwiyasa and M.-H. Lim, "A survey of problems and approaches in wireless-based
    indoor positioning," in *2016 International conference on indoor positioning and
    indoor navigation (IPIN)*. IEEE, 2016, pp. 1–7.

    - [5] G. Deak, K. Curran, and J. Condell, "A survey of active and passive indoor
    localisation systems," *Computer Communications*, vol. 35, no. 16, pp. 1939–1954,
    2012.

    - [6] P. Roy and C. Chowdhury, "A survey on ubiquitous wifi-based indoor localization
    system for smartphone users from implementation perspectives," *CCF Transactions
    on Pervasive Computing and Interaction*, vol. 4, no. 3, pp. 298–318, 2022.

    - [7] E. Pagliari, L. Davoli, and G. Ferrari, "Wi-fi-based real-time uav localization:
    A comparative analysis between rssi-based and ftm-based approaches," *IEEE Transactions
    on Aerospace and Electronic Systems*, vol. 60, no. 6, pp. 8757–8778, 2024.

    - [8] F. Zafari, A. Gkelias, and K. K. Leung, "A survey of indoor localization
    systems and technologies," *IEEE Communications Surveys & Tutorials*, vol. 21,
    no. 3, pp. 2568–2599, 2019.

    - [9] M. Cominelli, F. Gringoli, and F. Restuccia, "Exposing the csi: A systematic
    investigation of csi-based wi-fi sensing capabilities and limitations," in *2023
    IEEE International Conference on Pervasive Computing and Communications (PerCom)*.
    IEEE, 2023, pp. 81–90.

    - [10] A. Nessa, B. Adhikari, F. Hussain, and X. N. Fernando, "A survey of machine
    learning for indoor positioning," *IEEE access*, vol. 8, pp. 214 945–214 965,
    2020.

    - [11] B. Twala, "An empirical comparison of techniques for handling incomplete
    data using decision trees," *Applied Artificial Intelligence*, vol. 23, no. 5,
    pp. 373–405, 2009.

    - [12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. u. Kaiser, and I. Polosukhin, "Attention is all you need," in *Advances in
    Neural Information Processing Systems*, I. Guyon, U. V. Luxburg, S. Bengio, H.
    Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,
    Inc., 2017.

    - [13] T. Pires, E. Schlinger, and D. Garrette, "How multilingual is multilingual
    bert?" *arXiv preprint arXiv:1906.01502*, 2019.

    - [14] Google DeepMind, "Dolphingemma: How google ai is helping decode dolphin
    communication," https://blog.google/technology/ai/dolphingemma/, February 2024,
    accessed: 2025-05-03. [Online]. Available: https://blog.google/technology/ai/dolphingemma/

    - [15] H. Liu, H. Kamarthi, Z. Zhao, S. Xu, S. Wang, Q. Wen, T. Hartvigsen, F.
    Wang, and B. A. Prakash, "How can time series analysis benefit from multiple modalities?
    a survey and outlook," *arXiv preprint arXiv:2503.11835*, 2025.

    - [16] W. Aljedaani, A. Habib, A. Aljohani, M. Eler, and Y. Feng, "Does chatgpt
    generate accessible code? investigating accessibility challenges in llm-generated
    source code," in *Proceedings of the 21st International Web for All Conference*,
    ser. W4A ''24. New York, NY, USA: Association for Computing Machinery, 2024, p.
    165–176. [Online]. Available: https://doi.org/10.1145/3677846.3677854

    - [17] S. Pagano, S. Peirani, and M. Valle, "Indoor ranging and localisation algorithm
    based on received signal strength indicator using statistic parameters for wireless
    sensor networks," *IET Wireless Sensor Systems*, vol. 5, no. 5, pp. 243–249, 2015.

    - [18] H. P. Mistry and N. H. Mistry, "Rssi based localization scheme in wireless
    sensor networks: A survey," in *2015 Fifth International Conference on Advanced
    Computing & Communication Technologies*. IEEE, 2015, pp. 647–652.

    - [19] S. Yiu, M. Dashti, H. Claussen, and F. Perez-Cruz, "Wireless rssi fingerprinting
    localization," *Signal Processing*, vol. 131, pp. 235–244, 2017.

    - [20] N. Singh, S. Choe, and R. Punmiya, "Machine learning based indoor localization
    using wi-fi rssi fingerprints: An overview," *IEEE access*, vol. 9, pp. 127 150–127
    174, 2021.

    - [21] D. Lymberopoulos and J. Liu, "The microsoft indoor localization competition:
    Experiences and lessons learned," *IEEE Signal Processing Magazine*, vol. 34,
    no. 5, pp. 125–140, 2017.

    - [22] V. Barral Vales, O. C. Fernandez, T. Dom ´ ´ınguez-Bolano, C. J. Escudero,
    ˜ and J. A. Garc´ıa-Naya, "Fine time measurement for the internet of things: A
    practical approach using esp32," *IEEE Internet of Things Journal*, vol. 9, no.
    19, pp. 18 305–18 318, 2022.

    - [23] M. Ibrahim, H. Liu, M. Jawahar, V. Nguyen, M. Gruteser, R. Howard, B. Yu,
    and F. Bai, "Verification: Accuracy evaluation of wifi fine time measurements
    on an open platform," in *Proceedings of the 24th annual international conference
    on mobile computing and networking*, 2018, pp. 417–427.

    - [24] M. Bullmann, T. Fetzer, F. Ebner, M. Ebner, F. Deinzer, and M. Grzegorzek,
    "Comparison of 2.4 ghz wifi ftm-and rssi-based indoor positioning methods in realistic
    scenarios," *Sensors*, vol. 20, no. 16, p. 4515, 2020.

    - [25] S. Kumar, S. Gil, D. Katabi, and D. Rus, "Accurate indoor localization
    with zero start-up cost," in *Proceedings of the 20th annual international conference
    on Mobile computing and networking*, 2014, pp. 483–494.

    - [26] J. Xiong and K. Jamieson, "{ArrayTrack}: A {Fine-Grained} indoor location
    system," in *10th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 13)*, 2013, pp. 71–84.

    - [27] M. Kotaru, K. Joshi, D. Bharadia, and S. Katti, "Spotfi: Decimeter level
    localization using wifi," in *Proceedings of the 2015 ACM Conference on Special
    Interest Group on Data Communication*, ser. SIGCOMM ''15. New York, NY, USA: Association
    for Computing Machinery, 2015, p. 269–282. [Online]. Available: https://doi.org/10.1145/2785956.2787487

    - [28] K. Qian, C. Wu, Z. Yang, Z. Zhou, X. Wang, and Y. Liu, "Enabling phased
    array signal processing for mobile wifi devices," *IEEE Transactions on Mobile
    Computing*, vol. 17, no. 8, pp. 1820–1833, 2017.

    - [29] A. Paulraj, R. Roy, and T. Kailath, "Estimation of signal parameters via
    rotational invariance techniques-esprit," in *Nineteeth Asilomar Conference on
    Circuits, Systems and Computers, 1985.* IEEE, 1985, pp. 83–89.

    - [30] E. Systems, "Esp32 series datasheet," https://www.espressif.com/en/products/socs/esp32,
    2023, accessed: 2025-05-14.

    - [31] R. P. Foundation, "Raspberry pi documentation," https://www.raspberrypi.com/documentation/,
    2023, accessed: 2025- 05-14.

    - [32] M. Qorib, G. Moon, and H. T. Ng, "Are decoder-only language models better
    than encoder-only language models in understanding word meaning?" in *Findings
    of the Association for Computational Linguistics ACL 2024*, 2024, pp. 16 339–16
    347.

    - [33] M. V. Koroteev, "Bert: a review of applications in natural language processing
    and understanding," *arXiv preprint arXiv:2103.11943*, 2021.

    - [34] Z. Zhao, F. Meng, H. Li, X. Li, and G. Zhu, "Mining limited data sufficiently:
    A bert-inspired approach for csi time series application in wireless communication
    and sensing," 2024. [Online]. Available: https://arxiv.org/abs/2412.06861

    - [35] L. B. Cesar, M.-A. Manso-Callejo, and C.-I. Cira, "Bert (bidirectional
    ´ encoder representations from transformers) for missing data imputation in solar
    irradiance time series," *Engineering Proceedings*, vol. 39, no. 1, p. 26, 2023.

    - [36] O. AI, "Gpt-4 technical report," *arXiv preprint arXiv:2303.08774*, 2023.

    - [37] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Roziere, N. Goyal, E. Hambro, F. Azhar ` *et al.*, "Llama: Open and efficient
    foundation language models," *arXiv preprint arXiv:2302.13971*, 2023.

    - [38] D. Wu, X. Wang, Y. Qiao, Z. Wang, J. Jiang, S. Cui, and F. Wang, "Netllm:
    Adapting large language models for networking," in *Proceedings of the ACM SIGCOMM
    2024 Conference*, ser. ACM SIGCOMM ''24. ACM, Aug. 2024, p. 661–678. [Online].
    Available: http://dx.doi.org/10.1145/3651890.3672268

    - [39] J. Shao, J. Tong, Q. Wu, W. Guo, Z. Li, Z. Lin, and J. Zhang, "Wirelessllm:
    Empowering large language models towards wireless intelligence," 2024. [Online].
    Available: https://arxiv.org/abs/2405.17053

    - [40] S. Alnegheimish, L. Nguyen, L. Berti-Equille, and K. Veeramachaneni, "Large
    language models can be zero-shot anomaly detectors for time series?" 2024. [Online].
    Available: https://arxiv.org/abs/2405.14755

    - [41] Z. Zhao, T. Chen, F. Meng, H. Li, X. Li, and G. Zhu, "Finding the missing
    data: A bert-inspired approach against package loss in wireless sensing," in *IEEE
    INFOCOM 2024 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)*.
    IEEE, May 2024, p. 1–6. [Online]. Available: http://dx.doi.org/10.1109/INFOCOMWKSHPS61880.2024.10620769

    - [42] X. Zhao, G. Wang, Z. An, Q. Pan, and L. Yang, "Understanding localization
    by a tailored gpt," in *Proceedings of the 22nd Annual International Conference
    on Mobile Systems, Applications and Services*, ser. MOBISYS ''24. New York, NY,
    USA: Association for Computing Machinery, 2024, p. 318–330. [Online]. Available:
    https://doi.org/10.1145/3643832.3661869

    - [43] R. Vacareanu, V.-A. Negru, V. Suciu, and M. Surdeanu, "From words to numbers:
    Your large language model is secretly a capable regressor when given in-context
    examples," *arXiv preprint arXiv:2404.07544*, 2024.

    - [44] E. Tang, B. Yang, and X. Song, "Understanding llm embeddings for regression,"
    *arXiv preprint arXiv:2411.14708*, 2024.

    - [45] B. Agarwal, I. Joshi, and V. Rojkova, "Think inside the json: Reinforcement
    strategy for strict llm schema adherence," *arXiv preprint arXiv:2502.14905*,
    2025.

    - [46] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and
    D. Xiong, "Large language model alignment: A survey," *arXiv preprint arXiv:2309.15025*,
    2023.

    - [47] M. Community, "Llama-3.2-1b-instruct-bf16," https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-MLXTuned,
    2024, converted to MLX format from metallama/Llama-3.2-1B-Instruct using mlx-lm
    version 0.17.1. [Online]. Available: https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-MLXTuned

    - [48] ——, "Llama-3.2-3b-bf16," https://huggingface.co/mlxcommunity/Llama-3.2-3B-bf16,
    2024, converted to MLX format from meta-llama/Llama-3.2-3B using mlx-lm version
    0.17.1. [Online]. Available: https://huggingface.co/mlx-community/Llama-3.2-3B-bf16

    - [49] ——, "Deephermes-3-llama-3-8b-preview-bf16," https://huggingface.co/mlx-community/DeepHermes-3-Llama-3-
    8B-Preview-bf16, 2025, converted to MLX format from NousResearch/DeepHermes-3-Llama-3-8B-Preview
    using mlx-lm version 0.21.1. [Online]. Available: https://huggingface.co/mlxcommunity/DeepHermes-3-Llama-3-8B-Preview-bf16

    - [50] J. Zhao, Z. Zhang, L. Gao, Q. Zhang, T. Gui, and X. Huang, "Llama beyond
    english: An empirical study on language capability transfer," *arXiv preprint
    arXiv:2401.01055*, 2024.

    - [51] G. Dong, H. Yuan, K. Lu, C. Li, M. Xue, D. Liu, W. Wang, Z. Yuan, C. Zhou,
    and J. Zhou, "How abilities in large language models are affected by supervised
    fine-tuning data composition," *arXiv preprint arXiv:2310.05492*, 2023.

    - [52] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W.
    Chen *et al.*, "Lora: Low-rank adaptation of large language models." *ICLR*, vol.
    1, no. 2, p. 3, 2022.

    - [53] A. Hannun, J. Digani, A. Katharopoulos, and R. Collobert, "MLX: Efficient
    and flexible machine learning on apple silicon," 2023. [Online]. Available: https://github.com/ml-explore

    - [54] B. Zhang, Z. Liu, C. Cherry, and O. Firat, "When scaling meets llm finetuning:
    The effect of data, model and finetuning method," *arXiv preprint arXiv:2402.17193*,
    2024.

    - [55] U. Team, "Fine-tuning guide," https://docs.unsloth.ai/get-started/finetuning-guide,
    2025, accessed: 2025-05-11.

    - [56] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,
    K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan,
    E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, "Training compute-optimal large
    language models," 2022. [Online]. Available: https://arxiv.org/abs/2203.15556

    - [57] X. Feng, K. A. Nguyen, and Z. Luo, "WiFi RSS & RTT dataset with different
    LOS conditions for indoor positioning," Zenodo, Jun. 2024, accessed: May 11, 2025.
    [Online]. Available: https://zenodo.org/doi/10.5281/zenodo.11558791

    - [58] G. Wang, C. Qian, K. Cui, X. Shi, H. Ding, W. Xi, J. Zhao, and J. Han,
    "A universal method to combat multipaths for rfid sensing," in *IEEE INFOCOM 2020
    - IEEE Conference on Computer Communications*, 2020, pp. 277–286.

    - [59] X. Feng, K. A. Nguyen, and Z. Luo, "A wifi rss-rtt indoor positioning system
    using dynamic model switching algorithm," *IEEE Journal of Indoor and Seamless
    Positioning and Navigation*, 2024.

    - [60] M. Kotaru, K. Joshi, D. Bharadia, and S. Katti, "Spotfi: Decimeter level
    localization using wifi," *SIGCOMM Comput. Commun. Rev.*, vol. 45, no. 4, p. 269–282,
    Aug. 2015. [Online]. Available: https://doi.org/10.1145/2829988.2787487

    - [61] M. Ali, M. Fromm, K. Thellmann, R. Rutmann, M. Lubbering, J. Lev- ¨ eling,
    K. Klug, J. Ebert, N. Doll, J. Buschhoff *et al.*, "Tokenizer choice for llm training:
    Negligible or crucial?" in *Findings of the Association for Computational Linguistics:
    NAACL 2024*, 2024, pp. 3907–3924.

    - [62] S. Anagnostidis and J. Bulian, "How susceptible are llms to influence in
    prompts?" 2024. [Online]. Available: https://arxiv.org/abs/2408.11865

    - [63] H. M. Le, J.-P. Rossi, and D. Slock, "A geometric interpretation of trilateration
    for rss-based localization," in *2020 28th European Signal Processing Conference
    (EUSIPCO)*, 2021, pp. 1797–1801.

    - [64] S. Shekhar, T. Dubey, K. Mukherjee, A. Saxena, A. Tyagi, and N. Kotla,
    "Towards optimizing the costs of llm usage," *arXiv preprint arXiv:2402.01742*,
    2024.

    - [65] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J.
    Xue, and F. Wei, "The era of 1-bit llms: All large language models are in 1.58
    bits," *arXiv preprint arXiv:2402.17764*, vol. 1, 2024.


    [66] S. Ma, H. Wang, S. Huang, X. Zhang, Y. Hu, T. Song, Y. Xia, and F. Wei, "Bitnet
    b1.58 2b4t technical report," 2025. [Online]. Available: https://arxiv.org/abs/2504.12285'
- title: Characterization of Using Hybrid Beamforming in mmWave Virtual Reality
  abstract: 'Wireless Virtual Reality (VR) is increasingly in demand in Wireless LANs

    (WLANs). In this paper, a utility function for resource management in wireless

    VR is proposed. Maximizing the sum rate metric in transmitting VR audio or

    videos is an important factor for ascertaining low latency in obtaining QoS

    requirement of users in VR, so forth mmWave frequency band in WLAN technology

    should be used. This frequency band is presented in IEEE 802.11ad/ay. Resource

    access method in IEEE 802.11ay standard is MultiUser MIMO (MU-MIMO) with OFDM

    modulation. Operating at mmWave frequency band is equal to use massive number

    of antenna to enhance the received power in (Line of Sight) LoS direction by

    inducing sever propagation with small wavelength. Also for reducing the

    complexity of hardware in mmWave technology, designers should select some

    number of connected phase shifters to each antenna element by hybrid

    beamforming method. Processing delay, transmission delay and queue delay should

    be considered in acquiring QoS metric in terms of utility function. The optimal

    closed form expression of the multi-attribute utility function is based on

    these delays that are calculated by downlink and uplink rates in assistant with

    hybrid beamforming. Trends of transmission delay and multi-attribute utility

    function in various Es/N0 values and different scenarios are analyzed. Based on

    these results, 95.4% accuracy in comparison with ns3 in uplink and downlink

    channel modeling in IEEE 802.11ay standard''s indoor environment has been

    reported. Also, it is shown that min channel gain consideration can cause

    reduction in the value of the utility function and incursion in transmission

    delay in VR.'
  url: http://arxiv.org/abs/2505.15830v1
  keywords: '**— Virtual Reality (VR), QoS, mmWave, IEEE802.11ay, MU-MIMO-OFDM, utility
    function, hybrid beamforming.**'
  document: "# Characterization of Using Hybrid Beamforming in mmWave Virtual Reality\n\
    \nNasim Alikhani, Abbas Mohammadi, AUT-Wireless Research Lab. Electrical Engineering\
    \ Department, Amirkabir University of Technology Tehran, Iran\n\n#### *[abm125@aut.ac.ir](mailto:abm125@aut.ac.ir)*\n\
    \n*Abstract***—Wireless Virtual Reality (VR) is increasingly in demand in Wireless\
    \ LANs (WLANs). In this paper, a utility function for resource management in wireless\
    \ VR is proposed. Maximizing the sum rate metric in transmitting VR audio or videos\
    \ is an important factor for ascertaining low latency in obtaining QoS requirement\
    \ of users in VR, so forth mmWave frequency band in WLAN technology should be\
    \ used. This frequency band is presented in IEEE 802.11ad/ay. Resource access\
    \ method in IEEE 802.11ay standard is Multi-User MIMO (MU-MIMO) with OFDM modulation.\
    \ Operating at mmWave frequency band is equal to use massive number of antenna\
    \ to enhance the received power in (Line of Sight) LoS direction by inducing sever\
    \ propagation with small wavelength. Also for reducing the complexity of hardware\
    \ in mmWave technology, designers should select some number of connected phase\
    \ shifters to each antenna element by hybrid beamforming method. Processing delay,\
    \ transmission delay and queue delay should be considered in acquiring QoS metric\
    \ in terms of utility function. The optimal closed form expression of the multi-attribute\
    \ utility function is based on these delays that are calculated by downlink and\
    \ uplink rates in assistant with hybrid beamforming. Trends of transmission delay\
    \ and multi-attribute utility function in various Es/N0 values and different scenarios\
    \ are analyzed. Based on these results, 95.4% accuracy in comparison with ns3\
    \ in uplink and downlink channel modeling in IEEE 802.11ay standard's indoor environment\
    \ has been reported. Also, it is shown that min channel gain consideration can\
    \ cause reduction in the value of the utility function and incursion in transmission\
    \ delay in VR.**\n\n*Index Terms***— Virtual Reality (VR), QoS, mmWave, IEEE802.11ay,\
    \ MU-MIMO-OFDM, utility function, hybrid beamforming.**\n\n## 1.1. INTRODUCTION\n\
    \n Virtual reality (VR) technology provides a real-time immersive experience that\
    \ tightly blends physical and digital reality. This technology is obtaining attention\
    \ due to its ability to present an immersive viewing experience to users [1-6].\
    \ VR services are designed to build a virtual environment to mimic the real world\
    \ and immerse participants in virtual worlds. Although the quality of the experience\
    \ can be improved by changing the frame size and frame rate, there are limitations\
    \ on the values of these parameters in the wireless environment [7,8]. On the\
    \ other hand one of the main problems in wired VR application is the restriction\
    \ of mobility of VR devices. The second problem is that VR application is delay-sensitive.\
    \ To overcome these limitations, VR services can be supported with wireless cellular\
    \ connectivity that may provide better user experiences [9,10].\n\n A specific\
    \ feature of heavy traffic load on VR applications in wireless network is delay\
    \ sensitivity. The current Wi-Fi standards in Wireless Local Area Networks (WLAN)\
    \ are unable to guarantee the desired Quality of Service (QoS) requirements, especially\
    \ in dense environments [11]. On the other hand, mmWave technology can provide\
    \ a high transmission rate, low latency, and high transmission reliability. Built\
    \ upon 802.11ad, IEEE 802.11ay aims to offer about 100 Gbps throughput, ultra-low\
    \ latency by using technological advancements such as MIMO communication, channel\
    \ bonding/aggregation, and new beamforming techniques. Various VR services have\
    \ different traffic loads, delay, and bandwidth requirements [12]. IEEE 802.11ay\
    \ standard [13] works on the millimeter-wave frequency range and uses MU-MIMO\
    \ access method in Downlink (DL) with OFDM modulation. OFDM modulation in IEEE\
    \ 802.11ay increases the rate in DL by using a various number of subcarriers [14-16].\
    \ In MU-MIMO access mechanism, some users receive services simultaneously from\
    \ a transmitter such as Access Point (AP). AP has to schedule users in a parallel\
    \ manner to maximize the overall throughput. In order to realize the benefit of\
    \ the MU-MIMO access mechanism and guarantee the required QoS, it is essential\
    \ to acquire updated Channel State Information (CSI) from all the users. Hence,\
    \ there are a trade-off between the efficiency of the scheduler and the CSI overhead.\
    \ Generally, AP limits the number of users based on CSI feedback. The best user\
    \ CSI and suitable channels need to be obtained before the user is scheduled [17].\n\
    \n Meanwhile, in order to overcome the severe propagation loss of mmWave signals,\
    \ large antenna arrays in configuration of mmWave systems should be used [18-21].\
    \ The usage of a large number of antennas in mmWave systems is equal to a large\
    \ amount of RF chain that has great power consumption which makes full digital\
    \ beamforming impractical. Because of this limitation, hybrid beamforming can\
    \ be used in mmWave systems. Hybrid beamforming method divides the beamforming\
    \ method into digital and analog domains [10, 23]. So in order to increase the\
    \ energy efficiency in MU-MIMO mmWave systems and reduce the number of RF chains\
    \ at the cost of only slight performance degradation, the combination of a low-dimensional\
    \ digital baseband beamformer and a high-dimensional analog beamformer is essential\
    \ [18].\n\n In addition, important requirements on VR service are tracking accuracy,\
    \ transmission delay, processing delay and queue delay [24]. Authors in [25-28]\
    \ have considered utility function with less features that are almost dependent\
    \ to each other. Tracking accuracy, transmission delay, processing delay and queue\
    \ delay affect user experience. They are not independent from each other on affecting\
    \ requirements of QoS for users. In order to consider them jointly, we propose\
    \ a multi-attribute utility function that consider all of them based on the computed\
    \ rates in Downlink (DL) and Uplink (UL) with hybrid beamforming and infinite\
    \ buffer size in the transmitter in DL. For the hybrid beamforming method in this\
    \ paper, only once estimation of matrices in subcarrier frequencies (without iteratively\
    \ estimating beamforming matrices) is needed. Then, we explore hybrid beamforming\
    \ design for MU-MIMO OFDM systems over mmWave channels. Finally, by computing\
    \ the transmission delay, processing delay and queue delay at users and AP side,\
    \ the value of the utility function is evaluated. To the best of our knowledge,\
    \ this is the first method in VR applications in IEEE 802.11ay that uses the multi-attribute\
    \ utility function with considering transmission delay, processing delay, and\
    \ queue delay. The main contributions of our paper are summarized as follows,\n\
    \n1) Proposed hybrid beamforming method has low complexity in calculation of beamforming\
    \ matrices, because they are computed without iterative methods.\n\n2) A multi\
    \ attribute utility function by considering delay, transmission and processing\
    \ delay is proposed.\n\n3) The results with ns3 (network simulator) with consideration\
    \ of IEEE 802.11ay specifications in the PHY layer are extracted. Moreover, the\
    \ results of MATLAB simulation are compared with ns3 results.\n\n4) A lower bound\
    \ for DL channel is calculated based on subcarriers and the value of utility function\
    \ is calculated and compared with the utility function for DL channel by considering\
    \ the mean gain values of channel in all subcarriers. So we have considered two\
    \ scenarios as mean channel gain and min channel gain in the case of subcarriers\
    \ and compare their results of utility function and transmission delay line.\n\
    \n5) The results of the proposed optimal closed form expression of multi-attribute\
    \ utility function in MATLAB and ns3 simulations are compared with six different\
    \ codebooks that are generated in ns3: \"two-Antenna, one-RF chain\", \"two-Antenna,\
    \ two-RF chain\", \"four-Antenna, one-RF chain\", \"four-Antenna, two-RF chain\"\
    , \"eight-Antenna, one-RF chain\", \"eight-Antenna, two-RF chain\".\n\n The rest\
    \ of the paper is organized as follows. In Section 1.2, system model for channels\
    \ in the uplink and downlink are presented. In section 1.3, delay considerations\
    \ are explained. Also the routine for discovering the channel parameters of downlink\
    \ is described. In section 1.4 the optimization problem is analyzed. In section\
    \ 1.5, the indoor area and simulation results are explained. In the last section,\
    \ conclusion is expressed.\n\n## 1.2. SYSTEM MODEL\n\nIn this section formulas\
    \ for UL and DL channels, delay purposes and construction of utility function\
    \ are explained. The system model is shown in Fig. 1, where DL and UL channels\
    \ between each user and AP and intra-interference in DL and UL are shown in Fig.\
    \ 1. In this Fig, the black solid lines represent the main lobe beam direction\
    \ in UL from user to AP in *n*th subcarrier index. These channels are ℎ1,1, ,\
    \ ℎ2,2, in *n*th subcarrier. The black continues dotted lines represent the main\
    \ lobe beam direction from each antenna in DL from AP to each user. These channels\
    \ are shown by ℎ1,1 , ℎ2,2 . The grey dotted lines represent the inter interference\
    \ in DL direction from AP to user. These interference channels are ℎ2,1 , ℎ1,2\
    \ . The grey solid lines represent the inter interference in UL direction from\
    \ user to AP. These interference channels are denoted by ℎ2,1, , ℎ1,2, .\n\n(1)\n\
    \n**Fig. 1.** System Model configuration.\n\n# *1.2.1. UL channel*\n\nThe frequency\
    \ band that is considered in UL channel can be sub-6GHz. The channel matrix in\
    \ the UL is as below: −\n\n$$\\mathcal{H}^{\\rm UL}\\_{ljn}(\\mathbf{x}\\_l, y\\\
    _l, \\mathbf{z}\\_l) = g\\left(d\\_{lj}(\\mathbf{x}\\_l, y\\_l, \\mathbf{z}\\\
    _l)\\right)^{\\cdots}$$\n\nWhere channel matrix is denoted as ℎ for *i*th user,\
    \ *j*th AP in *n*th subcarrier. is the complex Gaussian channel gain in each subcarrier.\
    \ is the Euclidean vector distance between *i*th user and *j*th AP (each user\
    \ and AP has 3D vector for position). is the pathloss exponent value. The 3D coordinate\
    \ position of *i*th user is ( , , ).\n\n The complex Gaussian channel matrix in\
    \ uplink is considered as a complex Gaussian matrix with unit amplitude and different\
    \ phase values that are related to subcarrier frequencies as given by equation\
    \ (2),\n\n$$g = \\exp\\left(j\\left(\\frac{\\pi}{180}; \\frac{\\pi}{180}; \\frac{\\\
    pi}{180} \\times N\\_{\\rm SC}\\right)\\right) \\tag{2}$$\n\nWhere is the number\
    \ of subcarriers. So UL channel is ℎ = ∑ ℎ .\n\n# *1.2.2. DL channel*\n\nThe frequency\
    \ band that is considered in DL channel is mmWave technology. is the number of\
    \ transmit antenna at each AP. is the number of receive antenna for each user.\
    \ is the number of required bit stream for each user. is the number of RF chain\
    \ in antenna configuration of AP.\n\n# *1- Calculation of the parameters in DL\
    \ channel*\n\nDelay is assumed as = in DL where *d* is the distance between each\
    \ AP and user. *c* is the speed of light. Path gain at each subcarrier is denoted\
    \ as \"\" (between *i*th user and *j*th AP). is the wave length by considering\
    \ that carrier frequency is equal to 60 GHz. 20 <sup>10</sup> ( 4) is the free\
    \ space path-loss value in dB.\n\n$$pg\\_{lj} = 20\\log\\_{10}\\left(\\frac{\\\
    lambda}{4\\pi d\\_{lj}}\\right) \\times \\exp\\left(j\\left(\\frac{\\pi}{180}\
    \ \\colon \\frac{\\pi}{180} \\times \\mathbb{N}\\_{\\mathbb{S}\\mathbb{C}}\\right)\\\
    right) \\tag{3}$$\n \n$$\\text{By considering the position of } \\mathbb{A}\\\
    text{ } \\mathbb{B} \\text{ or } \\mathbb{T}\\mathbb{Y} \\text{ and position of\
    \ } \\mathbb{R}\\text{ } \\mathbb{R}\\text{, rotations in } \\mathbb{D}\\mathbb{L},\
    \ \\text{ the value of DOD/Dimension of } \\mathbb{D}\\text{-matrix of } \\mathbb{R}\\\
    text{, is defined as}$$\n\nBy considering the position of AP as TX and position\
    \ of user as RX notations in DL, the values of DOD (Direction of Departure) =RX-TX\
    \ and DOA (Direction of Arrival) =-DOD can be computed. By using the values of\
    \ DOA and DOD, AOA (Angle of Arrival) in azimuth direction and AOD (Angle of Departure)\
    \ in azimuth direction are computed.\n\n$$AOD\\_{AZ} = mod \\left( \\text{tg}^{-1}\
    \ \\left( \\frac{DOD(2)}{DOD(1)} \\right), 360 \\right) \\tag{4}$$\n\n$$AOA\\\
    _{AZ} = mod \\left( \\text{tg} \\, ^{-1} \\left( \\frac{DOA(2)}{DOA(1)} \\right),\
    \ 360 \\right) \\tag{5}$$\n\n DOA(2) and DOA(1) are the second and first dimension\
    \ of DOA respectively. Similarly, DOD(2) and DOD(1) are the second and first dimension\
    \ of DOD respectively. As a consequence , are converted from radians to degrees\
    \ and stored respectively as ,. ULA (Uniform Linear Array) antenna configuration\
    \ is considered on both the user and AP sides. The steering vector for this antenna\
    \ type is shown below:\n\n![](_page_2_Figure_19.jpeg)\n\n$$AOD\\_{AZ} = \\frac{1}{\\\
    sqrt{N\\_t}} \\exp\\left(-\\frac{j[0:N\\_t - 1]2\\pi d}{\\lambda} \\sin\\{\\theta\\\
    _{\\text{ad}\\_{AZ}}\\}\\right) \\tag{6}$$\n\n$$AOA\\_{AZ} = \\frac{1}{\\sqrt{N\\\
    _r}} \\exp\\left(-\\frac{f[0:N\\_r - 1]2\\pi d}{\\lambda} \\sin(\\phi\\_{aaa\\\
    _{AZ}})\\right) \\tag{7}$$\n\nSo complex channel in DL can be evaluated as equation\
    \ (8):\n\n$$h\\_{ij}^{DL} = \\sum\\_{\\mathbf{t}} \\sum\\_{\\mathbf{t}\\_{\\mathbf{G}\\\
    mathbf{C}}} \\left( \\mathbf{1}^{\\|\\mathbf{G}\\mathbf{1}\\|\\_{\\mathbf{L}}}\
    \ \\right) \\times A \\mathbf{O} \\mathbf{D}\\_{\\mathbf{A}\\mathbf{Z}} \\times\
    \ A \\mathbf{O} A\\_{\\mathbf{A}\\mathbf{Z}} \\times \\exp(-\\mathbf{t}/\\mathbf{\\\
    tau}) \\tag{8}$$\n\n$$\\mathbf{w} = \\begin{bmatrix} \\mathbf{w} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{0}\
    \ & \\mathbf{0} & \\mathbf{0} & \\mathbf{0} & \\mathbf{$$\n\n The variable *t*\
    \ in equation (8) is the time counter in processing the DL channel ℎ *.*This channel\
    \ is equal to the summation of complex values generated from components of delay\
    \ between each user and AP, AOA from each user and AOD from each AP and path-gain\
    \ over various times and subcarriers.\n\n# *2- Beamforming*\n\nThe beamforming\
    \ method is used to lower the hardware cost in mmWave systems. In this section,\
    \ the mathematical equations for both full digital beamforming and hybrid beamforming\
    \ are provided. We represent that the channel matrix in each subcarrier frequency\
    \ is denoted as ∈ ℂ ×.\n\n*a- Full digital beamforming*\n\nIn full digital beamforming,\
    \ each RF chains is connected to all antenna elements. In the first step, the\
    \ channel is decomposed by SVD (Singular Value Decomposition) as: [ ] = (). The\
    \ digital precoder in each subcarrier frequency can be estimated as follows:\n\
    \n$$P\\_{D\\_{\\rm SC}} = \\left[ U \\times \\left[ I\\_{N\\_{\\rm DS}} \\; \\\
    ; \\; 0\\_{N\\_{\\rm DS} \\times (N\\_{\\rm t} - N\\_{\\rm DS})} \\right] \\right]\
    \ \\tag{9}$$\n\nWhere is an identity matrix. 0×(−) is a zero matrix that is used\
    \ for dimensions with a fixed size in a specific number of bit streams and the\
    \ number of antennas at each AP. U and V are unitary matrices. So ∈ ℂ × is a complex\
    \ semi-unitary matrix.\n\n$$\\begin{aligned} \\text{The digital combinator matrix\
    \ is denoted by } \\mathcal{G}\\_{\\mathsf{D}\\_{\\mathsf{SC}}} & \\in \\mathbb{C}^{\\\
    mathsf{H} \\times \\mathsf{M}\\_{\\mathsf{DS}}} \\text{ and is defined by equation\
    \ (10):}\\\\ \\mathcal{G}\\_{\\mathsf{D}\\_{\\mathsf{SC}}} & = [V \\times [I\\\
    _{\\mathsf{N}\\mathsf{D}\\mathsf{S}} \\ \\mathsf{0}\\_{\\mathsf{N}\\_{\\mathsf{DS}}\
    \ \\times (\\mathsf{N}\\_{\\mathsf{V}} - \\mathsf{N}\\_{\\mathsf{DS}})}]] \\end{aligned}\
    \ \\tag{10}$$\n\nAnalog beamforming matrices are considered as a constant value\
    \ in each subcarrier frequency.\n\n*b- Estimation of analog beamforming matrices*\n\
    \n Initially, the sum of the values of channel matrix in all subcarrier frequencies\
    \ is computed, followed by the computation of the SVD from this matrix. To calculate\
    \ the analog combiner matrix, should be computed first which has dimensions of\
    \ × (because the complex channel has dimensions of × ), and then the sum of values\
    \ of the matrix in terms of all subcarrier frequencies is calculated.\n\n$$H\\\
    _{\\mathbb{S}\\_1} = \\sum\\_{\\mathbb{S}\\mathbb{C}=\\mathbb{1}}^{\\mathbb{N}\\\
    _{\\mathfrak{SC}}} H\\_{\\mathbb{S}\\mathbb{C}} H\\_{\\mathfrak{SC}}^H \\tag{11}$$\n\
    \nWhere is the total number of subcarriers.\n\nThen the decomposition of <sup>1</sup>\
    \ = 1<sup>1</sup> 1 is calculated by SVD. The analog combiner matrix is = 1 |1\
    \ | , where ∈ ℂ 1×. To estimate the analog precoder, the matrix which has dimensions\
    \ equal to × , is calculated in each subcarrier, and then it is denoted as <sup>2</sup>\
    \ .\n\n$$H\\_{S\\_2} = \\sum\\_{\\text{SC}=1}^{\\text{NSC}} H\\_{\\text{SC}}^H\
    \ H\\_{\\text{SC}} \\tag{12}$$\n\n Then the decomposition of <sup>2</sup> = 2<sup>2</sup>\
    \ 2 by SVD is calculated. The analog precoder matrix is represented by which is\
    \ equal to = 2 |2 | , where ∈ ℂ ×.\n\n*c- Hybrid beamforming for calculating digital\
    \ beamforming matrix*\n\nThe difference between full digital beamforming and hybrid\
    \ beamforming is that in the hybrid method, each antenna is connected to a series\
    \ of phase shifters due to the high energy consumption in the antenna configuration\
    \ in the mmWave frequency band. Therefore, the channel matrix should be updated\
    \ by considering the analog beamforming matrix and then using their values for\
    \ calculating the digital beamforming [16].\n\n As a consequence, = can be concluded.\
    \ The channel matrix in baseband in each subcarrier is denoted as and is equal\
    \ to = .\n\nSo, for computing , in each subcarrier, the SVD of channel is computed\
    \ as = . The digital beamforming matrices (digital precoder and digital combiner)\
    \ in each subcarrier frequency are respectively shown as below:\n\n$$\\mathcal{P}\\\
    _{D\\_{\\rm SC}} = V\\_{D\\_{\\rm SC}} \\times \\begin{bmatrix} l\\_{N\\_{\\rm\
    \ DS}} & 0\\_{N\\_{\\rm DS} \\times (V\\_{\\rm RF} - V\\_{\\rm DS})} \\end{bmatrix}\
    \ \\tag{13}$$\n\n$$\n\\hat{G}\\_{\\text{DSC}} = U\\_{\\text{DSC}} \\times \\left[\
    \ I\\_{\\text{N}\\_{\\text{DS}}} \\; \\mathbb{0}\\_{\\text{N}\\_{\\text{DS}} \\\
    times (1 - N\\_{\\text{DS}})} \\right] \\tag{14}\n$$\n\nDue to equation (14),\
    \ the number of RF chains on the user side is 1. Consequently, these equalities\
    \ can be concluded: ̂ = ̂ (,) , ̂ <sup>=</sup> ̂ (,) . The digital precoder depends\
    \ on power of the transmitter, therefore, normalizing the digital precoder by\
    \ the power of the AP in the DL is satisfactory.\n\nIn this hybrid beamforming\
    \ method, by once estimating matrices <sup>1</sup> , <sup>2</sup> , across all\
    \ subcarrier frequencies (without iteratively estimating beamforming matrices),\
    \ beamforming matrices are discovered by the SVD of channel matrices.\n\nTo discover\
    \ the DL rate for each user in each subcarrier, it should be noted that inter\
    \ and intra interferences has been computed. Intra interference occurs within\
    \ the user's own cell, while Inter interference occurs between cells.\n\nSINR\
    \ in UL is equivalent to:\n\n$$SINR\\_{l/n}^{\\text{UL}} = \\frac{P\\_{U\\_l}\
    \ \\left(h\\_{l/n}^{\\text{UL}}\\right)^2}{\\sigma^2 + \\sum\\_{l \\neq 1} P\\\
    _{U\\_l} \\left(h\\_{l/n}^{\\text{UL}}\\right)^2 + \\sum\\_{b=1, b \\neq l}^{B}\
    \ \\sum\\_{k=1, k \\neq l}^{U\\_b} P\\_{U\\_k} \\left(h\\_{bkm}^{\\text{UL}}\\\
    right)^2} \\tag{15}$$\n\nSINR in DL is equivalent to:\n\n$$SINR\\_{lj}^{DL} =\
    \ \\frac{P\\_{B\\_j} \\left\\{ h\\_{lj}^{DL} \\right\\}^2}{\\sigma^2 + \\sum\\\
    _{l \\neq l} P\\_{B\\_j} \\left\\{ h\\_{lj}^{DL} \\right\\}^2 + \\Sigma\\_{b=1,\
    \ b \\neq l}^B \\sum\\_{k=1, k \\neq l}^{U\\_b} P\\_{B\\_b} (h\\_{kh}^{DL})^2}\
    \ \\tag{16}$$\n\nWhere is the power of the *i*th user. is equal to users in the\
    \ coverage of *b*th AP*. B* is the number of APs. is the power of the *j*th AP.\
    \ 2 is the power of complex Gaussian noise, which is independent of inter and\
    \ intra interference signals.\n\n$$\\begin{aligned} \\text{The UL rate and DL\
    \ rate are respectively equivalent to:}\\\\ c\\_{l/n}^{\\text{UL}} &= BW \\times\
    \ \\log\\_2(1 + SINR\\_{l/n}^{\\text{UL}}) \\end{aligned} \\tag{17}$$\n\n$$c\\\
    _{l\\bar{l}}^{DL} = BW \\times \\log\\_2(1 + SINR\\_{l\\bar{l}}^{DL}) \\tag{18}$$\n\
    \n# 1.3. DELAY COMPUTATION AND UTILITY FUNCTION\n\nThis section discusses the\
    \ computations of delay and the utility function.\n\n# *1.3.1Delay*\n\n In considering\
    \ delays, three factors have been presented: 1-delay in UL, 2-delay in DL and\
    \ 3-queue delay. The transmission delay for the *i*th user, *j*th AP in the *n*th\
    \ subcarrier frequency is given by equation (19),\n\n$$D\\_{l/n}^T = \\frac{\\\
    mathcal{S}\\_l}{c\\_{l/l}^{DL}} + \\frac{A\\_l}{c\\_{l/n}^{UL}} \\tag{19}$$\n\n\
    \ represents the maximum number of bits in DL that each AP transmit to the *i*th\
    \ user. While denotes the size of the tracking vector in UL that each user transmits\
    \ to the specified AP.\n\nThe delay for processing information is as follows:\n\
    \n$$D\\_l^p\\left(\\mathbf{K}\\_{l/n}\\right) = \\frac{\\nu\\left(l\\left(\\mathbf{X}\\\
    _l\\{\\text{SNR}\\_{l/n}^{\\text{IL}}\\}\\right), l(\\mathbf{X}\\_l^R)\\right)}{\\\
    frac{M}{N\\_l}}\\tag{20}$$\n\nHere, 0 ≤ ( (( )) , ( )) ≤ , where *v* is the number\
    \ of bits in transmitting the image from (( )) to ( ), calculated by each AP.\
    \ *M* represents the overall processing limit of each AP. is the dedicated power\
    \ to each user from the *j*th AP, and signifies the accuracy in routing the *i*th\
    \ user in the *n*th subcarrier by the *j*th AP.\n\nRegarding the queue delay issue,\
    \ the distribution of users requests follows a poisson distribution with a mean\
    \ . The servicing time for the request of each user has an exponential distribution\
    \ with parameter . The buffer size in each AP is considered infinite. The inequality\
    \ > is assumed for the tradeoff between the time of user's requests and servicing\
    \ time. The delay time for the queue is represented as <sup>1</sup> and total\
    \ delay time is given by equation (21),\n\n$$D\\_{ljn} = D\\_l^p \\{ K\\_{ljn}\
    \ \\} + D\\_{ljn}^T + \\left( \\frac{1}{\\mu\\_j - \\lambda\\_l} \\right) \\tag{21}$$\n\
    \n# *1.3.2. Utility Function*\n\n The multi attribute utility function in this\
    \ paper has a joint function takes a conditional form with respect to the accuracy\
    \ of routing [24]. This function describes the utility function between the *i*th\
    \ user, *j*th AP as (,) in the *n*th subcarrier. The conditional utility function\
    \ is denoted as (|), that is given by equation (22),\n\n$$U\\_l \\{ D\\_{l/n}\
    \ \\} K\\_{l/n} = \\begin{cases} D\\_{\\max,l} \\{ K\\_{l/n} \\} - D\\_{l/n} &\
    \ D\\_{l/n} \\ge \\chi\\_{D\\_l} \\\\ D\\_{\\max,l} \\{ K\\_{l/n} \\} - \\chi\\\
    _{D\\_l} & D\\_{l/n} < \\chi\\_{D\\_l} \\\\ 1 & D\\_{l/n} < \\chi\\_{D\\_l} \\\
    end{cases} \\tag{22}$$\n\n represents the maximum tolerable delay for the *i*th\
    \ user. ,() = max () is the maximum delay for the *i*th user. Additionally, we\
    \ can infer that (, |) = 0 and ( |) = 1.\n\nWhen < , the value of the conditional\
    \ utility function is equal to 1. Therefore, the total utility function is as\
    \ follows: (,)\n\n$$\\dot{\\mathbf{U}} = \\mathbf{U}\\_l \\Big\\{ \\mathbf{D}\\\
    _{l/n} \\Big| \\mathbf{K}\\_{l/n} \\Big\\} \\mathbf{U}\\_l \\Big\\{ \\mathbf{K}\\\
    _{l/n} \\Big\\} \\tag{23}$$\n\n$$\\mathbf{U}l\\_{l}(\\mathbf{D}\\_{l\\mid ln},\\\
    mathbf{K}\\_{l\\mid ln}) = \\left(1 - \\frac{\\left\\|\\mathbf{x}\\_{l}[\\mathbf{x}\\\
    mathbf{N}\\mathbf{x}\\_{l\\mid ln}^{\\mathrm{UL}}] - \\mathbf{x}\\_{l}^{\\mathrm{R}}\\\
    right\\|}{\\max\\limits\\_{\\mathbf{n}} \\left\\|\\mathbf{x}\\_{l}[\\mathbf{x}\\\
    mathbf{N}\\mathbf{x}\\_{l\\mid ln}^{\\mathrm{UL}}] - \\mathbf{x}\\_{l}^{\\mathrm{R}}\\\
    right\\|}\\right) \\left(\\frac{\\mathbf{b}\\_{\\max,l}(\\mathbf{x}\\_{l\\mid\
    \ ln}) - \\mathbf{b}\\_{l\\mid ln}}{\\mathbf{b}\\_{\\max,l}(\\mathbf{x}\\_{l\\\
    mid ln}) - \\mathbf{y}\\_{\\mathbf{D}\\_{l}}}\\right) \\tag{24}$$\n\nWhere ()\
    \ is the value of utility function for the *i*th user.\n\n# 1.4. OPTIMIZATION\
    \ PROBLEM\n\n The purpose of the proposed method is to discover the maximum required\
    \ QoS of users simultaneously in VR systems. The corresponding optimization problem\
    \ is formulated as equation (25). This problem is the same as the optimal closed\
    \ form expression for utility function.\n\n$$\\max\\_{\\{u\\_{l/W}, W^{\\text{lin}}\\\
    }} \\sum\\_{l \\in \\mathbb{B}} \\sum\\_{l \\in \\mathcal{U}\\_l} \\sum\\_{n=1}^{N}\
    \ U\\_l(\\mathcal{D}\\_{l/n}, K\\_{l/n}) \\tag{25}$$\n\n$$\\begin{aligned} \\\
    text{s.t.} \\begin{bmatrix} U\\_{l} \\end{bmatrix} \\le V\\_{l}, \\forall j \\\
    in B\\\\ \\mathbf{c}\\_{lj}^{\\text{DL}} \\ge \\mathbf{R}\\_{lj}^{\\text{min}},\
    \ \\forall j \\in B, \\forall i \\in U\\_{l} \\end{aligned} \\tag{\\text{a}}$$\n\
    \n$$c\\_{ij}^{\\mu\\nu} \\le R\\_{ij}^{\\text{min}}, \\forall j \\in B, \\forall\
    \ l \\in U\\_{\\bar{l}} \\tag{9}$$\n\n$$\\sum\\_{l} P\\_{\\mathcal{U}\\_{l}} \\\
    le P\\_{\\mathcal{B}\\_{\\bar{l}}} \\tag{5}$$\n\n$$\\sum\\_{\\{\\mathbf{u}\\in\\\
    mathcal{U}\\_{f}\\}} \\mathbf{u}^{\\mathbf{u}} \\mathbf{u}\\_{f} \\le \\mathbf{u}\\\
    _{f} \\tag{7}$$\n\n$$P\\_{A\\_l} \\in N\\_{AP}^l \\left[ P\\_{A\\_l} \\right]\\\
    _{k,l} \\left[ P\\_{A\\_l} \\right]\\_{k,l}^\\* = \\frac{1}{N\\_l} \\tag{4}$$\n\
    \ \n$$G\\_{A\\_l} \\in N\\_{ST,l}^l \\left[ G\\_{A\\_l} \\right]\\_{k,l} \\left[\
    \ G\\_{A\\_l} \\right]\\_{k,l}^\\* = \\frac{1}{N\\_r} \\tag{5}$$\n\n Where | |\
    \ is the number of users related to the *j*th AP, and is the total number of users\
    \ within the coverage of the *j*th AP. Constraint (a) is related to the number\
    \ of users connected to the *j*th AP. The second constraint denoted that the minimum\
    \ level of SINR in DL for the *i*th user in contact with the *j*th AP is at-least\
    \ equal to . As noted in constraint (c), the sum of the power of all users should\
    \ not exceed the power of the *j*th AP denoted as , where the power is considered\
    \ equal for all users. = ̂ (,) represents the precoder matrix, and = ̂ (,) represents\
    \ the combiner matrix. Both and have unit amplitude. [ ] , is the element in the\
    \ *k*th row and *l*th column of the matrix. is the set of codebooks in the *j*th\
    \ AP, and [ ] , is the element in the *k*th row and *l*th column of the matrix.\
    \ is the number of codebooks for the *i*th user.\n\n The maximum value of this\
    \ utility function is equal to the maximum sum rate of users. The maximization\
    \ of the sum rate is estimated in the previous section using the SVD algorithm,\
    \ as explained thoroughly in System Model section. Then by using the values of\
    \ the rate in DL and UL and queue delay, the utility function is calculated.\n\
    \n## 1.5. SIMULATION RESULTS\n\nIn this section, we represent the performance\
    \ results of the proposed hybrid beamforming algorithm through computer simulations\
    \ in MATLAB. The environment considered is illustrated in Fig. 2, focusing on\
    \ an indoor area with two user (U=2) and two AP (B=2). The dimensions of the indoor\
    \ area is [0:10],[0:17],[0:3] in the X, Y and Z direction, respectively.\n\n![](_page_6_Figure_2.jpeg)\n\
    \n**Fig. 2.** Indoor simulation area.\n\nThe values of parameters used in simulations\
    \ are shown in Table. I. QAM (Quadrature Amplitude Modulation) with 64 subcarriers\
    \ is considered in OFDM.\n\nTABLE I PARAMETERS OF SIMULATION\n\n| parameter |\
    \ value   | parameter | value |\n|-----------|---------|-----------|-------|\n\
    | \U0001D453\U0001D450        | 60e9    | w         | 3.2   |\n| \U0001D441\U0001D446\
    \U0001D436       | 64      | \U0001D441\U0001D45F        | 1     |\n| \U0001D441\
    \U0001D461        | [2,4,8] | \U0001D441\U0001D445\U0001D439       | [1,2] |\n\
    | \U0001D446\U0001D456        | 512×24  | \U0001D434\U0001D456        | 6    \
    \ |\n| \U0001D463         | 5       | B         | 2     |\n| U         | 2   \
    \    | \U0001D443\U0001D435\U0001D457       | 10e-3 |\n| \U0001D707\U0001D457\
    \        | 4e-9    | \U0001D706\U0001D456        | 2e-9  |\n\n In order to simulate\
    \ this network in ns3, we've extract the contents of the PHY layer. At first we\
    \ have considered six codebooks as follows, for comparing simulation results:\n\
    \n1) \"two-Antenna, one-RF chain\",\n\n2) \"two-Antenna, two-RF chain\",\n\n3)\
    \ \"four-Antenna, one-RF chain\",\n\n4) \"four-Antenna, two-RF chain\",\n\n5)\
    \ \"eight-Antenna, one-RF chain\",\n\n6) \"eight-Antenna, two-RF chain\".\n\n\
    \ Figure. 3 shows the comparison of simulation results in ns3 with simulations\
    \ in MATLAB in the UL direction in all codebooks for the first user by considering\
    \ the first AP respectively.\n\n Figure. 4 shows the comparison of simulation\
    \ results in ns3 with simulations in MATLAB in the UL direction in all codebooks\
    \ in all codebooks for the first users by considering the second AP respectively.\n\
    \n Similarly Figure. 5 shows the comparison of simulation results in ns3 with\
    \ simulation in MATLAB in the DL direction in all codebooks for the first users\
    \ by considering the first AP, respectively.\n\n Similarly Figures. 6 shows the\
    \ comparison of simulation results in ns3 with simulation in MATLAB in the DL\
    \ direction in all codebooks for the first user by considering the second AP,\
    \ respectively.\n\n Consequently, as shown in these figures, the trends of increasing\
    \ and decreasing UL and DL rate are almost the same as the ns3 result for all\
    \ considered codebooks for two users and two APs, indicating that the modelling\
    \ of the channel in DL and UL in our computer simulation is relatively accurate.\
    \ So we can evaluate the value of our proposed utility function in considered\
    \ scenarios in the case of number of antennas and RF chains.\n\n![](_page_7_Figure_0.jpeg)\n\
    \n**Fig. 3.** Comparison of UL rate in both ns3 and simulation of MATLAB for the\
    \ first AP and the first user.\n\n![](_page_7_Figure_2.jpeg)\n\n**Fig. 4.** Comparison\
    \ of UL rate in both ns3 and simulation of MATLAB for the second AP and the first\
    \ user.\n\n![](_page_7_Figure_4.jpeg)\n\n**Fig. 5.** Comparison of DL rate in\
    \ both ns3 and simulation of MATLAB for the first AP and the first user.\n\n![](_page_8_Figure_0.jpeg)\n\
    \n**Fig. 6** Comparison of DL rate in both ns3 and simulation of MATLAB for the\
    \ second AP and the first user.\n\nAlso we have considered lower bound of channel\
    \ gain in our MATLAB simulation by using the min channel gain in subcarriers.\
    \ The value of utility function is computed for both of mean channel gain and\
    \ min channel gain that is shown in Fig.7.\n\n![](_page_8_Figure_3.jpeg)\n\n**Fig.\
    \ 7.** Values of utility function in all scenarios.\n\n In Fig.7, the utility\
    \ function value for each number of transmit antennas is shown. In the min channel\
    \ gain scenario, the value of utility function exhibits an increasing trend as\
    \ the number of antenna is increased. Notably, the values of utility function\
    \ in the mean channel gain scenario closely align with the simulations in ns3\
    \ and the values in all codebooks is approximately equal to 0.6. This alignment\
    \ is attributed to the consideration of the mean channel gain values in all subcarriers\
    \ for each users in each Es/N0 value.\n\n As illustrated in Fig 7, ns3 yields\
    \ higher values of utility function than other scenarios, signifying its superior\
    \ accuracy in simulating the indoor environment. The average error between ns3\
    \ utility function values and the simulation of the first scenario across all\
    \ codebooks is atleast 4.6%.\n\n In Figs. 8-10, the transmission delay for each\
    \ Es/N0 (dB) value in both scenarios (mean channel gain and min channel gain),\
    \ for each codebook, between each user and AP (in cases of varying numbers of\
    \ and ) is shown. Almostly the transmission delay exhibits a decreasing pattern\
    \ with increasing Es/N0 values. This trend is due to the incursion on received\
    \ power in relation to the power of noise, resulting in an increased rate in DL\
    \ and a reduction in transmission delay.\n\n*The mean channel gain scenario in\
    \ MATLAB simulation:*\n\nThe transmission delay for some considered codebooks\
    \ in the mean channel gain scenario is illustrated in Figures.8 through 10.\n\n\
    ![](_page_9_Figure_0.jpeg)\n\n**Fig. 8.** Transmission delay in mean channel gain\
    \ scenario for = 2, = 2.\n\n![](_page_9_Figure_2.jpeg)\n\n**Fig. 9.** Transmission\
    \ delay in mean channel gain scenario for = 4, = 2.\n\n![](_page_9_Figure_4.jpeg)\n\
    \n**Fig. 10** Transmission delay in mean channel gain scenario for = 8, = 2.\n\
    \n*MATLAB simulations of min channel gain scenario:*\n\nThe transmission delay\
    \ for some considered codebooks in the min channel gain scenario is illustrated\
    \ in Figures.11 through 13.\n\n![](_page_10_Figure_0.jpeg)\n\n**Fig. 11.** Transmission\
    \ delay in min channel gain scenario for = 2, = 2.\n\n![](_page_10_Figure_2.jpeg)\n\
    \n**Fig. 12.** Transmission delay in min channel gain scenario for = 4, = 2.\n\
    \n![](_page_10_Figure_4.jpeg)\n\n**Fig. 13.** Transmission delay in min channel\
    \ gain scenario for = 8, = 2.\n\n After simulating the transmission delay in various\
    \ Es/N0 values across scenarios, the min and mode values of transmission delay\
    \ at different Es/N0 values are compared with each other and with the ns3 results.\
    \ Mode statistics pertains to the most frequently occurring value in a vector.\
    \ The minimum and mode values of transmission delay for all considered codebooks\
    \ within the range of Es/N0 values in all scenarios are shown in Figs. 14-17.\n\
    \n![](_page_11_Figure_0.jpeg)\n\n**Fig. 14.** Min transmission delay based on\
    \ all Es/N0 values in scenarios and ns3 for the first AP and the first user.\n\
    \n![](_page_11_Figure_2.jpeg)\n\n**Fig. 15.** Min transmission delay based on\
    \ all Es/N0 values in scenarios and ns3 for the second AP and the first user.\n\
    \n![](_page_11_Figure_4.jpeg)\n\n**Fig. 16.** Mode of transmission delay based\
    \ on all Es/N0 values in scenarios and ns3 for the first AP and the first user.\n\
    \n![](_page_12_Figure_0.jpeg)\n\n**Fig. 17.** Mode of transmission delay based\
    \ on all Es/N0 values in scenarios and ns3 for the second AP and the first user.\n\
    \n As shown in Fig. 14 and Fig.15, ns3 consistently exhibits the lowest transmission\
    \ delay across all considered codebooks. This outcome is attributed to the higher\
    \ DL and UL rates obtained in ns3, surpassing the simulated rates in the mean\
    \ channel gain scenario using MATLAB. The minimum transmission delay in the min\
    \ channel gain scenario has higher values. In the min channel gain scenario, the\
    \ consideration of the minimum value of the channel at subcarriers in DL for calculating\
    \ the DL rates contributes to higher transmission delay.\n\n Figs.16-17 have shown\
    \ the mode value of transmission delay that are selected from all transmission\
    \ delay in all values of Es/N0. The mode value of transmission delay in the min\
    \ channel gain scenario is higher than others, indicating the presence of outliers\
    \ in transmission delay for this scenario. AS shown in these figures, the mode\
    \ of transmission delay is almost the same as the minimum value shown in Figs.14-15.\
    \ Interestingly, the mode of transmission delay in the mean channel gain scenario\
    \ exhibits approximately the same values as the minimum transmission values based\
    \ on Figs.14-15 and closely approaches ns3 results in Figs.16-17.\n\n## 1.6. CONCLUSION\n\
    \n In this paper, we have focused on hybrid beamforming design within mmWave multi-carrier\
    \ systems in DL to enhance the received power of users while minimizing hardware\
    \ complexity. Additionally, we proposed an optimal closed form expression of a\
    \ multi-attribute utility function to assess the QoS of users, taking into account\
    \ the specific limitations of VR systems. This multi attribute utility function\
    \ includes transmission delay, processing delay and queue delay. We investigate\
    \ the transmission delay in relation to increasing Es/N0 and the utility function\
    \ for mean channel gain and min channel gain scenarios in six different codebooks\
    \ with various number of transmit antenna and RF chains in DL. Furthermore, we\
    \ have discussed a lower bound consideration on channel gain that can lead to\
    \ a reduction in DL rate and ultimately increasing transmission delay. Consequently,\
    \ the utility function values have decreased by considering the lower bound of\
    \ channel gain.\n\nThe results reveal a consistent trend in the utility function\
    \ value for each codebook and on average, the accuracy of utility function in\
    \ ns3 is about 4.6% in comparison by MATLAB simulations across all considered\
    \ codebooks.\n\n## REFERENCES\n\n- [1] F. Guo, F. R. Yu, H. Zhang, H. Ji, V. C.\
    \ Leung, and X. Li, \"An adaptive wireless virtual reality framework in future\
    \ wireless networks: A distributed learning approach,\" IEEE Transactions on Vehicular\
    \ Technology, vol. 69, no. 8, pp. 8514-8528, 2020, doi:10.1109/TVT.2020.2995877.\n\
    - [2] N. Guo, R. C. Qiu, S. S. Mo, and K. Takahashi, \"60-GHz millimeter-wave\
    \ radio: Principle, technology, and new results,\" EURASIP journal on Wireless\
    \ Communications and Networking, vol. 2007, pp. 1-8, 2006.\n- [3] X. Yang et al.,\
    \ \"Communication-constrained mobile edge computing systems for wireless virtual\
    \ reality: Scheduling and tradeoff,\" IEEE Access, vol. 6, pp. 16665-16677, 2018,\
    \ doi:10.1109/ACCESS.2018.2817288.\n- [4] C. Chaccour, M. N. Soorki, W. Saad,\
    \ M. Bennis, and P. Popovski, \"Can terahertz provide high-rate reliable low-latency\
    \ communications for wireless VR?,\" IEEE Internet of Things Journal, vol. 9,\
    \ no. 12, pp. 9712-9729, 2022, doi:10.1109/JIOT.2022.3142674.\n- [5] I. Ahmed\
    \ et al., \"A survey on hybrid beamforming techniques in 5G: Architecture and\
    \ system model perspectives,\" IEEE Communications Surveys & Tutorials, vol. 20,\
    \ no. 4, pp. 3060-3097, 2018, doi:10.1109/COMST.2018.2843719.\n- [6] C. Chaccour,\
    \ M. N. Soorki, W. Saad, M. Bennis, P. Popovski, and M. Debbah, \"Seven defining\
    \ features of terahertz (THz) wireless systems: A fellowship of communication\
    \ and sensing,\" IEEE Communications Surveys & Tutorials, vol. 24, no. 2, pp.\
    \ 967-993, 2022, doi:10.1109/COMST.2022.3143454.\n- [7] T. Lin, J. Cong, Y. Zhu,\
    \ J. Zhang, and K. B. Letaief, \"Hybrid beamforming for millimeter wave systems\
    \ using the MMSE criterion,\" IEEE Transactions on Communications, vol. 67, no.\
    \ 5, pp. 3693-3708, 2019, doi: 10.1109/TCOMM.2019.2893632.\n- [8] A. Maltsev,\
    \ A. Pudeyev, A. Lomayev, and I. Bolotin, \"Channel modeling in the next generation\
    \ mmWave Wi-Fi: IEEE 802.11 ay standard,\" in European wireless 2016; 22th European\
    \ wireless conference, 2016: VDE, pp. 1-8.\n- [9] D. S. Rao and V. B. Hency, \"\
    QoS based radio resource management techniques for next generation MU-MIMO WLANs:\
    \ A survey,\" Journal of Telecommunication, Electronic and Computer Engineering\
    \ (JTEC), vol. 8, no. 1, pp. 97-105, 2016.\n- [10] P. Zhou et al., \"IEEE 802.11\
    \ ay-based mmWave WLANs: Design challenges and solutions,\" IEEE Communications\
    \ Surveys & Tutorials, vol. 20, no. 3, pp. 1654-1681, 2018, doi:10.1109/COMST.2018.2816920.\n\
    - [11] P. C. Jain, N. N. Srinivas, and Y. Vellisetty, \"Beam Forming Impact on\
    \ the Next Generation Wi-Fi IEEE802. 11ay in mm Wave Frequency Band,\" in International\
    \ Conference on Optical and Wireless Technologies, 2021: Springer, pp. 115-123,\
    \ https://doi.org/10.1007/978-981-19-1645-8\\_12\n- [12] S. H. Hong, J. Park,\
    \ S.-J. Kim, and J. Choi, \"Hybrid beamforming for intelligent reflecting surface\
    \ aided millimeter wave MIMO systems,\" IEEE Transactions on Wireless Communications,\
    \ vol. 21, no. 9, pp. 7343-7357, 2022. doi:10.1109/TWC.2022.3157880.\n- [13] F.\
    \ Sohrabi and W. Yu, \"Hybrid digital and analog beamforming design for large-scale\
    \ antenna arrays,\" IEEE Journal of Selected Topics in Signal Processing, vol.\
    \ 10, no. 3, pp. 501-513, 2016, doi:10.1109/JSTSP.2016.2520912.\n- [14] M. Chen,\
    \ W. Saad, and C. Yin, \"Virtual reality over wireless networks: Quality-of-service\
    \ model and learning-based resource management,\" IEEE Transactions on Communications,\
    \ vol. 66, no. 11, pp. 5621-5635, 2018, doi:10.1109/TCOMM.2018.2850303.\n- [15]\
    \ M. Kim, T. Ropitault, S. Lee, and N. Golmie, \"Efficient MU-MIMO beamforming\
    \ protocol for IEEE 802.11 ay WLANs,\" IEEE Communications Letters, vol. 23, no.\
    \ 1, pp. 144-147, 2018, doi:10.1109/LCOMM.2018.2879476.\n- [16] H. Gao and F.\
    \ Li, \"The application of virtual reality technology in the teaching of clarinet\
    \ music art under the mobile wireless network learning environment,\" Entertainment\
    \ Computing, vol. 49, p. 100619, 2024. <https://doi.org/10.1016/j.entcom.2023.100619>\n\
    - [17] X. Liu and Y. Deng, \"Learning-based prediction, rendering and association\
    \ optimization for MEC-enabled wireless virtual reality (VR) networks,\" IEEE\
    \ Transactions on Wireless Communications, vol. 20, no. 10, pp. 6356-6370, 2021,\
    \ doi:10.1109/TWC.2021.3073623\n- [18] X. Ge, L. Pan, Q. Li, G. Mao, and S. Tu,\
    \ \"Multipath cooperative communications networks for augmented and virtual reality\
    \ transmission,\" IEEE Transactions on Multimedia, vol. 19, no. 10, pp. 2345-2358,\
    \ 2017, doi:10.1109/TMM.2017.2733461.\n- [19] M. F. Hossain, A. Jamalipour, and\
    \ K. Munasinghe, \"A Survey on Virtual Reality over Wireless Networks: Fundamentals,\
    \ QoE, Enabling Technologies, Research Trends and Open Issues,\" Authorea Preprints,\
    \ 2023, do[i:10.36227/techrxiv.24585387.v1.](https://doi.org/10.36227/techrxiv.24585387.v1)\n\
    - [20] C. Michaelides and B. Bellalta, \"Buffer Resets: A Packet-Discarding Policy\
    \ for Timely Physiological Data Collection in Virtual Reality Gaming Systems,\"\
    \ IEEE Sensors Letters, vol. 7, no. 12, pp. 1-4, 2023, doi:10.1109/LSENS.2023.3334152.\n\
    - [21] D. Zhang, Y. Wang, X. Li, and W. Xiang, \"Hybridly connected structure\
    \ for hybrid beamforming in mmWave massive MIMO systems,\" IEEE Transactions on\
    \ Communications, vol. 66, no. 2, pp. 662-674, 2017, doi:10.1109/TCOMM.2017.2756882.\n\
    - [22] X. Song, T. Kühne, and G. Caire, \"Fully-/partially-connected hybrid beamforming\
    \ architectures for mmWave MU-MIMO,\" IEEE Transactions on Wireless Communications,\
    \ vol. 19, no. 3, pp. 1754-1769, 2019, doi:10.1109/TWC.2019.2957227.\n- [23] Y.\
    \ Zhang, J. Du, Y. Chen, X. Li, K. M. Rabie, and R. Kharel, \"Near-optimal design\
    \ for hybrid beamforming in mmWave massive multi-user MIMO systems,\" IEEE Access,\
    \ vol. 8, pp. 129153-129168, 2020, doi:10.1109/ACCESS.2020.3009238.\n- [24] H.\
    \ Yu, W. Qu, Y. Fu, C. Jiang, and Y. Zhao, \"A novel two-stage beam selection\
    \ algorithm in mmWave hybrid beamforming system,\" IEEE Communications Letters,\
    \ vol. 23, no. 6, pp. 1089-1092, 2019, doi:10.1109/LCOMM.2019.2913385.\n- [25]\
    \ M. Nekovee, Y. Qi, and Y. Wang, \"Distributed beam scheduling for multi-RAT\
    \ coexistence in mm-wave 5G networks,\" in 2016 IEEE 27th Annual International\
    \ Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), 2016:\
    \ IEEE, pp. 1-6, doi:10.1109/ICVES.2016.7548169.\n- [26] M. N. Anjum and H. Fang,\
    \ \"Coexistence in millimeter-wave WBAN: A game theoretic approach,\" in 2017\
    \ International Conference on Computing, Networking and Communications (ICNC),\
    \ 2017: IEEE, pp. 571-576, doi:10.1109/ICCNC.2017.7876192.\n- [27] M. A. Senouci,\
    \ S. Hoceini, and A. Mellouk, \"Utility function-based TOPSIS for network interface\
    \ selection in heterogeneous wireless networks,\" in 2016 IEEE international conference\
    \ on communications (ICC), 2016: IEEE, pp. 1-6, doi:10.1109/ICC.2016.7511563.\n\
    - [28] H. Dong et al., \"Quantitatively analysis of train communication network\
    \ based on multi-attribute utility function,\" in 17th International IEEE Conference\
    \ on Intelligent Transportation Systems (ITSC), 2014: IEEE, pp. 2330-2335, doi:10.1109/ITSC.2014.6958063."
- title: "Distributionally Robust Optimization for Digital Twin Service\n  Provisioning\
    \ over Edge Computing"
  abstract: 'Digital Twin (DT) is a transformative technology poised to revolutionize
    a

    wide range of applications. This advancement has led to the emergence of

    digital twin as a service (DTaaS), enabling users to interact with DT models

    that accurately reflect the real-time status of their physical counterparts.

    Quality of DTaaS primarily depends on the freshness of DT data, which can be

    quantified by the age of information (AoI). The reliance on remote cloud

    servers solely for DTaaS provisioning presents significant challenges for

    latency-sensitive applications with strict AoI demands. Edge computing, as a

    promising paradigm, is expected to enable the AoI-aware provision of real-time

    DTaaS for users. In this paper, we study the joint optimization of DT model

    deployment and DT model selection for DTaaS provisioning over edge computing,

    with the objective of maximizing the quality of DTaaS. To address the

    uncertainties of DT interactions imposed on DTaaS provisioning, we propose a

    novel distributionally robust optimization (DRO)-based approach, called

    Wasserstein DRO (WDRO), where we first reformulate the original problem to a

    robust optimization problem, with the objective of maximizing the quality of

    DTaaS under the unforeseen extreme request conditions. Then, we leverage

    multi-level dual transformations based on Wasserstein distance to derive a

    robust solution. Simulations are conducted to evaluate the performance of the

    proposed WDRO, and the results demonstrate its superiority over counterparts.'
  url: http://arxiv.org/abs/2505.15829v1
  keywords: ''
  document: "# I. INTRODUCTION\n\nDigital Twin (DT) is an advanced technology that\
    \ aims to create highly accurate, dynamic DT models of physical entities (PT),\
    \ serving as precise digital replicas that mirror the realtime status of their\
    \ counterparts. It is widely applied across fields such as intelligent manufacturing,\
    \ autonomous driving, and personalized healthcare [1]. This advancement has led\
    \ to the emergence of digital twin as a service (DTaaS), enabling users to interact\
    \ with DT models. High-quality of DTaaS depends on the freshness of DT service\
    \ response data, ensuring users to receive accurate and timely interaction experiences.\
    \ The freshness of DT data can be quantified by the age of information (AoI).\
    \ However, traditional remote cloud-based DTaaS provisioning, with DT models deployed\
    \ on solely cloud servers, often fails to meet the demands of latencysensitive\
    \ applications, resulting in high AoI and a degradation in the quality of DTaaS\
    \ [2]. Edge computing, as a promising paradigm, which deploys DT models on edge\
    \ servers (ESs), may be an alternative to provide AoI-aware real-time DTaaS for\
    \ users.\n\nAlthough some studies have focused on optimizing DT services over\
    \ edge networks [3] [4], there are some critical issues which have not yet been\
    \ well investigated. On one hand, the issue of updating DT models to preserve\
    \ their accuracy has been overlooked, which leads to outdated data of service\
    \ responses, and severely compromising quality of DTaaS. On the other hand, all\
    \ of these studies often rely on the assumption that all DT interaction information\
    \ is fully known in advance, which is impractical for DT with dynamic evolutions\
    \ and uncertain access requests. However, addressing these issues is very challenging\
    \ because of the following reasons. i) The quality of DTaaS is influenced by both\
    \ the fidelity of DT models and service latencies, but designing a metric that\
    \ integrates these two elements is not straightforward due to their significantly\
    \ heterogeneity. ii) Moreover, the uncertainty of the initiation of future DT\
    \ interaction requests exacerbates the complexity of the seamlessly provisioning\
    \ of DT services in the system optimization.\n\nIn this paper, we study a joint\
    \ optimization of DT deployment and DT model selection for DTaaS provisioning\
    \ over edge computing. Considering the enhancement of DTaaS provisioning on edge\
    \ servers rather than remote cloud servers, we design a utility gain based on\
    \ the difference of AoI as the metric to evaluate quality of DTaaS. We aim to\
    \ maximize the total utility gain of all DT interaction requests by jointly determining:\
    \ i) which DT models should be deployed on which ESs and ii) which DT models should\
    \ be selected to serve which DT interaction requests, subjected to the storage\
    \ resource constraints of ESs. Solving this problem is very challenging mainly\
    \ due to the uncertainties of DT interaction requests and the resulted impacts\
    \ to the DTaaS provisioning. To address this, we propose a novel distributionally\
    \ robust optimization (DRO)-based approach, called Wasserstein DRO (WDRO), where\
    \ we employ the DRO method to reformulate the original problem, and then apply\
    \ multi-level dual transformations based on Wasserstein distance to obtain a robust\
    \ optimization solution.\n\nThe main contributions of this paper are summarized\
    \ in the following.\n\n- A joint optimization of DT model deployment and DT model\
    \ selection for DTaaS provisioning over edge computing is first formulated, with\
    \ the objective of maximizing the quality of DTaaS, measured by the total utility\
    \ gain of all DT interaction requests.\n- A novel DRO-based approach, namely WDRO,\
    \ is proposed to obtain a robust solution. In WDRO, the original optimization\
    \ problem is first reformulated employing the DRO method to account for uncertainty.\
    \ Then, based on\n\n![](_page_1_Figure_0.jpeg)\n\nFig. 1: An illustration of the\
    \ considered DTaaS provisioning over edge computing.\n\nWasserstein distance,\
    \ multi-level dual transformations are applied to convert the problem into a form\
    \ solvable by the Gurobi optimizer, resulting in a robust optimization solution\
    \ that provides high quality of DTaaS under unforeseen extreme request conditions.\n\
    \n• Simulations are conducted to show the superiority of the proposed WDRO over\
    \ counterparts.\n\n# II. SYSTEM MODEL AND PROBLEM FORMULATION\n\n# *A. System\
    \ model*\n\nConsider DTaaS provisioning over edge computing, as illustrated in\
    \ Fig. 1, consisting of a set of geographically distributed edge servers (ESs)\
    \ V with |V| = V , each v ∈ V being associated with a base station, and a cloud\
    \ server c. Each of them maintains interactive DT models for providing DT services\
    \ for users. The system operates over continuous time, and we denote any time\
    \ instant by t. There is a set of PTs M with |M| = M, where each PT m ∈ M has\
    \ a set of corresponding DT models, denoted as H<sup>m</sup> = {DT <sup>g</sup>\
    \ <sup>m</sup> | g ∈ Vm} ∪ {DT <sup>c</sup> <sup>m</sup>}, where V<sup>m</sup>\
    \ is the subset of ES set V that maintain the DT models of PT m ∈ M, DT <sup>g</sup>\
    \ <sup>m</sup> represents the DT model of PT m ∈ M deployed on ES g ∈ Vm, and\
    \ DT <sup>c</sup> <sup>m</sup> represents the DT model of PT m ∈ M deployed on\
    \ the cloud server c. For convenience, let xm,v = 1 indicate that DT model of\
    \ PT m ∈ M is deployed on ES v ∈ V, xm,v = 0 otherwise, and thereby subset Vm,\
    \ m ∈ M can be further expressed as V<sup>m</sup> = {v | xm,v = 1, v ∈ V}.\n\n\
    To provide a high quality of DT services, the DT models within Hm, m ∈ M need\
    \ to be periodically updated for keep synchronized with the corresponding PT m\
    \ ∈ M by using data from PT m ∈ M. Similar to [5], we adopt AoI to measure the\
    \ performance of each DT model within Hm, m ∈ M in synchronization with the corresponding\
    \ PT m ∈ M. We use ADT gm (t) and ADT cm (t), m ∈ M, g ∈ V<sup>m</sup> to respectively\
    \ represent the AoI of DT models maintained on ESs and the cloud server at time\
    \ t. AoI is defined as the time elapsed since the most recent data for updating\
    \ the DT model was transmitted from the PT. Following this, at time t, the AoI\
    \ of DT model DT <sup>g</sup> <sup>m</sup>(t), m ∈ M maintained on ES g ∈ V<sup>m</sup>\
    \ can be expressed as ADT gm (t) = L update m,g + (t − n · τm),\n\n![](_page_1_Figure_8.jpeg)\n\
    \nFig. 2: An illustration of the AoI evolution of DT model DT <sup>g</sup> m.\n\
    \nwhere n ∈ {0, 1, ..., N} represents the index of update period, determined as\
    \ n = j t τ<sup>m</sup> k , and τ<sup>m</sup> is the time interval between two\
    \ consecutive updates. L update m,g denotes the data transmission latency from\
    \ PT m ∈ M to DT model DT <sup>g</sup> m.\n\nSpecifically, on the one hand, when\
    \ DT <sup>g</sup> <sup>m</sup>, m ∈ M, g ∈ V<sup>m</sup> and its corresponding\
    \ PT m ∈ M are in the same cell, the data for updating DT model DT <sup>g</sup>\
    \ <sup>m</sup> is transmitted directly from PT m ∈ M to ES g ∈ V<sup>m</sup> (i.e.,\
    \ intra-cell update) [6]. For intra-cell update, the transmission latency is expressed\
    \ as L update m,g = a<sup>m</sup> · I upload, where a<sup>m</sup> denotes the\
    \ size of data for updating from PT m ∈ M, and I upload represents the latency\
    \ of transmitting unit data to the local ES of PT m ∈ M. On the other hand, when\
    \ they are not in the same cell, the data for updating DT model DT <sup>g</sup>\
    \ <sup>m</sup> is transmitted from PT m ∈ M to DT model DT <sup>g</sup> <sup>m</sup>,\
    \ m ∈ M, g ∈ V<sup>m</sup> through its local ES and the core network (i.e., inter-cell\
    \ update). For inter-cell update, the transmission latency is given by L update\
    \ m,g = a<sup>m</sup> ·(I upload + I trans locm,g), where loc<sup>m</sup> ∈ V\
    \ indicates the coverage area of the ES in which PT m ∈ M is located, and I trans\
    \ locm,g represents the transmission latency of unit data between ES loc<sup>m</sup>\
    \ ∈ V and ES g ∈ V<sup>m</sup> through the core network [7]. We illustrate the\
    \ AoI evolution of DT <sup>g</sup> <sup>m</sup>, m ∈ M, g ∈ Vm, in Fig. 2. Similarly,\
    \ at time t, the AoI of DT model DT <sup>c</sup> <sup>m</sup>, m ∈ M maintained\
    \ on the cloud server is calculated as ADT cm (t) = L update m,c + (t − n · τm),\
    \ n ∈ {0, 1, ..., N}, and L update m,c = a<sup>m</sup> · I trans locm,c, where\
    \ I trans locm,c denotes the latency of transmitting unit data from ES loc<sup>m</sup>\
    \ ∈ V to cloud server c.\n\nUsers can initiate interaction requests to DT models\
    \ at any time through their local ESs, and then receive the DT service responses\
    \ from intra-cell, inter-cell ESs, or the cloud server [8]. We denote the set\
    \ R as all DT interaction requests initiated by all users, and we represent each\
    \ DT interaction request r ∈ R as a tuple r =< tr, locr, mr, s<sup>m</sup> <sup>r</sup>\
    \ >, where t<sup>r</sup> is initiation time, and loc<sup>r</sup> ∈ V is initiation\
    \ location. Requests initiated from a specific location may exhibit preferences\
    \ for certain types m<sup>r</sup> ∈ M. Particularly, m<sup>r</sup> ∈ M denotes\
    \ the type of DT model that the DT interaction request r ∈ R intends to interact\
    \ with, where the DT service is provided by one of the DT models within Hm, and\
    \ s m r is the data size of DT service response obtained from the DT model within\
    \ Hm. We denote the latency of DT service response for DT interaction request\
    \ r ∈ R served by DT model DT <sup>g</sup> m<sup>r</sup> , g ∈ V<sup>m</sup><sup>r</sup>\
    \ as L response g,r = s<sup>r</sup> · Ig,loc<sup>r</sup> , where Ig,loc<sup>r</sup>\n\
    \nrepresents the transmission latency of unit data between ES g ∈ V<sup>m</sup><sup>r</sup>\
    \ and loc<sup>r</sup> ∈ V. We use yr,g = 1 to signify that the DT model DT <sup>g</sup>\
    \ m<sup>r</sup> , g ∈ V<sup>m</sup><sup>r</sup> is selected to serve the DT interaction\
    \ request r ∈ R, and yr,g = 0, otherwise. Based on these, we define the AoI of\
    \ DT service response to DT interaction request r ∈ R served by DT <sup>g</sup>\
    \ m<sup>r</sup> , g ∈ V<sup>m</sup><sup>r</sup> as A<sup>g</sup> <sup>r</sup>\
    \ = P g∈Vmr yr,g · (ADT gmr (tr) + L response g,r ). Particularly, if there is\
    \ no DT model on ESs can provide the DT service for DT interaction request r ∈\
    \ R, it can only obtain the DT service response from the DT model deployed on\
    \ the cloud server c with AoI A<sup>c</sup> <sup>r</sup> = ADT cmr (tr) + L response\
    \ c,r . It should be noted that, DT interaction request r ∈ R is uncertain, including\
    \ its initiation time tr, location locr, target type DT model m<sup>r</sup> and\
    \ the size of DT service response s m r .\n\nDue to the long transmission latency\
    \ between users and the cloud server, DT interaction requests served by DT models\
    \ maintained on the cloud server results in high AoIs for the DT interaction requests\
    \ [5]. In contrast, DT models maintained on ESs can provide higher-quality DT\
    \ services with lower AoIs. We use the utility gain as a metric to evaluate the\
    \ quality of DT service achieved through edge computing. Following this, the utility\
    \ gain of DT interaction request r ∈ R served by DT <sup>g</sup> m<sup>r</sup>\
    \ , g ∈ Vm<sup>r</sup> is u<sup>r</sup> = P g∈Vmr yr,g · (A<sup>c</sup> <sup>r</sup>\
    \ − A<sup>g</sup> r ).\n\n# *B. Problem Formulation*\n\nWith the objective of\
    \ maximizing the total utility gain of all DT interaction requests within R while\
    \ satisfying the storage resource constraints of each ES v ∈ V, the joint optimization\
    \ of DT model deployment X = {xm,v}<sup>∀</sup>m∈M,v∈V and DT model selection\
    \ Y = {yr,g}<sup>∀</sup>r∈R,g∈Vmr can be formulated as\n\n$$\\mathcal{P}\\_1:\
    \ \\max\\_{\\mathbf{X}, \\mathbf{Y}} \\sum\\_{r \\in \\mathcal{R}} \\sum\\_{g\
    \ \\in \\mathcal{V}\\_{mr}} y\\_{r,g} \\cdot (\\mathcal{A}\\_r^c - \\mathcal{A}\\\
    _r^g) \\quad (1)$$\n\n$$s.t.\\ \\sum\\_{m\\in\\mathcal{M}} c\\_m \\cdot x\\_{m,v}\
    \ \\le \\Phi\\_v,\\ \\forall v\\in\\mathcal{V},\\tag{1a}$$\n\n$$\\sum\\_{g \\\
    in \\mathcal{V}\\_{mr}} y\\_{r,g} = 1, \\forall r \\in \\mathcal{R}, \\tag{1b}$$\n\
    \n$$\\mathcal{V}\\_m = \\left\\{ v \\mid x\\_{m,v} = 1, v \\in \\mathcal{V} \\\
    right\\}, \\,\\forall m \\in \\mathcal{M}, \\qquad (\\text{lc})$$\n\n$$y\\_{r,g}\
    \ \\in \\{0, 1\\}, \\forall r \\in \\mathcal{R}, g \\in \\mathcal{V}\\_{m\\_r},\
    \ \\tag{1d}$$\n\n$$x\\_{m,v} \\in \\{0, 1\\}, \\forall m \\in \\mathcal{M}, v\
    \ \\in \\mathcal{V}, \\tag{1e}$$\n\nwhere constraint (1a) is the storage constraint,\
    \ and c<sup>m</sup> denotes the storage resource required to maintain a DT model\
    \ of PT m ∈ M, and Φ<sup>v</sup> denotes the maximum storage capacity of the ES\
    \ v ∈ V; constraint (1b) indicates that each DT interaction request r ∈ R can\
    \ only be served by a single DT model DT <sup>g</sup> m<sup>r</sup> , g ∈ V<sup>m</sup><sup>r</sup>\
    \ . It is obvious that solving P<sup>1</sup> requires prior knowledge of DT interaction\
    \ requests, which is unavailable due to the uncertainty of all future DT interaction\
    \ requests within R. Intuitively, historical interaction information can provide\
    \ valuable insights into future uncertain interactions to some extent. However,\
    \ most existing approaches heavily rely on these historical information to guide\
    \ solution generation, making them susceptible to overfitting and unable to effectively\
    \ handle unforeseen extreme request conditions, resulting in the degradation of\
    \ total utility gain of all DT interaction requests. To address this, we propose\
    \ a DRO-based approach that aims to derive a robust solution to maintain a high\
    \ total utility gain of all DT interaction requests, even under the unforeseen\
    \ extreme request conditions, namely the distribution of future uncertain DT interaction\
    \ requests significantly deviates from historical ones.\n\n# III. DRO-BASED OPTIMIZATION\
    \ ALGORITHM\n\nIn this section, we proposed a DRO-based approach, called Wasserstein\
    \ DRO (WDRO), to obtain a robust solution. Specifically, we first reformulate\
    \ P<sup>1</sup> employing the DRO method with a confidence set, and then apply\
    \ multi-level dual transformations based on Wasserstein distance to make it solvable\
    \ by the Gurobi optimizer.\n\n# *A. DRO-based Problem Reformulation*\n\nAccording\
    \ to DRO [9], we first design a sample space based on all possible DT interaction\
    \ requests. Based on tolerance values and the reference distribution derived from\
    \ historical DT interaction requests, we construct a confidence set containing\
    \ the true distribution with a certain confidence level. Based on these, we reformulate\
    \ P<sup>1</sup> into a min-max problem, aiming to optimize the utility gain under\
    \ the unforeseen extreme request conditions that significantly deviates from historical\
    \ ones.\n\n*Design the sample space* Ω: For each uncertain DT interaction request\
    \ r =< tr, locr, mr, s<sup>m</sup> <sup>r</sup> >, r ∈ R, the uncertain initiation\
    \ time t<sup>r</sup> is cancelled within the calculation of utility gain, i.e.,\
    \ A<sup>c</sup> r−A<sup>g</sup> <sup>r</sup> = ADT cmr (tr)+L response c,r −(ADT\
    \ gmr (tr)+ L response g,r ) = L update <sup>m</sup>r,c − Lupdate <sup>m</sup>r,g\
    \ + L response c,r − Lresponse g,r , and the uncertain data size of DT service\
    \ response s m r is predetermined by target type mr. Therefore, the uncertain\
    \ elements of each DT interaction request r ∈ R are actually the request initiation\
    \ location loc<sup>r</sup> ∈ V and the target type of DT model m<sup>r</sup> ∈\
    \ M. The sample space Ω, denoted as Ω = {e1, e2, ..., ek, ..., eK} with K = |V|∗|M|\
    \ sample points, contains all possible combinations of initiation locations and\
    \ target DT model types. Each sample point e<sup>k</sup> ∈ Ω represents a basic\
    \ event, namely an DT interaction request that is initiated at a certain location\
    \ and targets a specific DT model type.\n\n*Construct the confidence set* D: We\
    \ denote the set of historical DT interaction request as R′ . For the sample space\
    \ Ω with K sample point, we design reference distribution P<sup>0</sup> = {p 0\
    \ 1 , p<sup>0</sup> 2 , ..., p<sup>0</sup> <sup>K</sup>} based on R′ . Additionally,\
    \ we set the form of ambiguity distribution of future uncertain DT interaction\
    \ requests as P = {p1, p2, ..., pK}, which represents the unknown true distribution\
    \ of future uncertain DT interaction requests. We use distance metric d(P, P0)\
    \ to represent the distance between P<sup>0</sup> and the ambiguity distribution\
    \ P. Then, we construct a confidence set D for the ambiguity distribution P, which\
    \ can be expressed as\n\n$$\\mathcal{D} = \\{ \\mathbb{P} : d(\\mathbb{P}, \\\
    mathbb{P}\\_0) \\le \\theta \\}, \\tag{2}$$\n\nwhere θ is the tolerance parameter\
    \ indicating the maximum distance between the ambiguity distribution P and the\
    \ reference distribution P0, and determines the size of the confidence set D.\
    \ Affected by the volume of historical DT interaction request |R′ | and confidence\
    \ level β ∈ [0, 1], the tolerance parameter θ is calculated as θ = K q <sup>2</sup>\
    \ |R′ | ln <sup>1</sup> 1−β [10].\n\nAdditionally, we construct the reference\
    \ distribution P<sup>0</sup> using a widely recognized step function, which increases\
    \ by 1/|R′ | at each historical DT interaction request. Specifically, we define\
    \ p 0 <sup>k</sup> = 1 |R′ | P|R′ | <sup>f</sup>=1 δ<sup>j</sup> (k), k ∈ {1,\
    \ ..., K}, where δ<sup>f</sup> (k) = 1 if the f data sample matches the basic\
    \ event e<sup>k</sup> ∈ Ω, and δ<sup>f</sup> (k) = 0 otherwise.\n\n*Reformulate\
    \ the problem*: According to DRO, instead of focusing on the unique true distribution,\
    \ we build the confidence set P of the ambiguous distribution such that the true\
    \ distribution exists with a certain confidence level in this set. Considering\
    \ risk aversion, we focus on the worst case distribution within the confidence\
    \ set D, ensuring robustness of DTaaS. To facilitate solution derivation, we convert\
    \ the maximization objective into the minimization of its negative. Then we minimize\
    \ the negative of the objective under the worst case in P under storage resource\
    \ constraints. Following this, we reformulate P<sup>1</sup> as a DRO problem\n\
    \n$$\\mathcal{P}\\_2: \\min\\_{\\mathbf{X}, \\mathbf{Y}} \\max\\_{\\mathbb{P}\
    \ \\in \\mathcal{D}} \\sum\\_{r \\in \\mathcal{R}} \\sum\\_{g \\in \\mathcal{V}\\\
    _{mr}} y\\_{r,g} \\cdot (\\mathcal{A}\\_r^g - \\mathcal{A}\\_r^c) \\quad (3)$$\n\
    \n$$s.t.\\ (1a) - (1e),\\tag{3a}$$\n\n$$\\mathcal{D} = \\{ \\mathbb{P} : d(\\\
    mathbb{P}, \\mathbb{P}\\_0) \\le \\theta \\}. \\tag{3b}$$\n\n*B. Multi-Level Dual\
    \ Transformation based on Wasserstein Distance*\n\nThe min-max format of P<sup>2</sup>\
    \ motivates us to transform the internal maximization problem into a convex problem,\
    \ so that it can be combined with the external minimization problem through dual\
    \ transformations. The process involves aggregating DT interaction requests and\
    \ multi-level dual transformations based on Wasserstein distance [11], which facilitates\
    \ the derivation of a robust solution.\n\n*DT Interaction Request Aggregation:*\
    \ We divide the DT interaction requests r ∈ R into K subsets R1, R2, ..., R<sup>K</sup>\
    \ according to initiation location loc<sup>r</sup> and type of target DT model\
    \ mr, assuming that DT interaction requests within the same subset are served\
    \ by the same DT model. Based on the definition of ur, DT interaction request\
    \ r ∈ R in the subset R<sup>k</sup> have the same ur. Then, we can get\n\n$$\\\
    begin{split} &\\sum\\_{r\\in\\mathcal{R}}\\sum\\_{g\\in\\mathcal{V}\\_{m\\_r}}y\\\
    _{r,g}\\cdot(\\mathcal{A}\\_r^g-\\mathcal{A}\\_r^c) \\\\ &=\\sum\\_{k=1}^K\\sum\\\
    _{r\\in\\mathcal{R}\\_k}\\sum\\_{g\\in\\mathcal{V}\\_{m\\_r}}y\\_{r,g}\\cdot(\\\
    mathcal{A}\\_r^g-\\mathcal{A}\\_r^c) \\\\ &=\\sum\\_{k=1}^K|\\mathcal{R}\\_k|\\\
    cdot\\sum\\_{g\\in\\mathcal{V}\\_{m\\_r}}y\\_{r\\_k,g}\\cdot(\\mathcal{A}\\_{r\\\
    _k}^g-\\mathcal{A}\\_{r\\_k}^c) \\\\ &=\\sum\\_{k=1}^Kp\\_k\\cdot\\sum\\_{g\\\
    in\\mathcal{V}\\_{m\\_r}}|\\mathcal{R}|\\cdot y\\_{r\\_k,g}\\cdot(\\mathcal{A}\\\
    _{r\\_k}^g-\\mathcal{A}\\_{r\\_k}^c) \\\\ &=\\sum\\_{k=1}^Kp\\_k\\cdot\\Psi[X,Y,r\\\
    _k], \\end{split} \\tag{4}$$\n\nwhere r<sup>k</sup> represents any DT interaction\
    \ request within Rk, k ∈ {1, ..., K}, and p<sup>k</sup> is the ratio of the size\
    \ of subset R<sup>k</sup> to the size of set R.\n\n*Problem-Level Dual Transformation\
    \ Based on Wasserstein distance:* We assume that X and Y are known in the external\
    \ minimization problem, and focus on the internal maximization problem, which\
    \ can be reformulated as\n\n$$\\max\\_{\\mathbb{P}\\in\\mathcal{D}}\\sum\\_{k=1}^{K}p\\\
    _k\\cdot\\Psi[\\mathbf{X},\\mathbf{Y},r\\_k] \\tag{5}$$\n\n$$s.t.\\ \\sum\\_{k=1}^{K}\
    \ p\\_k = 1,\\tag{5a}$$\n\n$$p\\_k \\ge 0, \\forall k \\in \\{1, \\ldots, K\\\
    }, \\tag{5b}$$\n\n$$\\mathfrak{m} \\quad \\mathfrak{m} \\quad \\mathfrak{m} \\\
    text{ and } \\mathfrak{m} > 0. \\tag{5c}$$\n\n$$\\mathcal{D} = \\{ \\mathbb{P}\
    \ : d(\\mathbb{P}, \\mathbb{P}\\_0) \\le \\theta \\}. \\tag{5c}$$\n\nThen based\
    \ on the definition of the Wasserstein distance, constraint (5c) can be transformed\
    \ into\n\n$$\\min\\_{\\pi \\ge 0} \\sum\\_{k,j=1 \\atop \\nu}^{K} ||\\xi\\_k -\
    \ \\xi\\_j||\\pi\\_{k,j} \\le \\theta \\tag{6}$$\n\n$$s.t.\\sum\\_{k=1 \\atop\
    \ K}^{K} \\pi\\_{k,j} = p\\_j^0, \\forall j \\in \\{1, \\ldots, K\\},\\tag{6a}$$\n\
    \n$$\\sum\\_{j=1}^{K} \\pi\\_{k,j} = p\\_k, \\forall k \\in \\{1, ..., K\\}, \\\
    qquad (6b)$$\n\nwhere ξ<sup>j</sup> = (T<sup>j</sup> , S<sup>j</sup> ) <sup>T</sup>\
    \ denotes the average response time and data size of historical service response\
    \ within subset R′ j , j ∈ {1, ..., K}. In contrast, ξ<sup>k</sup> = (Tk, Sk)\
    \ T represents the corresponding metrics for real DT interaction requests within\
    \ Rk, k ∈ {1, ..., K}. ||ξ<sup>k</sup> − ξ<sup>j</sup> || represents the distance\
    \ between two sample points e<sup>k</sup> and e<sup>j</sup> . πk,j represents\
    \ the amount of mass transported from ξ<sup>j</sup> to ξk. Then we replace p<sup>k</sup>\
    \ with equality constraints (6b) to completely eliminate pk. The internal maximization\
    \ problem is further transformed into\n\n$$\\max\\_{\\substack{\\pi \\ge 0 \\\\\
    \ \\text{even}}} \\sum\\_{k,j=1}^{K} \\pi\\_{k,j} \\cdot \\Psi[\\mathbf{X}, \\\
    mathbf{Y}, r\\_k] \\tag{7}$$\n\n$$\\begin{aligned} \\text{s.t. (6a)},\\\\ \\sum\\\
    _{k,j=1}^{K} ||\\xi\\_k - \\xi\\_j||\\pi\\_{k,j} \\le \\theta. \\end{aligned}\
    \ \\tag{7a}$$\n\nObviously, the objective is a linear combination in terms of\
    \ π, and the constraints are linear inequalities or equations with respect to\
    \ the variables. This is a convex linear programming problem, which can be dual\
    \ transformed as\n\n$$\\min\\_{\\lambda \\ge 0, h\\_j} \\lambda \\theta + \\sum\\\
    _{j=1}^{K} p\\_j^0 \\cdot h\\_j \\tag{8}$$\n \n$$\\text{s.t. } h\\_j + \\lambda\
    \ ||\\xi\\_k - \\xi\\_j|| \\ge \\Psi[\\mathbf{X}, \\mathbf{Y}, r\\_k], \\forall\
    \ j, k \\in \\{1, ..., K\\}, \\tag{8a}$$\n\nwhere λ and h<sup>j</sup> is the dual\
    \ variable introduced.\n\n*Norm-Level Dual Transformation for Constraint Optimization:*\
    \ Considering the constraints (8a) after dual transformation are typical discrete\
    \ constraints. The complexity of the solution increases as the number of j and\
    \ k combinations increases, which lead to computational difficulties. To this\
    \ end, we introduce the set Ξ and the maximization operation, which allow us to\
    \ partially de-discretize the problem and simplify the solution process. Specifically,\
    \ ξ<sup>k</sup> is an unknown numerical characteristics vector of real DT interaction\
    \ requests, and its value is continuous and uncertain. In order to describe this\
    \ uncertainty, we refer to the continuous set Ξ = {ξ|c · ξ ≤ d} to describe possible\
    \ ξk, which makes the problem shift towards dealing with continuous uncertainty.\
    \ We can transform the constraints (8a) as\n\n$$h\\_j \\ge \\max\\_{\\xi \\in\
    \ \\Xi} \\Psi[\\mathbf{X}, \\mathbf{Y}, r\\_k] - \\lambda ||\\xi - \\xi\\_j||,\
    \ \\forall j, k \\in \\{1, \\ldots, K\\}. \\tag{9}$$\n\nThen, express the norm\
    \ in the formula by the dual of the dual norm, and we further transform the constraint(8a)\
    \ as\n\n$$h\\_j \\ge \\max\\_{\\xi \\in \\Xi} \\Psi[\\mathbf{X}, \\mathbf{Y},\
    \ r\\_k] - \\max\\_{\\|n\\_j\\|\\_\\ast \\le \\hat{\\lambda}} \\|\\xi - \\xi\\\
    _j\\|^T n\\_j, \\forall j, k \\in \\{1, \\ldots, K\\}. \\tag{10}$$\n\nTheorem\
    \ 1. *norm* ∥x∥ *is the dual of its dual norm.*\n\n*Proof:* This proof is omitted\
    \ due to the page limit.\n\nTheorem 2. λ∥ξ −ξj∥ *can be expressed as* max∥n<sup>j</sup>\
    \ <sup>∥</sup>∗≤<sup>λ</sup> ∥ξ − ξj∥ <sup>T</sup> n<sup>j</sup> *.*\n\n*Proof:*\
    \ According to Theorem 1, for any vector x, we have\n\n$$\\|x\\| = \\max\\_{\\\
    |n\\|\\_\\ast \\le 1} |x|^T n.$$\n\nTherefore, for the vector ξ − ξ<sup>j</sup>\
    \ , we have\n\n$$\\|\\|\\xi - \\xi\\_j\\|\\| = \\max\\_{\\|n\\_j\\|\\_\\ast \\\
    le 1} \\|\\|\\xi - \\xi\\_j\\|\\|^T n\\_j \\dots$$\n\nSuch equivalence relation\
    \ can be illustrated as\n\n$$\\begin{aligned} \\max\\_{\\|\\|n\\_j\\|\\|\\ast\
    \ \\leq \\lambda} \\|\\xi - \\xi\\_j\\|\\|^T n\\_j &= \\max\\_{\\|\\|n\\_j\\|\\\
    |\\ast \\leq 1} \\|\\xi - \\xi\\_j\\|\\|^T (\\lambda n\\_j) \\\\ &= \\lambda \\\
    max\\_{\\|\\|n\\_j\\|\\|\\ast \\leq 1} \\|\\xi - \\xi\\_j\\|\\|^T n\\_j. \\end{aligned}$$\n\
    \nCombining the previous results, we have\n\n$$\\max\\_{\\|n\\_j\\|\\_\\ast \\\
    leq \\lambda} \\|\\xi - \\xi\\_j\\|^T n\\_j = \\lambda \\|\\xi - \\xi\\_j\\|.$$\n\
    \nTherefore, we can get\n\n$$\\|\\lambda \\|\\xi - \\xi\\_j\\| = \\max\\_{\\|n\\\
    _j\\|\\_\\ast \\le \\lambda} \\|\\xi - \\xi\\_j\\|^T n\\_j,$$\n\nwhich proves\
    \ that λ∥ξ − ξj∥ can be expressed as max<sup>∥</sup>n<sup>j</sup> <sup>∥</sup>∗≤<sup>λ</sup>\
    \ ∥ξ − ξj∥ <sup>T</sup> n<sup>j</sup> .\n\n*Constraint-Level Dual Transformation\
    \ and Order Exchange:* The right side of the equation (10) can be transformed\
    \ based on the minimax principle, which leads to maxξ∈<sup>Ξ</sup> min<sup>∥</sup>n<sup>j</sup>\
    \ <sup>∥</sup>∗≤<sup>λ</sup> Ψ[X, Y, rk] − ∥ξ − ξj∥ <sup>T</sup> n<sup>j</sup>\
    \ .\n\nWe exchange the order of solving max-min problems, the inner layer maximization\
    \ problem is transformed as\n\n$$\\min\\_{\\lambda \\ge 0, h\\_j, \\|\\|n\\_j\\\
    |\\|\\_\\ast \\le \\lambda} \\lambda \\theta + \\sum\\_{j=1}^K p\\_j^0 \\cdot\
    \ h\\_j \\tag{11}$$\n\n$$s.t. \\ h\\_j \\geq \\max\\_{\\xi \\in \\Xi} \\Psi[\\\
    mathbf{X}, \\mathbf{Y}, r\\_k] - ||\\xi - \\xi\\_j||^T n\\_j, \\forall j, k \\\
    in \\{1, ..., K\\}, \\tag{11.6}$$\n\n(11a)\n\n$$c \\cdot \\xi \\le d.\\tag{11b}$$\n\
    \nTheorem 3. min<sup>∥</sup>n<sup>j</sup> <sup>∥</sup>∗≤<sup>λ</sup> maxξ∈<sup>Ξ</sup>\
    \ Ψ[X, Y, rk] − ∥ξ − ξj∥ <sup>T</sup> n<sup>j</sup> *and* maxξ∈<sup>Ξ</sup> min<sup>∥</sup>n<sup>j</sup>\
    \ <sup>∥</sup>∗≤<sup>λ</sup> Ψ[X, Y, rk]−∥ξ−ξj∥ <sup>T</sup> n<sup>j</sup> *are\
    \ equivalent.*\n\n*Proof:* This proof is omitted due to the page limit. We transform\
    \ the constraints (11a) and (11b), obtaining a dual form as\n\n$$n\\_j^T \\xi\\\
    _j + \\Psi[\\mathbf{X}, \\mathbf{Y}, r\\_k] + z\\_j^T d \\le h\\_j, \\forall j,\
    \ k \\in \\{1, ..., K\\}, \\tag{12a}$$\n\n$$z\\_j^T d = n\\_j, \\forall j \\in\
    \ \\{1, \\ldots, K\\},\\tag{12b}$$\n\n$$\\{\\lambda, z\\_j \\ge 0, \\forall j\
    \ \\in \\{1, \\ldots, K\\}, \\tag{12c}$$\n\n$$\\|\\|n\\_j\\|\\|\\_\\* \\le \\\
    lambda, \\forall j \\in \\{1, \\ldots, K\\}. \\tag{12d}$$\n\nAfter completing\
    \ the dual transformation of the internal maximization problem, we combine it\
    \ with the external minimization problem, and eventually reformulate P<sup>2</sup>\
    \ as\n\n$$\\mathcal{P}\\_3: \\min\\_{\\lambda, h\\_j, n\\_j, z\\_j, \\mathbf{X},\
    \ \\mathbf{Y}} \\lambda \\theta + \\sum\\_{j=1}^{K} p\\_j^0 \\cdot h\\_j \\qquad\
    \ (13)$$\n\n$$s.t.\\ (1a) - (1e), (12a) - (12d).\\tag{13a}$$\n\nP<sup>3</sup>\
    \ is a mixed-integer nonlinear programming problem characterized by the dual norm\
    \ and the presence of both continuous and binary variables, making it NP-hard.\
    \ To effectively handle the mixed-integer programming and nonlinear components\
    \ in the objective function and constraints, we employ the Gurobi optimization\
    \ solver to solve the P3.\n\n# IV. SIMULATION RESULTS\n\nConsider DTaaS provisioning\
    \ over edge computing, the storage capacity of each ES is set within [8, 16] GB\
    \ randomly, and the size of DT model sm, m ∈ M is set within [0.5, 2] GB. The\
    \ size of data for updating of PT m ∈ M is set within [2, 5] MB, while the size\
    \ of the DT interaction request response data is set within [0.5, 2] MB. Additionally,\
    \ the transmission latency for sending a unit of data between two ESs is set within\
    \ [0.2, 1] ms. The transmission latency for sending a unit data between the remote\
    \ cloud and a ES through the core network is set within [2, 10] ms. We use the\
    \ realworld request dataset from edge networks in [12], and divided them into\
    \ historical DT interaction requests for constructing the reference distribution\
    \ and future DT interaction requests.\n\nFor the comparison purpose, the following\
    \ schemes are simulated as benchmarks. 1) Near-PT: Prioritize the deployment of\
    \ the DT model on the ES where the corresponding PT is located, and ignore the\
    \ uncertainty of DT interaction request; 2) Near-RQ: Deploy DT models based on\
    \ historical DT interaction information, targeting the ES in the area with the\
    \ highest visits, and ignore the update of DT models; 3) DRO-AVG: Utilize a DRO-based\
    \ approach to consider update of DT models and DT interaction requests, but use\
    \ average distribution as the reference distribution.\n\nFig. 3 shows the total\
    \ utility gain of different algorithms under different network sizes, thus demonstrating\
    \ the applicability of the WDRO in large-scale networks. The results demonstrate\
    \ that the proposed WDRO performs significantly better than the other three algorithms,\
    \ and the total utility gain shows a clear upward trend as the network size increases.\
    \ Compared with DRO-AVG, our algorithm exhibits a greater increase, which is due\
    \ to the fact that our WDRO takes into account the similarity between the historical\
    \ interaction information and the upcoming DT interaction requests. In contrast,\
    \ Near-RQ and Near-PT exhibit slower increases, as\n\n![](_page_5_Figure_0.jpeg)\n\
    \n![](_page_5_Figure_1.jpeg)\n\nFig. 3: Comparison of total utility gain w.r.t\
    \ network size.\n\nFig. 4: Comparison of total utility gain w.r.t PT number.\n\
    \nthey focus only on DT model updates or DT service response without optimizing\
    \ overall DT service quality.\n\nFig. 4 compares the total utility gain of different\
    \ algorithms as the PT number increases. The result shows the total utility gain\
    \ decrease as the number of PT rises, with WDRO exhibiting a significantly slower\
    \ rate of decline compared to the other algorithms. This is because, as the number\
    \ of PTs increases while the overall network size and structure remain unchanged,\
    \ storage limitations begin to become a major constraint. Once the PT count exceeds\
    \ a certain threshold, the available storage capacity is no longer sufficient\
    \ to deploy DT models at optimal locations. However, WDRO's inherent robustness\
    \ enables it to better mitigate the adverse effects of these environmental changes,\
    \ outperforming other algorithms.\n\nFig. 5 shows the total utility gain as the\
    \ maximum storage capacity of ES increases, with the components of utility gain\
    \ from DT model update and service response latency. The results indicate that\
    \ WDRO outperforms in both utility gain of DT model update and service response\
    \ latency. The utility gain rises and then plateaus as storage capacity increases,\
    \ because further deployment of DT models no longer contributes significantly\
    \ to the utility gain once the threshold is reached, which is sufficient to satisfy\
    \ DT interaction requests. Additionally, WDRO achieves utility gain from service\
    \ response similar to the Near-RQ, and its utility from DT model AoI close to\
    \ the Near-PT, demonstrating its effectiveness in fulfilling requests and maintaining\
    \ high-fidelity DT models.\n\n# V. CONCLUSION\n\nIn this paper, a joint optimization\
    \ of DT model deployment and DT model selection for DTaaS provisioning over edge\
    \ computing has been studied. To evaluate the quality of DTaaS, we introduce utility\
    \ gain based on the AoI difference between edge and cloud service provisioning.\
    \ With storage resource constraints, our goal is to maximize the total utility\
    \ gain of all DT interaction requests. Focusing on providing robust solutions\
    \ capable of adapting to unforeseen extreme request conditions, we propose a DRO-based\
    \ approach, called WDRO, that reformulates the problem employing the DRO method.\
    \ Then, by leveraging multi-level dual transformations based on Wasserstein distance,\
    \ we derive a robust solution. Compared\n\n![](_page_5_Figure_9.jpeg)\n\nFig.\
    \ 5: Comparison of total utility gain w.r.t storage capacity.\n\nto counterparts,\
    \ the proposed WDRO demonstrates the superiority in obtaining the total utility\
    \ gain of all DT interaction requests under unforeseen extreme request conditions.\n\
    \n# VI. ACKNOWLEDGMENTS\n\nThis work was supported by State Key Laboratory of\
    \ Massive Personalized Customization System and Technology No.H&C-MPC-2023-04-01,\
    \ and Postgraduate Research & Practice Innovation Program of Jiangsu Province\
    \ No.KYCX24\\_0596.\n\n# REFERENCES\n\n- [1] J. Chen, Y. Shi, C. Yi *et al.*,\
    \ \"Generative-AI-driven human digital twin in IoT healthcare: A comprehensive\
    \ survey,\" *IEEE Internet Things J.*, vol. 11, no. 21, pp. 34 749–34 773, 2024.\n\
    - [2] B. Fan, Y. Wu, Z. He, Y. Chen, T. Q. Quek, and C.-Z. Xu, \"Digital twin\
    \ empowered mobile edge computing for intelligent vehicular lanechanging,\" *IEEE\
    \ Netw.*, vol. 35, no. 6, pp. 194–201, 2021.\n- [3] Y. Zhou, R. Zhang, J. Liu,\
    \ T. Huang, Q. Tang, and F. R. Yu, \"A hierarchical digital twin network for satellite\
    \ communication networks,\" *IEEE Commun. Mag.*, vol. 61, no. 11, pp. 104–110,\
    \ 2023.\n- [4] Y. Shi, C. Yi, R. Wang *et al.*, \"Service migration or task rerouting:\
    \ A two-timescale online resource optimization for MEC,\" *IEEE Trans. Wirel.\
    \ Commun.*, vol. 23, no. 2, pp. 1503–1519, 2024.\n- [5] J. Li, S. Guo *et al.*,\
    \ \"AoI-aware, digital twin-empowered IoT query services in mobile edge computing,\"\
    \ *IEEE/ACM Trans. Netw.*, vol. 32, no. 4, pp. 3636–3650, 2024.\n- [6] R. Chen,\
    \ C. Yi, K. Zhu, B. Chen, J. Cai, and M. Guizani, \"A threeparty hierarchical\
    \ game for physical layer security aware wireless communications with dynamic\
    \ trilateral coalitions,\" *IEEE Trans. Wirel. Commun.*, vol. 23, no. 5, pp. 4815–4829,\
    \ 2024.\n- [7] C. Yi, J. Cai, T. Zhang, K. Zhu, B. Chen, and Q. Wu, \"Workload\
    \ reallocation for edge computing with server collaboration: A cooperative queueing\
    \ game approach,\" *IEEE Trans. Mob. Comput.*, vol. 22, no. 5, pp. 3095–3111,\
    \ 2023.\n- [8] Y. Yang, Y. Shi, C. Yi, J. Cai, K. Jiawen *et al.*, \"Dynamic human\
    \ digital twin deployment at the edge for task execution: A twotimescale accuracy-aware\
    \ online optimization,\" *IEEE Trans. Mob. Comput.*, vol. 23, no. 12, pp. 1–16,\
    \ 2024.\n- [9] E. Delage and Y. Ye, \"Distributionally robust optimization under\
    \ moment uncertainty with application to data-driven problems,\" *Math. Oper.\
    \ Res.*, vol. 58, no. 3, pp. 595–612, 2010.\n- [10] Y. Chen, M. Liu, B. Ai *et\
    \ al.*, \"Adaptive bitrate video caching in uavassisted mec networks based on\
    \ distributionally robust optimization,\" *IEEE Trans. Mob. Comput.*, vol. 23,\
    \ no. 5, pp. 5245–5259, 2024.\n- [11] R. Gao *et al.*, \"Distributionally robust\
    \ stochastic optimization with wasserstein distance,\" *Math. Oper. Res.*, vol.\
    \ 48, pp. 603–655, 2016.\n- [12] H. Liu, Y. Li, and S. Wang, \"Request scheduling\
    \ combined with load balancing in mobile-edge computing,\" *IEEE Internet Things\
    \ J.*, vol. 9, no. 21, pp. 20 841–20 852, 2022."
- title: "Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin\n  Interaction"
  abstract: "In this paper, we investigate a quality of experience (QoE)-aware resource\n\
    allocation problem for reconfigurable intelligent surface (RIS)-assisted\ndigital\
    \ twin (DT) interaction with uncertain evolution. In the considered\nsystem, mobile\
    \ users are expected to interact with a DT model maintained on a\nDT server that\
    \ is deployed on a base station, via effective uplink and downlink\nchannels assisted\
    \ by an RIS. Our goal is to maximize the sum of all mobile\nusers' joint subjective\
    \ and objective QoE in DT interactions across various DT\nscenes, by jointly optimizing\
    \ phase shift matrix, receive/transmit beamforming\nmatrix, rendering resolution\
    \ configuration and computing resource allocation.\nWhile solving this problem\
    \ is challenging mainly due to the uncertain evolution\nof the DT model, which\
    \ leads to multiple scene-specific problems, and require\nus to constantly re-solve\
    \ each of them whenever DT model evolves.\n  To this end, leveraging the dynamic\
    \ optimization capabilities of decision\ntransformers and the generalization strengths\
    \ of generative artificial\nintelligence (GAI), we propose a novel GAI-aided approach,\
    \ called the\nprompt-guided decision transformer integrated with zero-forcing\
    \ optimization\n(PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO,\n\
    demonstrating its effectiveness and superiority over counterparts."
  url: http://arxiv.org/abs/2505.15828v1
  keywords: ''
  document: "#### I. INTRODUCTION\n\nDigital twin (DT) is a groundbreaking technology\
    \ that can create high-fidelity and interactive virtual counterparts of physical\
    \ entities, known as DT models. DT is driving a paradigm shift across a wide-range\
    \ of industries, such as remote education, personalized healthcare, and intelligent\
    \ manufacturing, by delivering DT interaction services powered by these advanced\
    \ DT models [1]. The native way of providing DT interaction services is through\
    \ wireless networks, with the reconfigurable intelligent surface (RIS)-assisted\
    \ DT interactions being particularly important.\n\nAlthough the RIS-assisted virtual-reality\
    \ interaction services has been widely studied in literature [2], [3], they cannot\
    \ directly suit for the DT interaction services due to their unique and critical\
    \ features that have not yet been well investigated. Specifically, different from\
    \ the classical RIS-assisted virtualreality interaction services, RIS-assisted\
    \ DT interactions must consider the end-to-end round-trip quality due to their\
    \ joint uplink and downlink communications. Moreover, beyond the objective experiences\
    \ of mobile users commonly considered in virtual-reality interaction services,\
    \ the subjective experiences are vital important in DT interaction services. Additionally,\
    \ distinct from most of service providers, the DT models evolves uncertainly with\
    \ the ever-changing corresponding physical entities to real-time map them, resulting\
    \ in different and uncertainly changing DT scenes in DT interaction services.\
    \ To fill the gap in the literature, in this paper, we study a quality of experience\
    \ (QoE)-aware resource allocation problem for RISassisted DT interaction with\
    \ uncertain evolution, with the goal of maximizing the sum of all mobile users'\
    \ mixed subjective and objective QoE in DT interactions across various DT scenes,\
    \ by determining the phase shift matrix, receive/transmit beamforming matrix,\
    \ rendering resolution configuration and computing resource allocation. However,\
    \ simultaneously considering all aforementioned features of RIS-assisted DT interaction\
    \ in our problem is very challenging because of the following reasons. i) The\
    \ decision variables are not only highdimensional but also tightly coupled, meaning\
    \ that they have to be jointly optimized. ii) The uncertain evolution of DT models\
    \ leads to multiple scene-specific problems, each of them with distinct problem\
    \ formulation. This requires us to constantly re-solve each of them whenever the\
    \ DT model evolves.\n\nTo tackle these challenges, in this paper, we propose a\
    \ novel generative artificial intelligence (GAI)-aided approach to solve QoE-aware\
    \ resource allocation problem for RIS-assisted DT interaction with uncertain evolution.\
    \ In the considered system, mobile users are expected to interact with a DT model\
    \ maintained on a DT server that is deployed on a base station, via effective\
    \ uplink and downlink channels assisted by an RIS. By taking into account the\
    \ uncertain evolution of the DT model, we formulate a QoE-aware resource allocation\
    \ problem, which consists of unbounded growing amount of different scene-specific\
    \ problems. We aim to maximize the sum of all mobile users' mixed subjective and\
    \ objective QoE in DT interactions across all scene-specific problems. To circumvent\
    \ the difficulty in solving this complicated problem, we propose a GAI-aided approach,\
    \ called prompt-guided decision transformer integrated with zero-forcing optimization\
    \ (PG-ZFO). Specifically, we first reformulate each individual scenespecific problem\
    \ into an Markov decision process (MDP). Then, we design a prompt to elaborately\
    \ capture the scenespecific information. Based on this, we build a prompt-guided\
    \ decision transformer to generate phase shift matrix, rendering resolution configuration\
    \ and computing resource allocation. On top of this, we further develop a zero-forcing\
    \ (ZF)-based optimization algorithm to derive receive/transmit beamforming matrix.\
    \ Such a decision-making process is iteratively conducted during the DT scene,\
    \ and thus the scene-specific\n\n![](_page_1_Figure_0.jpeg)\n\nFig. 1. An illustration\
    \ of the considered RIS-assisted DT interaction system.\n\nproblem is solved.\
    \ The most significant advantage of the proposed PG-ZFO is its superior generalization\
    \ ability, which enables it to efficiently solve various scene-specific problems\
    \ without the need for time-consuming re-training.\n\nThe main contributions of\
    \ this paper are summarized in the following. i) To the best of our knowledge,\
    \ we are the first to study the RIS-assisted DT interaction with uncertain evolution.\
    \ We formulate a QoE-aware resource allocation problem, consisting of unbounded\
    \ growing amount of different scene-specific problems caused by the uncertain\
    \ evolution of the DT model. ii) We propose a novel GAI-aided approach, called\
    \ PG-ZFO. With its strong generalization capabilities, PG-ZFO efficiently solve\
    \ various scene-specific problem triggered by the uncertain evolution of the DT\
    \ model without time-consuming re-training. iii) We conduct simulations to evaluate\
    \ the performance of proposed PG-ZFO, and the results have shown its effectiveness\
    \ and superiority over counterparts.\n\n## II. SYSTEM MODEL AND PROBLEM FORMULATION\n\
    \n# *A. System Overview*\n\nConsider an RIS-assisted DT interaction system, as\
    \ illustrated in Fig. 1, consisting of a DT server a deployed on a base station\
    \ with a uniform linear array (ULA) of M receive/transmit antennas, located at\
    \ q<sup>a</sup> = [xa, ya, za], a set of single-antenna mobile users K with |K|\
    \ = K, and an RIS r with N passive reflecting elements at q<sup>r</sup> = [xr,\
    \ yr, zr]. The DT server maintains a high-fidelity and interactive DT model that\
    \ mirrors the real-time status of a physical entity to provide engaging DT interaction\
    \ services [4]. To ensure seamless connectivity and real-time interaction, the\
    \ RIS assists uplink and downlink.\n\nIt is worth noting that the DT model uncertainly\
    \ evolves with the ever-changing physical entity. Consequently, the DT scene i\
    \ ∈ I in which users engage continuously evolves, with the cardinality of I grows\
    \ with the evolution, i.e., |I| → +∞. Mobile users interact with the DT server\
    \ at a frequency F<sup>i</sup> during each DT scene i ∈ I. dividing it into T<sup>i</sup>\
    \ = 1/F<sup>i</sup> time slots with T<sup>i</sup> = {1, 2, ..., Ti}. Moreover,\
    \ each mobile user k ∈ K has a personalized weight for subjective and objective\
    \ experiences, defined as Wk(i) = (ϖ<sup>ϵ</sup> k (i), ϖ<sup>ι</sup> k (i)),\
    \ where ϖ<sup>ϵ</sup> k (i) and ϖ<sup>ι</sup> k (i) represent their degrees of\
    \ attentions to subjective and objective experiences towards DT scene i ∈ I, respectively,\
    \ and satisfying ϖ<sup>ϵ</sup> k (i) + ϖ<sup>ι</sup> k (i) = 1.\n\n# *B. Uplink\
    \ and Downlink Transmissions in DT Interaction*\n\nWithin any time slot t ∈ T<sup>i</sup>\
    \ when engaging in DT scene i ∈ I, each mobile user k ∈ K, located at qk(i) =\
    \ [xk(i), yk(i), zk(i)] transmits its interaction signals to the DT server through\
    \ an effective uplink channel h UL k,a(i, t) ∈ CM×1 , while obtaining feedback\
    \ signals from the DT server through an effective downlink channel h DL a,k (i,\
    \ t) <sup>∈</sup> <sup>C</sup>M×<sup>1</sup> , both assisted by RIS, which can\
    \ be respectively expressed as h UL k,a(i, t) = hk,a(i, t) + Hr,a(i, t)Θ(i, t)hk,r(i,\
    \ t), and h DL a,k (i, t) = ha,k(i, t) + Ha,r(i, t)Θ(i, t)hr,k(i, t), where vectors\
    \ hk,a(i, t) and ha,k(i, t) specify the direct uplink and downlink channels between\
    \ mobile user k ∈ K and the DT server; matrices Hr,a(i, t), Ha,r(i, t) indicate\
    \ the uplink and downlink channels between the RIS and the DT server; vectors\
    \ hk,r(i, t), hr,k(i, t) stand for the uplink and downlink channels between mobile\
    \ user k and the RIS; Θ(i, t) ≜ diag(A1(i, t)e jθ1(i,t) , ..., A<sup>N</sup> (i,\
    \ t)e jθ<sup>N</sup> (i,t) ) ∈ C <sup>N</sup>×<sup>N</sup> is the phase shift\
    \ matrix of the RIS, where An(i, t) and θn(i, t) represent the amplitude reflection\
    \ coefficient and the phase shift induced by n-th RIS element. Similar to [5],\
    \ [6], ideal reflections are assumed, satisfying |An(i, t)e jθn(i,t) | <sup>2</sup>\
    \ = 1.\n\nFor direct uplink communications of the DT interaction, the channel\
    \ between mobile user k ∈ K and the DT server is modeled as <sup>h</sup>k,a(i,\
    \ t) = <sup>q</sup> ρd<sup>−</sup>αk,a k,a (i)h NLoS k,a (i, t), where ρ is the\
    \ path loss coefficient, αk,a is the path loss exponent, dk,a(i) = p ||qk(i) −\
    \ qa||<sup>2</sup> is the distance between mobile user k and the DT server, and\
    \ h NLoS k,a (i, t) ∼ CN (0, 1) is the non-LoS (NLoS) component [7]. Meanwhile,\
    \ the channel between mobile user k ∈ K and the RIS is given by <sup>h</sup>k,r(i,\
    \ t) = <sup>q</sup> ρd<sup>−</sup>αk,r k,r (i)(<sup>q</sup> <sup>G</sup>k,r <sup>G</sup>k,r+1h\
    \ LoS k,r (i) + q <sup>1</sup> <sup>G</sup>k,r+1h NLoS k,r (i, t)), where dk,r(i)\
    \ = p ||qk(i) − qr||<sup>2</sup> is the distance between mobile user k and the\
    \ RIS, h NLoS k,r (i, t) ∼ CN (0, 1), and Gk,r is the Rician factor. Besides,\
    \ h LoS k,r (i) is the LoS component based on angle of arrival (AoA). Furthermore,\
    \ the channel between the q RIS and the DT server can be modeled as Hr,a(i, t)\
    \ = ρd<sup>−</sup>αr,a r,a ( q <sup>G</sup>r,a <sup>G</sup>r,a+1HLoS r,a + q <sup>1</sup>\
    \ <sup>G</sup>r,a+1HNLoS r,a (i, t)) where dr,a = p ||q<sup>r</sup> − qa||<sup>2</sup>\
    \ is the distance between the RIS and the DT server, HNLoS r,a (i, t) ∼ CN (0,\
    \ 1), Gr,a is the Rician factor, and HLoS r,a includes AoA and angle of departure\
    \ (AoD).\n\nThen, we can express the interaction signals of mobile user k ∈ K\
    \ received by the DT server via h UL k,a(i, t) as y UL k (i, t) = <sup>q</sup>\
    \ p UL k (vk(i, t))Hh UL k,a(i, t)δ UL k (i, t) + (vk(i, t))H( P<sup>K</sup> m=1,m̸=k\
    \ <sup>√</sup>pm<sup>h</sup> UL m,a(i, t)δ UL <sup>m</sup> (i, t)) + (vk(i, t))H(n\
    \ UL k (i, t)), where p UL k is the uplink transmission power, δ UL k (i, t) ∼\
    \ CN (0, 1) is the symbol, vk(i, t) ∈ CM×<sup>1</sup> is the receive beamforming\
    \ vector, and n UL k (i, t) ∼ CN (0,(σ UL k ) 2 ) is noise.\n\nAccordingly, the\
    \ transmission rate of uplink interaction is υ UL k (i, t) = b log<sup>2</sup>\
    \ (1 + γ UL k (i, t)), where b is the uplink channel bandwidth, and γ UL k (i,\
    \ t) is the uplink signal-tointerference-plus-noise ratio (SINR), calculated as\
    \ γ UL k (i, t) = (p UL k |(h UL k,a(i, t))Hvk(i, t)| 2 )/((vk(i, t))H((σ UL k\
    \ ) 2 I<sup>M</sup> + P K m=1,m̸=k p UL <sup>m</sup> h UL m,a(i, t)(h UL m,a(i,\
    \ t))H)vk(i, t)).\n\nSimilar to effective uplink channel, we can obtain the downlink\
    \ transmission rate as υ DL k (i, t) = b log<sup>2</sup> (1 + γ DL k (i, t)),\
    \ where γ DL k (i, t) = (|(h DL a,k (i, t))Hwk(i, t)| 2 )/((σ DL k ) <sup>2</sup>\
    \ + P<sup>K</sup> m=1,m̸=k |(h DL a,k (i, t))Hwm(i, t)| 2 ).\n\n#### *C. QoE Model\
    \ Analysis for DT Interaction*\n\nWe introduce a novel user-centric QoE metric,\
    \ i.e., QoE(Wk(i), t), ∀k ∈ K. Particularly, we consider two key performance indicators,\
    \ i.e., the Weber-Fechner Law based human perception quality Ek(i, t) and the\
    \ interaction roundtrip latency Lk(i, t), to quantify subjective and objective\
    \ experiences for DT interactions [8], respectively.\n\nAccording to Weber-Fechner\
    \ Law [9], we model the physical stimulus intensity as the rendering resolution\
    \ Ek(i, t), which determines the authenticity of the service. Based on this, Ek(i,\
    \ t) = ln( <sup>E</sup>k(i,t) Emin ), where Emin is the minimum resolution that\
    \ mobile users demand.\n\nWithin each time slot t ∈ T<sup>i</sup> when engaging\
    \ in any DT scene i ∈ I, each mobile user k ∈ K first transmits interaction signal\
    \ with size DUL k (i) via h UL k,a(i, t) to the DT server, with uplink latency\
    \ L UL k (i, t) = DUL k (i)/υUL k (i, t).\n\nThen, the DT server generates and\
    \ renders the feedback signal with resolution Ek(i, t). The processing latency\
    \ is L P RO k (i, t) = ξcEk(i, t)/fk(i, t), where ξ is the per-sample data size\
    \ in bits, c is the CPU cycles per bit, and fk(i, t) is the allocated computation\
    \ frequency.\n\nSubsequently, the rendered feedback signal with size ςEk(i, t)\
    \ is transmitted back to k ∈ K via the effective downlink channel h DL k,a (i,\
    \ t), resulting in a downlink latency L DL k (i, t) = ςEk(i, t)/υDL k (i, t)\n\
    \nTo sum up, the interaction round-trip latency is Lk(i, t) = DUL k (i) υ UL (i,t)\
    \ + ξcEk(i,t) <sup>f</sup>k(i,t) + ςEk(i,t) υDL (i,t) .\n\nk k By integrating\
    \ subjective and objective experience metrics through the personalized weight\
    \ Wk(i), we define QoE(Wk(i), t) = ϖ<sup>ϵ</sup> k (i)F ′ (Ek(i, t)) + ϖ<sup>ι</sup>\
    \ k (i)F ′′ (Lk(i, t)), where F ′ (Ek(i, t)) = <sup>E</sup>k(i,t) Emax , F′′ (Lk(i,\
    \ t)) = 1 − Lk(i,t) Lmax . Emax is the maximal perception quality that each mobile\
    \ user can achieve, and Lmax is the maximal interaction round-trip latency that\
    \ each mobile user can tolerate.\n\n#### *D. Problem Formulation*\n\nFor engaging\
    \ in each DT scene i ∈ I, we aim to maximize the sum of all mobile users' QoE\
    \ of DT interaction across all time slots, by jointly optimizing phase shift matrix\
    \ Θ(i, t), receive beamforming matrix V(i, t) := {v1(i, t), ..., vK(i, t)} and\
    \ transmit beamforming matrix W(i, t) := {w1(i, t), ..., wK(i, t)}, rendering\
    \ resolution configuration Ek(i, t) and computing resource allocation fk(i, t),\
    \ i.e.,\n\n$$\\mathcal{P}(i) \\colon \\max\\_{\\Theta(i,t), \\mathbf{V}(i,t),\
    \ \\mathbf{W}(i,t), E\\_k(i,t), f\\_k(i,t)} \\sum\\_{t \\in \\mathcal{T}\\_i}\
    \ \\sum\\_{k \\in \\mathcal{K}} QoE(\\mathcal{W}\\_k(i), t) \\tag{1}$$\n\n$$s.t.,\
    \ \\ \\theta\\_n(i, t) \\in [0, 2\\pi), \\forall n \\in \\mathcal{N}, i \\in \\\
    mathcal{T}, t \\in \\mathcal{T}\\_i,\\tag{1a}$$\n\n$$E\\_k(i, t) \\in [E\\_{min},\
    \ E\\_{max}], \\forall k \\in \\mathcal{K}, i \\in \\mathcal{I}, t \\in \\mathcal{T}\\\
    _i, \\quad (1b)$$\n\n$$\\sum\\_{k \\in \\mathcal{K}} f\\_k(i, t) \\le C, f\\_k(i,\
    \ t) > 0, \\forall i \\in \\mathcal{I}, t \\in \\mathcal{T}\\_i,\\qquad(1c)$$\n\
    \n$$\\sum\\_{k \\in \\mathcal{K}} ||\\mathbf{w}\\_k(i, t)||^2 \\le P^{max}, \\\
    forall i \\in \\mathcal{I}, t \\in \\mathcal{T}\\_i,\\tag{1d}$$\n\n$$\n\\mathcal{L}\\\
    _k(i, t) \\le \\mathcal{L}\\_{max}, \\forall k \\in \\mathcal{K}, \\forall i \\\
    in \\mathcal{I}, t \\in \\mathcal{T}\\_i,\\tag{1e}\n$$\n\nwhere constraint (1a)\
    \ signifies the continuous phase shift control; (1b) defines the range for feedback\
    \ signal resolution; (1c) limits computational resource allocation, with C as\
    \ the computational capacity of the DT server [10]; (1d) imposes a downlink transmission\
    \ power constraint, with P max as the maximum transmit power of the DT server;\
    \ (1e) guarantees that the interaction round-trip latency of each mobile user\
    \ stays within a pre-determined threshold.\n\nIt is worth noting that, due to\
    \ the uncertain DT evolution, the optimization of the considered system actually\
    \ involves a series of problems different scene-specific problems: P = {P(1),P(2),\
    \ ...,P(|I|)}|I|→+<sup>∞</sup>. However, solving P is very challenging because\
    \ i) each P(i) ∈ P is NP-hard due to non-convex quadratic terms, and spans multiple\
    \ time slots requiring a dynamic online algorithm. ii) Constant re-solving P(i)\
    \ ∈ P is needed as new DT scenes emerge, exacerbating the computational complexity.\n\
    \nFor solving different scene-specific problem within P, and inspired by prompt-based\
    \ learning in GAI models like Chat-GPT and DALL-E [11] , we extend the traditional\
    \ decision transformer [12] to a prompt-guided decision transformer. This model\
    \ leverages prompts to encode and exploit scene-specific information, enabling\
    \ it to generalize and efficiently solve new scene-specific problems without re-training\
    \ after being trained on historical data.\n\n# III. PROMPT-GUIDED DECISION TRANSFORMER\
    \ INTEGRATED WITH ZERO-FORCING OPTIMIZATION\n\n# *A. Problem Reformulation*\n\n\
    We reformulate each scene-specific problem P(i) ∈ P into an MDP M(i) ∈ M over\
    \ time slot T<sup>i</sup> , ∀i ∈ I. Each MDP M(i) ∈ M is formulated as follows:\n\
    \nStates: Within each time slot t ∈ T<sup>i</sup> , the current state s(i, t)\
    \ can be represented as s(i, t) = {Wk(i), DUL k (i), h UL k,a(i, t), h DL a,k\
    \ (i, t), QoE(Wk(i), t−1)}<sup>∀</sup>k∈K.\n\nActions: The action space is denoted\
    \ as A(i). Within time slot t ∈ T<sup>i</sup> , the action a(i, t) can be defined\
    \ as a(i, t) = {Θ(i, t), {Ek(i, t)}<sup>∀</sup>k∈K, {fk(i, t)}<sup>∀</sup>k∈K,\
    \ V(i, t),W(i, t)}.\n\nRewards: Observing from the formulation of P(i) ∈ P, within\
    \ each time slot t ∈ T<sup>i</sup> P , reward r(i, t) = <sup>k</sup>∈K QoE(Wk(i),\
    \ t) − δ P <sup>k</sup>∈K lk(i, t) if (1d) is met, and r(i, t) = − P <sup>k</sup>∈K\
    \ QoE(Wk(i), t) otherwise, where − P <sup>k</sup>∈K QoE(Wk(i), t) and −δ P <sup>k</sup>∈K\
    \ lk(i, t) are the penalties for violating constraints (1d) and (1e), respectively.\
    \ δ is the penalty coefficient, and the penalty term lk(i, t) = 0 if Lk(i) ≤ Lmax,\
    \ and lk(i, t) = Lmax otherwise.\n\nState transition probabilities: The state\
    \ transition probabilities is denoted as P r(i, s(i, t + 1)|s(i, t), a(i, t))\
    \ ∈ [0, 1].\n\n#### *B. Prompt-Guided Decision Transformer in PG-ZFO*\n\n*1) Design\
    \ of Prompt:* We use \"decision-making trajectory\" in designing the prompt. Specifically,\
    \ the prompt τ⋆(i) consists of a sequence of decision-making trajectory tuples.\
    \ Following this, τ⋆(i) = (r⋆(i, t), s⋆(i, t), ϑ⋆(i, t), ..., r⋆(i, t + T⋆), s⋆(i,\
    \ t + T⋆), ϑ⋆(i, t + T⋆)), where T<sup>⋆</sup> < T<sup>i</sup> denotes the amount\
    \ of tuples stored in the prompt, and the subscript ·<sup>⋆</sup> indicates the\
    \ elements related to prompts. τ⋆(i) implicitly captures the reward function and\
    \ state transition probabilities of M(i) ∈ M, thereby encoding the scenespecific\
    \ information of P(i) ∈ P. To offer the prompt-guided decision transformer a more\
    \ forward-looking scene-specific information, we replace r⋆(i, t) with the returns-to-go\
    \ (RTG), rˆ⋆(i, t), defined as rˆ⋆(i, t) = Rˆ(i) − Pj=<sup>t</sup> <sup>j</sup>=1\
    \ r⋆(i, j), where Rˆ(i) = P t∈T<sup>i</sup> P <sup>k</sup>∈K ϖ<sup>ϵ</sup> k (i)\
    \ + ϖ<sup>ι</sup> k (i) denotes the maximum total rewards over for M(i).\n\n*2)\
    \ Structure of Prompt-Guided Decision Transformer: Computing embeddings for input\
    \ tokens*: We take the prompt τ⋆(i) of MDP M(i) ∈ M and the most recent decisionmaking\
    \ trajectory with length of L prior to the current time slot t ∈ T<sup>i</sup>\
    \ , i.e., τ (i, t). To process the RTG, states, and decisions in the prompt, which\
    \ represent three distinct token modalities, we employ modality-specific trainable\
    \ linear layers for embedding. A trainable linear layer is also used to add positional\
    \ embeddings to token embeddings within the same decision-making trajectory tuple\
    \ in the prompt. Similarly, trainable linear layers are applied to τ (i, t).\n\
    \n*Obtaining decisions using causal transformer*: The embedded input tokens are\
    \ fed into a causal transformer comprising G-stacked identical decoders. Each\
    \ decoder processes the tokens through a masked multi-head self-attention module\
    \ to capture dependencies across tokens. A feedforward layer enhances these representations\
    \ through position-wise transformations, while layer normalization stabilizes\
    \ training and facilitates gradient updates. The causal transformer processes\
    \ tokens sequentially across the G-stacked decoders, ultimately outputting the\
    \ hidden states. Then, we input them to a trainable linear decision prediction\
    \ layer to generate the decisions.\n\n## *C. ZF-based Optimization Algorithm Integrated\
    \ in PG-ZFO*\n\nThe prompt-guided decision transformer struggles to generate receive/transmit\
    \ beamforming matrix due to the curse of dimensionality. To address this, we propose\
    \ a low-complexity ZF-based optimization algorithm.\n\nFor each time slot t ∈\
    \ T<sup>i</sup> of MDP M(i) ∈ M, given the decisions ϑ(i, t) = {Θ(i, t), {Ek(i,\
    \ t)}<sup>∀</sup>k∈K, {fk(i, t)}<sup>∀</sup>k∈K}, the optimization of κ(i, t)\
    \ = {V(i, t),W(i, t)} for MDP M(i) ∈ M can be formulated as\n\n$$\\min\\_{\\mathbf{V}(i,t),\\\
    mathbf{W}(i,t)} \\sum\\_{k \\in \\mathcal{K}} \\varpi\\_k^t(i) (\\frac{D\\_k^{UL}(i)}{\\\
    upsilon\\_k^{UL}(i,t)} + \\frac{\\varsigma E\\_k(i,t)}{\\upsilon\\_k^{DL}(i,t)}),\
    \ \\quad (2)$$\n\ns.t., (1d),\n\n$$\n\\mathcal{L}\\_k(i, t) \\le \\mathcal{L}\\\
    _{max} - \\mathcal{L}\\_k^{PRO}(i, t), \\forall k \\in \\mathcal{K}. \\tag{2a}\n\
    $$\n\n*Deriving* V(i, t): Based on the ZF-based optimization algorithm for the\
    \ receive beamforming matrix [13], we can obtain V(i, t) = HUL(i, t)((HUL(i, t))<sup>H</sup>HUL(i,\
    \ t))−<sup>1</sup> , where HUL(i, t) = {h UL <sup>1</sup>,a (i, t), ...., h UL\
    \ K,a(i, t)} ∈ CM×K.\n\n*Deriving* W(i, t): Given V(i, t), problem (2) can be\
    \ reformulated as\n\n$$\\min\\_{\\mathbf{W}(i,t)} \\sum\\_{k \\in \\mathcal{K}}\
    \ \\varpi\\_k^{\\iota}(i) \\frac{\\varsigma E\\_k(i,t)}{\\upsilon\\_k^{DL}(i,t)},\\\
    tag{3}$$\n\ns.t., (1d),\n\n$$\\mathcal{L}\\_k^{DL}(i, t) \\le \\mathcal{L}\\_{max}\
    \ - \\mathcal{L}\\_k^{UL}(i, t) - \\mathcal{L}\\_k^{PRO}(i, t). \\quad \\text{(3a)}$$\n\
    \nBased on the ZF-based optimization algorithm [13], we can obtain as W(i, t)\
    \ = W˜ (i, t)P 1 <sup>2</sup> , where HDL(i, t) = {h DL <sup>1</sup>,a (i, t),\
    \ ...., h DL K,a(i, t)} ∈ CM×<sup>K</sup>, W˜ (i, t) = HDL(i, t)((HDL(i, t))HHDL(i,\
    \ t))<sup>−</sup><sup>1</sup> , and P is a diagonal matrix whose k-th diagonal\
    \ element is the receive power at the mobile user k ∈ K within time slot t ∈ T<sup>i</sup>\
    \ under DT scene i ∈ I, i.e., p DL k (i, t). Additionally, the following constraints\
    \ should be satisfied: |(h DL a,k (i, t))Hwk(i, t)| = q p DL k (i, t) and |(h\
    \ DL a,k (i, t))Hwm(i, t)| = 0, ∀k ̸= m. Then, with these two constraints, we\
    \ reformulated problem (3) as:\n\n$$\\max\\_{\\{p\\_k^{DL}(i,t)\\}} \\sum\\_{k\
    \ \\in \\mathcal{K}} \\frac{b \\log\\_2(1 + \\frac{p\\_k^{DL}(i,t)}{(\\sigma\\\
    _k^{DL})^2})}{\\varsigma E\\_k(i,t) \\varpi\\_k^{\\iota}(i)},\\tag{4}$$\n\ns.t.,\
    \ (1d),\n\n$$\\begin{aligned} &\\log\\_2(1+\\frac{p\\_k^{DL}(i,t)}{(\\sigma\\\
    _k^{DL})^2})\\\\ &\\geq \\frac{\\varsigma E\\_k(i,t)}{b(\\mathcal{L}\\_{max}-\\\
    mathcal{L}\\_k^{UL}(i,t)-\\mathcal{L}\\_k^{PRO}(i,t))}, \\forall k \\in \\mathcal{K}.\
    \ \\end{aligned} \\tag{4a}$$\n\nWe apply the water-filling algorithm to derive\
    \ problem (4). The closed-form solution can be obtained as p DL k (i, t) = 1 <sup>v</sup>k(i,t)\
    \ max{ 1 <sup>ϱ</sup> − vk(i, t)(σ DL k ) 2 , vk(i, t)p DL,min k (i, t)}, where\
    \ vk(i, t) is the k-th diagonal element of (W˜ (i, t))<sup>H</sup>W˜ (i, t), ϱ\
    \ P is normalization factor which is selected for ensuring that <sup>k</sup>∈K\
    \ max{ 1 <sup>ϱ</sup> − vk(σ DL k ) 2 , vkp DL,min k } = P max. Besides, p DL,min\
    \ k (i, t) = (σ DL k ) 2 (2 ςEk(i,t) <sup>b</sup>(Lmax−LUL k (i,t)−LP RO k (i,t))\
    \ − 1) is the minimum received power constraint.\n\n#### *D. Implementation of\
    \ PG-ZFO*\n\n*1) Offline Training:* We provide a dataset Dtra = {D(1tra), D(2tra),\
    \ ..., D(|Itra|)} for offline training. Each D(i tra) ∈ Dtra contains multiple\
    \ episodes corresponding to MDP M(i tra) ∈ Mtra. We further divide each D(i tra)\
    \ into D⋆(i tra) for sampling the prompt, and D⋄(i tra) for sampling the most\
    \ recent decision-making trajectory.\n\n*Batching*: For each M(i tra) ∈ Mtra,\
    \ we build a minibatch B(i tra) = {In(τ⋆(i tra, g), τ (i tra, tg, g))} G <sup>g</sup>=1\
    \ includes G input tokens. Each token In(τ⋆(i tra, g), τ (i tra, tg, g)), g ∈\
    \ {1, ..., G} consists of a prompt τ⋆(i tra, g) sampled from D⋆(i tra), and a\
    \ most recent decision-making trajectory τ (i tra, tg, g) sampled from D⋄(i tra).\
    \ By combining all minibatches, we can obtain a batch B = {B(i tra), ∀i tra ∈\
    \ Itra}.\n\n*Model training*: We allow the prompt-guided decision transformer\
    \ X<sup>ω</sup> to traverse all input tokens in the batch B. For each input token,\
    \ the prompt-guided decision transformer generates a set of decisions. Then, we\
    \ combine them as a vector of generated decisions, represented as ϑ gen. After\
    \ that, we compute the mean-squared error (MSE) loss LMSE between ϑ gen and the\
    \ corresponding decisions ϑ in B, which is calculated as LMSE = 1 |B| P(ϑ gen\
    \ − ϑ) 2 . With the aim of minimizing LMSE, we update the prompt-guided decision\
    \ transformer using Adam optimizer with learning rate β.\n\n*2) Online Execution:*\
    \ In the online execution of PG-ZFO, we integrate the offline-trained prompt-guided\
    \ decision transformer X ∗ <sup>ω</sup> with the ZF-based optimization algorithm.\n\
    \n*Input token acquisition*: Specifically, for any P(i) ∈ P, we first reformulate\
    \ it as MDP M(i) ∈ M. Then, we obtain the prompt τ⋆(i) through a deep reinforcement\
    \ learning (DRL) integrated with ZF-based optimization algorithm based prompt\
    \ acquisition approach [13]. Then, we take current state s(i, t) and the RTG rˆ(i,\
    \ t) = Rˆ(i) − Pj=T<sup>⋆</sup> j=1<sup>⋆</sup> r⋆(i, j), where current time slot\
    \ t = T⋆+ 1, forming the most recent decision-making trajectory τ (i, t) with\
    \ sequence length of L.\n\n*Action generation*: We input In(τ⋆(i), τ (i, t)) to\
    \ PG-ZFO, which use the prompt-guided decision transformer X ∗ <sup>ω</sup> to\
    \ generate ϑ(i, t) = {Θ(i, t), Ek(i, t), fk(i, t)}k∈K. Then, it uses ZF-based\
    \ optimization algorithm to derive κ(i, t) = {V(i, t),W(i, t)} by taking ϑ(i,\
    \ t) as the input. Subsequently, PG-ZFO outputs the action a(i, t) = {ϑ(i, t),\
    \ κ(i, t)} for the MDP M(i) ∈ M, obtaining the new RTG and next state, and update\
    \ the input. We interactively conduct such decisionmaking process until reach\
    \ T<sup>i</sup> .\n\n#### IV. SIMULATION RESULTS\n\n#### *A. Experimental Settings*\n\
    \nWe consider a RIS-assisted DT interaction system in a 100m × 100m area, where\
    \ the DT server is located at q<sup>a</sup> = [0m, 0m, 40m] and the RIS is located\
    \ at q<sup>r</sup> = [75m, 100m, 20m]. The DT server maintains a DT model that\
    \ evolves in response to the uncertain changes of an educational system to provide\
    \ DT interaction services [1]. Besides, to construct the dataset Dtra, we take\
    \ |Itra| = 40 DT scenes arisen\n\nTABLE I MAIN SIMULATION SETTING PARAMETERS.\n\
    \n| Parameter               | Value        | Parameter            | Value    \
    \     |\n|-------------------------|--------------|----------------------|---------------|\n\
    | K                       | 10           | M                    | 64         \
    \   |\n| N                       | 16           | B                    | 2 MHz\
    \         |\n| ξ                       | 8 Mb         | P max                |\
    \ 43 dBm        |\n| UL<br>p<br>k            | 500 mW       | C              \
    \      | 10 GHz        |\n| c                       | 50 cycle/bit | Dk(i)   \
    \             | [0.1, 1.0] MB |\n| Ek(i)                   | [1.0, 2.0]   | Eth\
    \                  | 1.0           |\n| ς                       | 1 MB       \
    \  | Lmax                 | 0.5 s         |\n| UL<br>2<br>(σ<br>)<br>k | -60 dBm\
    \      | (σDL<br>2<br>)<br>k  | -50 dBm       |\n| ρ                       | -20\
    \ dB       | {Gk,r, Gr,k}K<br>k=1 | 8 dB          |\n| Gr,a                  \
    \  | 6 dB         | Ga,r                 | 7 dB          |\n| Ti             \
    \         | 100          | T⋆                   | 10            |\n| G       \
    \                | 12           | L                    | 20            |\n| G\
    \                       | 16           | β                    | 1e-4         \
    \ |\n\n![](_page_4_Figure_13.jpeg)\n\nFig. 2. Convergence of the proposed PG-ZFO\
    \ approach.\n\nfrom the DT model and collect 100 episodes per scene-specific problem\
    \ P(i tra). Additionally, 3 distinct DT scenes from I are selected as examples\
    \ for performance evaluation, with corresponding problems denoted as P(1new),P(2new),P(3new).\
    \ All simulations are implemented using PyTorch and Huggingface Transformers on\
    \ an NVIDIA RTX 4090 GPU. Table I lists the values of main simulation setting\
    \ parameters, and all simulation results are obtained by taking averages over\
    \ 1000 runs.\n\nThe following schemes are simulated as benchmarks.\n\nRigid optimization\
    \ method (ROM) [14]: ROM tailors a solution policy for a random historical scene-specific\
    \ problem through DRL, and then the solution policy is deployed to solve any new\
    \ scene-specific problem.\n\nDecision transformer without prompt (DF-WP) [12]:\
    \ DF-WP is first offline trained on the same dataset used in PG-ZFO, and then\
    \ deployed for online execution to solve any new scenespecific problem. SDF does\
    \ not consider the augmentation of prompts during either offline training and\
    \ online execution.\n\n#### *B. Performance Evaluations*\n\nFig. 2 illustrates\
    \ the convergence of PG-ZFO. The MSE loss LMSE decreases steadily and converges\
    \ to a small value. This indicates that PG-ZFO learns a generalized solution\n\
    \n![](_page_5_Figure_0.jpeg)\n\nFig. 3. Illustration of the sum of all mobile\
    \ users' QoE of DT interaction across all time slots under various DT scenes within\
    \ I.\n\n![](_page_5_Figure_2.jpeg)\n\nFig. 4. Comparison of the sum of all mobile\
    \ users' QoE of DT interaction across all time slots w.r.t. maximum transmit power.\n\
    \npolicy applicable across historical scene-specific problems {P(i tra)}<sup>∀</sup><sup>i</sup>\
    \ tra∈Itra . Additionally, we evaluate the PG-ZFO in the unseen problem P(i +\
    \ 1) during the offline training. The results shows that P t∈Ti+1 P <sup>k</sup>∈K\
    \ QoE(Wk(i + 1), t), increases as LMSE decreases and stabilizes when LMSE converges.\
    \ This indicates that PG-ZFO generalizes well to unseen problems, attributed to\
    \ its use of prompts in PG-ZFO, which enriches the representation of scene-specific\
    \ information and enhance generalization.\n\nFig. 3 illustrates the sum of all\
    \ mobile users' QoE in DT interactions across all time slots under various DT\
    \ scenes in I, optimized by ROM, DF-WP and the proposed PG-ZFO. The proposed PG-ZFO\
    \ outperforms DF-WP, as its prompts provide rich scene-specific information to\
    \ guide action generation, improving policy generalization. DF-WP performs better\
    \ than ROM, as its RTG mechanisms partially captures scene-specific information,\
    \ helping it maximize rewards, which helps guide it to generate actions toward\
    \ the maximum rewards Rˆ(i) that expected to be achieved in the M(i) ∈ M.\n\n\
    Fig. 4 examines the sum of all mobile users' QoE in DT interactions across all\
    \ time slots under various DT scenes within I with different maximum transmit\
    \ power P max .\n\n Proposed PG-ZFO DF-WP ROM As P max increases, P t∈T<sup>i</sup>\
    \ P <sup>k</sup>∈K QoE(Wk(i), t), i ∈ I increases because higher P max reduces\
    \ downlink latency L DL k (i, t), enhancing mobile users' objective experience.\
    \ The proposed PG-ZFO outperforms both ROM and DF-WP. The explanations for these\
    \ results are similar to those for Fig. 3.\n\n#### V. CONCLUSION\n\nScene-Specific\
    \ Problem In this paper, we investigate a QoE-aware resource allocation problem\
    \ for RIS-assisted DT interaction with uncertain evolution. We aim to maximize\
    \ the sum of all mobile users' QoE in DT interactions across all scene-specific\
    \ problems, by jointly determining phase shift matrix, receive/transmit beamforming\
    \ matrix, rendering resolution configuration and computing resource allocation.\
    \ To solve this complicated problem, we propose a novel GAI-aided approach, called\
    \ PG-ZFO. Simulation results show that, the proposed PG-ZFO has superior performance\
    \ compared to counterparts.\n\n#### VI. ACKNOWLEDGMENTS\n\nThis work was supported\
    \ by Postgraduate Research & Practice Innovation Program of Jiangsu Province No.KYCX24\
    \ 0596.\n\n#### REFERENCES\n\n- [1] S. Mihai, M. Yaqoob, D. V. Hung *et al.*,\
    \ \"Digital twins: A survey on enabling technologies, challenges, trends and future\
    \ prospects,\" *IEEE Commun. Surv. Tutor.*, vol. 24, no. 4, pp. 2255–2291, 2022.\n\
    - [2] S. Jiang, X. Wang, J. Lin *et al.*, \"A delay-oriented joint optimization\
    \ approach for RIS-assisted MEC-MIMO system,\" *IEEE Trans. Mob. Comput.*, pp.\
    \ 1–15, 2024.\n- [3] X. Zhang, H. Zhang, K. Sun *et al.*, \"Human-centric irregular\
    \ RISassisted multi-UAV networks with resource allocation and reflecting design\
    \ for metaverse,\" *IEEE J. Sel. Areas Commun.*, vol. 42, no. 3, pp. 603–615,\
    \ 2024.\n- [4] J. Chen, C. Yi, S. D. Okegbile, J. Cai, and X. Shen, \"Networking\
    \ architecture and key supporting technologies for human digital twin in personalized\
    \ healthcare: A comprehensive survey,\" *IEEE Commun. Surv. Tutor.*, vol. 26,\
    \ no. 1, pp. 706–746, 2024.\n- [5] Q. Chen, R. Li, X. Xu, J. Wu, H. Jiang, and\
    \ M. Qiu, \"Human-aware dynamic hierarchical network control for distributed metaverse\
    \ services,\" *IEEE J. Select. Areas Commun.*, vol. 42, no. 3, pp. 629–642, 2024.\n\
    - [6] S. Kurma, M. Katwe, K. Singh *et al.*, \"RIS-empowered MEC for URLLC systems\
    \ with digital-twin-driven architecture,\" *IEEE Trans. Commun.*, vol. 72, no.\
    \ 4, pp. 1983–1997, 2024.\n- [7] S. Gong, X. Lu, D. T. Hoang *et al.*, \"Toward\
    \ smart wireless communications via intelligent reflecting surfaces: A contemporary\
    \ survey,\" *IEEE Commun. Surv. Tutor.*, vol. 22, no. 4, pp. 2283–2314, 2020.\n\
    - [8] Y. Yang, Y. Shi, C. Yi *et al.*, \"Dynamic human digital twin deployment\
    \ at the edge for task execution: A two-timescale accuracy-aware online optimization,\"\
    \ *IEEE Trans. Mob. Comput.*, vol. 23, no. 12, pp. 12 262– 12 279, 2024.\n- [9]\
    \ P. Reichl, B. Tuffin, and R. Schatz, \"Logarithmic laws in service quality perception:\
    \ where microeconomics meets psychophysics and quality of experience,\" *Telecommun.\
    \ Syst.*, vol. 52, pp. 587–600, 2013.\n- [10] Y. Shi, C. Yi, R. Wang *et al.*,\
    \ \"Service migration or task rerouting: A two-timescale online resource optimization\
    \ for MEC,\" *IEEE Trans. Wirel. Commun.*, vol. 23, no. 2, pp. 1503–1519, 2024.\n\
    - [11] T. Brown, B. Mann *et al.*, \"Language models are few-shot learners.\"\n\
    - [12] L. Chen, K. Lu, A. Rajeswaran *et al.*, \"Decision transformer: Reinforcement\
    \ learning via sequence modeling,\" vol. 34, 2021, pp. 15 084–15 097.\n- [13]\
    \ C. B. Peel, B. M. Hochwald, and A. L. Swindlehurst, \"A vector-perturbation\
    \ technique for near-capacity multiantenna multiuser communication-Part I: channel\
    \ inversion and regularization,\" *IEEE Trans. Commun.*, vol. 53, no. 1, pp. 195–202,\
    \ 2005.\n- [14] R. Chen, C. Yi, K. Zhu *et al.*, \"A three-party hierarchical\
    \ game for physical layer security aware wireless communications with dynamic\
    \ trilateral coalitions,\" *IEEE Trans. Wirel. Commun.*, vol. 23, no. 5, pp. 4815–4829,\
    \ 2024."
- title: Decoupling the Device and Identity in Cellular Networks with vSIM
  abstract: 'Cellular networks are now fundamental infrastructure, powering not just

    smartphones for daily communication and commerce, but also enabling the

    expansion of IoT and edge computing through last-mile connectivity. At the core

    of this infrastructure is the SIM card, which provides essential network

    authentication and subscriber identification through subscriber cryptographic

    key and profile information. More recently, the SIM card has evolved from a

    separate pluggable card, to a card integrated into the board (i.e., soldered

    onto the board with the same electrical interface) (eSIM), to one that is

    integrated into the System on Chip (iSIM). However, a fundamental limitation

    persists across SIM evolution: subscriber identity remains coupled to hardware.

    eSIM and iSIM technologies, despite enabling remote provisioning, still bind

    digital identities to specific hardware elements. This makes it complex to

    support emerging use cases like moving a phone number to a cloud AI service or

    transferring credentials between different devices while maintaining cellular

    connectivity. Furthermore, although eSIM and iSIM support multiple profiles

    (multiple phone numbers or carrier profiles on a single device), all profiles

    still link back to the same hardware identity. For users seeking to maintain

    privacy through identity rotation or separation (like having different numbers

    for different purposes), they are limited by the hardware-bound nature of the

    security architecture. In this paper, we seek to decouple identity from the

    device, enhancing privacy and flexibility compared to various SIM designs. By

    breaking this coupling, we enable scenarios like real identity rotation,

    integration with virtual assistants, or temporary use of backup phones while

    maintaining consistent cellular connectivity.'
  url: http://arxiv.org/abs/2505.15827v1
  keywords: ''
  document: 'I. INTRODUCTION


    Cellular networks are now fundamental infrastructure, powering not just smartphones
    for daily communication and commerce, but also enabling the expansion of IoT and
    edge computing through last-mile connectivity. At the core of this infrastructure
    is the SIM card, which provides essential network authentication and subscriber
    identification through subscriber cryptographic key and profile information. More
    recently, the SIM card has evolved from a separate pluggable card, to a card integrated
    into the board (i.e., soldered onto the board with the same electrical interface)
    (eSIM), to one that is integrated into the System on Chip (iSIM).


    However, a fundamental limitation persists across SIM evolution: subscriber identity
    remains coupled to hardware. eSIM and iSIM technologies, despite enabling remote
    provisioning, still bind digital identities to specific hardware elements. This
    makes it complex to support emerging use cases like moving a phone number to a
    cloud AI service or transferring credentials between different devices while maintaining
    cellular connectivity. Furthermore, although eSIM and iSIM support multiple profiles
    (multiple phone numbers or carrier profiles on a single device), all profiles
    still link back to the same hardware identity. For users seeking to maintain privacy
    through identity rotation or separation (like having different numbers for different
    purposes), they are limited by the hardware-bound nature of the security architecture.


    In this paper, we seek to decouple identity from the device, enhancing privacy
    and flexibility compared to various SIM designs. By breaking this coupling, we
    enable scenarios like real identity rotation, integration with virtual assistants,
    or temporary use of backup phones while maintaining consistent cellular connectivity.


    ## II. VSIM


    Modern SIM security fundamentally relies on hardware binding for authentication
    and trust. eSIM and iSIM rely on manufacturing-time ID, certificates, and keys
    installed in their secure hardware elements for profile provisioning and management.
    These credentials, used to establish secure channels with profile management servers,
    create a permanent binding between the subscriber profile and the hardware identity.
    To break this binding, we introduce vSIM, a software-based


    Eric Keller Tamara Lehman University of Colorado Boulder eric.keller@colorado.edu
    tamara.lehman@colorado.edu


    SIM implementation that operates within Trusted Execution Environments (TEEs)
    and enables secure digital provisioning. Instead of using hardcoded IDs and keys
    for identity verification, vSIM proves the execution of specific trusted software.
    This is achieved by leveraging Enhanced Privacy ID (EPID) [1], where each CPU
    maintains a unique private key while sharing a group public key. This approach
    enables anonymous attestation—devices can prove their authenticity to carriers
    without revealing individual identities—while supporting fine-grained security
    management through multiple revocation mechanisms (e.g., signature pattern blacklisting).
    During profile provisioning, vSIM signs its attestation quote using its EPID private
    key, allowing providers to verify authenticity and trustworthiness of vSIM against
    group keys and revocation lists before establishing a secure channel for profile
    delivery.


    #### *A. Design Overview*


    Figure 1 illustrates the high-level architecture of vSIM. (1) vSIM executes on
    the device''s main CPU within a TEE, providing hardware-enforced isolation between
    vSIM''s secure execution and the normal operating system. It implements standard
    SIM card functions for 5G authentication and adds secure provisioning capabilities.
    (2) The trusted hardware includes EPID private key which is used for authentication.
    (3) The remote attestation measures the vSIM binary and signs it using EPID private
    key. (4) The provider vSIM manager, residing on the provider''s infrastructure,
    handles profile management and attestation verification for vSIMs requesting new
    profiles. (5) After successfully downloding a profile, vSIM will store it permenantly
    in a secure storage.


    # *B. Establishing Trust and Secret Key Provisioning*


    vSIM is initially installed without a subscriber profile and must securely obtain
    one from a user-specified provisioner.


    Trust Establishment. As a software solution, vSIM builds trust through secure
    boot and remote attestation. Secure boot provides the foundation by creating a
    chain of trust where each boot layer verifies the next layer''s signature, ultimately
    ensuring the TEE and attestation system boot securely. The attestation system,
    running within this verified TEE environment, then generates cryptographically
    signed quotes that prove vSIM''s integrity and authenticity to remote parties.


    ![](_page_1_Figure_0.jpeg)


    Fig. 1: vSIM Architecture (Blue arrow shows profile provisioning and red arrow
    represents authentication procedure).


    Secure Channel Protocol. The provisioning process begins when vSIM generates a
    nonce and ephemeral public key, then transmits them encrypted with the provisioner''s
    public key. The provisioner transmits back nonces, an attestation request, and
    its ephemeral public key, encrypted with vSIM''s public key. Both parties derive
    a session key from their ephemeral keys. vSIM then sends its attestation quote
    (signed by the private key) and nonces, encrypted with the session key. Upon verifying
    the quote and signature by the provisioner, the secure channel is established.
    The provisioner then securely transfers the subscriber profile, which includes
    both the user''s subscription data and the secret key needed for 5G authentication.
    This protocol ensures mutual authentication between vSIM and provisioner, protection
    against replay attacks through nonces, verification of vSIM''s trusted execution
    state, and forward secrecy through session keys.


    #### *C. Using vSIM in 5G*


    After obtaining the subscriber profile and the secret key, we need to securely
    persist them in storage. In order to minimize the software running in the TEE,
    the data is encrypted and then passed to the file system running in the untrusted
    OS.


    Finally, vSIM provides all essential functions to support 5G authentication. The
    process follows the traditional flow: when the Mobile Equipment (ME) receives
    a network challenge, it forwards it to vSIM. The vSIM retrieves and decrypts the
    secret key from storage, generates the required cryptographic keys and response,
    and securely transmits them back to the ME for network verification (Figure 1
    red arrow).


    #### III. PRELIMINARY WORK


    We implemented vSIM in the Keystone [2], an opensource TEE framework for RISC-V,
    deploying it in a Qemuemulated environment. For cryptographic functions, we ported
    Libsodium library along with tiny AES libraries into the enclave runtime. In accordance
    with specifications in 3GPP TS 33.501 standard [3], we implemented 5G authentication
    using AES-CBC with 256-bit keys. We provided a remote attestation program which
    resides within Keystone and provides vSIM''s binary measurement and its signature
    when requested.


    For profile provisioning, we developed a Python-based vSIM Manager server that
    handles profile provisioning and attestation verification.


    # IV. INTEGRATION WITH 5G STRUCTURE


    To verify that vSIM seamlessly integrates with 5G infrastructure, we leveraged
    srsRAN with ZeroMQ, extending srsUE''s source code to interface with vSIM instead
    of USIM.


    We measured the traffic with/without vSIM. The 5G authentication process is done
    only once when the UE wants to attach to the network, and therefore communication
    can occur without vSIM''s involvement afterwards. Using iPerf3, we measured TCP
    traffic performance by sending 200 MB through the UE. Figure 4(a) illustrates
    network usage rates using vSIM, and Figure 4(b) shows it with USIM. Apart from
    some noise, they are roughly identical, illustrating the fact that there is no
    overhead.


    ![](_page_1_Figure_12.jpeg)


    Fig. 2: Inbound and Outbound network usage of UE. (a) With vSIM, (b) With USIM


    ## V. CONCLUSIONS AND FUTURE WORKS


    In this paper, we introduce vSIM, a software-based reimagining of SIM functionality
    that addresses limitations of traditional SIM cards. Our implementation integrates
    with commercial off-the-shelf CU/DU solutions like srsRAN, validating vSIM''s
    compatibility with existing network infrastructure.


    However, our work here is only the beginning. As part of our future work, we plan
    to integrate vSIM into an FPGA implementation of Keystone for IoT devices. With
    this, we aim to implement discussed features such as multi-profile support which
    enables comprehensive evaluations of performance, compatibility, and cost benefits
    compared to traditional SIM solutions in real-world deployments.


    #### REFERENCES


    - [1] E. Brickell and J. Li, "Enhanced privacy id: A direct anonymous attestation
    scheme with enhanced revocation capabilities," *IEEE Transactions on Dependable
    and Secure Computing*, vol. 9, no. 3, pp. 345–360, 2012.

    - [2] D. Lee, D. Kohlbrenner, S. Shinde, K. Asanovic, and D. Song, ´ "Keystone:
    an open framework for architecting trusted execution environments," in *Proceedings
    of the Fifteenth European Conference on Computer Systems*, ser. EuroSys ''20.
    New York, NY, USA: Association for Computing Machinery, 2020. [Online]. Available:
    https://doi.org/10.1145/3342195.3387532

    - [3] "3GPP TS 33.501 version 16.3.0 Release 16," 3rd Generation Partnership Project
    (3GPP), Tech. Rep., 2020, accessed: 2025-01-01.'
- title: A Novel Compound AI Model for 6G Networks in 3D Continuum
  abstract: 'The 3D continuum presents a complex environment that spans the terrestrial,

    aerial and space domains, with 6Gnetworks serving as a key enabling technology.

    Current AI approaches for network management rely on monolithic models that

    fail to capture cross-domain interactions, lack adaptability,and demand

    prohibitive computational resources. This paper presents a formal model of

    Compound AI systems, introducing a novel tripartite framework that decomposes

    complex tasks into specialized, interoperable modules. The proposed modular

    architecture provides essential capabilities to address the unique challenges

    of 6G networks in the 3D continuum, where heterogeneous components require

    coordinated, yet distributed, intelligence. This approach introduces a

    fundamental trade-off between model and system performance, which must be

    carefully addressed. Furthermore, we identify key challenges faced by Compound

    AI systems within 6G networks operating in the 3D continuum, including

    cross-domain resource orchestration, adaptation to dynamic topologies, and the

    maintenance of consistent AI service quality across heterogeneous environments.'
  url: http://arxiv.org/abs/2505.15821v1
  keywords: 3D Continuum, Compound AI, 6G Networks
  document: '# I. INTRODUCTION


    The integrated 3D continuum represents a fundamental shift from traditional ground-based
    infrastructure, seamlessly connecting terrestrial, aerial, and space-based components
    into a unified ecosystem. 6G networks serve as one of the key enabling technologies
    for this continuum, facilitating interactions between ground networks, high-altitude
    platforms, unmanned aerial vehicles, and satellite constellations [\[1\]](#page-3-0).
    This 3D continuum extends beyond mere connectivity to encompass distributed compute
    and storage resources across heterogeneous domains, creating a fabric that supports
    ubiquitous digital services regardless of geographical location or altitude. Through
    6G''s orchestration capabilities, the system promises unprecedented global coverage,
    enhanced reliability, and improved throughput across all segments of this multidimensional
    environment.


    However, managing this 3D environment presents significant challenges due to the
    high mobility, heterogeneity, and dynamic nature of its components, particularly
    satellite nodes that continuously reshape network topologies in real time [\[2\]](#page-3-1).
    Although current AI-assisted approaches to network management predominantly rely
    on monolithic models, these face critical limitations within the 3D continuum
    context. Singlemodel strategies typically specialize in optimizing specific domains
    without effectively capturing cross-domain interactions, lack the adaptability
    required for rapidly changing network conditions, and often require substantial
    computational resources unavailable across all segments of the network. Furthermore,
    training and operating such large monolithic models incurs prohibitive costs in
    terms of computation, energy, and maintenance. System performance metrics such
    as latency and throughput also suffer significantly when these models attempt
    to handle large-scale, heterogeneous network environments, creating bottlenecks
    that undermine real-time decision-making capabilities.


    Compound AI systems [\[4\]](#page-3-2), [\[5\]](#page-3-3), [\[6\]](#page-3-4)
    offer distinct advantages over these current state-of-the-art approaches. By distributing
    intelligence across the network, Compound AI systems enable localized decision
    making while maintaining global coordination, efficiently utilize heterogeneous
    computational resources, and adapt to the unique characteristics of each domain.
    This paper presents Compound AI systems to address the key management challenges
    of the 6G networks in the 3D continuum, demonstrating how this modular yet integrated
    approach can enable autonomous adaptation while maintaining consistent service
    quality and operational efficiency across terrestrial, aerial, and space domains.


    #### II. OVERVIEW OF COMPOUND AI SYSTEMS


    #### *A. Towards system design approach to AI*


    Rather than relying on a single model to handle all aspects of a complex problem,
    Compound AI systems decompose tasks into manageable sub-components. This modular
    approach enables developers to leverage specialized models or tools for different
    sub-tasks and control information flow. For example, lightweight models can make
    real-time decisions at terrestrial base stations, while different specialized
    models simultaneously optimize aerial and satellite resource allocation. Supporting
    infrastructure components such as vector databases for similarity search, orchestration
    frameworks for module coordination, API gateways for external tool access, and
    monitoring systems for quality assurance create a rich ecosystem of components
    to build upon. This diversity allows the system to function effectively even when
    parts have varying computational capacities or connectivity constraints. Similarly,
    routing inputs to different-sized models based on task complexity and overall
    system load can optimize computational resources while maintaining quality of
    service across heterogeneous network segments. As network conditions evolve, individual
    AI components can be updated independently, preserving adaptability without requiring
    retraining of the entire system.


    # *B. Example Compound AI System and Main Design Principles*


    To better understand the concept of Compound AI systems, we turn to the following
    explanatory example.


    ![](_page_1_Figure_3.jpeg)


    <span id="page-1-0"></span>Fig. 1. VATE: A Compound AI System for Edge-Cloud Object
    Detection and Tracking. [\[7\]](#page-3-5)


    Fig [1](#page-1-0) illustrates a Compound AI system for edge-cloud collaborative
    object detection and tracking. It demonstrates how multiple specialized modules
    collaborate in a unified system. At the edge, a lightweight detector identifies
    potential objects, while a tracking module maintains object persistence across
    frames. On the cloud side, a more powerful detector offers greater accuracy for
    challenging cases. A fusion module combines detections from both edge and cloud
    sources to create an improved understanding of the scene. Furthermore, a dedicated
    orchestrator module makes intelligent decisions about when to process input locally,
    when to offload to the cloud, and when to rely on tracking rather than detection,
    balancing between computational efficiency and accuracy.


    With these considerations, we define Compound AI systems as systems composed of
    specialized, interoperable modules that collectively address complex AI tasks.
    Each module performs a distinct function and interacts via well-defined defined
    interfaces. The following key characteristics define the structural and functional
    principles that underpin Compound AI systems, enabling them to address complex
    tasks in a scalable and efficient manner:


    - Modularity Separate parts of Compound AI systems can be developed, tested and
    maintained independently while minimizing impacts on the overall system. This
    represents a separation of concerns that facilitates parallel development and
    iterative improvement by specialized teams [\[8\]](#page-3-6).

    - Adaptability The modular design of Compound AI systems enables rapid adaptation
    to new requirements or changing conditions by allowing individual components to
    be replaced, enhanced, or reconfigured without rebuilding the entire system.

    - Abstraction Internal complexities of modules are hidden behind well-defined
    interfaces, ensuring that changes


    to a module''s implementation don''t affect other components. This creates a clear
    separation between what a module does and how it accomplishes its task.


    - Interaction-defined Architecture The architecture of a Compound AI system is
    fundamentally defined by how modules interact with each other. These interactions
    establish the data flow through the system and determine how information is processed,
    transformed and utilized across modules.

    - Cost-Effectiveness Designing with cost-effectiveness in mind ensures that resource
    utilization, energy consumption, and computational requirements are optimized.
    This principle is crucial not only for reducing operational expenses but also
    for enabling an architecture where individual components can be updated or replaced
    without necessitating a complete retraining of the entire system


    # III. COMPOUND AI SYSTEM MODEL


    # *A. Definition and Core Components*


    ![](_page_1_Figure_15.jpeg)


    <span id="page-1-1"></span>Fig. 2. Tripartite model for Compound AI systems


    In Fig [2](#page-1-1) we present a model and reference architecture for Compound
    AI system. This reference architecture offers a conceptual framework through which
    to analyze, conceptualize, and implement Compound AI (CAI) systems as multimodular,
    interactive, and adaptive entities. These systems are composed from a comprehensive
    ecosystem encompassing various AI/ML models, vector databases, retrieval mechanisms,
    external APIs, and domain-specific algorithmic solutions.


    We define a Compound AI System formally as a triple:


    $$\mathbf{CAI} = (S, F, O)$$


    Where:


    - S represents the Structural component

    - F represents the Functional component

    - O represents the Operational component


    While each component serves a distinct role, they interact in important ways:


    - 1) The Structural component (S) defines what modules exist and how they connect,
    constraining the possible implementations in the Functional component (F).

    - 2) The Functional component (F) realizes the abstract architecture defined in
    S through concrete implementations, potentially informing structural changes based
    on implementation constraints.

    - 3) The Operational component (O) monitors and maintains S and F, providing feedback
    for optimization and adaptation.


    The tripartite model naturally supports key system design principles such as modularity,
    adaptability, and abstraction. By clearly separating architectural, behavioral,
    and operational concerns, it enables more systematic approaches to the development,
    deployment, and maintenance of complex, multimodel AI systems.


    #### *B. Structural Component (S)*


    The Structural component defines the high-level architecture of the system, specifying
    what each module does and how information flows between modules. Formally, we
    represent this as a directed graph:


    $$S = G(M, E)$$


    Where:


    - M = {M1, M2, . . . , Mn} is the set of modules

    - E ⊆ M × M is the set of connections between modules


    For any two modules M<sup>i</sup> and M<sup>j</sup> , an edge E(M<sup>i</sup>
    , M<sup>j</sup> ) indicates that the output of module M<sup>i</sup> serves as
    an input to module M<sup>j</sup> .


    Each module M<sup>i</sup> is characterized by its input space I<sup>i</sup> and
    output space O<sup>i</sup> , which define the types and formats of data the module
    can accept and produce. The Structural component thus establishes a "schema" for
    the Compound AI system, defining the roles and relationships of its constituent
    parts without specifying their implementation details.


    #### *C. Functional Component (F)*


    The Functional component defines how the system behaves by mapping the abstract
    architecture to concrete implementations. It encompasses:


    $$F = (P, \Phi, \delta)$$


    Where:


    - P = {P1, P2, . . . , Pn} is a implementation set where each P<sup>i</sup> is
    an implementation pool for module M<sup>i</sup>

    - Φ is the composition function that determines the overall system behavior

    - δ is the mapping function that selects specific implementations from pools


    For each module M<sup>i</sup> , the implementation pool P<sup>i</sup> = {fi1,
    fi2, . . . , fik} contains multiple possible implementations that fulfill the
    same functional role but may differ in their performance characteristics, resource
    requirements, or other properties.


    The mapping function δ : M → S P<sup>i</sup> selects a specific implementation
    for each module, such that δ(Mi) ∈ P<sup>i</sup> .


    The composition function Φ integrates these implementations according to the structural
    blueprint to produce the overall system behavior:


    $$

    \Phi: I \to O

    $$


    Where I is the input space and O is the output space of the entire system.


    Critically, this function ensures that the compound system''s behavior matches
    what would be expected from a monolithic model:


    $$y = \Phi(x) \quad \text{where } x \in I, \, y \in O\_1$$


    The composition function can be formally defined in terms of the graph execution:


    $$\Phi(x) = \Psi(G(M, E), \delta(M\_1), \delta(M\_2), \dots, \delta(M\_n), x)$$


    Where Ψ is a graph execution function that propagates inputs through the implementation
    graph according to the connection pattern defined in S.


    # *D. Operational Component (O)*


    The Operational component encompasses the infrastructure and processes that enable,
    maintain, and optimize the running system. This includes:


    $$O = (Mon, Sec, Gov, Ord)$$


    Where:


    - Mon represents monitoring and observability systems

    - Sec represents security and compliance mechanisms

    - Gov represents governance frameworks and policies

    - Orch represents orchestration and resource management


    The Operational component serves as the foundation that supports both the Structural
    and Functional components, providing:


    - 1) System monitoring that tracks performance, resource utilization, and failure
    modes

    - 2) Security controls that protect system integrity and data privacy

    - 3) Governance mechanisms that ensure compliance with regulations and ethical
    standards

    - 4) Orchestration tools that manage deployment, scaling, and resource allocation


    This component parallels DevOps and MLOps practices in software engineering but
    extends them to address the unique challenges of compound AI systems.


    # IV. OPEN CHALLENGES FOR COMPOUND AI IN 6G NETWORKS FOR 3D CONTINUUM


    To realize our vision of Compound AI, the following challenges need to be addressed.


    ### *A. Cross-Domain Resource Orchestration*


    Orchestrating Compound AI resources across the 3D continuum faces unique constraints
    that existing AI solutions fail to address. Terrestrial components can leverage
    high computational capacity but are limited in coverage, aerial platformbased
    components faces energy and computational constraints, while satellite-hosted
    systems can provide wide coverage but with significant computational and latency
    limitations. Current AI orchestration approaches treat these domains separately,
    creating inefficiencies at domain boundaries. Compound AI for 6G networks requires
    intelligent decomposition and coordination mechanisms that can distribute AI tasks
    optimally across these diverse domains while accounting for their unique characteristics
    and computational limitations.


    #### *B. Adaptation to Dynamic Network Topologies*


    The constantly evolving network topologies of the 3D continuum challenge traditional
    deployment strategies. As satellite constellations orbit, aerial platforms move,
    and terrestrial demand shifts, Compound AI systems must continuously reconfigure
    themselves to maintain performance. Current AI composition algorithms struggle
    with this dynamism, leading to suboptimal configurations where component distribution
    becomes misaligned with actual network conditions. Compound AI for 6G networks
    requires adaptive systems capable of predicting topology changes and proactively
    reconfiguring component distribution and communication patterns, thus maintaining
    overall model performance while optimizing system performance metrics.


    #### *C. Maintaining AI Service Consistency*


    Delivering consistent AI service quality across the 3D continuum presents significant
    technical challenges. As AI requests and data transition between terrestrial,
    aerial, and space segments, maintaining continuity of AI inference quality becomes
    increasingly difficult. Current approaches typically react to AI service degradations
    after they occur, particularly at domain boundaries where computational resources
    vary dramatically. Compound AI for 6G networks requires predictive capabilities
    that can anticipate performance variations across the continuum and implement
    proactive measures in order to maintain service level objectives despite the inherent
    heterogeneity and dynamic nature of the underlying 3D infrastructure.


    #### *D. Balancing the Trade-offs*


    Compound AI systems present trade-offs between improving model performance and
    maintaining system efficiency [\[3\]](#page-3-7). In scenarios requiring advanced
    reasoning, adding specialized components to a monolithic model can enhance accuracy,
    precision, and recall. However, this often leads to increased latency, energy
    consumption, and computational demands. Conversely, when deploying on the edge,
    Compound AI aims to retain comparable model performance while significantly improving
    system efficiency to suit resource-constrained environments. These trade-offs
    become especially challenging across the 3D continuum, which spans from powerful
    data centers to limited-capacity edge devices. Addressing this complexity requires
    adaptive frameworks capable of dynamically reconfiguring Compound AI systems based
    on fluctuating network conditions, available resources, and application-specific
    requirements. Such adaptability would allow systems to strike the right balance
    between model and system performance, depending on the deployment context. Advancing
    this research is key to ensuring that the benefits of Compound AI outweigh the
    added complexity it introduces in real-world scenarios.


    #### V. CONCLUSION


    In this paper, we introduced the concept of Compound AI systems and presented
    a formal tripartite model that captures their structural, functional, and operational
    dimensions. By breaking down complex tasks into specialized, interoperable modules,
    Compound AI systems offer a scalable and adaptable alternative to traditional
    monolithic AI architectures. We explored how this general framework can address
    the unique challenges of 6G networks in the 3D continuum, highlighting the advantages
    of modularity and distributed intelligence in such complex scenarios.


    Looking ahead, our future work will focus on applying this general Compound AI
    system model to real-world 6G use cases and other dynamic, distributed environments.
    This includes implementing adaptive orchestration strategies, developing proactive
    reconfiguration mechanisms, and validating system performance across varying network
    and resource conditions. By doing so, we aim to demonstrate how Compound AI systems
    can be effectively deployed to meet the demands of next-generation networks and
    beyond.


    Acknowledgments: This research was partially funded by the EU''s Horizon Europe
    Research and Innovation Program as part of the NexaSphere project (GA No. 101192912).


    #### REFERENCES


    - <span id="page-3-0"></span>[1] Massod Khorsandi Bahare, Anastasius Gavras, Marco
    Gramaglia, John Cosmas, Xi Li, Omer Bulakci, Arifur Rahman, Alexandros Kostopoulos,
    ¨ Agapi Mesodiakaki, Dimitris Tsolkas, Marten Ericson, Mauro Boldi, ˚ Mikko Uusitalo,
    Mir Ghoraishi, and Patrik Rugeland. The 6g architecture landscape - european perspective,
    2023. Working paper, Version v1.

    - <span id="page-3-1"></span>[2] D. Bhattacherjee, W. Aqeel, I. N. Bozkurt, A.
    Aguirre, B. Chandrasekaran, P. B. Godfrey, G. Laughlin, B. Maggs, and A. Singla.
    Gearing up for the 21st century space race, 2018.

    - <span id="page-3-7"></span>[3] Gohar Irfan Chaudhry, Esha Choukse, ´Inigo Goiri,
    Rodrigo Fonseca, ˜ Adam Belay, and Ricardo Bianchini. Towards resource-efficient
    compound ai systems, 2025.

    - <span id="page-3-2"></span>[4] Lingjiao Chen, Jared Quincy Davis, Boris Hanin,
    Peter Bailis, Matei Zaharia, James Zou, and Ion Stoica. Optimizing model selection
    for compound ai systems, 2025.

    - <span id="page-3-3"></span>[5] Jared Quincy Davis, Boris Hanin, Lingjiao Chen,
    Peter Bailis, Ion Stoica, and Matei Zaharia. Networks of networks: Complexity
    class principles applied to compound ai systems design, 2024.

    - <span id="page-3-4"></span>[6] Swayambhoo Jain, Ravi Raju, Bo Li, Zoltan Csaki,
    Jonathan Li, Kaizhao Liang, Guoyao Feng, Urmish Thakkar, Anand Sampat, Raghu Prabhakar,
    and Sumati Jairath. Composition of experts: A modular compound ai system leveraging
    large language models, 2024.

    - <span id="page-3-5"></span>[7] Maximilian Maresch and Stefan Nastic. Vate: Edge-cloud
    system for object detection in real-time video streams. In *2024 IEEE 8th International
    Conference on Fog and Edge Computing (ICFEC)*, pages 27–34, 2024.

    - <span id="page-3-6"></span>[8] Helena Zhang, Jakobi Haskell, and Yosef Frost.
    Flow state: Humans enabling ai systems to program themselves, 2025.'
- title: 'Edge-First Language Model Inference: Models, Metrics, and Tradeoffs'
  abstract: 'The widespread adoption of Language Models (LMs) across industries is
    driving

    interest in deploying these services across the computing continuum, from the

    cloud to the network edge. This shift aims to reduce costs, lower latency, and

    improve reliability and privacy. Small Language Models (SLMs), enabled by

    advances in model compression, are central to this shift, offering a path to

    on-device inference on resource-constrained edge platforms. This work examines

    the interplay between edge and cloud deployments, starting from detailed

    benchmarking of SLM capabilities on single edge devices, and extending to

    distributed edge clusters. We identify scenarios where edge inference offers

    comparable performance with lower costs, and others where cloud fallback

    becomes essential due to limits in scalability or model capacity. Rather than

    proposing a one-size-fits-all solution, we present platform-level comparisons

    and design insights for building efficient, adaptive LM inference systems

    across heterogeneous environments.'
  url: http://arxiv.org/abs/2505.16508v1
  keywords: Edge Computing, Language Model Inference, Small Language Models (SLMs),
    Distributed AI, Performance-Cost Tradeoffs.
  document: '# I. INTRODUCTION


    In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities
    in handling a wide array of language comprehension tasks, including answering
    complex queries, arithmetic reasoning, contextual understanding, sentiment analysis,
    text categorization, factual retrieval, and code generation. These models have
    revolutionized numerous applications by showcasing unprecedented accuracy and
    contextual awareness in natural language processing (NLP) tasks. However, their
    computational intensity—driven by self-attention mechanisms and large-scale training
    requirements—makes them best suited for cloud-hosted deployment, where storage,
    compute, and scalability are readily available.


    Despite their success, cloud-based LLMs come with significant drawbacks, including
    dependency on consistent network connectivity, latency issues, high operational
    costs, and privacy concerns. For instance, cloud-based inference introduces nonnegligible
    round trip times which may hinder real-time or mission-critical applications (see
    Section II for detailed measurements). Additionally, service availability can
    be impacted by rate limits or server overloads, and user prompts may be logged
    or used for model training, further amplifying privacy concerns [1]. These limitations
    are fueling a shift towards edge-centric solutions.


    Central to this shift is the growing interest in Small Language Models (SLMs),
    which, enabled by techniques like quantization, pruning, and distillation, allow
    language capabilities to be executed on resource-constrained edge devices such
    as smartphones [2], [3]. By reducing model size or precision [4]–[6], SLMs make
    on-device inference viable, addressing many of the challenges posed by centralized
    LLMs, particularly in terms of cost, latency, and data locality. However, SLMs
    still fall short of matching the performance and generality of their larger counterparts.
    This leads to a fundamental question:


    *How can SLMs be leveraged at edge to reduce cloud dependency while maintaining
    performance, privacy and responsiveness?*


    In this work, we adopt an edge-first perspective and explore how to deploy and
    evaluate SLMs across a distributed edge environment. We begin by benchmarking
    the performance, responsiveness, and energy consumption of opensourced SLMs on
    off-the-shelf edge devices and extend our investigation to a distributed edge
    cluster. We then assess conditions under which cloud fallback becomes necessary
    due to limitations in edge scalability. Rather than promoting a one-size-fits-all
    deployment, we focus on the tradeoffs across model types, hardware platforms,
    and performance-cost metrics—reflecting the core theme of this paper: *Models,
    Metrics, and Tradeoffs*.


    # II. BACKGROUND


    Conventional cloud-based LLM services such as Chat-GPT [7] offers users with highly
    accurate response, to demonstrate impressive capabilities in tasks. For example,
    ChatGPT-4o demonstrates impressive benchmark scores across various areas, including
    general knowledge (MMLU) , mathematical and logical reasoning (GSM8K) , coding
    (HumanEval) , while also supporting audio and visual multimodal capabilities [8].
    Additionally, cloud-based LLM inference offers significant advantages in terms
    of accessibility and scalability. By hosting language models in the cloud, users
    can utilize the service via internet connectivity, regardless of their geographical
    location. Moreover, cloud infrastructures offers dynamic resource allocation,
    automatically adapting compute resources to workload


    This paper has been accepted for publication and presentation at the 45th IEEE
    International Conference on Distributed Computing Systems (IEEE ICDCS 2025). The
    copyright will be transferred to IEEE upon publication in the conference proceedings.


    ![](_page_1_Figure_0.jpeg)


    Fig. 1: Top: Timeline of sub-4B SLMs designed for on-device deployments. Only
    the smallest version per model family is shown. Bottom: Average network latency
    to cloud-based LLM services (ChatGPT, Claude, Perplexity) from various global
    regions. Error bars indicate variability.


    demands without requiring user intervention, simplifying enduser experience.


    Despite their remarkable performance and benefits, cloudbased LLM inference comes
    with their limitations. Firstly, the massive computational requirements of cloud-based
    LLMs result in high operational costs and significant CO<sup>2</sup> footprint
    due to the extensive energy consumption. For instance, training large-scale models
    like GPT-3 is estimated to consume the equivalent of the annual electricity usage
    of a U.S. household for 120 years [9], which is also equivalent to 1287 MWh of
    electricity, and resulted in carbon emissions of 502 metric tons [10]. From an
    LLM inference perspective, based on a recent study [11], a similar size model
    BLOOM consumes on average 3.96 Wh per request, with energy usage scaling significantly
    as user demand increases.


    Second, long latency is a common issue in cloud-based systems, as data must be
    transmitted over the network, processed remotely, and then returned, introducing
    delays that impact real-time applications. Figure 1-bottom shows network latency
    measurement of various cloud-based LLM services. Popular LLM services, such as
    those provided by OpenAI, are hosted in centralized server locations (e.g., the
    US), which can impact the Quality of Experience (QoE) for users situated far from
    these hosting regions due to increased latency and reliability issues, with network
    latency ranging from around 48 milliseconds to hundreds of milliseconds depending
    on the service provider and user location. The Content Delivery Network (CDN)
    systems can help alleviate long latency delays by efficient network management,
    they cannot fully resolve the issue since the core processing of LLMs remains
    on remote infrastructure.


    ![](_page_1_Figure_5.jpeg)


    Fig. 2: Pass@1 accuracy of LMs with varying parameter sizes on OpenAI gsm8k. Blue
    bars indicate general-purpose models; the red bar (m-1.5) shows a 1.5B fine-tuned
    model, highlighting accuracy gains from specialization.


    Lastly, recent examples of prolonged service downtime [12] for popular LLM services
    highlight another critical limitation of centralized systems. This kind of outages
    can severely disrupt operations, especially for applications requiring high availability.


    On the other hand, edge-based SLM inference aims to execute the model directly
    on-devices, reducing the dependency on the cloud. In response to such enormous
    cost of cloud-based LLMs, there has been advancements in model compression techniques,
    such as quantization [4], pruning [5], and knowledge distillation [6], are helping
    to make edge AI a viable option for deploying SLMs on mobile [2] and at the edge.
    For instance, Gemma2 is available in multiple versions, with the smaller model
    consisting of 2B parameters [13], enabling local laptops and even mobiles to run
    SLMs. Figure 1-top shows recent trend of dramatic increased number of sub-4B language
    models over the past few years [14]. Notably, these newer models are also becoming
    multi-modal, with capabilities extending beyond text processing to include image
    to text capabilities (Llama3.2-vision [15]). This current shift reflects a broader
    industry trend toward developing versatile models that can handle a range of input
    types, enabling them to support complex application that require integrating information
    from multiple data sources– capability especially necessary at the network edge.


    # III. BENCHMARKING SLMS ON EDGE DEVICES


    # *A. General vs. Specialized Models*


    The challenge with deploying SLMs on mobile and edge devices lies in their limited
    computing capability, which directly restricts the loading of the weights to the
    memory. Since general purpose language models are inherently large, drastically
    reducing the model size leads to performance degradation. To evaluate the impact
    of the number of parameters on accuracy, we conducted an experiment using 100
    queries sampled from OpenAI/GSM8K math test dataset, applying the length-stratified
    sampling technique to ensure a well-distributed query set. We then assessed the
    performance of the Qwen-2.5 language models, a versatile, general-purpose pretrained
    model. Qwen-2.5 comes in few variations, differentiated by the number of parameters:
    0.5B, 1.5B, 3B, 7B, 14B,


    ![](_page_2_Figure_0.jpeg)


    Fig. 3: Top: Token generation speed (bars) and time-to-first-token (dots) on Jetson
    Nano (blue) and AGX (orange). The red dashed line marks human reading speed. Bottom:
    Power (dots) and energy per query (bars) on Nano and AGX.


    32B, and 72B. We evaluated the performance of the model by comparing the model
    response to the correct answer.


    In Figure 2, the x-axis represents the number of parameters (in billions), while
    the y-axis illustrates the pass@1 accuracy, which indicates the percentage of
    queries answered correctly on the first try. As shown through blue bar graphs,
    SLMs with fewer parameters exhibit a significant drop in accuracy. Interestingly,
    accuracy improves with more parameters but plateaus around 91%, where further
    increases yield only marginal gains.


    A SLM pretrained on a specific domain may outperform a general purpose model when
    handling domain specific queries. In other words, rather than relying on an all-purpose
    model, a model trained or fine tuned on a specific domain may exhibit less knowledge
    across other areas or capability but will achieve greater accuracy within its
    fine-tuned field. For example, BERT-based models like HealthBERT for healthcare
    [16], and FinBERT for finance [17], are known to have less than 110M parameters
    fine tuned exclusively on their respective domains. These models excel in classification,
    identification, and prediction tasks. Similarly, Code Llama [18], DeepSeekMath
    [19], and BioMistral [20] all having 7B parameters, demonstrate exceptional capabilities
    in their respective domains: Code Llama excels in code generation, DeepSeekMath
    in mathematical reasoning, and BioMistral in medical question-answering tasks.


    To assess this, we applied the same length-stratified sampling technique to the
    *Qwen-2-math:1.5B* model, which shares the same architecture as Qwen-2.5 but is
    optimized for answering math questions. We conducted the same set of 100 queries,
    and the results, shown in the red bar graph labeled *m-1.5* in Figure 2, demonstrate
    that despite having only 1.5 billion parameters, the model achieves accuracy comparable
    to that of 7B general-purpose variant while requiring just 19.8% of the model
    space.


    # *B. On-device SLM inference*


    Computation capability of device in edge environment is also an important aspect
    of running various SLMs at the edge. A key performance metric in this context
    is token generation speed (TGS), time-to-first-token (TTFT), along with average
    power usage and energy consumption per query, all of which can vary significantly
    based on the hardware platform. TGS reflects the rate of token production during
    SLM inference which provides insights into throughput efficiency, while TTFT measures
    the latency for generating the first token which highlights the responsiveness
    of the model. To evaluate the performance of different models, we used two edge
    devices—Jetson AGX Orin and Jetson Nano Orin, commonly found in edge AI applications
    like drones and small robots, along with a smartphone—to run several SLMs. All
    experiments on the Jetson used its on-board GPU to execute SLM query inference.


    Figure 3-top depicts the TGS and TTFT results for these two devices while running
    various SLMs. The x-axis represents the model''s parameter size, the left y-axis
    indicates TGS (in tokens per second), and the right y-axis indicates TTFT (in
    seconds). The orange indicates the Jetson Agx and blue indicates Jetson Nano.
    Notably, some models could not run on Jetson Nano due to memory limitations, and
    these cases are labeled as ''OOM'' (out of memory) in the figure. The horizontal
    red line in Figure 3-top represents the average human words per minute reading
    speed, providing a baseline for decent TGS.


    While there are some discrepancies, models with fewer parameters typically generate
    tokens faster and require less time to generate the first token. This is largely
    because they have fewer layers, which reduces the depth of sequential computations
    required for token generation (e.g., Qwen2.5:0.5b has 24 layers compared to Qwen2.5:32b
    with 64 layers). Additionally, embedding length plays a significant role in performance,
    as it directly affects the size of the matrices used in self-attention and feedforward
    layers. Larger embedding sizes result in more computational overhead and memory
    usage during inference, slowing down both token generation speed and time-to-first-token
    (e.g., Qwen2.5:0.5b has 896, while Qwen2.5:32b uses 5120). TTFT starts to increase
    as the number of model parameters approaches the device''s memory capacity (e.g.,
    7b for Nano and 27b for Agx), where memory constraints and computational overhead
    become significant bottlenecks.


    Figure 3-bottom depicts average power consumption and energy consumption for processing
    a query. The x-axis represents the model''s parameter size, the left y-axis represent
    average energy consumption (in Wh), right y-axis represent average power consumption
    (in W). Jetson Agx consumes 1.7–3.91 times more power than the Nano; however,
    the Nano requires 1.49 times more time to generate a response. This time difference
    increases to 3.25 times for larger models, such as those with 7 billion parameters,
    where Nano approaches its performance limits. Despite this, on-device inference
    demonstrates significantly higher efficiency compared to cloud-based models. For
    comparison, an average energy consumption per query for cloud-based 176B parameter
    model like BLOOM, is estimated to be more than 3.96Wh [11].


    Results from Figure 2, and Figure 3 highlight that in edge computing environments,
    rather than relying solely on generalpurpose shallow SLMs with limited performance,
    it becomes crucial to understand how to best deploy LMs across heterogeneous edge
    devices. Factors such as model specialization, device capacity, and workload characteristics
    have an important role in determining whether an edge deployment is sufficient,
    or whether cloud fallback becomes necessary. This motivates the need to define
    new metrics that can capture tradeoffs across multiple dimensions, which we explore
    next through the lens of energy, cost, inference latency, and response quality.


    # IV. EDGE EFFICIENCY, COST AND QUALITY TRADEOFFS


    While metrics like service latency, response quality, and cost per query are well
    established for evaluating cloudbased LLMs, hey must be redefined for edge SLM
    agents to reflect variations in language model architectures as well as the unique
    characteristics and constraints of edge devices. On constrained edge devices,
    energy consumption differs significantly (Figure 3-bottom), and response quality
    varies by model architecture (Figure 2). This implies that a combined metric would
    be necessary for capturing trade-offs between resource, cost, and response quality.
    This section introduces considerations for evaluation metric for SLM deployment
    at edge.


    From a system perspective, a holistic measurement of how the edge agent operates
    provides insights into its efficiency, hence we introduce an example metric Performance-Cost
    Ratio (PCR), a metric that evaluates how effectively resources are used to deliver
    both quality and speed. To achieve this, we begin by defining a utility function
    (U) that combines quality (Q) and responsiveness (R), assigning a flexible weight
    α to reflect their relative importance:


    $$\mathbf{U} = \alpha \cdot Q + (1 - \alpha) \cdot R \tag{1}$$


    Here, Q represents the quality score (e.g., accuracy, BLEU, ROUGE, etc) and R
    indicates a responsiveness measure (e.g., TTFT or TGS, etc). The parameter α controls
    the weight given to quality versus speed.


    Then, PCR can be defined as:


    $$\text{PCR}\_{\text{platform}} = \frac{\text{U}}{\text{CPR}} \tag{2}$$


    Here, Cost per Response (CPR) represents the monetary cost of generating a single
    response. For cloud-based LLMs, this cost is directly determined by usage fees
    tied to API calls, often based on the number of tokens generated.


    $$\text{CPR}\_{\text{cloud}} = \text{API usage in } \mathfrak{\mathfrak{g}} \tag{3}$$


    While edge operations do not incur direct service costs like cloud services, the
    device still consumes electricity, which can be translated into a monetary equivalent.
    Thus, for edge deployments, CPR can be calculated as:


    $$\text{CPR}\_{\text{edge}} = \text{Energy (kWh)} \times \text{Electricity Rate
    } (\sharp \text{kWh}) \quad (4)$$


    TABLE I: Comparison of platform metrics. CPR: Cost per request, Q: Query score
    (Accuracy), R: Responsiveness (Time to first token), PCR: Platform-cost ratio.


    | CPR (¢) | Q    | R    | PCR    |

    |---------|------|------|--------|

    | 1.65    | 0.97 | 0.71 | 0.51   |

    | 0.0041  | 0.91 | 0.28 | 145.12 |

    | 0.0017  | 0.80 | 0.33 | 332.35 |

    |         |      |      |        |


    To demonstrate the practical application of these metrics, we compare a cloud-based
    LLM platform using GPT-4 with an edge-based SLM platform using Qwen-2.5:7b deployed
    on a Jetson Agx device and Qwen-2.5:3b deployed on a Jetson Nano device. On the
    cloud, an average input of 48 tokens (from GSM8K dataset) and output of 249 tokens
    for GPT-4 costs approximately 1.65 cents per response (Equation 3). On the edge
    (Agx), energy consumption of 0.1646 Wh/query at ¢25/kWh results in a CPR of 0.0041
    cents/response (Equation 4), while on the edge Nano, 0.0687 Wh/query yields a
    CPR of 0.0017 cents/response. In terms of quality score Q , GPT-4 achieves 0.97
    on GSM8K, compared to 0.91 for Qwen-2.5:7B and 0.80 for Qwen-2.5:3B. Responsiveness,
    measured as TTFT ( R ), is 0.71s for GPT-4, 0.28s for the Agx, and 0.33s for the
    Nano.


    Finally, considering the PCR using α = 0.5 to weight quality and responsiveness
    equally, the combined utility scores (Equation 1) are 0.84 for GPT-4, 0.595 for
    Qwen-2.5:7B, and 0.565 for Qwen-2.5:3B. However, when normalized by their respective
    costs, the PCR values (Equation 2) are 0.51 for the cloud (GPT-4), 145.12 for
    the edge Agx (Qwen-2.5:7B), and 332.25 for the Nano (Qwen-2.5:3B), making the
    edge platforms significantly efficient.


    Despite the cloud''s higher quality and speed, the edge platform uses its resources
    extremely efficiently in this example, producing better combined performance per
    unit cost. This example demonstrates how these integrated metrics—CPR, and PCR—can
    illuminate nuanced trade-offs, enabling informed decisions about where and how
    to deploy language models for various use cases.


    # V. BALANCING EDGE LIMITS AND CLOUD RE-DIRECTIONS


    Although SLMs deployed at the edge (on-device) demonstrate strong performance
    under typical conditions, real-world deployments often encounter dynamic situations
    where resource scarcity, energy limitations, user preferences, or task complexity
    exceed the capabilities of on-device models. To manage these challenges, systems
    must not only balance local execution with selective offloading to cloud-based
    LLMs but also adopt mechanisms to optimize limited edge resources. Similar to
    how cloud-based LLM systems impose rate limits to manage resource usage and ensure
    fair access [21], edge deployments must implement comparable strategies, particularly
    given the strict compute, memory, and power constraints typical of edge devices.
    These challenges are further amplified in heterogeneous environments, where devices
    vary significantly in capability, available energy budgets, and inference performance.


    In such contexts, *edge rate-limiting strategies* serve a dual purpose: optimizing
    local resource usage while offloading excess demand to the cloud when necessary.
    To analyze the impact of these strategies on system behavior and cost, we developed
    a simulation framework that enables controlled experimentation under diverse conditions.


    We focus on a *sliding window approach*, which enforces rate limits over a rolling
    time window [22]. The window size is a critical parameter that governs system
    responsiveness and stability. A smaller window size enables the system to recover
    quickly from temporary overloads, making it particularly effective for handling
    bursty workloads (queries). In contrast, larger window sizes smooth out resource
    utilization over time but may lead to increased cloud redirection under sustained
    high-demand conditions.


    Given the heterogeneity of edge devices, an additional challenge lies in how requests
    are assigned to devices for processing. We investigate the impact of three device
    selection methods that determine how incoming requests are distributed across
    available resources.


    - *Random method* requests are assigned uniformly to devices without consideration
    of their capacity.

    - *Weighted method* improves this by selecting device''s probabilistically, favoring
    those with higher maximum capabilities.

    - *Load-Aware method* dynamically directs requests to devices with the most available
    resources at a given time, mitigating overload and enhancing system efficiency.


    We evaluate the sliding window strategy under realistic conditions with 30 users
    generating requests over an hour in one-minute steps. Workloads includes steady
    and bursty pattern, with bursts every 10 minutes with a higher probability of
    user activity. Token demands for each request are selected from a set of typical
    sizes (50, 100, 200, 300, and 500 tokens), weighted to favor smaller requests.
    The cloud cost for offloading requests is calculated as an average of OpenAI API
    pricing, considering both input and output token costs. The system comprises heterogeneous
    edge devices modeled on token generation performance (Figure 3) with defined token
    capacities and parallel request limits. Each device is assigned a maximum token
    capacity and a limit on parallel requests. Table II summarizes the experimental
    setup, including user activity, token demand distribution, and device capacities.
    Overall, our study considers four representative scenarios that combine workload
    types and device selection methods to understand their effect on request handling
    and cloud redirection. These scenarios include steady loads with random and capacityweighted
    device selection, as well as bursty loads with random and load-aware selection
    methods.


    TABLE II: Experimental Simulation Setup


    | Parameter                                                                                                                                                                   |
    Value                                                    |  |

    |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|--|

    | Number of Users                                                                                                                                                             |
    30                                                       |  |

    | Simulation Duration                                                                                                                                                         |
    1 hour (60 one-minute step)                              |  |

    | Sliding Window Size                                                                                                                                                         |
    5 minutes                                                |  |

    | Cloud Cost per Token                                                                                                                                                        |
    \$0.00000625 (average OpenAI API GPT-4 cost)             |  |

    | Steady Request Probability                                                                                                                                                  |
    30% per user per minute                                  |  |

    | Bursty Request Probability                                                                                                                                                  |
    80% per user per minute                                  |  |

    | Token Demand Distribution                                                                                                                                                   |
    50 (20%), 100 (30%), 200 (20%), 300 (20%), 500 (10%)     |  |

    | Node Types                                                                                                                                                                  |
    High-capacity (1), Medium-capacity (3), Low-capacity (4) |  |

    | High-capacity: 6 requests, 1800 tokens per minute<br>Node Capacity<br>Medium-capacity:
    3 requests, 900 tokens per minute<br>Low-capacity: 1 requests, 300 tokens per
    minute |                                                          |  |


    Figure 4 illustrates the impact of device selection strategies (Random, Weighted,
    and Load-Aware) under steady and bursty workloads when combined with the sliding
    window ratelimiting approach. The first plot (left) shows that under steady workloads,
    the *Weighted* processes the most requests but rejects fewer compared to *Random*,
    highlighting its efficiency in leveraging high-capacity device. For bursty workloads,
    the *Load-Aware* outperforms *Random* by reducing the number of rejected requests,
    demonstrating its ability to dynamically distribute requests to underutilized
    devices'' during high-demand periods.


    The second plot (middle) further quantifies the effect of device selection strategies
    on cloud redirection and costs. For steady workloads, *Weighted* significantly
    reduces tokens redirected to the cloud and associated costs compared to


    ![](_page_5_Figure_0.jpeg)


    Fig. 4: Performance comparison of device selection strategies under steady and
    bursty workloads with the sliding window ratelimiting approach. (Left) Number
    of requests processed and rejected. (Middle) Tokens redirected to the cloud and
    associated costs. (Right) Device utilization under bursty workloads, comparing
    the Random and Load-Aware strategies.


    *Random*. In bursty workloads, the *Load-Aware* leads to the lowest redirection
    and costs, highlighting its effectiveness in minimizing reliance on cloud resources
    during demand spikes.


    The third plot (right) focuses on device utilization under bursty workloads. The
    *Random* strategy results in an imbalanced load across devices, with high variability
    in utilization, particularly underutilizing low-power devices. In contrast, the
    *Load-Aware* achieves a more even distribution of load across the devices, reflected
    by its lower variability and closer alignment to the average utilization. This
    balanced load improves overall system efficiency and reduces cloud redirection.


    These results reflect the specific workload characteristics and the types of devices
    used in this study and experimental setup, including heterogeneous edge devices
    with varying capacities. However, the framework is flexible and allows testing
    of alternative setups, such as different device configurations, other rate-limiting
    strategies like Fixed Window or Token Bucket, or more complex workloads. To support
    further research and real-world experimentation, we will release the code for
    the simulator, the dataset, the classifier (discussed in previous sections), and
    the deployment code for running the system on a real-world cluster of edge devices,
    upon acceptance of this paper.


    # VI. CONCLUSION


    In this work, we assessed the limitations of cloud-based LLM inference and explored
    an edge-first perspective for deploying SLMs. Through empirical evaluation on
    off-theshelf devices and distributed edge setups, we highlighted scenarios where
    SLMs offer competitive performance, and others where cloud fallback is still necessary.
    We introduced new metrics to capture tradeoffs across quality, latency, energy,
    and cost. Rather than promoting a one-size-fits-all solution, our results emphasize
    the importance of metric-driven decisions for efficient and adaptive LM inference
    across heterogeneous environments.


    # REFERENCES


    - [1] OpenAI, "Data usage for consumer services faq," https://help.openai.com/en/articles/7039943-data-usage-for-consumerservices-faq,
    accessed: 2024-11-01.

    - [2] S. Laskaridis, K. Kateveas, L. Minto, and H. Haddadi, "Melting point: Mobile
    evaluation of language transformers," *arXiv preprint arXiv:2403.12844*, 2024.

    - [3] D. Xu, W. Yin, X. Jin, Y. Zhang, S. Wei, M. Xu, and X. Liu, "Llmcad: Fast
    and scalable on-device large language model inference," *arXiv preprint arXiv:2309.04255*,
    2023.

    - [4] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang,
    C. Gan, and S. Han, "Awq: Activation-aware weight quantization for on-device llm
    compression and acceleration," *Proceedings of Machine Learning and Systems*,
    vol. 6, pp. 87–100, 2024.

    - [5] X. Ma, G. Fang, and X. Wang, "Llm-pruner: On the structural pruning of large
    language models," *Advances in neural information processing systems*, vol. 36,
    pp. 21 702–21 720, 2023.

    - [6] Y. Gu, L. Dong, F. Wei, and M. Huang, "Minillm: Knowledge distillation of
    large language models," in *The Twelfth International Conference on Learning Representations*,
    2024.

    - [7] OpenAI, "Chatgpt: A conversational ai service," OpenAI ChatGPT, 2023, https://openai.com/chatgpt.
    [Online]. Available: https://openai.com/chatgpt

    - [8] ——, "Gpt-4o: Enhanced performance across diverse tasks," https://platform.openai.com/docs/models/gpt-4o,
    2024, accessed: 2024-11-02.

    - [9] Association of Data Scientists, "How much energy do llms consume? unveiling
    the power behind ai," 2024, accessed: 2024-09- 13. [Online]. Available: https://adasci.org/how-much-energy-do-llmsconsume-unveiling-the-power-behind-ai/

    - [10] Columbia University Earth Institute, "Ai''s growing carbon footprint,"
    2023, accessed: 2024-09-13. [Online]. Available: https://news.climate.columbia.edu/2023/06/09/ais-growing-carbonfootprint/

    - [11] A. S. Luccioni, S. Viguier, and A.-L. Ligozat, "Estimating the carbon footprint
    of bloom, a 176b parameter language model," *Journal of Machine Learning Research*,
    vol. 24, no. 253, pp. 1–15, 2023.

    - [12] OpenAI. (2024, Dec.) Incident report: Service down on december 12, 2024.
    Accessed on 2024-12-18. [Online]. Available: https://status.openai.com/incidents/ctrsv3lwd797

    - [13] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju,
    L. Hussenot, T. Mesnard, B. Shahriari, A. Rame´ *et al.*, "Gemma 2: Improving
    open language models at a practical size," *arXiv preprint arXiv:2408.00118*,
    2024.

    - [14] Z. Lu, X. Li, D. Cai, R. Yi, F. Liu, X. Zhang, N. D. Lane, and M. Xu, "Small
    language models: Survey, measurements, and insights," *arXiv preprint arXiv:2409.15790*,
    2024.

    - [15] M. AI, "Llama models," https://github.com/meta-llama/llama-models, 2024,
    accessed: 2024-11-09.

    - [16] Y. Kim, X. Xu, D. McDuff, C. Breazeal, and H. W. Park, "Health-llm: Large
    language models for health prediction via wearable sensor data," *arXiv preprint
    arXiv:2401.06866*, 2024.

    - [17] A. H. Huang, H. Wang, and Y. Yang, "Finbert: A large language model for
    extracting information from financial text," *Contemporary Accounting Research*,
    vol. 40, no. 2, pp. 806–841, 2023.

    - [18] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, R. Sauvestre, T. Remez *et al.*, "Code llama: Open foundation models for
    code," *arXiv preprint arXiv:2308.12950*, 2023.

    - [19] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y.
    Li, Y. Wu *et al.*, "Deepseekmath: Pushing the limits of mathematical reasoning
    in open language models," *arXiv preprint arXiv:2402.03300*, 2024.

    - [20] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and R. Dufour,
    "Biomistral: A collection of open-source pretrained large language models for
    medical domains," *arXiv preprint arXiv:2402.10373*, 2024.

    - [21] OpenAI, "Rate limits openai," https://platform.openai.com/docs/guides/ratelimits,
    2024, accessed: 2024-09-12.

    - [22] W. Iqbal, J. L. Berral, D. Carrera *et al.*, "Adaptive sliding windows
    for improved estimation of data center resource utilization," *Future Generation
    Computer Systems*, vol. 104, pp. 212–224, 2020.'
- title: Recursive Offloading for LLM Serving in Multi-tier Networks
  abstract: 'Heterogeneous device-edge-cloud computing infrastructures have become
    widely

    adopted in telecommunication operators and Wide Area Networks (WANs), offering

    multi-tier computational support for emerging intelligent services. With the

    rapid proliferation of Large Language Model (LLM) services, efficiently

    coordinating inference tasks and reducing communication overhead within these

    multi-tier network architectures becomes a critical deployment challenge.

    Existing LLM serving paradigms exhibit significant limitations: on-device

    deployment supports only lightweight LLMs due to hardware constraints, while

    cloud-centric deployment suffers from resource congestion and considerable

    prompt communication overhead caused by frequent service requests during peak

    periods. Although the model-cascading-based inference strategy adapts better to

    multi-tier networks, its reliance on fine-grained, manually adjusted thresholds

    makes it less responsive to dynamic network conditions and varying task

    complexities. To address these challenges, we propose RecServe, a recursive

    offloading framework tailored for LLM serving in multi-tier networks. RecServe

    integrates a task-specific hierarchical confidence evaluation mechanism that

    guides offloading decisions based on inferred task complexity in progressively

    scaled LLMs across device, edge, and cloud tiers. To further enable intelligent

    task routing across tiers, RecServe employs a sliding-window-based dynamic

    offloading strategy with quantile interpolation, enabling real-time tracking of

    historical confidence distributions and adaptive offloading threshold

    adjustments. Experiments on eight datasets demonstrate that RecServe

    outperforms CasServe in both service quality and communication efficiency, and

    reduces the communication burden by over 50\% compared to centralized

    cloud-based serving.'
  url: http://arxiv.org/abs/2505.16502v1
  keywords: Large language models, services computing, task offloading, edge-cloud
    collaboration, communication efficiency
  document: "## I. INTRODUCTION\n\nT HE rapid proliferation of the Internet of Things\
    \ (IoT), Mobile Edge Computing (MEC), and Artificial Intelligence (AI) has driven\
    \ unprecedented demand for intelligent services. To accommodate the computational\
    \ intensity of these applications, telecommunication operators and Wide Area Networks\
    \ (WANs) increasingly adopt multi-tier heterogeneous computing infrastructures\
    \ consisting of end devices, edge computing nodes, and cloud data centers [\\\
    [1\\]](#page-14-0). This hierarchical architecture exhibits an inverse relationship\
    \ between computational capacity and user proximity: end devices directly interact\
    \ with user data but typically possess limited computing power; edge nodes offer\
    \ moderate processing capabilities with low-latency connectivity; cloud data centers\
    \ host powerful CPU/GPU clusters but are situated remotely from data sources [\\\
    [2\\]](#page-14-1), [\\[3\\]](#page-14-2). Such multi-tier network systems offer\
    \ remarkable flexibility for optimizing resource allocation, minimizing service\
    \ latency, and enhancing user experience.\n\nAmong the emerging intelligent applications,\
    \ services powered by Large Language Models (LLMs) have demonstrated remarkable\
    \ impacts. Built upon Transformer [\\[4\\]](#page-14-3) architecture and self-attention\
    \ [\\[5\\]](#page-14-4) mechanism, LLMs effectively capture long-range contextual\
    \ dependencies within text, demonstrating capabilities that rival or even surpass\
    \ human performance across tasks such as code generation, multi-turn conversations,\
    \ and mathematical reasoning [\\[6\\]](#page-14-5). From ChatGPT [\\[7\\]](#page-14-6),\
    \ [\\[8\\]](#page-14-7) leading the global wave of AI applications to DeepSeek-R1\
    \ [\\[9\\]](#page-14-8) redefining the landscape of reasoning LLMs, LLMs have\
    \ emerged as core engines driving next-generation intelligent interaction, content\
    \ creation, and decision-making processes.\n\nAs user reliance on LLM services\
    \ continues to grow, efficient orchestration and intelligent task allocation across\
    \ multi-tier computing infrastructures become increasingly critical deployment\
    \ challenges. Existing LLM serving paradigms, however, present significant limitations.\
    \ On-device deployments offer enhanced privacy protection and eliminate transmission\
    \ overhead, but are constrained by the limited local computational capabilities,\
    \ supporting only lightweight LLMs [\\[10\\]](#page-14-9), [\\[11\\]](#page-14-10),\
    \ thereby restricting service quality for demanding tasks such as long-text generation\
    \ and sophisticated reasoning. Centralized cloud-based deployment strategies [\\\
    [12\\]](#page-14-11), [\\[13\\]](#page-14-12) can deliver high-quality inference\
    \ services but necessitate the transmission of full request prompts and contextual\
    \ information across WANs to remote servers, potentially creating severe resource\
    \ bottlenecks and network congestion during peak periods due to frequent service\
    \ calls [\\[14\\]](#page-14-13), [\\[15\\]](#page-15-0). While the model cascading-based\
    \ multi-tier inference approach [\\[16\\]](#page-15-1) can partially mitigate\
    \ these issues by distributing inference tasks across device, edge, and cloud\
    \ nodes based on predefined hyperparameter thresholds. However, these static thresholds\
    \ require fine-grained manual calibration based on task types, deployed LLMs,\
    \ and network conditions, severely limiting flexibility and adaptability to dynamic\
    \ real-world scenarios.\n\nTo address these challenges, we propose RecServe, a\
    \ recursive offloading framework for multi-tier networks designed to dynamically\
    \ allocate LLM inference tasks based on customized confidence feedback. RecServe\
    \ is structured around three core components: (1) a hierarchical LLM deployment\
    \ framework that positions progressively capable LLMs across device, edge and\
    \ cloud tiers; (2) task-specific confidence evaluation mechanisms tailored to\
    \ different NLP task categories; and (3) a dynamic offloading strategy leverages\
    \ quantile interpolation over a sliding historical confidence window to adaptively\
    \ determine offloading decision thresholds. Upon receiving user requests, RecServe\
    \ first performs local inference and computes a task-specific confidence score.\
    \ Once this score surpasses the dynamically determined threshold, the local result\
    \ is immediately returned. Otherwise, the task is recursively offloaded to the\
    \ next higher tier until either a sufficiently confident result is obtained or\
    \ the task reaches the cloud tier. This design ensures simpler tasks are handled\
    \ locally on devices or at the edge, reserving cloud resources for computationally\
    \ demanding requests. Our theoretical analysis identifies ranges of offloading\
    \ parameters under which RecServe achieves reduced expected communication burden\
    \ and computational cost compared to centralized cloud-based deployment. Experiments\
    \ across eight datasets confirm that RecServe significantly outperforms the existing\
    \ multi-tier LLM serving method, while reducing communication burden by more than\
    \ 50% compared to cloud-centric deployment.\n\nIn summary, the contributions of\
    \ this paper are as follows:\n\n- We propose RecServe, a recursive offloading\
    \ framework for LLM serving in multi-tier networks, which progressively offloads\
    \ complex inference tasks to highercapability tiers.\n- We design a dynamic strategy\
    \ based on adaptive confidence thresholds, leveraging differentiated confidence\
    \ evaluations tailored to distinct NLP tasks and quantile interpolation over historical\
    \ confidence queue.\n- We develop a theoretical model for RecServe, establishing\
    \ hyperparameter bounds for reducing expected communication burden and computational\
    \ costs, respectively.\n- We conduct extensive experiments on eight datasets.\
    \ Results show that RecServe outperforms CasServe in both service quality and\
    \ communication burden, and achieves over 50% reduction in communication burden\
    \ compared to cloud-based deployment.\n\n## II. PRELIMINARIES\n\nThis section\
    \ provides foundational knowledge necessary to understand the proposed framework.\
    \ We will elaborate on\n\nTABLE I MAIN NOTATIONS WITH DESCRIPTIONS.\n\n<span id=\"\
    page-1-0\"></span>\n\n| Notation    | Description                            \
    \       |  |  |  |  |  |\n|-------------|-----------------------------------------------|--|--|--|--|--|\n\
    |             | Total number of tiers in hierarchical         |  |  |  |  |  |\n\
    | n           | deployment                                    |  |  |  |  |  |\n\
    | Mi          | LLM deployed on tier i                        |  |  |  |  |  |\n\
    | τ           | LLM task type                                 |  |  |  |  |  |\n\
    | x           | Input text sequence                           |  |  |  |  |  |\n\
    | y           | Prediction generated by LLM                   |  |  |  |  |  |\n\
    | HM,τ        | Historical confidence queue for LLM M         |  |  |  |  |  |\n\
    |             | and task type τ                               |  |  |  |  |  |\n\
    | k           | Maximum capacity of the historical            |  |  |  |  |  |\n\
    |             | confidence queue                              |  |  |  |  |  |\n\
    | CM,τ        | Confidence score from LLM M with task         |  |  |  |  |  |\n\
    |             | type τ                                        |  |  |  |  |  |\n\
    | PM(y = i x) | Softmax probability assigned to class i       |  |  |  |  |  |\n\
    |             | given input x                                 |  |  |  |  |  |\n\
    | zi          | Logit corresponding to class i                |  |  |  |  |  |\n\
    |             | Perplexity of the output sequence y           |  |  |  |  |  |\n\
    | PPLM(y x)   | given input x by LLM M                        |  |  |  |  |  |\n\
    | β           | Offloading parameter used for                 |  |  |  |  |  |\n\
    |             | dynamic threshold computation                 |  |  |  |  |  |\n\
    | TM,τ        | Dynamic offloading threshold for LLM M        |  |  |  |  |  |\n\
    |             | and task type τ                               |  |  |  |  |  |\n\
    | D           | Recursive offloading policy                   |  |  |  |  |  |\n\
    | M∗          | Final LLM that completes inference via D      |  |  |  |  |  |\n\
    | pi          | Expected probability that a task is offloaded |  |  |  |  |  |\n\
    |             | from LLM Mi<br>to Mi+1                        |  |  |  |  |  |\n\
    |             | Cumulative Distribution Function (CDF) of     |  |  |  |  |  |\n\
    | FMi,τ       | confidence scores for Mi<br>and task type τ   |  |  |  |  |  |\n\
    | A           | Availability check function                   |  |  |  |  |  |\n\
    | Dut         | Recursive offloading policy with              |  |  |  |  |  |\n\
    |             | unavailability tolerance                      |  |  |  |  |  |\n\
    | M∗∗         | Final LLM that completes inference            |  |  |  |  |  |\n\
    |             | considering node unavailability via Dut       |  |  |  |  |  |\n\
    \nthe basics of LLM, the architecture of multi-tier networks, and the concept\
    \ of task offloading. For clarity, a list of the main notations and descriptions\
    \ used throughout this paper is provided in Table [I.](#page-1-0)\n\n## *A. Large\
    \ Language Models*\n\nIn recent years, LLMs have demonstrated remarkable capabilities\
    \ in natural language understanding and generation, primarily driven by the Transformer\
    \ module [\\[4\\]](#page-14-3) and the selfattention [\\[5\\]](#page-14-4) mechanism.\
    \ Mainstream approaches [\\[9\\]](#page-14-8), [\\[17\\]](#page-15-2) typically\
    \ utilize a stack of attention layers augmented with positional encodings to form\
    \ the backbone of LLMs, with the core computation represented as:\n\n$$\\text{Attention}(\\\
    mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Softmax}\\left(\\frac{\\mathbf{Q}\\\
    mathbf{K}^{\\top}}{\\sqrt{d\\_k}}\\right) \\cdot \\mathbf{V}, \\qquad (\\text{l})$$\n\
    \nwhere Q, K, and V denote the query, key, and value matrices, respectively. d<sup>k</sup>\
    \ is a scaling factor used to stabilize gradients during the training process.\
    \ To facilitate coherent and context-aware understanding and generation during\
    \ inference, LLMs typically adopt pretraining objectives such as Masked Language\
    \ Modeling [\\[18\\]](#page-15-3), [\\[19\\]](#page-15-4) or Causal Language Modeling\
    \ (CLM) [\\[4\\]](#page-14-3), [\\[20\\]](#page-15-5). Taking widely adopted CLM\
    \ as an example, an LLM M optimizes its model parameters θ by maximizing the probability\
    \ of generating the current token conditioned on all previous tokens:\n\n$$\\\
    max\\_{\\theta} \\text{P}\\_M(x\\_1, \\dots, x\\_m) = \\max\\_{\\theta} \\prod\\\
    _{i=1}^m \\text{P}\\_M(x\\_i \\mid x\\_1, \\dots, x\\_{i-1}),\\tag{2}$$\n\nwhere\
    \ x = (x1, x2, . . . , xm) is the input text sequence, x<sup>i</sup> represents\
    \ the i-th token in x, and m denotes the sequence length. Besides, P(x<sup>i</sup>\
    \ | x1, . . . , xi−1) represents the probability of predicting the i-th token\
    \ given the preceding tokens. Accordingly, the training objective of CLM is to\
    \ minimize the negative log-likelihood loss L<sup>M</sup> as follows:\n\n$$\\\
    min\\_{\\theta} \\ \\mathcal{L}\\_M = \\min\\_{\\theta} - \\sum\\_{i=1}^n \\log\
    \ \\mathbf{P}\\_M(x\\_i \\mid x\\_1, \\dots, x\\_{i-1}). \\tag{3}$$\n\nBeyond\
    \ pretraining, LLMs typically undergo additional processes such as task-specific\
    \ fine-tuning [\\[21\\]](#page-15-6), [\\[22\\]](#page-15-7) or reinforcement\
    \ learning from human feedback [\\[23\\]](#page-15-8), bridging the gap between\
    \ general language modeling capabilities and practical application requirements.\
    \ During inference, LLMs exhibit the ability to handle multiple task formats [\\\
    [24\\]](#page-15-9), which can be broadly categorized into Seq2Class (Sequence-to-Class)\
    \ and Seq2Seq (Sequence-to-Sequence) categories. The former refers to tasks that\
    \ map an input sequence to a single class label, and the latter involves generating\
    \ an output sequence conditioned on the input sequence. These capabilities empower\
    \ LLMs to serve as foundational models across a wide range of downstream tasks\
    \ in real-world applications.\n\n## *B. Multi-tier Networks*\n\nMulti-tier network\
    \ integrates heterogeneous computing resources through a vertically tiered design\
    \ paradigm, which typically organizes computing nodes into the device tier, the\
    \ edge node tier, and the cloud data center tier [\\[3\\]](#page-14-2), [\\[25\\\
    ]](#page-15-10)–[\\[27\\]](#page-15-11). This hierarchical division is motivated\
    \ by significant differences across tiers in computational capacity and communication\
    \ latency [\\[28\\]](#page-15-12), [\\[29\\]](#page-15-13). At the device tier,\
    \ embedded chips with limited computational power are deployed, primarily suited\
    \ for sensor data collection and lightweight inference tasks. The edge node tier\
    \ incorporates more powerful computing infrastructures that support real-time\
    \ service coordination and handle moderately complex computational workloads.\
    \ In contrast, the cloud data center tier is equipped with powerful GPU or TPU\
    \ clusters but with relatively high communication latencies.\n\nIn the context\
    \ of LLM serving, multi-tier networks can potentially support distributed inference\
    \ and enable dynamic communication-precise trade-offs via intelligent tiered deployment\
    \ strategies. By integrating flexible task-routing pathways within this architecture,\
    \ on-demand LLM inference workload distribution becomes feasible. As a result,\
    \ most lightweight requests can be effectively processed at the device or edge\
    \ tiers, while only a small subset of computationally intensive tasks necessitate\
    \ cloud-based computation. Such hierarchical cooperation substantially prevents\
    \ congestion at the cloud tier, reduces data transmission across public networks,\
    \ and minimizes communication overheads with maintenance on service quality.\n\
    \n## *C. Task Offloading*\n\nTask offloading refers to the process of migrating\
    \ computational tasks from local nodes to remote ones to optimize system performance\
    \ and resource utilization [\\[30\\]](#page-15-14), [\\[31\\]](#page-15-15). In\
    \ a typical two-tier computing system, the objective of task offloading Csystem\
    \ can be formulated as an optimization problem minimizing total execution cost:\n\
    \n$$\\min\\_{\\tau \\in \\{0, 1\\}} \\mathcal{C}\\_{system} = (1 - \\tau) \\cdot\
    \ \\mathcal{C}\\_{local} + \\tau \\cdot (\\mathcal{C}\\_{comm} + \\mathcal{C}\\\
    _{remove}), \\tag{4}$$\n\nwhere τ is the offloading decision variable, with τ\
    \ = 0 denoting local execution and τ = 1 representing offloading to a remote node.\
    \ Clocal denotes the cost of local execution, Ccomm represents the communication\
    \ overhead incurred during offloading, and Cremote corresponds to the cost of\
    \ remote execution. These costs are primarily determined by task characteristics,\
    \ computational capacity, and network conditions [\\[32\\]](#page-15-16). Such\
    \ a formulation encapsulates the core principle of task offloading: dynamically\
    \ balancing between local execution and remote execution plus communication, and\
    \ selecting the path with the lower overall cost.\n\nIn practical LLM serving\
    \ scenarios, Clocal corresponds to the accuracy loss incurred by executing a lightweight\
    \ LLM locally. Cremote reflects the computational resource consumption required\
    \ to process the task using a large LLM on a remote node, and is closely related\
    \ to inference quality. Ccomm is heavily influenced by transmitted sequence length.\
    \ Based on this system model, task-aware or network-aware strategies can be employed\
    \ to dynamically determine the offloading path, facilitating collaborative task\
    \ execution and resource adaptation in multi-tier networks.\n\n## III. RECURSIVE\
    \ OFFLOADING\n\nThis section details the proposed RecServe framework. We will\
    \ begin with an overview of the RecServe framework, followed by explanations of\
    \ its core components: the historical confidence queue, task-specific confidence\
    \ evaluation mechanisms, and the recursive offloading strategy with dynamic decision\
    \ thresholds. Finally, we provide the formal description of RecServe.\n\n## *A.\
    \ Framework Overview*\n\nThe schematic diagram of RecServe is illustrated in Fig.\
    \ [1,](#page-3-0) which spans across end devices, edge nodes, and cloud data centers.\
    \ Specifically, RecServe deploys LLMs with progressively increasing model sizes\
    \ and inference capabilities at the end, edge, and cloud, respectively, to align\
    \ with the heterogeneous computational resources of different tiers. Each computational\
    \ node maintains a historical confidence queue to store the computed confidence\
    \ scores previously processed by its local LLM. To support a broad range of Natural\
    \ Language Processing (NLP) tasks, RecServe incorporates task-specific\n\n![](_page_3_Figure_1.jpeg)\n\
    \nFig. 1. Schematic diagram of RecServe.\n\nconfidence evaluation mechanisms:\
    \ peak softmax probability for Sequence to Class (Seq2Class) tasks and normalized\
    \ perplexity for Sequence to Sequence (Seq2Seq) tasks. Based on a dynamic offloading\
    \ strategy relying on a sliding window of historical confidence scores, RecServe\
    \ continuously tracks the distribution of past confidence levels and dynamically\
    \ adjusts its offloading threshold. This adaptive mechanism enables each node\
    \ to decide whether to process a task locally or escalate it to a higher-tier\
    \ node based on the quality of the current inference. As only a small subset of\
    \ complex tasks are transmitted over the network, the majority of tasks are processed\
    \ locally without cross-tier communication. This design enables RecServe to distribute\
    \ the inference load across different tiers, reducing pressure on cloud data centers\
    \ and mitigating resource congestion during peak periods, while also achieving\
    \ an effective balance between communication burden and service performance.\n\
    \nIn the following subsections, we will provide a detailed explanation of the\
    \ three core components of RecServe: (1) the historical confidence queue, (2)\
    \ the task-specific confidence evaluation mechanism, and (3) the recursive offloading\
    \ strategy. A formal description of the execution procedure of RecServe is presented\
    \ at the end of this section.\n\n## *B. Historical Confidence Queue*\n\nBy maintaining\
    \ an up-to-date confidence history, each computational node can effectively analyze\
    \ historical inference quality, estimate the complexity distribution of incoming\
    \ tasks, and dynamically adjust offloading thresholds to make informed decisions\
    \ about whether to handle tasks locally or forward <span id=\"page-3-0\"></span>them\
    \ to higher-tier nodes. To facilitate this, we design a sliding-window-based historical\
    \ confidence queue to capture the confidence information of LLMs in the past inference\
    \ process, which can be formally defined as:\n\n<span id=\"page-3-1\"></span>\n\
    $$H\\_{M,\\tau} = \\{C\\_{M,\\tau}(x\\_1), C\\_{M,\\tau}(x\\_2), \\dots, C\\_{M,\\\
    tau}(x\\_k)\\},\\quad(\\mathfrak{S})$$\n\nwhere HM,τ denotes the historical confidence\
    \ queue for LLM M and task type τ , CM,τ (xi) denotes the confidence score for\
    \ input x<sup>i</sup> , and k represents the maximum queue capacity. Each node\
    \ initializes its historical confidence queue as empty and maintains it following\
    \ the First-In-First-Out (FIFO) policy, so as to ensure the queue reflects the\
    \ most recent k predictions:\n\n<span id=\"page-3-2\"></span>\n$$\\begin{cases}\
    \ H\\_{M,\\tau}^{\\text{new}} = \\\\ H\\_{M,\\tau} \\cup \\{ C\\_{M,\\tau}(x\\\
    _{\\text{new}}) \\}, & \\text{if } |H\\_{M,\\tau}| < k \\\\ (H\\_{M,\\tau} - \\\
    { C\\_{M,\\tau}(x\\_1) \\}) \\cup \\{ C\\_{M,\\tau}(x\\_{\\text{new}}) \\}, &\
    \ \\text{if } |H\\_{M,\\tau}| = k, \\end{cases} \\tag{6}$$\n\nwhere xnew denotes\
    \ the latest input and Hnew M,τ is the updated queue.\n\n## *C. Task-Specific\
    \ Confidence Evaluation*\n\nThe confidence measure associated with an LLM's output\
    \ should be adapted to the characteristics of the current inference task, as it\
    \ plays a critical role in determining the reliability and suitability of offloading\
    \ decisions. Accordingly, we employ task-specific confidence evaluation mechanisms\
    \ for Seq2Class and Seq2Seq tasks in LLM serving.\n\nFor Seq2Class tasks, we define\
    \ output confidence as the maximum softmax probability. First, given an input\
    \ x, the softmax probability assigned to each class i is computed as follows:\n\
    \n<span id=\"page-4-0\"></span>\n$$P\\_M(y=i|x) = \\frac{\\exp(z\\_i)}{\\sum\\\
    _{j=1}^C \\exp(z\\_j)},\\tag{7}$$\n\nwhere z<sup>i</sup> is the logit corresponding\
    \ to class i output by LLM M, and C is the total number of classes. Then, the\
    \ confidence score can be formulated as follows:\n\n<span id=\"page-4-1\"></span>\n\
    $$C\\_{M, \\text{Seq2Class}}(x) = \\max\\_{i \\in [1, C]} P\\_M(y = i | x). \\\
    tag{8}$$\n\nIn Seq2Class scenarios, the primary goal is to identify the most probable\
    \ category for a given input text. Therefore, the maximum softmax probability\
    \ naturally provides a direct measure of the LLM's certainty in its prediction\
    \ [\\[33\\]](#page-15-17). A higher confidence score indicates greater LLM certainty,\
    \ thereby reducing the necessity of offloading the task to alternative resources.\n\
    \nFor Seq2Seq tasks, we adopt perplexity [\\[4\\]](#page-14-3) with customized\
    \ normalization as the confidence metric, aiming to assess the coherence and consistency\
    \ of the entire token sequence. Consider the output text y as a generated sequence\
    \ comprising individual tokens:\n\n<span id=\"page-4-2\"></span>\n$$y = (t\\_1,\
    \ t\\_2, \\dots, t\\_L) \\tag{9}$$\n\nwhere t<sup>i</sup> denotes the i-th token\
    \ generated by the LLM, and L is the sequence length. The perplexity is computed\
    \ based on the average negative log-likelihood of the token sequence:\n\n<span\
    \ id=\"page-4-3\"></span>\n$$\\text{PPL}\\_M(y|x) = \\exp\\left(-\\frac{1}{L}\
    \ \\sum\\_{i=1}^{L} \\log P\\_M(t\\_i|t\\_1, \\dots, t\\_{i-1}, x)\\right),\\\
    tag{10}$$\n\nwhere PM(t<sup>i</sup> |t1, . . . , ti−1, x) denotes the softmax\
    \ probability assigned by LLM M to token t<sup>i</sup> given the input x and the\
    \ previous generated tokens. Specifically,\n\n<span id=\"page-4-4\"></span>\n\
    $$P\\_M(t\\_i|t\\_1, \\dots, t\\_{i-1}, x) = \\frac{\\exp(z\\_{t\\_i})}{|V|},\
    \ \\qquad \\text{(11)}$$\n\n$$\\sum\\_{j=1}^{|V|} \\exp(z\\_{V\\_j})$$\n\nwhere\
    \ z<sup>t</sup><sup>i</sup> is the logit associated with token t<sup>i</sup> ,\
    \ and V is the vocabulary set. Since lower perplexity values reflect higher confidence\
    \ and greater coherence in sequence generation, we normalize perplexity into a\
    \ confidence score in the range (0, 1) as follows:\n\n<span id=\"page-4-5\"></span>\n\
    $$C\\_{M, \\text{Seq2seq}}(x) = \\frac{1}{1 + \\text{PPL}\\_M(y|x)}.\\tag{12}$$\n\
    \nThis normalization step provides an intuitive interpretation of confidence scores,\
    \ where larger values indicate stronger LLM certainty.\n\n# *D. Recursive Offloading\
    \ with Dynamic Decision Thresholds*\n\nBuilding upon the historical confidence\
    \ queue and taskadaptive confidence evaluation, RecServe implements a strategy\
    \ with dynamic decision thresholds. Unlike the prior method that depends on fixed\
    \ thresholds and tier-specific hyperparameters [\\[16\\]](#page-15-1), our approach\
    \ dynamically adjusts the offloading threshold based on statistical characteristics\
    \ derived from the historical confidence queue, effectively reducing unnecessary\
    \ communication while preserving inference quality. Formally, given the historical\
    \ confidence queue HM,τ , we first obtain a sorted version of this queue in ascending\
    \ order:\n\n<span id=\"page-4-6\"></span>\n$$H\\_{M,\\tau}^{\\text{sorted}} =\
    \ \\{c\\_{(1)}, c\\_{(2)}, \\dots, c\\_{(k)}\\} \\tag{13}$$\n\nwhere c(i) denotes\
    \ the i-th confidence score in the sorted list, satisfying:\n\n<span id=\"page-4-7\"\
    ></span>\n$$c\\_{(1)} \\le c\\_{(2)} \\le \\dots \\le c\\_{(k)} \\tag{14}$$\n\n\
    Based on the sorted queue, RecServe defines the dynamic threshold TM,τ (β) as\
    \ the β-th quantile of the confidence distribution in HM,τ , with β ∈ (0, 1).\
    \ Specifically, the threshold computation employs linear interpolation as follows:\n\
    \n<span id=\"page-4-8\"></span>\n$$T\\_{M,\\tau}(\\beta) = c\\_{(\\lfloor r \\\
    rfloor + 1)} \\cdot (1 - (r - \\lfloor r \\rfloor)) + c\\_{(\\lceil r \\rceil\
    \ + 1)} \\cdot (r - \\lfloor r \\rfloor) \\tag{15}$$\n\nwhere r = β·(k−1). A smaller\
    \ β results in a stricter threshold, reducing the proportion of offloaded tasks,\
    \ while a larger β relaxes the threshold, leading to more frequent offloading.\
    \ This dynamically computed threshold guides the recursive decisionmaking process\
    \ for task offloading at each computational node. For a given non-top-tier LLM\
    \ M, the input text x is processed locally if the confidence score satisfies:\n\
    \n<span id=\"page-4-9\"></span>\n$$C\\_{M,\\tau}(x) \\ge T\\_{M,\\tau}(\\beta).\
    \ \\tag{16}$$\n\nOtherwise, the task is recursively offloaded to the next highertier\
    \ LLM M′ . As a result, the recursive offloading decision policy D(x, M, τ ) can\
    \ be formalized as:\n\n<span id=\"page-4-10\"></span>\n$$\\begin{cases} \\mathcal{D}(x,M,\\\
    tau) = \\\\ M, & \\text{if } C\\_{M,\\tau}(x) \\ge T\\_{M,\\tau}(\\beta) \\\\\
    \ \\mathcal{D}(x,M',\\tau), & \\text{if } M \\ne M\\_n \\text{ and } C\\_{M,\\\
    tau}(x) < T\\_{M,\\tau}(\\beta), \\end{cases} \\tag{17}$$\n\nwhere M′ represents\
    \ the LLM on the upstream computational node of M. The recursion terminates either\
    \ when an LLM achieves sufficient confidence or when the task reaches the highest-tier\
    \ computational node. Consequently, the final inference output produced by RecServe\
    \ is given by:\n\n$$y = M^\\*(x), M^\\* = \\mathcal{D}(x, M\\_1, \\tau),\\tag{18}$$\n\
    \nwhere M<sup>∗</sup> (x) is the final LLM that completes inference. M<sup>1</sup>\
    \ denotes the initial processing node, typically an end device that close to users.\n\
    \n# *E. Formal Description of RecServe*\n\nAs illustrated in Algorithm [1](#page-5-0)\
    \ and Fig. [1,](#page-3-0) RecServe is deployed within a multi-tiered network\
    \ architecture comprising end devices, edge nodes, and cloud data centers. The\
    \ proposed framework employs a recursive offloading strategy based on a hierarchical\
    \ evaluation of confidence scores guided by dynamic thresholds.\n\nThe serving\
    \ process of RecServe initiates at end devices equipped with a lightweight local\
    \ LLM M1. For each input text x with task type τ , the recursive offloading function\
    \ TaskOffloading(x, M<sup>i</sup> , τ, β) (as detailed in Algorithm [1\\)](#page-5-0)\
    \ is invoked. At each computational tier, the LLM M<sup>i</sup> first performs\
    \ local inference to generate its prediction output y\n\n# Algorithm 1: Recursive\
    \ Offloading for LLM Serving\n\n```\nFunction RecServe(x, τ, β, {M1, M2, . . .\
    \ , Mn}):\n   y ← TaskOffloading(x, M1, τ, β)\n   return y\nFunction TaskOffloading(x,\
    \ Mi\n                                   , τ, β):\n   (logits, y) ← Mi(x)\n  \
    \ if τ = Seq2Class then\n       Compute CMi,τ (x) following Eqs. (7,8)\n   else\n\
    \       Compute CMi,τ (x) following Eqs. (9,10,11,12)\n   Update HM,τ following\
    \ Eqs. (5,6)\n   Compute TM,τ (β) following Eqs. (13,14,15)\n   if CMi,τ (x) ≥\
    \ TM,τ (β) or Mi\n                                 is Mn then\n       if Mi\n\
    \             is not M1 then\n          Transmit y from Mi\n                 \
    \             to Mi−1\n       return y\n   else\n       Transmit x from Mi\n \
    \                          to Mi+1\n       y ← TaskOffloading(x, Mi+1, τ, β)\n\
    \        return y\n```\nand computes the corresponding confidence score CMi,τ\
    \ (x) using either maximum softmax probability for Seq2Class tasks (Eqs. [\\(7](#page-4-0)[,8\\\
    )](#page-4-1)) or normalized perplexity for Seq2Seq tasks (Eqs. [\\(9,](#page-4-2)[10](#page-4-3)[,11](#page-4-4)[,12\\\
    )](#page-4-5)). Subsequently, M<sup>i</sup> updates its historical confidence\
    \ queue HMi,τ (Eqs. [\\(5,](#page-3-1) [6\\)](#page-3-2)) and computes a dynamic\
    \ offloading threshold TMi,τ (β) based on the interpolated βquantile of the updated\
    \ confidence history (Eqs. [\\(13](#page-4-6)[,14](#page-4-7)[,15\\)](#page-4-8)).\
    \ The inference result y is finalized at the current tier if either the computed\
    \ confidence CMi,τ (x) meets or exceeds the threshold (Eq. [\\(16\\)](#page-4-9))\
    \ or if the current LLM M<sup>i</sup> represents the top-tier cloud LLM Mn. In\
    \ such cases, the result is directly returned or propagated back to lower-tier\
    \ nodes. Otherwise, the input x is recursively offloaded to the next-tier LLM\
    \ Mi+1 for further inference (Eq. [\\(17\\)](#page-4-10)). This recursive procedure\
    \ continues until a sufficiently confident result is obtained or the highest-tier\
    \ cloud LLM is reached.\n\nBy leveraging historical feedback and adaptive confidence\
    \ thresholds, RecServe ensures that only challenging and uncertain tasks are transmitted\
    \ upward through the network, while the majority of requests are efficiently processed\
    \ at local computational nodes. Through this design, RecServe reduces the load\
    \ on cloud resources to mitigate potential congestion while achieving a fine-grained\
    \ balance between inference quality and communication overhead.\n\n## IV. THEORETICAL\
    \ ANALYSIS FOR RECSERVE\n\nThis section provides the theoretical support for RecServe.\
    \ We will present an approximation of the communication burden and an analysis\
    \ of the computational costs associated with the proposed RecServe under specific\
    \ assumptions.\n\n# <span id=\"page-5-1\"></span>*A. Communication Burden Approximation*\n\
    \nTo estimate the total communication burden incurred by RecServe across multi-tier\
    \ networks, we analyze both the probability of task offloading between hierarchical\
    \ tiers and the transmission cost associated with each offloading event.\n\nAssume\
    \ RecServe deploy n LLMs of increasing scale across end, edge, and cloud tiers,\
    \ denoted as M1, M2, . . . , Mn, where M<sup>1</sup> is the smallest LLM and M<sup>n</sup>\
    \ is the most powerful one. Each task is initially processed by M<sup>1</sup>\
    \ and may be offloaded to higher-tier LLMs based on its confidence score. Let\
    \ x denote an input text of length |x| and y denote the ultimate inference result\
    \ of length |y|. Communication burden occurs primarily when transmitting x to\
    \ a higher-tier node and returning y to the lower-tier node. Each time a task\
    \ is offloaded from M<sup>i</sup> to Mi+1, the input x is uploaded, incurring\
    \ a communication burden of |x| for both the sending and receiving nodes. Upon\
    \ task completion at a certain tier M<sup>∗</sup> , the inference result y is\
    \ propagated back through each intermediate tier from M<sup>∗</sup> to M1, incurring\
    \ a communication burden of |y| at each node along the return path.\n\nLet p<sup>i</sup>\
    \ denote the expected probability that a task is offloaded from M<sup>i</sup>\
    \ to Mi+1. According to the recursive offloading strategy in RecServe, p<sup>i</sup>\
    \ is defined as the probability that the task's confidence score at M<sup>i</sup>\
    \ falls below a quantile threshold β computed from the historical confidence queue\
    \ HMi,τ , that is:\n\n$$p\\_i = P(C\\_{M\\_i, \\tau}(x) < T\\_{M\\_i, \\tau}(\\\
    beta)) \\tag{19}$$\n\nWe can approximate p<sup>i</sup> under the following assumptions.\
    \ Assumption 1: Independent and Identically Distributed Confidence Scores. The\
    \ confidence score CMi,τ (x) for the current task is assumed to be independently\
    \ and identically distributed with respect to the historical confidence scores\
    \ stored in the queue. Let FMi,τ (c) denote the Cumulative Distribution Function\
    \ (CDF) of CMi,τ (x):\n\n$$F\\_{M\\_i, \\tau}(c) = P(C\\_{M\\_i, \\tau}(x) \\\
    le c). \\tag{20}$$\n\nWe assume that FMi,τ (c) is determined solely by the task\
    \ difficulty and the capacity of the current LLM M<sup>i</sup> .\n\nAssumption\
    \ 2: Sufficient Sample in Historical Queue. The historical confidence queue contains\
    \ a sufficiently large number of samples such that the threshold TMi,τ (β) closely\
    \ approximates the β-quantile of FMi,τ (c):\n\n$$P(C\\_{M\\_i, \\tau}(x) \\le\
    \ T\\_{M\\_i, \\tau}(\\beta)) \\approx \\beta. \\tag{21}$$\n\nAssumption 3: Monotonic\
    \ Continuity of CDF. The CDF F<sup>M</sup>i,τ (c) is monotonically continuous\
    \ everywhere.\n\nAssumption 4: Independence Between Confidence and Input Length.\
    \ The text length |x| is assumed to be uncorrelated with the confidence score\
    \ CM,τ (x) produced during LLM inference.\n\nAssumption 5: Output Length Distribution\
    \ Invariance Across Tiers: The distribution of output lengths |y| remains consistent\
    \ across different processing tiers.\n\nAccording to Assumption 1, the confidence\
    \ scores in the historical queue H<sup>M</sup>i,τ are sufficient independent samples\
    \ drawn from the distribution F<sup>M</sup>i,τ (c). Due to the monotonicity nature\
    \ of F<sup>M</sup>i,τ (c) following Assumption 3, we can define the inverse function\
    \ F −1 <sup>M</sup>i,τ (β), satisfying:\n\n$$F\\_{M\\_i, \\tau}(F\\_{M\\_i, \\\
    tau}^{-1}(\\beta)) = \\beta. \\tag{22}$$\n\nThis implies:\n\n$$P(C\\_{M\\_i, \\\
    tau}(x) \\le F\\_{M\\_i, \\tau}^{-1}(\\beta)) = \\beta,\\tag{23}$$\n\nindicating\
    \ that F −1 <sup>M</sup>i,τ (β) represents the theoretical β-quantile of the distribution\
    \ FMi,τ (c). By Assumption 2, we approximate:\n\n$$P(C\\_{M\\_i, \\tau}(x) \\\
    le F\\_{M\\_i, \\tau}^{-1}(\\beta)) = \\beta \\approx P(C\\_{M\\_i, \\tau}(x)\
    \ \\le T\\_{M\\_i, \\tau}(\\beta)). \\tag{24}$$\n\nHence, we obtain the approximation:\n\
    \n$$T\\_{M\\_i, \\tau}(\\beta) \\approx F\\_{M\\_i, \\tau}^{-1}(\\beta), \\tag{25}$$\n\
    \nwhich further implies:\n\n$$p\\_i = P(C\\_{M\\_i, \\tau}(x) < T\\_{M\\_i, \\\
    tau}(\\beta)) \\approx P(C\\_{M\\_i, \\tau}(x) < F\\_{M\\_i, \\tau}^{-1}(\\beta)).\
    \ \\tag{26}$$\n\nUnder Assumption 3 where FMi,τ (c) is continuous at F −1 <sup>M</sup>i,τ\
    \ (β), we have:\n\n$$\\lim\\_{\\substack{c\\uparrow F\\_{M\\_i,\\tau}^{-1}(\\\
    beta)}} F\\_{M\\_i,\\tau}(c) = F\\_{M\\_i,\\tau}(F\\_{M\\_i,\\tau}^{-1}(\\beta))\
    \ = \\beta. \\tag{27}$$\n\nTherefore, the jump at point F −1 <sup>M</sup>i,τ (β)\
    \ is zero, which means:\n\n$$P(C\\_{M\\_i, \\tau}(x) = F\\_{M\\_i, \\tau}^{-1}(\\\
    beta)) = 0,\\tag{28}$$\n\nwhich leads to:\n\n$$P(C\\_{M\\_i, \\tau}(x) < F\\_{M\\\
    _i, \\tau}^{-1}(\\beta)) = P(C\\_{M\\_i, \\tau}(x) \\le F\\_{M\\_i, \\tau}^{-1}(\\\
    beta)) = \\beta. \\tag{29}$$\n\nTherefore, we can conclude that:\n\n<span id=\"\
    page-6-0\"></span>\n$$p\\_i \\approx \\beta. \\tag{30}$$\n\nBased on the approximation\
    \ in Eq. [\\(30\\)](#page-6-0), we derive the probability of task completion at\
    \ each tier and the corresponding communication burden. According to the serving\
    \ process of RecServe, tasks are initially processed at M<sup>1</sup> and are\
    \ offloaded to higher tiers only when the confidence is insufficient. Let P <sup>C</sup>\
    \ (M) denote the probability that a task is completed at LLM M. Then, the probability\
    \ of completing the inference task at LLM M<sup>i</sup> at each tier can be approximated\
    \ as follows.\n\n• Task completed at at M<sup>1</sup> without offloading:\n\n\
    $$P^C(M\\_1) = 1 - p\\_1 \\approx 1 - \\beta. \\tag{31}$$\n\n• Task completed\
    \ at M<sup>i</sup> , i ∈ {2, . . . , n − 1} after i − 1 offloads without further\
    \ offloading to higher tiers:\n\n$$P^C(M\\_i) = \\prod\\_{j=1}^{i-1} p\\_j \\\
    cdot (1 - p\\_i) \\approx \\beta^{i-1} (1 - \\beta). \\tag{32}$$\n\n• Task completed\
    \ at Mn, after being offloaded to the highest tier:\n\n$$P^C(M\\_n) = \\prod\\\
    _{i=1}^{n-1} p\\_i \\approx \\beta^{n-1}.\\tag{33}$$\n\nFollowing Assumptions\
    \ 4 and 5, the expected communication burden for each type of inference can be\
    \ derived as follows:\n\n• When the task is completed at M1, no communication\
    \ burden is incurred.\n\n• When the task is completed at M<sup>i</sup> , i ∈ {2,\
    \ . . . , n − 1}, the input x is uploaded i − 1 times, and the output y is downloaded\
    \ i−1 times. Therefore, the total communication burden introduced to the pathway\
    \ computing nodes is:\n\n$$2(i-1)(|x|+|y|).\\tag{34}$$\n\n• When the task is completed\
    \ at M<sup>n</sup> in the highest tier, the input x is uploaded n − 1 times, and\
    \ the output y is downloaded n − 1 times. Thus, the incurred total communication\
    \ burden is computed as:\n\n$$2(n-1)(|x|+|y|).\\tag{35}$$\n\nThus, the expected\
    \ communication burden under RecServe E[Comm-RecServe] is as follows:\n\n$$\\\
    begin{aligned} &E[\\text{comm-RecServer}]\\\\ &=\\sum\\_{i=2}^{n-1}P^C(M\\_i)\\\
    cdot 2(i-1)(|x|+|y|)\\\\ &+P^C(M\\_n)\\cdot 2(n-1)(|x|+|y|)\\\\ &\\approx 2\\\
    sum\\_{i=2}^{n-1}(i-1)(|x|+|y|)\\times \\beta^{i-1}(1-\\beta)\\\\ &+2(n-1)(|x|+|y|)\\\
    times \\beta^{n-1}\\\\ &\\approx 2(|x|+|y|)\\left[(1-\\beta)\\sum\\_{i=2}^{n-1}(i-1)\\\
    beta^{i-1}+(n-1)\\beta^{n-1}\\right]\\\\ &\\approx 2(|x|+|y|)\\left[\\beta\\frac{1-(n-1)\\\
    beta^{n-2}+(n-2)\\beta^{n-1}}{1-\\beta}\\right] \\end{aligned} \\tag{36}$$\n\n\
    In particular, for a three-tier device-edge-cloud network architecture, i.e.,\
    \ n = 3, we have:\n\n$$\\begin{array}{l} E[\\text{Comm-RecServer}] \\\\ \\approx\
    \ 2(|x| + |y|) \\left[ \\frac{\\beta(1 - 2\\beta + \\beta^2)}{1 - \\beta} + 2\\\
    beta^2 \\right] \\\\ \\approx 2(|x| + |y|) \\cdot \\beta(1 + \\beta). \\end{array}\
    \ \\tag{37}$$\n\nFor comparison, consider the cloud-centric LLM serving scheme\
    \ (CloudServe), where all task requests are sent directly to the cloud data center.\
    \ The expected total communication burden for this serving paradigm is:\n\n$$E[\\\
    text{Common-ClouldServer}] = 2(|x| + |y|). \\tag{38}$$\n\nTherefore, the ratio\
    \ of communication burden between the two serving paradigms can be computed as:\n\
    \n$$\\frac{E[\\text{Common-RecServer}]}{E[\\text{Common-CloudServe}]} = \\beta(1+\\\
    beta). \\tag{39}$$\n\nFor RecServe to be more communication-efficient than cloudonly\
    \ serving, we require:\n\n$$\\frac{E[\\text{Common-RecServer}]}{E[\\text{Common-CloudServe}]}\
    \ \\in (0,1) \\land \\beta \\in (0,1). \\qquad (40)$$\n\nThis yields:\n\n$$\n\\\
    beta \\in \\left( 0, \\frac{\\sqrt{5} - 1}{2} \\right) \\tag{41}\n$$\n\nWithin\
    \ this range, RecServe is expected to be more communication-efficient than the\
    \ cloud-only serving paradigm under our assumptions.\n\n## *B. Computation Cost\
    \ Analysis*\n\nFor simplicity, our analysis focuses solely on the computational\
    \ cost of LLM inference, excluding other factors such as data communication overhead\
    \ between neighboring nodes and the management of historical confidence queues.\
    \ Let the inference cost of the n hierarchical LLMs M1, M2, . . . , M<sup>n</sup>\
    \ be denoted as Cost1, Cost2, . . . , Costn, respectively, satisfying Cost<sup>1</sup>\
    \ < Cost<sup>2</sup> < · · · < Costn. Based on the assumptions in section [IV-A,](#page-5-1)\
    \ the task completion probabilities at M1, M<sup>i</sup> (2 ≤ i ≤ n − 1), and\
    \ M<sup>n</sup> can be approximated as 1−β, β i−1 (1−β), and β n−1 , respectively.\
    \ Thus, the expected computation cost of RecServe E[Comp-RecServe] is given by:\n\
    \nE[Comp-RecServe]\n\n$$\\begin{aligned} &= P^C(M\\_1) \\cdot \\text{Cost}\\_1\
    \ + \\sum\\_{i=2}^{n-1} P^C(M\\_i) \\cdot \\sum\\_{j=1}^i \\text{Cost}\\_j\\\\\
    \ &+ P^C(M\\_n) \\cdot \\sum\\_{j=1}^n \\text{Cost}\\_j\\\\ &\\approx (1 - \\\
    beta) \\cdot \\text{Cost}\\_1 + \\sum\\_{i=2}^{n-1} \\beta^{i-1} (1 - \\beta)\
    \ \\cdot \\sum\\_{j=1}^i \\text{Cost}\\_j\\\\ &+ \\beta^{n-1} \\cdot \\sum\\_{j=1}^n\
    \ \\text{Cost}\\_j \\end{aligned} \\tag{42}$$\n\nIn a three-tier device-edge-cloud\
    \ architecture, we can specify Cost<sup>1</sup> = Costdevice, Cost<sup>2</sup>\
    \ = Costedge, and Cost<sup>3</sup> = Costcloud, yielding:\n\n$$\\begin{array}{l}\
    \ E[\\text{Comp-RecerSere}]\\\\ \\approx (1 - \\beta) \\cdot \\text{Cost}\\_{\\\
    text{device}} + \\beta(1 - \\beta) \\cdot (\\text{Cost}\\_{\\text{device}} + \\\
    text{Cost}\\_{\\text{edge}})\\\\ + \\beta^2 \\cdot (\\text{Cost}\\_{\\text{device}}\
    \ + \\text{Cost}\\_{\\text{edge}} + \\text{Cost}\\_{\\text{cloud}})\\\\ \\approx\
    \ \\text{Cost}\\_{\\text{device}} + \\beta \\cdot \\text{Cost}\\_{\\text{edge}}\
    \ + \\beta^2 \\cdot \\text{Cost}\\_{\\text{cloud}} \\end{array} \\tag{43}$$\n\n\
    For comparison, the expected computation cost of cloud-only inference E[Comp-CloudServe]\
    \ is:\n\n$$E[\\text{Comp-ClouldServe}] = \\text{Cost}\\_{\\text{cloud}} \\tag{44}$$\n\
    \nTherefore, the relative computation cost of RecServe compared to cloud-only\
    \ inference can be expressed as:\n\n$$\\begin{array}{l} \\frac{E[\\text{Comp-RecServer}]}{E[\\\
    text{Comp-CloudServe}]}\\\\ \\approx \\frac{\\text{Cost}\\_{\\text{device}} +\
    \ \\beta \\cdot \\text{Cost}\\_{\\text{edge}} + \\beta^2 \\cdot \\text{Cost}\\\
    _{\\text{cloud}}}{\\text{Cost}\\_{\\text{cloud}}} \\end{array} \\tag{45}$$\n\n\
    For RecServe to outperform cloud-only inference in terms of computational efficiency,\
    \ we require:\n\n$$\\frac{E[\\text{Comp-RecServer}]}{E[\\text{Comp-CloadServer}]}\
    \ < 1\\tag{46}$$\n\nTherein, we can infer that:\n\n$$\\beta \\in \\left( 0, \\\
    frac{-\\text{Cost}\\_{\\text{edge}} + \\sqrt{\\text{Cost}\\_{\\text{edge}}^2 +\
    \ 4\\text{Cost}\\_{\\text{cloud}}(\\text{Cost}\\_{\\text{cloud}} - \\text{Cost}\\\
    _{\\text{device}})}}{2\\text{Cost}\\_{\\text{cloud}}} \\right) \\tag{47}$$\n\n\
    Within this range, RecServe is expected to incur lower computation cost than cloud-only\
    \ inference.\n\n## V. EXPERIMENTS\n\nThis section validates the performance of\
    \ RecServe through a series of extensive experiments. We will detail the comprehensive\
    \ experimental setup, followed by a presentation and analysis of the comparative\
    \ results.\n\n# *A. Experimental Setup*\n\n*1) Tasks and Datasets:* To evaluate\
    \ RecServe's performance across diverse NLP applications, we consider two distinct\
    \ task categories with different datasets:\n\n- Sequence-to-Class (Seq2Class).\
    \ We evaluate on five widely-acknowledged benchmark datasets: IMDB [\\[34\\]](#page-15-18),\
    \ SST-2 [\\[35\\]](#page-15-19), Rotten Tomatoes [\\[36\\]](#page-15-20), Yelp\
    \ Polarity [\\[37\\]](#page-15-21), and Amazon Polarity [\\[37\\]](#page-15-21),\
    \ which span diverse textual domains such as movie reviews and product feedback.\n\
    - Sequence-to-Sequence (Seq2Seq). We select German-to-English (De-En) translation\
    \ benchmarks from WMT16 [\\[38\\]](#page-15-22), WMT19 [\\[39\\]](#page-15-23),\
    \ and OPUS100 [\\[40\\]](#page-15-24), which encompass a diverse range of linguistic\
    \ contexts and complexities.\n\n*2) Hierarchical LLM Deployment:* To simulate\
    \ realistic multi-tier network environments, we establish a three-tier serving\
    \ hierarchy comprising device, edge, and cloud nodes. At each computational tier,\
    \ we deploy LLMs with capacity and complexity appropriate to the node's computational\
    \ resources. For Seq2Class tasks, we deploy DistilRoBERTa [\\[41\\]](#page-15-25),\
    \ [\\[42\\]](#page-15-26), RoBERTa-Base [\\[19\\]](#page-15-4), [\\[43\\]](#page-15-27),\
    \ RoBERTa-Large [\\[44\\]](#page-15-28) on end, edge and cloud nodes, respectively.\
    \ For Seq2Seq tasks, we deploy T5-Small [\\[24\\]](#page-15-9), [\\[45\\]](#page-15-29),\
    \ opus-mt [\\[46\\]](#page-15-30), [\\[47\\]](#page-15-31) and fairseq-based wmt19\
    \ transformer [\\[48\\]](#page-15-32), [\\[49\\]](#page-15-33) at the corresponding\
    \ tiers. All LLMs are fine-tuned following the official guidelines provided in\
    \ their respective repositories, with pre-trained weights downloaded from the\
    \ Huggingface platform.\n\n*3) Baselines:* We compare RecServe against several\
    \ representative inference serving paradigms or baseline systems:\n\n- Local inference\
    \ on end devices (EndServe). Tasks are processed entirely on end devices without\
    \ any offloading to remote servers [\\[10\\]](#page-14-9), [\\[11\\]](#page-14-10).\n\
    - Full offloading to edge server (EdgeServe). Tasks are fully offloaded to the\
    \ edge server for inference [\\[50\\]](#page-15-34).\n- Full offloading to cloud\
    \ data center (CloudServe). Tasks are fully offloaded to a centralized cloud data\
    \ center for inference [\\[12\\]](#page-14-11), [\\[13\\]](#page-14-12).\n- Multi-tier\
    \ collaborative serving with quality-independent partial offloading (ColServe).\
    \ Tasks are partially offloaded to higher-tier servers without considering inference\
    \ quality. In this paper, we consider a fixed probability α to control the offloading\
    \ process.\n- Multi-tier collaborative serving with model cascades (CasServe).\
    \ Tasks are sequentially processed using hierarchical model cascades deployed\
    \ across multiple tiers. Offloading to a higher-tier server occurs only if the\
    \ inference confidence at the current tier does not meet predefined thresholds\
    \ [\\[16\\]](#page-15-1). In this paper, we define thresholds tend and tedge for\
    \ end devices and edge nodes, respectively.\n\n<span id=\"page-8-0\"></span>\n\
    \n|                 |                      |              | Communication Burden\
    \ (B)                                     |                          |       \
    \  |         |  |  |\n|-----------------|----------------------|--------------|--------------------------------------------------------------|--------------------------|---------|---------|--|--|\n\
    |                 | Method               | Accuracy (%) | Device             \
    \                                          | Edge                     | Cloud\
    \   | Total   |  |  |\n|                 | EndServe             | 90.92      \
    \  | -                                                            | -        \
    \                | -       | -       |  |  |\n|                 | EdgeServe  \
    \          | 92.29        | 30.41M                                           \
    \            | 30.41M                   | -       | 60.82M  |  |  |\n|       \
    \          | CloudServe           | 94.25        | 30.41M                    \
    \                                   | -                        | 30.41M  | 60.82M\
    \  |  |  |\n| IMDB            | ColServe(α<br>= 0.2) | 91.32        | 6.09M  \
    \                                                      | 7.33M               \
    \     | 1.23M   | 14.65M  |  |  |\n|                 | CasServe(setting 1)  |\
    \ 92.33        | 3.92M                                                       \
    \ | 4.40M                    | 0.48M   | 8.80M   |  |  |\n|                 |\
    \ RecServe(β<br>= 0.1) | 92.35        | 3.73M                                \
    \                        | 4.15M                    | 0.42M   | 8.30M   |  | \
    \ |\n|                 | ColServe(α<br>= 0.5) | 91.88        | 15.08M        \
    \                                               | 22.72M                   | 7.64M\
    \   | 45.44M  |  |  |\n|                 | CasServe(setting 2)  | 93.32      \
    \  | 12.27M                                                       | 15.35M   \
    \                | 3.08M   | 30.70M  |  |  |\n|                 | RecServe(β<br>=\
    \ 0.3) | 93.74        | 10.97M                                               \
    \        | 14.61M                   | 3.64M   | 29.22M  |  |  |\n|           \
    \      | Method               | Accuracy (%) | Communication Burden (B)      \
    \                               |                          |         |       \
    \  |  |  |\n|                 |                      |              | Device \
    \                                                      | Edge                \
    \     | Cloud   | Total   |  |  |\n|                 | EndServe             |\
    \ 91.17        | -                                                           \
    \ | -                        | -       | -       |  |  |\n|                 |\
    \ EdgeServe            | 94.38        | 89.80K                               \
    \                        | 89.80K                   | -       | 179.60K |  | \
    \ |\n|                 | CloudServe           | 95.76        | 89.80K        \
    \                                               | -                        | 89.80K\
    \  | 179.60K |  |  |\n| SST-2           | ColServe(α<br>= 0.2) | 91.74       \
    \ | 18.77K                                                       | 22.60K    \
    \               | 3.83K   | 45.20K  |  |  |\n|                 | CasServe(setting\
    \ 3)  | 93.35        | 12.38K                                                \
    \       | 13.58K                   | 1.20K   | 27.16K  |  |  |\n|            \
    \     | RecServe(β<br>= 0.1) | 93.58        | 11.67K                         \
    \                              | 12.97K                   | 1.30K   | 25.94K \
    \ |  |  |\n|                 | ColServe(α<br>= 0.5) | 92.43        | 43.97K  \
    \                                                     | 63.18K               \
    \    | 19.21K  | 126.36K |  |  |\n|                 | CasServe(setting 4)  | 94.95\
    \        | 37.29K                                                       | 44.57K\
    \                   | 7.28K   | 89.14K  |  |  |\n|                 | RecServe(β<br>=\
    \ 0.3) | 95.30        | 29.76K                                               \
    \        | 40.56K                   | 10.80K  | 81.12K  |  |  |\n|           \
    \      | Method               | Accuracy (%) |                               \
    \                               | Communication Burden (B) |         |       \
    \  |  |  |\n|                 |                      |              | Device \
    \                                                      | Edge                \
    \     | Cloud   | Total   |  |  |\n|                 | EndServe             |\
    \ 89.40        | -                                                           \
    \ | -                        | -       | -       |  |  |\n|                 |\
    \ EdgeServe            | 90.34        | 120.86K                              \
    \                        | 120.86K                  | -       | 241.72K |  | \
    \ |\n|                 | CloudServe           | 91.84        | 120.86K       \
    \                                               | -                        | 120.86K\
    \ | 241.72K |  |  |\n| Rotten Tomatoes | ColServe(α<br>= 0.2) | 89.68        |\
    \ 21.62K                                                       | 25.23K      \
    \             | 3.61K   | 50.46K  |  |  |\n|                 | CasServe(setting\
    \ 3)  | 90.71        | 13.30K                                                \
    \       | 14.27K                   | 0.96K   | 28.53K  |  |  |\n|            \
    \     | RecServe(β<br>= 0.1) | 90.62        | 11.30K                         \
    \                              | 13.05K                   | 1.75K   | 26.10K \
    \ |  |  |\n|                 | ColServe(α<br>= 0.5) | 89.77        | 63.02K  \
    \                                                     | 93.45K               \
    \    | 30.43K  | 186.90K |  |  |\n|                 | CasServe(setting 4)  | 91.18\
    \        | 47.94K                                                       | 57.69K\
    \                   | 9.75K   | 115.38K |  |  |\n|                 | RecServe(β<br>=\
    \ 0.3) | 91.46        | 41.34K                                               \
    \        | 53.53K                   | 12.19K  | 107.06K |  |  |\n|           \
    \      | Method               | Accuracy (%) | Communication Burden (B)<br>Device<br>Edge<br>Cloud<br>Total\
    \ |                          |         |         |  |  |\n|                 |\
    \ EndServe             | 92.46        | -                                    \
    \                        | -                        | -       | -       |  | \
    \ |\n| Yelp Polarity   | EdgeServe            | 95.28        | 25.90M        \
    \                                               | 25.90M                   | -\
    \       | 51.80M  |  |  |\n|                 | CloudServe           | 96.54  \
    \      | 25.90M                                                       | -    \
    \                    | 25.90M  | 51.80M  |  |  |\n|                 | ColServe(α<br>=\
    \ 0.2) | 92.91        | 5.04M                                                \
    \        | 6.03M                    | 0.99M   | 12.06M  |  |  |\n|           \
    \      | CasServe(setting 5)  | 94.58        | 2.80M                         \
    \                               | 3.62M                    | 0.81M   | 7.23M \
    \  |  |  |\n|                 | RecServe(β<br>= 0.1) | 94.63        | 3.21M  \
    \                                                      | 3.55M               \
    \     | 0.34M   | 7.10M   |  |  |\n|                 | ColServe(α<br>= 0.5) |\
    \ 93.89        | 13.01M                                                      \
    \ | 19.55M                   | 6.54M   | 39.10M  |  |  |\n|                 |\
    \ CasServe(setting 6)  | 95.62        | 11.69M                               \
    \                        | 13.93M                   | 2.24M   | 27.86M  |  | \
    \ |\n|                 | RecServe(β<br>= 0.3) | 96.06        | 9.68M         \
    \                                               | 12.39M                   | 2.71M\
    \   | 24.78M  |  |  |\n|                 |                      |            \
    \  |                                                              | Communication\
    \ Burden (B) |         |         |  |  |\n|                 | Method         \
    \      | Accuracy (%) | Device                                               \
    \        | Edge                     | Cloud   | Total   |  |  |\n|           \
    \      | EndServe             | 89.66        | -                             \
    \                               | -                        | -       | -     \
    \  |  |  |\n|                 | EdgeServe            | 92.88        | 154.19M\
    \                                                      | 154.19M             \
    \     | -       | 308.38M |  |  |\n| Amazon Polarity | CloudServe           |\
    \ 95.11        | 154.19M                                                     \
    \ | -                        | 154.19M | 308.38M |  |  |\n|                 |\
    \ ColServe(α<br>= 0.2) | 90.20        | 30.76M                               \
    \                        | 36.88M                   | 6.12M   | 73.76M  |  | \
    \ |\n|                 | CasServe(setting 7)  | 91.71        | 18.72M        \
    \                                               | 21.04M                   | 2.33M\
    \   | 42.09M  |  |  |\n|                 | RecServe(β<br>= 0.1) | 91.88      \
    \  | 17.19M                                                       | 19.09M   \
    \                | 1.90M   | 38.18M  |  |  |\n|                 | ColServe(α<br>=\
    \ 0.5) | 91.38        | 77.21M                                               \
    \        | 115.86M                  | 38.65M  | 231.72M |  |  |\n|           \
    \      | CasServe(setting 6)  | 93.42        | 56.75M                        \
    \                               | 72.27M                   | 15.52M  | 144.54M\
    \ |  |  |\n|                 |                      |              |         \
    \                                                     |                      \
    \    |         |         |  |  |\n\nTABLE II INFERENCE ACCURACY VS COMMUNICATION\
    \ BURDEN IN SEQ2CLASS TASKS.\n\n<span id=\"page-9-0\"></span>\n\n|         | \
    \                     |          | Communication Burden (B) |         |      \
    \   |          |  |\n|---------|----------------------|----------|--------------------------|---------|---------|----------|--|\n\
    |         | Method               | BLEU (%) | Device                   | Edge\
    \    | Cloud   | Total    |  |\n|         | EndServe             | 23.18    |\
    \ -                        | -       | -       | -        |  |\n|         | EdgeServe\
    \            | 28.87    | 689.87K                  | 689.87K | -       | 1379.74K\
    \ |  |\n|         | CloudServe           | 29.26    | 727.11K                \
    \  | -       | 727.11K | 1454.22K |  |\n| WMT16   | ColServe(α = 0.5)    | 26.44\
    \    | 356.49K                  | 545.08K | 188.59K | 1090.15K |  |\n|       \
    \  | CasServe(setting 8)  | 26.00    | 442.04K                  | 674.09K | 232.04K\
    \ | 1348.17K |  |\n|         | RecServe(β = 0.5)    | 26.60    | 309.67K     \
    \             | 454.55K | 144.88K | 909.10K  |  |\n|         | ColServe(α = 0.3)\
    \    | 25.00    | 205.70K                  | 269.89K | 64.19K  | 539.78K  |  |\n\
    |         | CasServe(setting 9)  | 24.30    | 199.91K                  | 258.58K\
    \ | 58.67K  | 517.16K  |  |\n|         | RecServe(β = 0.3)    | 25.16    | 167.60K\
    \                  | 214.57K | 46.97K  | 429.14K  |  |\n|         | Method   \
    \            | BLEU (%) | Communication Burden (B) |         |         |     \
    \     |  |\n|         |                      |          | Device             \
    \      | Edge    | Cloud   | Total    |  |\n|         | EndServe             |\
    \ 24.77    | -                        | -       | -       | -        |  |\n| \
    \        | EdgeServe            | 31.28    | 702.66K                  | 702.66K\
    \ | -       | 1405.32K |  |\n|         | CloudServe           | 31.37    | 731.13K\
    \                  | -       | 731.13K | 1462.26K |  |\n| WMT19   | ColServe(α\
    \ = 0.5)    | 28.01    | 359.49K                  | 546.55K | 187.06K | 1093.10K\
    \ |  |\n|         | CasServe(setting 10) | 27.54    | 433.04K                \
    \  | 660.52K | 227.48K | 1321.04K |  |\n|         | RecServe(β = 0.5)    | 28.38\
    \    | 332.13K                  | 488.38K | 156.25K | 976.76K  |  |\n|       \
    \  | ColServe(α = 0.3)    | 26.52    | 211.43K                  | 278.63K | 67.20K\
    \  | 557.26K  |  |\n|         | CasServe(setting 8)  | 25.75    | 188.79K    \
    \              | 251.65K | 62.86K  | 503.30K  |  |\n|         | RecServe(β = 0.3)\
    \    | 26.85    | 177.97K                  | 224.53K | 46.56K  | 449.06K  |  |\n\
    |         | Method               | BLEU (%) | Communication Burden (B) |     \
    \    |         |          |  |\n| OPUS100 |                      |          |\
    \ Device                   | Edge    | Cloud   | Total    |  |\n|         | EndServe\
    \             | 15.70    | -                        | -       | -       | -  \
    \      |  |\n|         | EdgeServe            | 21.44    | 293.71K           \
    \       | 293.71K | -       | 587.42K  |  |\n|         | CloudServe          \
    \ | 19.44    | 388.19K                  | -       | 388.19K | 776.38K  |  |\n\
    |         | ColServe(α = 0.5)    | 18.40    | 178.09K                  | 278.96K\
    \ | 100.87K | 557.92K  |  |\n|         | CasServe(setting 11) | 17.84    | 216.83K\
    \                  | 364.33K | 147.50K | 728.66K  |  |\n|         | RecServe(β\
    \ = 0.5)    | 18.63    | 152.67K                  | 237.16K | 84.49K  | 474.32K\
    \  |  |\n|         | ColServe(α = 0.3)    | 16.98    | 90.56K                \
    \   | 127.31K | 36.75K  | 254.62K  |  |\n|         | CasServe(setting 12) | 17.10\
    \    | 112.45K                  | 178.93K | 66.48K  | 357.86K  |  |\n|       \
    \  | RecServe(β = 0.3)    | 17.67    | 66.98K                   | 86.53K  | 19.55K\
    \  | 173.06K  |  |\n\nTABLE III BLEU VS COMMUNICATION BURDEN IN SEQ2SEQ TASKS.\n\
    \n*4) Hyper-parameters:* We outline the hyperparameter configurations for both\
    \ the considered baselines and our proposed RecServe, aiming to ensure fair and\
    \ valid comparisons across tasks and datasets.\n\n- For EndServe, EdgeServe, and\
    \ CloudServe, no additional hyperparameters are required.\n- For ColServe, we\
    \ consider the cases where α ∈ {0.2, 0.3, 0.5}.\n- For CasServe, we apply careful\
    \ tuning of tend and tedge to achieve comparability with relevant baselines. Specifically,\
    \ twelve different hyper-parameter settings (setting 1-12) are considered with\
    \ threshold combinations (tend, tedge) as follows: (0.85, 0.6), (0.99, 0.7), (0.97,\
    \ 0.7), (0.998, 0.95), (0.85, 0.75), (0.995, 0.8), (0.9, 0.6), (0.05, 0.001),\
    \ (0.025, 0.0002), (0.055, 0.001), (0.07, 0.01), and (0.03, 0.001).\n- For our\
    \ proposed RecServe, we set k = 10000, and consider the cases where β ∈ {0.1,\
    \ 0.3, 0.5}.\n\n*5) Evaluation Metrics:* We evaluate the performance of serving\
    \ systems based on the following metrics:\n\n- Precision. We measure inference\
    \ accuracy for Seq2Class tasks and BLEU scores for Seq2Seq tasks to quantify predictive\
    \ performance.\n- Communication burden. We quantify data transmission volume per\
    \ computing node and aggregate this across all nodes to assess system-wide communication\
    \ efficiency.\n- Precision-communication trade-off. We analyze the relationship\
    \ between precision and communication burden through comparative performance plots.\n\
    \n# *B. Experimental Results*\n\n*1) Evaluation on Seq2Class Tasks:* TABLE [II](#page-8-0)\
    \ reports the inference accuracy and communication burden of various methods across\
    \ five datasets: IMDB, SST-2, Rotten Tomatoes, Yelp Polarity, and Amazon Polarity.\
    \ As displayed, CloudServe achieves the highest accuracy across all datasets,\
    \ with values of 94.25% (IMDB), 95.76% (SST-2), 91.84% (Rotten Tomatoes), 96.54%\
    \ (Yelp Polarity), and 95.11% (Amazon Polarity). However, this comes at the cost\
    \ of substantial total communication burden, with 60.82MB, 179.60KB, 241.72KB,\
    \ 51.80MB, and\n\n![](_page_10_Figure_1.jpeg)\n\nFig. 2. Visualization of precision\
    \ vs communication burden for multi-tier serving methods across eight datasets.\n\
    \n308.38MB, on these five datasets, respectively. EdgeServe offers slightly lower\
    \ accuracy with no improvement in communication burden compared to CloudServe.\
    \ In contrast, our proposed RecServe (β = 0.3) delivers competitive accuracies\
    \ compared to CloudServe and EdgeServe, which are 93.74% on IMDB, 95.30% on SST-2,\
    \ 91.46% on Rotten Tomatoes, 96.06% on Yelp Polarity, and 94.20% on Amazon Polarity.\
    \ At the same time, the total communication burden of RecServe (β = 0.3) is significantly\
    \ reduced by over 50% compared to CloudServe and EdgeServe, which are only 29.22MB\
    \ on IMDB, 81.12KB on SST-2, 107.06KB on Rotten Tomatoes, 24.78MB on Yelp Polarity,\
    \ and 136.39MB on Amazon Polarity, respectively. In addition, RecServe significantly\
    \ improves accuracy across all datasets compared to EndServe. Against multi-tier\
    \ baselines such as ColServe and CasServe, RecServe demonstrates superior performance\
    \ in both accuracy and communication efficiency. On the IMDB dataset, for instance,\
    \ ColServe (α = 0.5) achieves 91.88% accuracy with 45.44MB total communication\
    \ burden, while CasServe (setting 2) yields 93.32% with 30.70MB. RecServe (β =\
    \ 0.3), by comparison, achieves 93.74% accuracy with only 29.22MB total communication\
    \ burden. Similar patterns are observed across other datasets, with RecServe outperforming\
    \ these multi-tier serving methods in most configurations. A visualization of\
    \ these tradeoffs is provided in subsection [V-B3.](#page-11-0)\n\n*2) Evaluation\
    \ on Seq2Seq Tasks:* TABLE [III](#page-9-0) summarizes the BLEU scores and communication\
    \ burden over WMT16, WMT19, and OPUS100 datasets. As illustrated, CloudServe achieves\
    \ the highest BLEU scores in most cases, but at the cost of the largest communication\
    \ burden. EdgeServe, while slightly more efficient in communication, still incurs\n\
    \n<span id=\"page-10-0\"></span>![](_page_10_Figure_5.jpeg)\n\n<span id=\"page-10-1\"\
    ></span>Fig. 3. Comparison of RecServe and ColServe with different offload configurations.\n\
    \nconsiderable overhead. On the contrary, RecServe (β = 0.5) offers a strong balance\
    \ between serving performance and communication efficiency, achieving BLEU scores\
    \ comparable to CloudServe while reducing communication burden by over 33%. At\
    \ lower values of β (e.g., β = 0.3), RecServe further reduces communication burden\
    \ by more than 50%, still maintaining BLEU scores that significantly outperform\
    \ EndServe. Compared to ColServe and CasServe, RecServe consistently delivers\
    \ higher BLEU scores with lower communication burden. For example, on WMT19, ColServe\
    \ (α = 0.3) achieves a BLEU score of 26.52% with a 557.26KB communication bur-\n\
    \n![](_page_11_Figure_1.jpeg)\n\n<span id=\"page-11-1\"></span>Fig. 4. Effect\
    \ of maximum queue capability on RecServe's inference accuracy (β = 0.1) and communication\
    \ burden.\n\nden, and CasServe (setting 8) obtains 25.75% with 503.30KB. In contrast,\
    \ RecServe (β = 0.3) achieves 26.85% with only 449.06KB. Further precision-communication\
    \ comparisons of these multi-tier serving methods are elaborated in subsection\
    \ [V-B3.](#page-11-0)\n\n<span id=\"page-11-0\"></span>*3) Visualization of Precision-Communication\
    \ Trade-off:* Fig. [2](#page-10-0) presents the comparison of the performance\
    \ of multitier serving methods (ColServe, CasServe, and RecServe) across eight\
    \ datasets, evaluating both precision (measured by accuracy or BLEU score) and\
    \ communication burden. Notably, RecServe (represented by the brown dots and lines)\
    \ is positioned in the upper left relative to both ColServe (red) and CasServe\
    \ (purple) in nearly all cases, indicating higher accuracy achieved at lower communication\
    \ burden. For instance, on the IMDB dataset, RecServe achieves the highest accuracy\
    \ at all levels of communication burden, with its performance reaching up to 93.5%\
    \ with less than 30MB total communication burden. In contrast, ColServe and CasServe\
    \ start with lower accuracy, and their accuracy improvements are slower as the\
    \ communication burden increases. RecServe continues to excel in tasks involving\
    \ BLEU scores. For example, in the WMT16 dataset, RecServe maintains a significant\
    \ lead in BLEU scores, surpassing 26.5% at total communication burdens below 1000KB.\
    \ Meanwhile, both CasServe and ColServe consistently fall short of this threshold.\n\
    \n## VI. ABLATION STUDY\n\nThis section further investigates the performance characteristics\
    \ of RecServe by examining the impact of key parameters. We will illustrate on\
    \ the ablation studies focusing on three aspects: the impact of the dynamic offloading\
    \ threshold quantile β, the maximum queue capability k, and the choice of cloudside\
    \ LLM. All the experimental results are conducted on the IMDB dataset, following\
    \ the same experimental configuration as in the main experiments.\n\n![](_page_11_Figure_7.jpeg)\n\
    \n<span id=\"page-11-2\"></span>Fig. 5. Comparison of RecServe and ColServe with\
    \ DeBERTa-large deployed on the cloud.\n\n# *A. Impact of Dynamic Offloading Threshold\
    \ Quantile*\n\nWe first investigate how the offloading threshold quantile β affects\
    \ the trade-off between inference accuracy and total communication burden. Fig.\
    \ [3](#page-10-1) illustrates the performance of RecServe with different values\
    \ of β (ranging from 0.1 to 0.5), in comparison with ColServe under varying offloading\
    \ probabilities α. As shown in the figure, increasing β leads to higher accuracy\
    \ but also increases the total communication burden. Specifically, when β is set\
    \ to 0.1, RecServe achieves an accuracy of 92.3% with a communication burden less\
    \ than 10 MB. As β increases to 0.5, accuracy rises to 94.2% while the communication\
    \ burden increases to more than 50 MB. Comparing RecServe with ColServe, there\
    \ exists at least one RecServe configuration that simultaneously achieves higher\
    \ accuracy and lower communication burden in each configuration evaluated for\
    \ ColServe. For example, at a communication burden of around 30 MB, RecServe with\
    \ β = 0.3 attains an accuracy of about 93.7%, substantially outperforming ColServe\
    \ with α = 0.5, which only reaches less than 91.9%. These results demonstrate\
    \ the effectiveness of RecServe's recursive offloading strategy in achieving a\
    \ superior trade-off between communication efficiency and serving performance.\n\
    \n# *B. Impact of Maximum Queue Capability*\n\nWe further analyze the influence\
    \ of the maximum queue capability k in the historical confidence queue on both\
    \ communication burden and inference accuracy. Fig. [4](#page-11-1) illustrates\
    \ the results of RecServe as k varies from 10 to 10000. The bar charts show the\
    \ communication burden at each network tier (device, edge, cloud), as well as\
    \ the total burden, while the black line represents the corresponding inference\
    \ accuracy. As k increases, the communication burden remains largely stable, while\
    \ accuracy initially improves before reaching a plateau beyond k = 300. For larger\
    \ values of k (e.g., k = 1000 or higher), the accuracy stabilizes at approximately\
    \ 92.35%, with little further change in communication burden. This behavior can\
    \ be attributed to the stability of the dynamic threshold estimation: a larger\
    \ queue provides a more reliable and representative history of LLM confidence,\
    \ enabling more\n\n![](_page_12_Figure_1.jpeg)\n\n<span id=\"page-12-0\"></span>Fig.\
    \ 6. Confidence score and sample distribution across text lengths. Results are\
    \ derived from T5-Small on the Rotten Tomatoes dataset.\n\naccurate offloading\
    \ decisions. However, excessively large k yields minimal additional benefits,\
    \ as the confidence distribution converges. Therefore, in practical deployments,\
    \ setting k to a moderate value (e.g., 300–1000) is sufficient to achieve near-optimal\
    \ performance without unnecessary storage or computational overhead.\n\n## *C.\
    \ Impact of Cloud-side LLM*\n\nTo evaluate the robustness of RecServe against\
    \ variations in the cloud-side LLM, we replace the original cloud-tier model with\
    \ a version of DeBERTa-Large fine-tuned on SST-2 [\\[51\\]](#page-15-35), [\\\
    [52\\]](#page-15-36) without any tuning for multi-tier coordination. As shown\
    \ in Fig. [5,](#page-11-2) RecServe achieves a substantial improvement in serving\
    \ performance, whose accuracy is approximately 1% higher than ColServe. In terms\
    \ of communication, RecServe still maintains superior efficiency. The total communication\
    \ burden of RecServe is less than 30MB, significantly lower than ColServe's approximately\
    \ 45MB, representing a reduction of over 30%. These results demonstrate that RecServe\
    \ is robust to the choice of the cloud-side LLM, confirming its effectiveness\
    \ across different model configurations in practical deployment scenarios.\n\n\
    ## VII. DISCUSSION\n\nThis section extends the analysis of RecServe by considering\
    \ its wider implications and inherent challenges. We will elaborate on the potential\
    \ advantages of end-to-end service latency, investigate the reasons behind deviations\
    \ from our theoretical communication models, and discuss limitations and how they\
    \ can be mitigated in real-world deployments.\n\n# *A. Potential Benefits for\
    \ End-to-End Service Latency*\n\nRecServe offers potential benefits in end-to-end\
    \ service latency reduction for most inference tasks. By intelligently routing\
    \ dominant simple tasks to smaller LLMs at lower tiers, RecServe simultaneously\
    \ avoids the higher inference times of larger LLMs and eliminates the substantial\
    \ round-trip delays\n\n![](_page_12_Figure_10.jpeg)\n\n<span id=\"page-12-1\"\
    ></span>Fig. 7. Output length distribution across device, edge, and cloud tiers.\
    \ Results are derived from WMT16 dataset, taking β = 0.5.\n\nassociated with device-cloud\
    \ communications. Furthermore, RecServe's distributed inference architecture naturally\
    \ loadbalances inference tasks across the network hierarchy, reducing congestion\
    \ at higher tiers during peak periods when centralized services often face exponential\
    \ increases in queuing delays. These latency benefits are particularly valuable\
    \ for interactive applications where perceived responsiveness directly impacts\
    \ user engagement and satisfaction.\n\n# <span id=\"page-12-2\"></span>*B. Discrepancies\
    \ Between Theoretical and Actual Communication Burdens*\n\nWhile our theoretical\
    \ analysis offers an approximation of communication burden under idealized assumptions,\
    \ the actual results differ somewhat in practice due to several factors.\n\n-\
    \ Cold start of historical confidence queues. The theoretical model presumes a\
    \ historical confidence queue with sufficient samples to estimate dynamic offloading\
    \ thresholds accurately. However, during system initialization or earlystage deployment,\
    \ the queues may lack representative samples, leading to suboptimal threshold\
    \ estimation and irregular offloading decisions.\n- Correlation between input\
    \ length and confidence. The theoretical analysis assumes independence between\
    \ input sequence length |x| and the output confidence score C<sup>M</sup>i,τ (x),\
    \ which is impractical in reality, as shown in Fig. [6.](#page-12-0)\n- Variation\
    \ in output length distribution across tiers. The theoretical framework assumes\
    \ uniform output length |y| across LLMs deployed on all tiers. However, actual\
    \ output lengths may differ due to variations in LLM capacity, decoding strategies,\
    \ or sample-specific generation behavior, as illustrated in Fig. [7.](#page-12-1)\n\
    \nThese aforementioned factors collectively explain the observed deviations of\
    \ the actual communication burden from that predicted by our theoretical model.\n\
    \n# *C. Limitations and Countermeasures*\n\n*1) Upper-Tier Node Unavailability:*\
    \ In Eq. [\\(17\\)](#page-4-10), RecServe assumes that upper-tier nodes (e.g.,\
    \ edge servers and cloud data centers) are continuously available to process offloaded\
    \ tasks. However, in real-world deployments, these nodes may occasionally become\
    \ temporarily unavailable due to factors such as resource overload, network congestion,\
    \ or scheduled maintenance. To address this limitation, we define a function A(M′\
    \ ) that evaluates the availability of the next-tier node M′ for task offloading.\
    \ If offload is advised (CM,τ (x) < TM,τ (β) and M ̸= Mn) but the target node\
    \ M′ is unavailable (¬A(M′ )), than the current node M should cease further upward\
    \ recursion for that specific task and shoulder the responsibility for final task\
    \ execution. Incorporating this availability check, the unavailability-tolerant\
    \ recursive offloading decision policy Dut can be formulated as:\n\n$$\\begin{cases}\
    \ \\mathcal{D}\\_{ut}(x, M, \\tau) = \\\\ M, \\text{if } C\\_{M,\\tau}(x) \\ge\
    \ T\\_{M,\\tau}(\\beta) \\text{ or } \\neg A(M') \\\\ \\mathcal{D}\\_{ut}(x, M',\
    \ \\tau), \\\\ \\text{if } M \\ne M\\_n \\text{ and } C\\_{M,\\tau}(x) < T\\_{M,\\\
    tau}(\\beta) \\text{ and } A(M'), \\end{cases} (48)$$\n\nAccordingly, the revised\
    \ inference output is given by the following equations:\n\n$$y = M^{\\*\\*}(x),\
    \ \\\\ M^{\\*\\*} = \\mathcal{D}\\_{ut}(x, M\\_1, \\tau). \\tag{49}$$\n\nThis\
    \ fault-tolerant mechanism modification allows RecServe to gracefully degrade\
    \ service by completing the task at the current available tier when an intended\
    \ offload to a highertier node is prevented by that node's unavailability, ensuring\
    \ that RecServe maintains operational resilience in the face of unpredictable\
    \ network or node failures within the higher tiers.\n\n*2) Inference Serving Under\
    \ Communication Budget:* As discussed in subsection [VII-B,](#page-12-2) we observe\
    \ the systematic bias between the theoretical expected communication burden (denoted\
    \ as Etheo[Comm-RecServe(β)]) and the expected actual communication burden (denoted\
    \ as Eact[Comm-RecServe(β)]). This discrepancy complicates selecting an offloading\
    \ quantile β such that the actual burden meets a precise communication budget\
    \ Bcomm:\n\n$$E\\_{act}[\\text{Comm-RecServer}(\\beta)] \\approx B\\_{comm}.\\\
    tag{50}$$\n\nTo address this challenge and ensure that RecServe adheres to a predefined\
    \ communication budget, we propose an online feedback-based calibration mechanism\
    \ that dynamically adjusts β to align the actual communication burden with the\
    \ target budget. The designed procedure is as follows:\n\n1) Select an initial\
    \ value β<sup>0</sup> such that the theoretical expected communication burden\
    \ equals the target budget:\n\n$$E\\_{theo}[\\text{Comm-RecServe}(\\beta\\_0)]\
    \ = B\\_{comm}.\\tag{51}$$\n\n- 2) Over a window of R requests, measure the actual\
    \ expected communication burden for the current βt, where t indicates the current\
    \ calibration round.\n- 3) Compute the feedback ratio γ(βt) as follows:\n\n$$\\\
    gamma(\\beta\\_t) = \\frac{E\\_{act}[\\text{Common-RecService}(\\beta\\_t)]}{B\\\
    _{comm}}.\\qquad(52)$$\n\nWhen γ(βt) > 1, the actual burden exceeds the budget,\
    \ and vice versa.\n\n4) Update β using a proportional control strategy:\n\n$$\n\
    \\beta\\_{t+1} = \\frac{\\beta\\_t}{\\gamma(\\beta\\_t)^{\\eta}} \\tag{53}\n$$\n\
    \nwhere η is a hyperparameter that controls the adjustment rate.\n\n5) Repeat\
    \ steps 2–4 until convergence (γ(βt) ≈ 1), ensuring that the actual expected communication\
    \ burden closely matches the target budget.\n\nThis iterative calibration mechanism\
    \ enables RecServe to operate within a specified communication budget by adaptively\
    \ tuning its offloading parameter β based on real-time inference feedback.\n\n\
    # VIII. RELATED WORK\n\n# *A. Large Language Model*\n\nThe development of LLM\
    \ is fundamentally rooted in the Transformer architecture [\\[5\\]](#page-14-4),\
    \ evolving from foundation model design to generalized task adaptation and large-scale\
    \ deployment. One of the earliest representative LLMs, BERT [\\[18\\]](#page-15-3),\
    \ introduced a bidirectional Transformer encoder to effectively capture contextual\
    \ text information, achieving significant success in natural language understanding\
    \ tasks. Its successor, RoBERTa [\\[19\\]](#page-15-4), further enhanced performance\
    \ by increasing the training corpus size, extending the number of training epochs,\
    \ and removing the next sentence prediction objective. Since 2020, LLMs have entered\
    \ the era of hundred-billionparameter models, which became a mainstream trend\
    \ by 2022. A key milestone in this evolution was GPT-3.5 [\\[7\\]](#page-14-6),\
    \ [\\[8\\]](#page-14-7), which demonstrated notable capability in natural language\
    \ generation. GPT-4 series [\\[53\\]](#page-15-37), [\\[54\\]](#page-15-38) further\
    \ achieve superior performance in multimodal understanding and daily tasks. Concurrently,\
    \ the Claude series [\\[55\\]](#page-15-39)–[\\[57\\]](#page-15-40) has distinguished\
    \ itself through a strong focus on safety and alignment, enhancing the model's\
    \ ability to understand and respond to human intent while maintaining robust language\
    \ comprehension. In the open-source community, LLaMA series [\\[58\\]](#page-15-41)–\
    \ [\\[60\\]](#page-15-42) has been widely adopted in both academia and industry\
    \ for its efficiency, performance, and flexibility. Alibaba's Qwen series [\\\
    [61\\]](#page-16-0)–[\\[63\\]](#page-16-1) focuses on multilingual capabilities\
    \ and multimodal inference. DeepSeek-R1 [\\[9\\]](#page-14-8), leveraging a reinforcement\
    \ learning-driven pretraining paradigm, endows models with powerful reasoning\
    \ capabilities and achieves outstanding performance in tasks such as mathematics,\
    \ coding, and logic.\n\n# *B. Large Language Model Serving*\n\nEfficient inference\
    \ services for LLMs are essential to unlocking their full potential in real-world\
    \ applications. Mainstream LLM serving solutions rely on cloud-centric infrastructure,\
    \ leveraging abundant computing resources available in cloud data centers to achieve\
    \ multi-GPU parallel inference. PagedAttention [\\[64\\]](#page-16-2) applies\
    \ a virtual memory-inspired paging mechanism to optimize key-value (KV) cache\
    \ management, reducing memory waste and improving throughput. CacheGen [\\[65\\\
    ]](#page-16-3) introduces a KV cache compression method for improving bandwidth\
    \ utilization and dynamically adjusting compression levels to balance loading\
    \ delay and generation quality. ServerlessLLM [\\[12\\]](#page-14-11) significantly\
    \ reduces inference latency in serverless environments through fast multi-tier\
    \ checkpoint loading, efficient live migration, and optimized model scheduling.\
    \ DistServe [\\[13\\]](#page-14-12) separates pre-filling and decoding computations\
    \ onto different GPUs to improve parallelism and resource allocation. In edge-based\
    \ serving scenarios, optimizing resource efficiency becomes a central challenge\
    \ due to the scarce nature of computing capabilities on edge computing nodes.\
    \ AdaInf [\\[66\\]](#page-16-4) conducts a data drift-aware scheduling method\
    \ that combines incremental retraining and servicelevel-objective-compliant inference\
    \ with optimized CPU-GPU memory communications. EdgeShard [\\[67\\]](#page-16-5)\
    \ takes advantage of model partition by distributing model shards across multiple\
    \ edge resource-constrained devices and applying dynamic programming algorithms\
    \ to optimize inference latency and throughput. Additionally, edge-cloud collaborative\
    \ serving offers a promising approach for balancing performance and resource efficiency.\
    \ PerLLM [\\[14\\]](#page-14-13) introduces a constraintaware personalized scheduling\
    \ framework in edge-cloud environments to meet latency requirements while reducing\
    \ energy consumption.\n\n## *C. Task Offloading for Artificial Intelligence*\n\
    \nTask offloading has long been a research focus in the fields of edge and cloud\
    \ computing [\\[30\\]](#page-15-14), [\\[31\\]](#page-15-15), which offload an\
    \ application module from the local computing node to a remote server for execution\
    \ and retrieve the results upon completion. With the rapid development of artificial\
    \ intelligence (AI), task offloading has been extended to the training and inference\
    \ processes of both Deep Neural Networks (DNN) and LLMs. Split Computing [\\[68\\\
    ]](#page-16-6) executes the initial layers of a neural network on a local device\
    \ and offloads the remaining layers to edge servers, achieving low-latency response\
    \ with acceptable accuracy. Mohammed [\\[69\\]](#page-16-7) proposes a DNN offloading\
    \ strategy based on adaptive fine-grained partitioning and a swap-matching-based\
    \ algorithm to dynamically divide DNNs into sub-layer components and efficiently\
    \ distribute inference tasks across the network, significantly reducing inference\
    \ latency. Mandheling [\\[70\\]](#page-16-8) introduces the first efficient on-device\
    \ training system by integrating mixed-precision training with on-chip Digital\
    \ Signal Processing (DSP) offloading, saving both convergence time and energy\
    \ consumption. In addition, InstInfer [\\[71\\]](#page-16-9) offloads attention\
    \ computation and KV cache management to Computational Storage Drives (CSDs) during\
    \ decoding, alleviating PCIe bottlenecks and improving inference efficiency for\
    \ long-context LLMs.\n\n## IX. CONCLUSION\n\nThis paper proposes RecServe, a recursive\
    \ offloading framework designed for LLM serving in multi-tier networks, which\
    \ avoid unnecessary cloud-based LLM invocations while addressing the critical\
    \ balance between inference quality and communication burden. RecServe deploys\
    \ LLMs with progressively increasing capabilities across device, edge, and cloud\
    \ nodes. By combining task-specific confidence evaluations with dynamic offloading\
    \ guided by sliding historical knowledge queues, RecServe enables the majority\
    \ of inference tasks to be efficiently completed at lower tiers, while selectively\
    \ offloading only the more complex tasks with insufficient confidence to higher-tier\
    \ nodes. Theoretical analysis provides parameter bounds that facilitate the alleviation\
    \ of expected communication burden and computational costs, respectively. Extensive\
    \ experiments demonstrate that RecServe significantly outperforms existing multi-tier\
    \ LLM serving approaches in both service quality and communication efficiency,\
    \ and achieves substantial reductions in communication burden compared to cloud-only\
    \ deployments.\n\n## ACKNOWLEDGEMENT\n\nWe gratefully acknowledge Zixuan Li (Institute\
    \ of Automation, Chinese Academy of Sciences) and Yanyu Ren (Tsinghua University)\
    \ for their insightful technical guidance related to large language models.\n\n\
    ## REFERENCES\n\n- <span id=\"page-14-0\"></span>[1] J. Ren, D. Zhang, S. He,\
    \ Y. Zhang, and T. Li, \"A survey on endedge-cloud orchestrated network computing\
    \ paradigms: Transparent computing, mobile edge computing, fog computing, and\
    \ cloudlet,\" *ACM Computing Surveys (CSUR)*, vol. 52, no. 6, pp. 1–36, 2019.\n\
    - <span id=\"page-14-1\"></span>[2] S. Duan, D. Wang, J. Ren, F. Lyu, Y. Zhang,\
    \ H. Wu, and X. Shen, \"Distributed artificial intelligence empowered by end-edge-cloud\
    \ computing: A survey,\" *IEEE Communications Surveys & Tutorials*, vol. 25, no.\
    \ 1, pp. 591–624, 2022.\n- <span id=\"page-14-2\"></span>[3] Y. Wang, C. Yang,\
    \ S. Lan, L. Zhu, and Y. Zhang, \"End-edge-cloud collaborative computing for deep\
    \ learning: A comprehensive survey,\" *IEEE Communications Surveys & Tutorials*,\
    \ 2024.\n- <span id=\"page-14-3\"></span>[4] A. Radford, J. Wu, R. Child, D. Luan,\
    \ D. Amodei, I. Sutskever *et al.*, \"Language models are unsupervised multitask\
    \ learners,\" *OpenAI blog*, vol. 1, no. 8, p. 9, 2019.\n- <span id=\"page-14-4\"\
    ></span>[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\
    \ Ł. Kaiser, and I. Polosukhin, \"Attention is all you need,\" *Advances in neural\
    \ information processing systems*, vol. 30, 2017.\n- <span id=\"page-14-5\"></span>[6]\
    \ H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes,\
    \ and A. Mian, \"A comprehensive overview of large language models,\" *arXiv preprint\
    \ arXiv:2307.06435*, 2023.\n- <span id=\"page-14-6\"></span>[7] OpenAI, \"Gpt-3.5\
    \ turbo: Legacy gpt model for cheaper chat and non-chat tasks,\" 2022. [Online].\
    \ Available: [https://platform.openai.com/](https://platform.openai.com/docs/models/gpt-3.5-turbo)\
    \ [docs/models/gpt-3.5-turbo](https://platform.openai.com/docs/models/gpt-3.5-turbo)\n\
    - <span id=\"page-14-7\"></span>[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J.\
    \ D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell *et al.*,\
    \ \"Language models are few-shot learners,\" *Advances in neural information processing\
    \ systems*, vol. 33, pp. 1877–1901, 2020.\n- <span id=\"page-14-8\"></span>[9]\
    \ D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang,\
    \ X. Bi *et al.*, \"Deepseek-r1: Incentivizing reasoning capability in llms via\
    \ reinforcement learning,\" *arXiv preprint arXiv:2501.12948*, 2025.\n- <span\
    \ id=\"page-14-9\"></span>[10] Z. Liu, C. Zhao, F. Iandola, C. Lai, Y. Tian, I.\
    \ Fedorov, Y. Xiong, E. Chang, Y. Shi, R. Krishnamoorthi *et al.*, \"Mobilellm:\
    \ Optimizing subbillion parameter language models for on-device use cases,\" in\
    \ *Forty-first International Conference on Machine Learning*, 2024.\n- <span id=\"\
    page-14-10\"></span>[11] S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng,\
    \ Y. Fang, Y. Huang, W. Zhao *et al.*, \"Minicpm: Unveiling the potential of small\
    \ language models with scalable training strategies,\" *arXiv preprint arXiv:2404.06395*,\
    \ 2024.\n- <span id=\"page-14-11\"></span>[12] Y. Fu, L. Xue, Y. Huang, A.-O.\
    \ Brabete, D. Ustiugov, Y. Patel, and L. Mai, \"{ServerlessLLM}:{Low-Latency}\
    \ serverless inference for large language models,\" in *18th USENIX Symposium\
    \ on Operating Systems Design and Implementation (OSDI 24)*, 2024, pp. 135–153.\n\
    - <span id=\"page-14-12\"></span>[13] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu,\
    \ X. Liu, X. Jin, and H. Zhang, \"{DistServe}: Disaggregating prefill and decoding\
    \ for goodput-optimized large language model serving,\" in *18th USENIX Symposium\
    \ on Operating Systems Design and Implementation (OSDI 24)*, 2024, pp. 193–210.\n\
    - <span id=\"page-14-13\"></span>[14] Z. Yang, Y. Yang, C. Zhao, Q. Guo, W. He,\
    \ and W. Ji, \"Perllm: Personalized inference scheduling with edge-cloud collaboration\
    \ for diverse llm services,\" *arXiv preprint arXiv:2405.14636*, 2024.\n- <span\
    \ id=\"page-15-0\"></span>[15] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K.\
    \ Huang, \"Pushing large language models to the 6g edge: Vision, challenges, and\
    \ opportunities,\" *arXiv preprint arXiv:2309.16739*, 2023.\n- <span id=\"page-15-1\"\
    ></span>[16] L. Lebovitz, L. Cavigelli, M. Magno, and L. K. Muller, \"Efficient\
    \ inference with model cascades,\" *Transactions on Machine Learning Research*,\
    \ 2023.\n- <span id=\"page-15-2\"></span>[17] A. Yang, B. Yang, B. Zhang, B. Hui,\
    \ B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei *et al.*, \"Qwen2. 5 technical\
    \ report,\" *arXiv preprint arXiv:2412.15115*, 2024.\n- <span id=\"page-15-3\"\
    ></span>[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training\
    \ of deep bidirectional transformers for language understanding,\" in *Proceedings\
    \ of the 2019 conference of the North American chapter of the association for\
    \ computational linguistics: human language technologies, volume 1 (long and short\
    \ papers)*, 2019, pp. 4171–4186.\n- <span id=\"page-15-4\"></span>[19] Y. Liu,\
    \ M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\
    \ and V. Stoyanov, \"Roberta: A robustly optimized bert pretraining approach,\"\
    \ *arXiv preprint arXiv:1907.11692*, 2019.\n- <span id=\"page-15-5\"></span>[20]\
    \ A. Radford, K. Narasimhan, T. Salimans, I. Sutskever *et al.*, \"Improving language\
    \ understanding by generative pre-training,\" 2018.\n- <span id=\"page-15-6\"\
    ></span>[21] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\
    \ W. Chen *et al.*, \"Lora: Low-rank adaptation of large language models.\" *ICLR*,\
    \ vol. 1, no. 2, p. 3, 2022.\n- <span id=\"page-15-7\"></span>[22] N. Houlsby,\
    \ A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan,\
    \ and S. Gelly, \"Parameter-efficient transfer learning for nlp,\" in *International\
    \ conference on machine learning*. PMLR, 2019, pp. 2790–2799.\n- <span id=\"page-15-8\"\
    ></span>[23] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\
    \ C. Zhang, S. Agarwal, K. Slama, A. Ray *et al.*, \"Training language models\
    \ to follow instructions with human feedback,\" *Advances in neural information\
    \ processing systems*, vol. 35, pp. 27 730–27 744, 2022.\n- <span id=\"page-15-9\"\
    ></span>[24] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\
    \ Y. Zhou, W. Li, and P. J. Liu, \"Exploring the limits of transfer learning with\
    \ a unified text-to-text transformer,\" *Journal of machine learning research*,\
    \ vol. 21, no. 140, pp. 1–67, 2020.\n- <span id=\"page-15-10\"></span>[25] S.\
    \ Duan, D. Wang, J. Ren, F. Lyu, Y. Zhang, H. Wu, and X. Shen, \"Distributed artificial\
    \ intelligence empowered by end-edge-cloud computing: A survey,\" *IEEE Communications\
    \ Surveys & Tutorials*, vol. 25, no. 1, pp. 591–624, 2023.\n- [26] Z. Yang, S.\
    \ Fu, W. Bao, D. Yuan, and A. Y. Zomaya, \"Hierarchical federated learning with\
    \ momentum acceleration in multi-tier networks,\" *IEEE Transactions on Parallel\
    \ and Distributed Systems*, 2023.\n- <span id=\"page-15-11\"></span>[27] Z. Wu,\
    \ S. Sun, Y. Wang, M. Liu, K. Xu, Q. Pan, B. Gao, and T. Wen, \"Beyond model scale\
    \ limits: End-edge-cloud federated learning with self-rectified knowledge agglomeration,\"\
    \ *arXiv preprint arXiv:2501.00693*, 2025.\n- <span id=\"page-15-12\"></span>[28]\
    \ W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, \"Edge computing: Vision and challenges,\"\
    \ *IEEE internet of things journal*, vol. 3, no. 5, pp. 637–646, 2016.\n- <span\
    \ id=\"page-15-13\"></span>[29] Z. Wu, S. Sun, Y. Wang, M. Liu, B. Gao, Q. Pan,\
    \ T. He, and X. Jiang, \"Agglomerative federated learning: Empowering larger model\
    \ training via end-edge-cloud collaboration,\" in *IEEE INFOCOM 2024- IEEE Conference\
    \ on Computer Communications*. IEEE, 2024, pp. 131– 140.\n- <span id=\"page-15-14\"\
    ></span>[30] A. Islam, A. Debnath, M. Ghose, and S. Chakraborty, \"A survey on\
    \ task offloading in multi-access edge computing,\" *Journal of Systems Architecture*,\
    \ vol. 118, p. 102225, 2021.\n- <span id=\"page-15-15\"></span>[31] F. Saeik,\
    \ M. Avgeris, D. Spatharakis, N. Santi, D. Dechouniotis, J. Violos, A. Leivadeas,\
    \ N. Athanasopoulos, N. Mitton, and S. Papavassiliou, \"Task offloading in edge\
    \ and cloud computing: A survey on mathematical, artificial intelligence and control\
    \ theory solutions,\" *Computer Networks*, vol. 195, p. 108177, 2021.\n- <span\
    \ id=\"page-15-16\"></span>[32] S. Dong, J. Tang, K. Abbas, R. Hou, J. Kamruzzaman,\
    \ L. Rutkowski, and R. Buyya, \"Task offloading strategies for mobile edge computing:\
    \ A survey,\" *Computer Networks*, p. 110791, 2024.\n- <span id=\"page-15-17\"\
    ></span>[33] T. Pearce, A. Brintrup, and J. Zhu, \"Understanding softmax confidence\
    \ and uncertainty,\" *arXiv preprint arXiv:2106.04972*, 2021.\n- <span id=\"page-15-18\"\
    ></span>[34] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,\
    \ \"Learning word vectors for sentiment analysis,\" in *Proceedings of the 49th\
    \ annual meeting of the association for computational linguistics: Human language\
    \ technologies*, 2011, pp. 142–150.\n- <span id=\"page-15-19\"></span>[35] R.\
    \ Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts,\
    \ \"Recursive deep models for semantic compositionality over a sentiment treebank,\"\
    \ in *Proceedings of the 2013 conference on empirical methods in natural language\
    \ processing*, 2013, pp. 1631–1642.\n- <span id=\"page-15-20\"></span>[36] B.\
    \ Pang and L. Lee, \"Seeing stars: Exploiting class relationships for sentiment\
    \ categorization with respect to rating scales,\" *arXiv preprint cs/0506075*,\
    \ 2005.\n- <span id=\"page-15-21\"></span>[37] X. Zhang, J. Zhao, and Y. LeCun,\
    \ \"Character-level convolutional networks for text classification,\" *Advances\
    \ in neural information processing systems*, vol. 28, 2015.\n- <span id=\"page-15-22\"\
    ></span>[38] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck,\
    \ A. J. Yepes, P. Koehn, V. Logacheva, C. Monz *et al.*, \"Findings of the 2016\
    \ conference on machine translation (wmt16),\" in *First conference on machine\
    \ translation*. Association for Computational Linguistics, 2016, pp. 131–198.\n\
    - <span id=\"page-15-23\"></span>[39] L. Barrault, O. Bojar, M. R. Costa-Jussa,\
    \ C. Federmann, M. Fishel, Y. Graham, B. Haddow, M. Huck, P. Koehn, S. Malmasi\
    \ *et al.*, \"Findings of the 2019 conference on machine translation (wmt19).\"\
    \ ACL, 2019.\n- <span id=\"page-15-24\"></span>[40] J. Tiedemann, \"Parallel data,\
    \ tools and interfaces in opus.\" in *Lrec*, vol. 2012. Citeseer, 2012, pp. 2214–2218.\n\
    - <span id=\"page-15-25\"></span>[41] V. Sanh, L. Debut, J. Chaumond, and T. Wolf,\
    \ \"Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,\"\
    \ *ArXiv*, vol. abs/1910.01108, 2019.\n- <span id=\"page-15-26\"></span>[42] azizbarank,\
    \ \"azizbarank/distilroberta-base-sst2-distilled,\" 2022. [Online]. Available:\
    \ [https://huggingface.co/azizbarank/](https://huggingface.co/azizbarank/distilroberta-base-sst2-distilled)\
    \ [distilroberta-base-sst2-distilled](https://huggingface.co/azizbarank/distilroberta-base-sst2-distilled)\n\
    - <span id=\"page-15-27\"></span>[43] textattack, \"textattack/roberta-base-sst-2,\"\
    \ 2023. [Online]. Available: <https://huggingface.co/textattack/roberta-base-SST-2>\n\
    - <span id=\"page-15-28\"></span>[44] howey, \"howey/roberta-large-sst2,\" 2021.\
    \ [Online]. Available: [https:](https://huggingface.co/howey/roberta-large-sst2)\
    \ [//huggingface.co/howey/roberta-large-sst2](https://huggingface.co/howey/roberta-large-sst2)\n\
    - <span id=\"page-15-29\"></span>[45] SEBIS, \"Sebis/legal t5 small trans de en\
    \ small finetuned,\" 2021. [Online]. Available: [https://huggingface.co/SEBIS/legal](https://huggingface.co/SEBIS/legal_t5_small_trans_de_en_small_finetuned)\
    \ t5 small trans de en small [finetuned](https://huggingface.co/SEBIS/legal_t5_small_trans_de_en_small_finetuned)\n\
    - <span id=\"page-15-30\"></span>[46] J. Tiedemann and S. Thottingal, \"Opus-mt–building\
    \ open translation services for the world,\" in *Annual Conference of the European\
    \ Association for Machine Translation*. European Association for Machine Translation,\
    \ 2020, pp. 479–480.\n- <span id=\"page-15-31\"></span>[47] PontifexMaximus, \"\
    Pontifexmaximus/opus-mt-de-en-finetuned-de-toen,\" 2022. [Online]. Available:\
    \ [https://huggingface.co/PontifexMaximus/](https://huggingface.co/PontifexMaximus/opus-mt-de-en-finetuned-de-to-en)\
    \ [opus-mt-de-en-finetuned-de-to-en](https://huggingface.co/PontifexMaximus/opus-mt-de-en-finetuned-de-to-en)\n\
    - <span id=\"page-15-32\"></span>[48] J. Kasai, N. Pappas, H. Peng, J. Cross,\
    \ and N. A. Smith, \"Deep encoder, shallow decoder: Reevaluating non-autoregressive\
    \ machine translation,\" *arXiv preprint arXiv:2006.10369*, 2020.\n- <span id=\"\
    page-15-33\"></span>[49] allenai, \"allenai/wmt19-de-en-6-6-big,\" 2023. [Online].\
    \ Available: <https://huggingface.co/allenai/wmt19-de-en-6-6-big>\n- <span id=\"\
    page-15-34\"></span>[50] A. Padmanabhan, N. Agarwal, A. Iyer, G. Ananthanarayanan,\
    \ Y. Shu, N. Karianakis, G. H. Xu, and R. Netravali, \"Gemel: Model merging for\
    \ {Memory-Efficient},{Real-Time} video analytics at the edge,\" in *20th USENIX\
    \ Symposium on Networked Systems Design and Implementation (NSDI 23)*, 2023, pp.\
    \ 973–994.\n- <span id=\"page-15-35\"></span>[51] P. He, X. Liu, J. Gao, and W.\
    \ Chen, \"Deberta: Decoding-enhanced bert with disentangled attention,\" *arXiv\
    \ preprint arXiv:2006.03654*, 2020.\n- <span id=\"page-15-36\"></span>[52] Tomor0720,\
    \ \"Tomor0720/deberta-large-finetuned-sst2,\" 2023. [Online]. Available: [https://huggingface.co/Tomor0720/](https://huggingface.co/Tomor0720/deberta-large-finetuned-sst2)\
    \ [deberta-large-finetuned-sst2](https://huggingface.co/Tomor0720/deberta-large-finetuned-sst2)\n\
    - <span id=\"page-15-37\"></span>[53] J. Achiam, S. Adler, S. Agarwal, L. Ahmad,\
    \ I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat\
    \ *et al.*, \"Gpt-4 technical report,\" *arXiv preprint arXiv:2303.08774*, 2023.\n\
    - <span id=\"page-15-38\"></span>[54] S. Shahriar, B. D. Lund, N. R. Mannuru,\
    \ M. A. Arshad, K. Hayawi, R. V. K. Bevara, A. Mannuru, and L. Batool, \"Putting\
    \ gpt-4o to the sword: A comprehensive evaluation of language, vision, speech,\
    \ and multimodal proficiency,\" *Applied Sciences*, vol. 14, no. 17, p. 7782,\
    \ 2024.\n- <span id=\"page-15-39\"></span>[55] Anthropic, \"The Claude 3 Model\
    \ Family: Opus, Sonnet, Haiku,\" 2024. [Online]. Available: [https://www-cdn.anthropic.com/](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)\
    \ [de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)\
    \ Card Claude [3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)\n\
    - [56] Anthropic, \"Claude 3-5 sonnet,\" 2024. [Online]. Available: [https:](https://www.anthropic.com/news/claude-3-5-sonnet)\
    \ [//www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)\n\
    - <span id=\"page-15-40\"></span>[57] Anthropic, \"Claude 3.7 sonnet and claude\
    \ code,\" 2025. [Online]. Available:<https://www.anthropic.com/news/claude-3-7-sonnet>\n\
    - <span id=\"page-15-41\"></span>[58] H. Touvron, T. Lavril, G. Izacard, X. Martinet,\
    \ M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar ` *et al.*,\
    \ \"Llama: Open and efficient foundation language models,\" *arXiv preprint arXiv:2302.13971*,\
    \ 2023.\n- [59] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\
    \ N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale *et al.*, \"Llama 2: Open foundation\
    \ and fine-tuned chat models,\" *arXiv preprint arXiv:2307.09288*, 2023.\n- <span\
    \ id=\"page-15-42\"></span>[60] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey,\
    \ A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan *et al.*,\
    \ \"The llama 3 herd of models,\" *arXiv preprint arXiv:2407.21783*, 2024.\n-\
    \ <span id=\"page-16-0\"></span>[61] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang,\
    \ X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang *et al.*, \"Qwen technical report,\"\
    \ *arXiv preprint arXiv:2309.16609*, 2023.\n- [62] P. Wang, S. Bai, S. Tan, S.\
    \ Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge *et al.*, \"Qwen2-vl:\
    \ Enhancing vision-language model's perception of the world at any resolution,\"\
    \ *arXiv preprint arXiv:2409.12191*, 2024.\n- <span id=\"page-16-1\"></span>[63]\
    \ S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang,\
    \ J. Tang *et al.*, \"Qwen2. 5-vl technical report,\" *arXiv preprint arXiv:2502.13923*,\
    \ 2025.\n- <span id=\"page-16-2\"></span>[64] W. Kwon, Z. Li, S. Zhuang, Y. Sheng,\
    \ L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, \"Efficient memory\
    \ management for large language model serving with pagedattention,\" in *Proceedings\
    \ of the 29th Symposium on Operating Systems Principles*, 2023, pp. 611–626.\n\
    - <span id=\"page-16-3\"></span>[65] Y. Liu, H. Li, Y. Cheng, S. Ray, Y. Huang,\
    \ Q. Zhang, K. Du, J. Yao, S. Lu, G. Ananthanarayanan *et al.*, \"Cachegen: Kv\
    \ cache compression and streaming for fast large language model serving,\" in\
    \ *Proceedings of the ACM SIGCOMM 2024 Conference*, 2024, pp. 38–56.\n- <span\
    \ id=\"page-16-4\"></span>[66] S. S. Shubha and H. Shen, \"Adainf: Data drift\
    \ adaptive scheduling for accurate and slo-guaranteed multiple-model inference\
    \ serving at edge servers,\" in *Proceedings of the ACM SIGCOMM 2023 Conference*,\
    \ 2023, pp. 473–485.\n- <span id=\"page-16-5\"></span>[67] M. Zhang, X. Shen,\
    \ J. Cao, Z. Cui, and S. Jiang, \"Edgeshard: Efficient llm inference via collaborative\
    \ edge computing,\" *IEEE Internet of Things Journal*, 2024.\n- <span id=\"page-16-6\"\
    ></span>[68] Y. Matsubara, M. Levorato, and F. Restuccia, \"Split computing and\
    \ early exiting for deep learning applications: Survey and research challenges,\"\
    \ *ACM Computing Surveys*, vol. 55, no. 5, pp. 1–30, 2022.\n- <span id=\"page-16-7\"\
    ></span>[69] T. Mohammed, C. Joe-Wong, R. Babbar, and M. Di Francesco, \"Distributed\
    \ inference acceleration with adaptive dnn partitioning and offloading,\" in *IEEE\
    \ INFOCOM 2020-IEEE conference on computer communications*. IEEE, 2020, pp. 854–863.\n\
    - <span id=\"page-16-8\"></span>[70] D. Xu, M. Xu, Q. Wang, S. Wang, Y. Ma, K.\
    \ Huang, G. Huang, X. Jin, and X. Liu, \"Mandheling: Mixed-precision on-device\
    \ dnn training with dsp offloading,\" in *Proceedings of the 28th Annual International\
    \ Conference on Mobile Computing And Networking*, 2022, pp. 214–227.\n- <span\
    \ id=\"page-16-9\"></span>[71] X. Pan, E. Li, Q. Li, S. Liang, Y. Shan, K. Zhou,\
    \ Y. Luo, X. Wang, and J. Zhang, \"Instinfer: In-storage attention offloading\
    \ for cost-effective long-context llm inference,\" *arXiv preprint arXiv:2409.04992*,\
    \ 2024.\n\n![](_page_16_Picture_12.jpeg)\n\nZhiyuan Wu is currently a research\
    \ assistant with the Institute of Computing Technology, Chinese Academy of Sciences.\
    \ He has contributed several technical papers to top-tier conferences and journals\
    \ as the first author in the fields of computer architecture, computer networks,\
    \ and intelligent systems, including IEEE Transactions on Parallel and Distributed\
    \ Systems (TPDS), IEEE Transactions on Mobile Computing (TMC), IEEE International\
    \ Conference on Computer Communications (INFOCOM), and ACM Transactions on Intelligent\
    \ Systems and\n\nTechnology (TIST). His research interests include edge intelligence,\
    \ distributed systems, and services computing.\n\n![](_page_16_Picture_15.jpeg)\n\
    \nYuwei Wang (Member, IEEE) received his Ph.D. degree in computer science from\
    \ the University of Chinese Academy of Sciences, Beijing, China. He is currently\
    \ an associate professor at the Institute of Computing Technology, Chinese Academy\
    \ of Sciences. He has been responsible for setting over 30 international and national\
    \ standards, and also holds various positions in both international and national\
    \ industrial standards development organizations (SDOs) as well as local research\
    \ institutions, including the associate rapporteur at the ITU-T\n\nSG21 Q5, and\
    \ the deputy director of China Communications Standards Association (CCSA) TC1\
    \ WG1. His current research interests include federated learning, mobile edge\
    \ computing, and next-generation network architecture.\n\n![](_page_16_Picture_18.jpeg)\n\
    \nMin Liu (Senior Member, IEEE) received her Ph.D degree in computer science from\
    \ the Graduate University of the Chinese Academy of Sciences, China. Before that,\
    \ she received her B.S. and M.S. degrees in computer science from Xi'an Jiaotong\
    \ University, China. She is currently a professor at the Institute of Computing\
    \ Technology, Chinese Academy of Sciences, and also holds a position at the Zhongguancun\
    \ Laboratory. Her current research interests include mobile computing and edge\
    \ intelligence.\n\n![](_page_16_Picture_20.jpeg)\n\nBo Gao (Member, IEEE) received\
    \ his M.S. degree in electrical engineering from the School of Electronic Information\
    \ and Electrical Engineering at Shanghai Jiaotong University, Shanghai, China\
    \ in 2009, and his Ph.D. degree in computer engineering from the Bradley Department\
    \ of Electrical and Computer Engineering at Virginia Tech, Blacksburg, USA in\
    \ 2014. He was an Assistant Professor with the Institute of Computing Technology\
    \ at Chinese Academy of Sciences, Beijing, China from 2014 to 2017. He was a Visiting\
    \ Researcher with the School of Com-\n\nputing and Communications at Lancaster\
    \ University, Lancaster, UK from 2018 to 2019. He is currently an Associate Professor\
    \ with the School of Computer and Information Technology at Beijing Jiaotong University,\
    \ Beijing, China. He has directed a number of research projects sponsored by the\
    \ National Natural Science Foundation of China (NSFC) or other funding agencies.\
    \ He is a member of IEEE, ACM, and China Computer Federation (CCF). His research\
    \ interests include wireless networking, mobile/edge computing, multiagent systems,\
    \ and machine learning.\n\n![](_page_16_Picture_23.jpeg)\n\nSheng Sun is currently\
    \ an associate professor at the Institute of Computing Technology, Chinese Academy\
    \ of Sciences. She received her bachelor's degree from Beihang University, and\
    \ her Ph.D. from the Institute of Computing Technology, Chinese Academy of Sciences.\
    \ Dr. Sun has led or executed 5 major funded research projects and published over\
    \ 20 technical papers in journals and conferences related to computer network\
    \ and distributed systems, including IEEE Transactions on Parallel and Distributed\
    \ Systems (TPDS), IEEE Transactions on\n\nMobile Computing (TMC), and IEEE International\
    \ Conference on Computer Communications (INFOCOM). Her research interests include\
    \ federated learning, edge intelligence, and privacy computing.\n\n![](_page_16_Picture_26.jpeg)\n\
    \nJinda Lu is currently a Phd candidate with the School of Cyber Science and Technology,\
    \ University of Science and Technology of China. Before that, he received his\
    \ bachelor's degree with honor at Jilin University. His research interests include\
    \ large language models and multi modality.\n\n![](_page_17_Picture_1.jpeg)\n\n\
    Zheming Yang received his B.Sc. degree in electronic engineering from the North\
    \ China University of Science and Technology in 2019 and his Ph.D. degree in computer\
    \ science from the Institute of Computing Technology, Chinese Academy of Sciences,\
    \ in 2024. He is currently an assistant professor with the Institute of Computing\
    \ Technology, Chinese Academy of Sciences, Beijing, China. He is also a visiting\
    \ scholar with the School of Computing, National University of Singapore. His\
    \ current research interests include multimedia systems, edge/cloud\n\ncomputing,\
    \ LLM inference optimization, and efficient AI system design. In addition, his\
    \ research won the IFTC Best Paper Award, the IEEE ISPA Outstanding Paper Award,\
    \ and the CCF HPC China Outstanding Paper Award.\n\n![](_page_17_Picture_4.jpeg)\n\
    \nTian Wen is currently a research assistant with the Institute of Computing Technology,\
    \ Chinese Academy of Sciences. His research interests include federated learning,\
    \ edge computing, and information security."
- title: 'Smaller, Smarter, Closer: The Edge of Collaborative Generative AI'
  abstract: 'The rapid adoption of generative AI (GenAI), particularly Large Language

    Models (LLMs), has exposed critical limitations of cloud-centric deployments,

    including latency, cost, and privacy concerns. Meanwhile, Small Language Models

    (SLMs) are emerging as viable alternatives for resource-constrained edge

    environments, though they often lack the capabilities of their larger

    counterparts. This article explores the potential of collaborative inference

    systems that leverage both edge and cloud resources to address these

    challenges. By presenting distinct cooperation strategies alongside practical

    design principles and experimental insights, we offer actionable guidance for

    deploying GenAI across the computing continuum.'
  url: http://arxiv.org/abs/2505.16499v1
  keywords: Generative AI at the Edge, Collaborative Inference, Agentic Systems, Edge
    Intelligence, Edge-Cloud Orchestration.
  document: '# Smaller, Smarter, Closer: The Edge of Collaborative Generative AI


    Roberto Morabito *EURECOM* Biot, France roberto.morabito@eurecom.fr


    SiYoung Jang *Nokia Bell Labs* Cambridge, United Kingdom siyoung.jang@nokia-bell-labs.com


    *Abstract*—The rapid adoption of generative AI (GenAI), particularly Large Language
    Models (LLMs), has exposed critical limitations of cloud-centric deployments,
    including latency, cost, and privacy concerns. Meanwhile, Small Language Models
    (SLMs) are emerging as viable alternatives for resourceconstrained edge environments,
    though they often lack the capabilities of their larger counterparts. This article
    explores the potential of collaborative inference systems that leverage both edge
    and cloud resources to address these challenges. By presenting distinct cooperation
    strategies alongside practical design principles and experimental insights, we
    offer actionable guidance for deploying GenAI across the computing continuum.
    Ultimately, this work underscores the great potential of *edgefirst* approaches
    in realizing the promise of GenAI in diverse, real-world applications.


    *Index Terms*—Generative AI at the Edge, Collaborative Inference, Agentic Systems,
    Edge Intelligence, Edge-Cloud Orchestration.


    It is no longer necessary to elaborate extensively on the transformative impact
    of generative AI (GenAI) models, particularly Large Language Models (LLMs), across
    various sectors of society. From healthcare to education, entertainment to software
    development and IoT [\[1\]](#page-6-0), it is evident that nearly every application
    domain is ready (or already is) to be influenced by these technologies. LLMs like
    GPT-4, powered by transformer architectures with billions of parameters, excel
    in diverse NLP tasks (e.g., summarization, translation, query answering) and high-level
    reasoning. In this paper, we often refer to language models as the most advanced
    and widely adopted category of GenAI. However, the considerations and strategies
    discussed are equally relevant to other types of GenAI models, such as those used
    for image generation or multimodal applications, as their characteristics and
    challenges often overlap with those of language models. For example, models such
    as ChatGPT-4o achieve top benchmarks in *general knowledge* (MMLU), *math reasoning*
    (GSM8K), and *code generation* (HumanEval) while integrating multimodal capabilities
    [\[2\]](#page-6-1), redefining machine potential in modern digital ecosystems.
    These capabilities come with high computational and storage demands, typically
    managed via cloud hosting.


    While the transformative potential of GenAI is undeniable, from the perspective
    of distributed AI and systems, significant *pre-GenAI era* efforts aimed to shift
    AI execution closer to the edge. The motivations that drove these efforts—reducing
    latency, enhancing reliability, and improving user experiences—remain equally
    relevant in the context of GenAI. However, with LLMs, for example, additional
    challenges emerge. A persistent issue with cloud-based LLMs is the high cost of
    provisioning these services [\[3\]](#page-6-2). Beyond costs, there are growing
    concerns regarding security and privacy. For instance, content submitted to popular
    LLM platforms—such as user prompts, model responses, and other data—is often used
    to improve model performance. This occurs routinely, as services like ChatGPT
    have become as ubiquitous as web search engines, with little thought given to
    the implications of such data sharing. Moreover, the centralized nature of cloud-based
    LLMs introduces risks of service disruption. For example, a four-hour outage of
    OpenAI''s services caused significant disruptions for applications heavily reliant
    on its APIs [\[4\]](#page-6-3), underscoring the reliability challenges of centralized
    systems. These issues are further compounded by the impact of GenAI on network
    traffic, as highlighted in [\[5\]](#page-6-4). Increased use of GenAI-driven video
    assistants and immersive interactions is expected to drive significant growth
    in both uplink and downlink traffic, placing additional strain on network infrastructure.
    On the other hand, GenAI''s compression capabilities are likely to find utility
    in closed ecosystem applications, including edge-based deployments, where they
    could play a key role in reducing traffic demands by processing and semantically
    compressing data closer to users, thereby alleviating strain on network infrastructure.
    These concrete issues from the perspective of *GenAI over the Internet* are driving
    the push toward edgebased solutions.


    <span id="page-0-0"></span>![](_page_0_Figure_10.jpeg)


    Fig. 1: Interaction between edge-based and cloud-based SLMenabled agents.


    As the reliance on edge-based solutions grows, Small Lan-


    This paper is currently under review for publication in an IEEE magazine. If accepted,
    the copyright will be transferred to IEEE.


    guage Models (SLMs) are emerging as a key enabler for addressing these challenges
    through lightweight, *agentic* systems designed for collaborative and distributed
    AI, as highlighted in [\[6\]](#page-6-5). Adopting SLMs at the edge, therefore,
    should not be viewed solely as an engineering or data science endeavor to shrink
    models while maintaining quality. Instead, from a distributed AI standpoint, this
    shift represents an opportunity to address these challenges holistically.


    While SLMs are the way forward to efficiently bring GenAI to resource-constrained
    environments, they do not fully resolve the challenges associated with deploying
    AI at the edge. The limitations of SLMs, particularly in delivering the qualitative
    and quantitative performance of their larger counterparts, make it unlikely that
    the cloud can be completely replaced [\[7\]](#page-7-0). Instead, we argue for
    a complementary relationship between the edge and the cloud (as exemplified in
    Figure [1\)](#page-0-0), where SLMenabled agents deployed at the edge collaborate
    not only with centralized infrastructures, but also among themselves. This approach
    prioritizes the edge as the primary execution environment, relying on the cloud
    only when QoE from the end user''s perspective and QoS from the system''s capabilities
    cannot be fully satisfied at the edge. Furthermore, this collaboration introduces
    additional dimensions compared to pre-GenAI applications, where edge cooperation
    was primarily driven by *computation* needs. In fact, thanks to the versatility
    and multipurpose nature of these models, we envision a broader scope for cooperation,
    encompassing not just computation but also *data* and *knowledge* sharing across
    heterogeneous systems.


    Building on these considerations, this article explores the potential of *collaborative
    GenAI at the edge*, offering practical insights into designing such systems. From
    abstracting how to enable dynamic task delegation across heterogeneous devices
    to introducing the scalability of distributed LLM inference, our aim is to provide
    a roadmap for deploying collaborative intelligence across the computing continuum.


    #### I. FROM CLOUD TO EDGE: MOTIVATION


    Recent trends reveal a notable shift toward reducing the size of LLMs, with a
    growing emphasis on compactness and efficiency. This shift is motivated by factors
    spanning both the capabilities of smaller models themselves and the challenges
    posed by large models provisioning from a network perspective. In this section,
    we explore these motivations and their implications for the future of GenAI.


    # *A. Smaller*


    In response to the high cost [\[3\]](#page-6-2) of cloud-based LLM services, advances
    in model compression techniques—such as quantization [\[8\]](#page-7-1), pruning
    [\[9\]](#page-7-2), and knowledge distillation [\[10\]](#page-7-3)—are making
    edge AI increasingly viable for deploying SLMs on mobile devices [\[11\]](#page-7-4)
    and at the edge. The upper part of Figure [2,](#page-1-0) *''Smaller''*, depicts
    the recent surge in sub-4B language models developed over the past few years.
    These models vary in size, with the smallest, Qwen2.5, featuring 0.5B parameters
    and a storage footprint of just 398 megabytes. Notably, these models are also
    becoming multi-modal, with


    <span id="page-1-0"></span>![](_page_1_Figure_7.jpeg)


    Fig. 2: *Smaller, Smarter, Closer*: (1) Progression towards smaller language models
    with comparable or improved efficiency (top); (2) Improved accuracy of smaller
    models as measured by Pass@1 scores (middle); and (3) Infrastructurerelated latency
    comparisons across global locations for major AI systems, highlighting geographical
    disparities in access speed (bottom).


    **Closer**


    **3**


    capabilities extending beyond text processing to include image-to-text capabilities
    (Llama3.2-vision). This current shift reflects a broader industry trend toward
    developing *smaller* and versatile models that can handle a range of input types,
    enabling them to support complex application that require integrating information
    from multiple data sources–capability especially necessary at the network edge.


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Fig. 3: Task-oriented cooperation types: Data, Computation, and Knowledge cooperation
    strategies for multi-agent collaboration. The Capability Metadata Store (CMS),
    introduced later in the paper, is depicted here as a key enabler for coordination
    and metadata-driven interactions among agents.


    #### *B. Smarter*


    The challenge with SLMs lies in their reduced computational capability resulting
    from quantization and compression. Drastically reducing a general-purpose model
    leads to significant performance degradation. Blue bar in Figure [2,](#page-1-0)
    *''Smarter''*, depicts Pass@1 accuracy (percentage of queries answered correctly
    on the first try) of OpenAI/GSM8K math test accuracy drop as the number of language
    model parameters drop. A language model pre-trained on a specific domain may outperform
    a general-purpose model when handling domain-specific queries. Instead of an all-purpose
    model, domain-specific models like Code Llama (code generation), DeepSeekMath
    (mathematical reasoning), and BioMistral (medical Q&A), each with 7B parameters,
    achieve greater accuracy within their specialized fields. Red bar in Figure [2](#page-1-0)
    middle, demonstrate that despite having only 1.5 billion parameters, the math
    domain specific model *m-1.5* achieves accuracy comparable to that of 7B general-purpose
    variant while requiring just 19.8% of the memory. This demonstrates that while
    SLMs may have fewer parameters and reduced general-purpose capabilities, their
    domainspecific optimization allows them to achieve exceptional accuracy and efficiency,
    highlighting the potential for SLMs to become *smarter* and more reliable within
    targeted domain.


    # *C. Closer*


    Long latency is a common issue in cloud-based LLM services, as data must be transmitted
    over the network, processed remotely, and then returned, introducing delays that
    impact real-time applications. Figure [2,](#page-1-0) *''Closer''*, shows network
    latency measurement of various cloud-based LLM services. Popular LLM services,
    such as those provided by OpenAI, are hosted in centralized server locations (e.g.,
    the US), which can impact the QoE for users situated far from these hosting regions
    due to increased latency and reliability issues, with network latency ranging
    from around tens of milliseconds to hundreds of milliseconds depending on the
    service provider and user location. Content Delivery Network (CDN) systems can
    help to alleviate long latency delays by efficient network tunneling and management.
    However, CDNs cannot fully resolve the issue since the core processing of LLMs
    remains on remote infrastructure. This inherent limitation further drives research
    into moving LLM services *closer* to the edge, enabling localized inference to
    significantly reduce networking latency and enhancing QoE for users.


    ## II. EDGE-CENTRIC COLLABORATION


    ## *A. Collaboration Approaches*


    Advancing collaborative GenAI inference at the edge requires a structured understanding
    of the diverse strategies enabling agents to cooperate and process tasks efficiently.
    These strategies address critical aspects of performance, scalability, and adaptability,
    particularly in resource-constrained environments. As shown in Figure [3,](#page-2-0)
    task-oriented cooperation is categorized into three key types: *Data*, *Computation*,
    and *Knowledge*. Each category addresses distinct operational challenges: Data
    Cooperation ensures synchronized observations, Computation Cooperation optimizes
    resource usage through task distribution, and Knowledge Cooperation enables dynamic
    access to domain-specific expertise. These approaches collectively form the foundation
    of scalable and efficient edgebased GenAI systems. Each cooperation type is illustrated
    in Figure [3,](#page-2-0) with a high-level conceptual overview at the top and
    an example showcasing specific interactions between agents below.


    Data cooperation ensures agents maintain consistent and upto-date knowledge through
    the real-time exchange of raw or processed data. A key enabler is the exchange
    of embeddings, compact representations that encode domain-specific information
    [\[12\]](#page-7-5). By sharing embeddings with metadata (e.g., domain, type,
    dimensionality), agents can reuse preprocessed data for tasks like classification
    or summarization, reducing computational overhead and enhancing scalability.


    This approach is particularly valuable in situational awareness scenarios, such
    as environmental monitoring and realtime analytics. For example, an air quality
    monitoring agent can generate embeddings enriched with metadata (e.g., urban vs.
    industrial), which are shared with peers for trend detection or anomaly analysis.
    This synchronization enables agents to construct a unified environmental representation,
    improving responsiveness while minimizing redundant processing.


    Computation cooperation involves dynamically allocating and distributing computational
    tasks to balance workloads and optimize resource utilization. Unlike data cooperation,
    which focuses on synchronization, this approach considers both agents'' computational
    capabilities and the complexity of the request [\[13\]](#page-7-6). By matching
    tasks with the most appropriate resources, agents collaborate effectively. For
    instance, a complex query can be decomposed into subtasks, with lightweight tasks
    assigned to lower-capability agents and resource-intensive operations handled
    by specialized nodes. The results are then aggregated, showcasing how distributed
    inference pipelines leverage both agent expertise and computational power to handle
    demanding workloads.


    Knowledge cooperation addresses knowledge gaps by enabling agents to access domain-specific
    expertise or generalpurpose reasoning capabilities [\[14\]](#page-7-7). Unlike
    data cooperation, which focuses on synchronization, and computation cooperation,
    which emphasizes execution, knowledge cooperation facilitates the dynamic exchange
    of higher-order insights for deeper contextual understanding. For instance, an
    agent processing a math or code query may detect a knowledge gap and query a peer
    with domain-specific expertise. Metadata ensures the retrieved knowledge is accurate,
    contextually aligned, and relevant, allowing it to be seamlessly integrated into
    operations. Beyond addressing immediate gaps, this approach supports long-term
    adaptability. Through iterative exchanges, agents refine their shared knowledge
    base, enhancing their capabilities and decision-making over time. This evolving
    collaboration is particularly valuable for reasoning-intensive tasks in dynamic
    and complex environments, where a unified and adaptive approach is essential.


    # *B. Requirements*


    Seamless collaboration among distributed agents requires understanding the diverse
    dimensions of such systems, which operate in heterogeneous environments with varying
    computational capacities, network latencies, and data domains. Instead of proposing
    a one-size-fits-all architecture—neither feasible nor suitable—this effort emphasizes
    foundational principles and mechanisms for achieving flexible, adaptable, and
    scalable collaboration.


    Building on the cooperation strategies discussed earlier, successful systems must
    prioritize flexibility in capability management. This involves enabling agents
    to dynamically discover and interpret peers'' capabilities, such as sharing data
    or leveraging domain-specific expertise. Metadata plays a pivotal role, guiding
    decisions about the relevance of an agent''s resources for specific tasks. For
    example, an agent specializing in healthcare could tag outputs with metadata describing
    their domain and type, allowing others to reuse these resources efficiently.


    Another requirement is adaptability to task complexity, given the variety of SLM
    applications. Tasks can range from simple fact retrieval to complex reasoning
    workflows. Systems must support task decomposition, where a query is split into
    subtasks handled collaboratively by agents. For instance, one agent might extract
    context while another performs detailed inferences, ensuring collaboration meets
    latency, accuracy, or resource efficiency priorities.


    Collaborative systems must also address knowledge gaps, particularly in scenarios
    where agents encounter queries beyond their expertise. Agents should detect insufficient
    knowledge, query peers with relevant expertise, and seamlessly integrate external
    insights into their inference processes. For example, an agent handling a marine
    biology query might dynamically retrieve domain-specific knowledge from a peer,
    ensuring contextually aligned and accurate responses.


    Finally, practical implementations of distributed GenAI inference must balance
    trade-offs between competing priorities. Systems must optimize performance while
    avoiding resource overburden and manage decentralization''s scalability benefits
    alongside its coordination overhead.


    #### *C. Elements Supporting Collaborative SLM Inference*


    To address the outlined requirements, we identify key elements essential for enabling
    collaboration among distributed SLM-enabled agents. The upper part of Figure [4](#page-4-0)
    illustrates the conceptual framework of these components. While we do not aim
    to propose a fully-fledged architecture, this figure offers an overview of key
    building blocks that align with our view


    The *CMS* acts as a decentralized repository for storing and retrieving metadata
    on agent capabilities, including model configurations, system nodes, and resources.
    As shown on the bottom of Figure [4,](#page-4-0) it contains entries such as model
    architecture, parameters, quantization settings, and system metrics like task
    progress, node health, and load status. Information sharing among devices is performed
    through gossiplike protocols [\[15\]](#page-7-8), favoring broad and adaptive
    metadata dissemination across agents. Overall, all these different entries provide
    a structured way to enable dynamic service discovery and metadata-driven decision-making,
    as discussed earlier.


    <span id="page-4-0"></span>


    | SLM-enabled Agent                                                                                                                                                                                                                                                       |                                                                                                                                                                                |

    |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Capability Metadata Store<br>SLM 1<br>SLM 2<br>Semantic Discovery &<br>Routing
    Engine                                                                                                                                                                                   |
    Task Orchestrator<br>SLM 3<br>SLM 4<br>Classifier Engine<br>SLM Lake<br>Local<br>Decentralized<br>Component<br>Component                                                       |

    | Capability Metadata Store (CMS)                                                                                                                                                                                                                                         |
    Example                                                                                                                                                                        |

    | Model Details: The CMS stores configurations like model architecture,<br>number
    of training parameters, quantization, context length, etc.                                                                                                                              |
    /slm/model_A/architecture_parameters -> "llama_1.2B"<br>/slm/model_C/context_length
    -> "131072"<br>/slm/model_B/quantization = "Q8_0"                                          |

    | Configuration<br>Management:<br>LM<br>services<br>may<br>have<br>different<br>configurations
    based on deployment environments (e.g., edge, cloud,<br>hybrid setups). The CMS
    stores configurations like model versions, API<br>rate limits, and hardware preferences.   |
    /slm/configs/model_A/version -> "v1.2.0"<br>/slm/configs/model_B/max_tokens ->
    "2048"<br>/slm/configs/fallback_endpoint -> "cloud.gpt.com"<br>/slm/configs/max_tokens
    = "1024" |

    | Fine-Tuning (or RAG) State Management: Fine-tuning multiple LMs<br>concurrently
    across nodes. The CMS tracks the progress of fine-tuning<br>tasks for synchronization.                                                                                                  |
    /slm/fine_tuning/job_1234/progress -> "80%"<br>/slm/fine_tuning/job_1234/status
    -> "running"                                                                                   |

    | Language<br>Model<br>Dynamic<br>Parameter<br>Updates:<br>Update<br>LM<br>hyperparameters
    (e.g., temperature, top-k, top-p) dynamically based on<br>user<br>needs<br>or<br>model<br>performance.<br>The<br>CMS<br>stores<br>and<br>retrieve<br>parameter
    configurations. | /slm/parameters/model_A/temperature -> "0.8"<br>/slm/parameters/model_A/top_k
    -> "50"                                                                                          |

    | Logging and Metrics Aggregation: Aggregate performance metrics (e.g.,<br>response
    time, token usage) from distributed LM services. The CMS logs<br>metrics and compute
    aggregated statistics for monitoring.                                                            |
    /slm/metrics/model_A/inference_time_avg -> "125ms"<br>/slm/metrics/model_A/tokens_used_total
    -> "500000"                                                                       |

    | State Management for Collaborative Inference: LM inference tasks<br>require
    splitting across devices (e.g., edge/cloud collaboration). The CMS<br>stores intermediate
    states or task allocations.                                                                       |
    /slm/inference/task_1/phase_1_output -><br>"partial_embedding_123"<br>/slm/inference/task_001/assigned_device
    -> "edge_node_1"                                                 |

    | Model-Specific Caching: Cache frequently used prompts or embeddings<br>to optimize
    repeated requests. The CMS stores precomputed outputs or<br>embeddings.                                                                                                              |
    /slm/cache/prompt_hash_abc12 -> "precomputed_embedding_x"                                                                                                                      |

    | Model Multi-tenancy: Serve multiple tenants with isolated resources and<br>configurations.<br>The<br>CMS<br>stores<br>tenant-specific<br>configurations<br>and<br>metadata.                                                                                             |
    /slm/tenants/tenant_123/config/model -> "model_B"<br>/slm/tenants/tenant_123/usage_limit
    -> "100 tokens/month"                                                                 |

    | Dynamic Scaling: Scale up/down SLM instances dynamically based on<br>traffic.
    The CMS tracks available resources and trigger scaling events.                                                                                                                            |
    /slm/scaling/model_A/current_instances -> "3"<br>/slm/scaling/model_A/desired_instances
    -> "5"                                                                                 |

    | User and Session Management: Track user-specific preferences or session<br>context<br>in<br>a<br>multi-user<br>system.<br>The<br>CMS<br>stores<br>session-specific<br>information
    for personalized inference.                                                           |
    /slm/sessions/user_001/context -> "{''topic'': ''science''}"<br>/slm/sessions/user_001/tokens_used
    -> "1500"                                                                       |


    Fig. 4: On top, our conceptual framework for enabling collaboration among distributed
    SLM-enabled agents. On the bottom, the table illustrates examples of CMS entries
    for coordinating data, computation, and knowledge cooperation across agents.


    *Semantic Discovery and Routing* capabilities are also essential. Adopting a simple,
    semantically structured notations akin to DNS-style naming conventions (e.g.,
    data.gpt/domain/processed\_embeddings for data, computation.gpt/task\_distribution
    for computations, or knowledge.gpt/arithmetic for domainspecific knowledge), agents
    can quickly identify and interact with relevant services. This approach simplifies
    coordination across heterogeneous systems and facilitates seamless interoperability.


    The *Task Orchestrator* complements this by managing task decomposition and dynamic
    allocation of subtasks across agents. It leverages information from the CMS and
    classifier outputs to adaptively balance workloads, ensuring that computational
    resources are used efficiently. The *Classifier Engine* [\[16\]](#page-7-9), in
    turn, runs specialized classifiers to assess task complexity and relevance, gap
    detection, and capability matching, allowing the orchestration process to align
    with the adaptability and scalability principles highlighted in the requirements
    section.


    These components can collectively support the execution of the collaboration strategies
    discussed earlier—data, computation, and knowledge cooperation—by providing the
    necessary infrastructure for agents to interact cohesively. They leverage structured
    metadata, semantically simple discovery notations, and adaptive task orchestration,
    ensuring consistent coordination across heterogeneous environments.


    Once again, while this framework is not intended to be exhaustive, it can provide
    a baseline for designing collaborative systems that fulfill the requirements outlined
    previously.


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Fig. 5: A collaborative language model inference scenario illustrating (a) sequential
    collaboration and (b) parallel collaboration.


    ## *D. Security and Privacy Considerations*


    Before moving forward to practical application scenarios, it is important to briefly
    acknowledge security and privacy considerations that arise in collaborative edge
    inference scenarios. While these systems offer benefits in latency reduction and
    resource efficiency, they also introduce potential security vulnerabilities. Sensitive
    information could be leaked during inter-agent communication or task delegation,
    and adversaries could target intermediate inference states. Addressing these challenges
    requires incorporating privacy-preserving mechanisms, such as encrypted data exchanges,
    secure multi-party computation, and fine-grained access controls. Further discussions
    on attacks and defense mechanisms in collaborative inference systems can be found
    in [\[17\]](#page-7-10).


    #### III. APPLICATION SCENARIOS


    Next, we analyze practical use case scenarios for collaborative SLMs-enabled agents
    at the edge, demonstrating how the previously discussed cooperation strategies
    and components can be applied.


    ## *A. Mobile & Wearable Healthcare.*


    Wearable AI enables real-time health monitoring, integrating multi-modal data
    for personalized care. The advent of multi-modal GenAI models, including SLMs,
    further enhances these capabilities by enabling devices to integrate multiple
    functions within a single model. This allows for context-aware analysis, collaborative
    decision-making, and more efficient use of on-device resources.


    This is particularly valuable as the number of AI-enabled wearables on our bodies
    increases [\[18\]](#page-7-11), along with the variety of available sensors. These
    devices now provide a much broader range of sensory data, including heart rate,
    respiration rate, blood oxygen level, activity levels, sleep patterns, and environmental
    conditions. Each wearable device can process such data locally using lightweight
    SLM models, transforming raw sensor inputs into actionable insights. This capability
    reduces reliance on centralized processing systems, enhancing the overall efficiency
    and scalability of health monitoring solutions from a technical perspective. From
    an application perspective, it enables devices to provide deeper insights into
    a person''s health status by integrating and analyzing diverse data streams collectively
    gathered from multiple devices.


    As depicted in Figure [5](#page-5-0) (a), in a scenario where a user queries their
    symptoms, multiple on-body computing devices can sequentially and collaboratively
    perform SLM inference by leveraging a combination of data and knowledge cooperation.
    These agents analyze real-time vitals, share insights to create a unified understanding
    of the user''s health status, and dynamically query relevant knowledge bases when
    needed. This enables the system to proactively provide personalized advice to
    the wearer while auto-generating detailed reports for healthcare providers, all
    without requiring the user''s intervention.


    # *B. Collaborative Urban Intelligence*


    In public spaces, ML model-equipped agents can seamlessly integrate into the smart
    city ecosystem, embedding themselves within public infrastructures such as smart
    cameras, environmental sensors, traffic lights, and roadside units. Each of these
    systems is equipped with processing power and specialized onboard sensors—for
    instance, visual and audio sensors on public smart cameras or environmental sensors
    on roadside units—enabling them to function intelligently within urban infrastructure.
    Imagine a scenario in the context of increasingly integrated smart cities where
    a user''s device cannot accurately address a query using only its onboard data,
    model, and information. The device can enhance its understanding and decision-making
    capabilities, by collaborating with nearby IoT/Edge devices.


    As depicted in Figure [5](#page-5-0) (b), a user''s device can query their device
    with a question such as *Evaluate environmental risks for setting up an outdoor
    event*. In response, the user''s device decompose the root query and start collaborating
    with nearby IoT agents, leveraging both computation cooperation and data cooperation
    to provide a comprehensive answer. Each of these queries can then be concurrently
    addressed with high accuracy by specific IoT devices. For example, a smart camera
    can leverage its visual language model and its data to report recent observation
    while also conducting an online search for records in the area. Simultaneously,
    an environment monitoring identifies recent pollution levels. The aggregated insights
    allow the user''s device to form a holistic understanding of the environment,
    enabling better-informed decisions about urban safety and quality of life.


    <span id="page-6-8"></span>![](_page_6_Figure_0.jpeg)


    Fig. 6: Impact of request allocation methods on the number of requests processed,
    rejected, and cloud redirection costs in collaborative edge-cloud systems.


    # IV. BALANCING EDGE CONSTRAINTS AND CLOUD DEPENDENCIES


    While cooperation strategies enable efficient edge deployments, many edge systems
    face inherent computational limitations [\[19\]](#page-7-12). As demand rises,
    edge devices can quickly reach capacity limits, making cloud-based GenAI a necessary
    complement, particularly when edge resources saturate. However, reliance on cloud
    services introduces non-negligible costs, emphasizing the need for efficient scheduling
    and orchestration to minimize cloud redirections [\[20\]](#page-7-13).


    To evaluate this, we deployed a system consistent of eight off-the-shelf edge
    devices, including NVIDIA Jetson AGX Orin and Jetson Orin Nano[0](#page-6-6) ,
    featuring built-in GPUs. Each device had predefined token generation (where a
    *token* represents a unit of text processed during inference) and concurrency
    limits, reflecting constraints derived from benchmarking GenAI workloads [\[11\]](#page-7-4).


    The system configuration was tailored to replicate realworld conditions. Over
    a one-hour evaluation, the system was subjected to bursty workloads, with periodic
    surges in user requests. Token demands for each request were selected from typical
    sizes (e.g., 50, 100, 200, 300, and 500 tokens), weighted toward smaller requests
    to reflect common usage patterns. Cloud offloading costs were based on OpenAI
    API GPT-4 pricing[1](#page-6-7) , accounting for input and output tokens.


    A critical challenge in such deployments is optimally assigning requests to nodes.
    We evaluated three requestallocation methods: *Random*, which distributes requests
    uniformly across nodes regardless of capacity; *Weighted*, which prioritizes higher-capability
    nodes; and *Load-Aware*, which dynamically directs requests to the least utilized
    nodes, balancing the workload and mitigating bottlenecks.


    As shown in Figure [6,](#page-6-8) scheduling strategies significantly impact
    system performance and cloud redirections. The Random method led to uneven workload
    distribution, increasing task rejections and cloud reliance. In contrast, Weighted
    allocation reduced cloud offloading by prioritizing higher-capability nodes, while
    Load-Aware scheduling further improved efficiency by dynamically balancing the
    workload, minimizing rejected tasks, and lowering cloud costs.


    These results are not intended to lead to generalizations, as edge deployments
    can vary widely in their configurations, the GenAI applications they provision,
    and the degree of heterogeneity in their infrastructure. However, they underscore
    the significance of even a simple change in scheduling strategy, which in this
    case halved cloud costs. This reduction becomes even more impactful when considering
    the hyperscale nature of the use cases discussed earlier, where millions of requests
    may be processed daily.


    These findings open the door to numerous research opportunities. Advancing scheduling
    algorithms that dynamically adapt to GenAI workload variations and edge heterogeneity
    will be paramount in shaping the next generation of edge intelligence networks.
    Additionally, investigating their scalability across diverse deployment scenarios
    and multi-modal applications could further enhance real-world applicability. Exploring
    the feasibility of interoperability among heterogeneous SLMs, devices, platforms
    will also be critical in ensuring seamless integration across the ecosystem. Finally,
    we envision that incorporating *incentive mechanisms* could foster deeper collaboration
    between edge and cloud resources, ensuring that stakeholders are fairly compensated
    for their contributions, thereby strengthening the overall ecosystem of collaborative
    GenAI inference.


    ## V. CONCLUSION


    Edge computing will play a key role in addressing the limitations of cloud-centric
    GenAI, favoring low-latency, costeffective, and privacy-preserving solutions.
    In this article, we explored collaboration strategies and practical design principles
    for deploying GenAI across the computing continuum. While challenges such as resource
    constraints and scalability persist, edge-centric approaches offer a viable path
    to enabling efficient and adaptable GenAI systems for future applications.


    ## REFERENCES


    - <span id="page-6-0"></span>[1] X. Wang, Z. Wan, A. Hekmati, M. Zong, S. Alam,
    M. Zhang, and B. Krishnamachari, "The internet of things in the era of generative
    ai: Vision and challenges," *IEEE Internet Computing*, vol. 28, no. 5, pp. 57–64,
    2024.

    - <span id="page-6-1"></span>[2] OpenAI, "Gpt-4o system card," *arXiv preprint
    arXiv:2410.21276*, 2024.

    - <span id="page-6-2"></span>[3] A. S. Luccioni, S. Viguier, and A.-L. Ligozat,
    "Estimating the carbon footprint of bloom, a 176b parameter language model," *Journal
    of Machine Learning Research*, vol. 24, no. 253, pp. 1–15, 2023.

    - <span id="page-6-3"></span>[4] OpenAI. (2024, Dec.) Incident report: Service
    down on december 12, 2024. Accessed on 2024-12-18. [Online]. Available: [https:](https://status.openai.com/incidents/ctrsv3lwd797)
    [//status.openai.com/incidents/ctrsv3lwd797](https://status.openai.com/incidents/ctrsv3lwd797)

    - <span id="page-6-4"></span>[5] Ericsson, "Impact of genai on mobile network
    traffic," *Ericsson Mobility Report*, Nov. 2024. [Online]. Available: [https://www.ericsson.com/en/reports-and-papers/mobility-report/](https://www.ericsson.com/en/reports-and-papers/mobility-report/articles/genai-impact-on-mobile-network-traffic)
    [articles/genai-impact-on-mobile-network-traffic](https://www.ericsson.com/en/reports-and-papers/mobility-report/articles/genai-impact-on-mobile-network-traffic)

    - <span id="page-6-5"></span>[6] T. Meuser, L. Loven, M. Bhuyan, S. G. Patil,
    S. Dustdar, A. Aral, ´ S. Bayhan, C. Becker, E. De Lara, A. Y. Ding *et al.*,
    "Revisiting edge ai: Opportunities and challenges," *IEEE Internet Computing*,
    vol. 28, no. 4, pp. 49–59, 2024.


    <span id="page-6-6"></span><sup>0</sup>https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/


    <span id="page-6-7"></span><sup>1</sup>https://openai.com/api/pricing/


    - <span id="page-7-0"></span>[7] S. Laskaridis, S. I. Venieris, A. Kouris, R.
    Li, and N. D. Lane, "The future of consumer edge-ai computing," *IEEE Pervasive
    Computing*, 2024.

    - <span id="page-7-1"></span>[8] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen,
    W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, "Awq: Activation-aware weight
    quantization for on-device llm compression and acceleration," *Proceedings of
    Machine Learning and Systems*, vol. 6, pp. 87–100, 2024.

    - <span id="page-7-2"></span>[9] X. Ma, G. Fang, and X. Wang, "Llm-pruner: On
    the structural pruning of large language models," *Advances in neural information
    processing systems*, vol. 36, pp. 21 702–21 720, 2023.

    - <span id="page-7-3"></span>[10] Y. Gu, L. Dong, F. Wei, and M. Huang, "Minillm:
    Knowledge distillation of large language models," in *The Twelfth International
    Conference on Learning Representations*, 2024.

    - <span id="page-7-4"></span>[11] S. Laskaridis, K. Katevas, L. Minto, and H.
    Haddadi, "Melting point: Mobile evaluation of language transformers," in *Proceedings
    of the 30th Annual International Conference on Mobile Computing and Networking*,
    2024, pp. 890–907.

    - <span id="page-7-5"></span>[12] Z. Yao, Z. Tang, J. Lou, P. Shen, and W. Jia,
    "Velo: A vector databaseassisted cloud-edge collaborative llm qos optimization
    framework," *arXiv preprint arXiv:2406.13399*, 2024.

    - <span id="page-7-6"></span>[13] M. Zhang, X. Shen, J. Cao, Z. Cui, and S. Jiang,
    "Edgeshard: Efficient llm inference via collaborative edge computing," *IEEE Internet
    of Things Journal*, 2024.

    - <span id="page-7-7"></span>[14] A. Das, S.-C. Chen, M.-L. Shyu, and S. Sadiq,
    "Enabling synergistic knowledge sharing and reasoning in large language models
    with collaborative multi-agents," in *2023 IEEE 9th International Conference on
    Collaboration and Internet Computing (CIC)*. IEEE, 2023, pp. 92–98.

    - <span id="page-7-8"></span>[15] X. Zhang, X. Zhu, W. Bao, L. T. Yang, J. Wang,
    H. Yan, and H. Chen, "Distributed learning on mobile devices: a new approach to
    data mining in the internet of things," *IEEE Internet of Things Journal*, vol.
    8, no. 13, pp. 10 264–10 279, 2020.

    - <span id="page-7-9"></span>[16] V. Yadav, Z. Tang, and V. Srinivasan, "Pag-llm:
    Paraphrase and aggregate with large language models for minimizing intent classification
    errors," in *Proceedings of the 47th International ACM SIGIR Conference on Research
    and Development in Information Retrieval*, 2024, pp. 2569– 2573.

    - <span id="page-7-10"></span>[17] Z. He, T. Zhang, and R. B. Lee, "Attacking
    and protecting data privacy in edge–cloud collaborative inference systems," *IEEE
    Internet of Things Journal*, vol. 8, no. 12, pp. 9706–9716, 2020.

    - <span id="page-7-11"></span>[18] T. Gong, S. Y. Jang, U. G. Acer, F. Kawsar,
    and C. Min, "Collaborative inference via dynamic composition of tiny ai accelerators
    on mcus," *arXiv preprint arXiv:2401.08637*, 2023.

    - <span id="page-7-12"></span>[19] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and
    J. Zhang, "Edge intelligence: Paving the last mile of artificial intelligence
    with edge computing," *Proceedings of the IEEE*, vol. 107, no. 8, pp. 1738–1762,
    2019.

    - <span id="page-7-13"></span>[20] X. Zhang, Z. Huang, E. O. Taga, C. Joe-Wong,
    S. Oymak, and J. Chen, "Efficient contextual llm cascades through budget-constrained
    policy learning," in *The Thirty-eighth Annual Conference on Neural Information
    Processing Systems*, 2024.'
- title: "Graph Attention Network for Optimal User Association in Wireless\n  Networks"
  abstract: 'With increased 5G deployments, network densification is higher than ever
    to

    support the exponentially high throughput requirements. However, this has meant

    a significant increase in energy consumption, leading to higher operational

    expenditure (OpEx) for network operators creating an acute need for

    improvements in network energy savings (NES). A key determinant of operational

    efficacy in cellular networks is the user association (UA) policy, as it

    affects critical aspects like spectral efficiency, load balancing etc. and

    therefore impacts the overall energy consumption of the network directly.

    Furthermore, with cellular network topologies lending themselves well to

    graphical abstractions, use of graphs in network optimization has gained

    significant prominence. In this work, we propose and analyze a graphical

    abstraction based optimization for UA in cellular networks to improve NES by

    determining when energy saving features like cell switch off can be activated.

    A comparison with legacy approaches establishes the superiority of the proposed

    approach.'
  url: http://arxiv.org/abs/2505.16347v1
  keywords: Network Energy Efficiency, User Association, Graph Neural Networks, Attention
    Mechanisms.
  document: '#### I. INTRODUCTION


    With the fifth generation (5G) and beyond 5G (B5G) rollout, various use cases
    have been enabled that go beyond just providing connectivity for mobile devices.
    In particular, they can be categorized into enhanced Mobile Broadband (eMBB) for
    high data rate, Ultra-reliable low latency (URLLC) for low latency and highly
    reliable use cases and machine-tomachine Type Communication (mMTC) for the internet
    of things (IoT) devices. While enabling these features is widely anticipated,
    each of these use cases place stringent demands on the network resources along
    with a commensurate increase in complexity of network design and operation. Network
    operators are challenged not only by the management of such complex mobile environments
    but also by an aspect that has received very little attention in previous generations:
    network energy consumption.


    Recently, increased energy consumption of wireless networks due to greater BS
    deployment density compared to previous generation of networks has gained widescale
    attention. In fact, the radio access network (RAN) is deemed to be responsible
    for greater than 70% of network''s total power consumption [\[1\]](#page-5-0).
    While legacy networks were forced to maintain BSs in a constant ON state regardless
    of the traffic level, leading to reduced energy efficiencies, current 3GPP standards
    have had a significant focus on developing energy saving features (ESFs) for RAN
    elements. Moreover, it is well-known that traffic loads of cellular networks exhibit
    spatio-temporal variation, which essentially implies the BS doesn''t need to be
    configured for full-power operation of the radio at all times. Changes in user
    location and behaviour combined with temporal dynamics of the network imply that
    a user may not always be connected to the network in a way that optimizes energy
    usage. Load-balancing the network to maximize network energy savings (NES) is
    therefore required to ensure that UEs are connected to the BS that leads to the
    least amount of energy consumption. Such an action may be actuated either based
    on a periodic trigger or due to certain BSs within a cluster reaching a critical
    power consumption threshold determined by the network operator. Identifying such
    issues and re-distributing the UEs per a minimal network energy consumption criterion
    is a non-trivial task as it can have a cascading effect through the network and
    thus needs sophisticated solutions that are hard to derive analytically given
    the dynamics of the cellular environment.


    While modeling wireless networks have been pursued in various ways for operational
    optimization, graph neural networks (GNNs) have only recently been used to that
    end [\[2\]](#page-5-1)–[\[6\]](#page-5-2). They allow graph-structured data to
    be processed effectively, enabled by global parameterizations, rotational and
    shift invariance [\[2\]](#page-5-1), which give rise to their generalizability
    and scalability over different network topology. They exploit the underlying graph-structure
    of data as well as contextual information that facilitate the optimization of
    operational aspects of wireless networks [\[6\]](#page-5-2).


    In this paper, we aim to minimize the overall network energy consumption to improve
    NES, by jointly minimizing the fixed and traffic dependent power consumption.
    The former is done by employing cell switch off (CSO) when feasible while the
    latter is performed by optimally associating the UEs to the best BS. To do so,
    we propose a GNN-based technique with attention mechanism for user association
    in cellular networks to improve the NES. The wireless network is first represented
    as a homogenous graph, whereby the UEs are represented as the nodes of the graph
    and the edges denote the connection between the UEs and BS. Leveraging the underlying
    graph structure of the network, GNN determines when energy saving features can
    be activated. These features include re-associating the UEs to the neighbouring
    BSs, thereby managing physical resource block (PRB) utilization per BS, as well
    as the initiating cell switch off. Compared with a legacy user association techniques,
    such as RSRP and a genie-aided per-PRB SINR metric, the proposed GNN-based technique
    shows significant improvement in NES.


    #### II. SYSTEM MODEL


    Given that network energy consumption increases in proportion to the throughput
    requirements of the UEs, this section provides a brief description of the scenarios
    that we consider in order to exhibit the nuances of the problem itself and the
    capabilities of a GNN-based approach to effectively address them. We consider
    a cellular network with N cells that are adjacent to each other as shown exemplarily
    in Fig[.1.](#page-1-0) Each cell site is equipped with the NT x transmit antennas
    at a height of hT x. In the region of coverage (RoC) of this cluster of N cells,
    K different UEs are uniformly distributed with the height of hUE and NRx received
    antennas at each UE location. The UEs are able to connect to one or more of the
    BSs that are part of the cluster and their connectivity is jointly optimized to
    maximize NES through the UA policy. Data and control information flows between
    each UE and BS over Nsc OFDM subcarriers, with the carrier frequency of fc, Wsc
    subcarrier spacing, and bandwidth W in a 5G NR compliant format.


    #### <span id="page-1-2"></span>*A. Power Consumption Model*


    Modeling the power consumption of the BS is an important first step towards developing
    methods to optimize it. It is wellknown that each BS has a fixed portion of power
    consumption that depends on the type of deployed BS e.g. macro, smallcell etc,
    and a traffic load dependent portion that varies per UE demands. In particular,
    the overall power consumption of the n th BS comprising of the traffic dependent
    and fixed portions is given by P <sup>n</sup> = P n f ixed + P n T r−dep, where
    P n f ixed is the power consumption of the n th BS when carrying no traffic, and
    P n T r−dep is the traffic dependent power consumption of the corresponding BS.
    P n T r−dep is further composed of contributions from the baseband and the radio
    components [\[7\]](#page-5-3)


    $$P\_{Tr - dep}^{n}(\eta) = P\_{BB}^{n}(\eta) + P\_{Radio}^{n}(\eta),\tag{1}$$


    where P n Radio(η) <sup>≜</sup> <sup>N</sup>Tx <sup>h</sup> 1 (1+ϵ)σmax η + ϵP
    <sup>n</sup> max,P A i , and P n BB(η) denotes the power consumed by the baseband
    circuitry. In Eqn. [\(1\)](#page-1-1), σmax is the maximum efficiency of PA, when
    transmitting the maximum output power P n max,P A with η being the load on the
    PA and ϵ is a PA design-dependent parameter [\[8\]](#page-5-4). Using the PRB
    utilization ratio as a proxy for η, we define η ≜ NPRB−used NPRB−total , where
    NPRB−used is the number of allocated PRBs, NPRB−total is the total number of available
    PRBs, and 0 ≤ η ≤ 1.


    #### III. WIRELESS NETWORK GRAPHS


    The radio units (RUs) that can influence the energy consumption of each other,
    through actions such as user migration involving handovers through a change in
    UA policy triggered


    <span id="page-1-0"></span>![](_page_1_Figure_9.jpeg)


    Fig. 1. Logical connectivity graph representation between cell sites (BSs) and
    UEs.


    by load-balancing or NES considerations, are considered to be part of a network
    cluster. Fig. [1](#page-1-0) provides a logical representation of the network
    graph, where the RUs and UEs are the nodes of this graph and the UE-RU connectivity
    is represented by the edges of the graph. While the corresponding network graph
    can be either represented heterogenous or homogenous, in this work, we consider
    a homogenous graph where all nodes and edges are considered to be of the same
    type. Mathematically, graphs can be expressed as G = (V, E) with V and E being
    the sets of nodes and edges, respectively. Let v<sup>i</sup> ∈ V be a node in
    the graph and eij = (v<sup>i</sup> , v<sup>j</sup> ) ∈ E be an edge from node
    v<sup>i</sup> to node v<sup>j</sup> , then eij = 1 indicates that v<sup>i</sup>
    and v<sup>j</sup> are connected. The graph representation of the wireless network
    can be constructed by letting G(A) = (V, E, H(0)) denote an equivalent network
    graph, where H(0) ∈ RK×<sup>d</sup> represents the features'' matrix associated
    with the G, and d represents the number of features for each node of the graph.
    Let h (0) k denote the k th row in H(0), representing the ddimensional feature
    vector for the k th node. To construct h (0) k , we use a set of physically measured
    KPIs. In particular, we define h (0) <sup>k</sup> <sup>≜</sup> [pk, <sup>q</sup>k,
    <sup>r</sup>k], where <sup>p</sup><sup>k</sup> <sup>≜</sup> [pk1, pk2, · · · ,
    pkN ], q<sup>k</sup> ≜ [qk1, qk2, · · · , qkN ] and r<sup>k</sup> ≜ [rk1, rk2,
    · · · , rkN ] are the PRB, distance, and SINR vectors of the k th , ∀k, UE, respectively,
    where pkn, qkn, and rkn, ∀n, are the PRB requirement, distance and the measured
    SINR between the k th UE and the n th BS, respectively.


    <span id="page-1-1"></span>Assuming G to be a homogenous graph, and let A ∈ {0,
    1} <sup>K</sup>×<sup>K</sup> denote the adjacency matrix of G, a non-zero (i,
    j) th entry of A denoted by eij ∈ E, represents the edge connecting the nodes
    i, j ∈ V. The adjacency matrix A, is defined based on the SINR measurements such
    that for each BS, all the UEs whose measured SINR is greater than γ th, are connected.
    Mathematically, we define U<sup>n</sup> ≜ (i, j)|rin > γth and rjn > γth , and
    U ≜ S<sup>N</sup> <sup>n</sup>=1 Un, where rin, ∀n, being the SINR measurement
    at the i-th UE from cell n. Therefore,


    $$\mathbf{A}(i,j) \triangleq \begin{cases} 1 & (i,j) \in \mathcal{U} \\ 0 & \text{Otherwise.}
    \end{cases} \tag{2}$$


    The next section delves into the details on how GNNs are leveraged to achieve
    the NES employing a combination of CSO and improved UA policies.


    #### IV. GNN FOR NES-OPTIMIZED USER ASSOCIATION


    Most NES opportunities present themselves when network traffic is low in the coverage
    area of a cell. This provides an opportunity for cell sites with negligible traffic
    to be completely switched off resulting in significant energy savings [\[1\]](#page-5-0),
    [\[9\]](#page-5-5), [\[10\]](#page-5-6). However, when a cell with a few UEs generating
    a small amount of traffic is switched off, those UEs need to be migrated to active
    neighboring cell(s) that can handle additional traffic from the cell being switched
    off. Note that while this may increase the energy consumption of the cells to
    which the UEs are migrated, since a cell site is switched off completely, it is
    likely to still lead to a net benefit from a global network energy consumption
    perspective, if the energy saved by switching off the cell is greater than the
    nominal increase experienced by the new anchor cell.


    We define the cell association process, by denoting s<sup>k</sup> ∈ [0, 1]1×<sup>N</sup>
    , k = 1, 2, · · · , K, the association of the k th UE to all cells. The n th entry
    of s<sup>k</sup> denotes the association probability of k th UE to the BS in the
    n th cell. Therefore, the entries of s<sup>k</sup> are summed to 1, i.e., |sk|<sup>1</sup>
    = 1. Stacking sk, k = 1, 2, · · · , K, we define the cell association matrix,
    S ∈ [0, 1]K×<sup>N</sup> , where S(k, n) represents the probability of association
    of the k th node (UE) to the BS in the n th cell. Furthermore, by stacking pk,
    ∀k, we construct P ∈ R <sup>K</sup>×<sup>N</sup> , the UE-BS PRB allocation matrix.
    This allows us to define pˆ ≜ S <sup>T</sup> P, a 1 × N vector, whose n th entry
    denotes the total PRBs needed to serve the UEs associated with the BS in n th
    cell for a given cell association matrix S.


    #### *A. NES-based Optimization Objective*


    The overall objective is to minimize the total energy consumption (EC) of the
    network. Additionally, we aim to make sure that the network traffic is well supported
    within the EC constraint. Expressing this mathematically, the total power consumption
    of the network is given by PNW = P<sup>N</sup> <sup>n</sup>=1 P n, where P <sup>n</sup>
    is descirbed in Sec[.II-A.](#page-1-2) Note that the fixed power consumption component
    of an active BS cannot be reduced by influencing network behaviour. However, when
    a BS can be switched off, a significant amount of saving is achieved if no serious
    degradation to network KPIs is anticipated. Such decisions are made as part of
    an overall objective of NES based UA and can be formalized as follows to minimize
    the average network energy consumption,


    $$\min\_{\mu} P\_{NW}(\mu) = \sum\_{n=1}^{N} P\_{fixed}^{n} + P\_{Tr-dep}^{n},\tag{3}$$


    where µ is the policy that leads to the least power consumption by user re-association
    with or without cell switch off. Additionally, denoting the aggregate throughput
    of the network cluster being optimized at the time t by Lt(µ), a further constraint
    is put in order to ensure that the guaranteed bit rate (GBR) traffic doesn''t
    suffer as given by


    $$\min\_{\mu} L\_t(\mu) \ge L\_{\min},\tag{4}$$


    where Lmin = P<sup>N</sup>GBR <sup>n</sup>=1 L n t , and NGBR denotes the total
    number of BSs that are carrying GBR traffic. Note that the optimization in [\(3\)](#page-2-0),
    given the constraint in [\(4\)](#page-2-1), is an NPhard problem. This typically
    requires an exhaustive search that is extremely computationally demanding due
    to the high dimensionality of the search space.


    #### *B. Learning on GNN to achieve Improved NES*


    Here, the ability of GNNs to be scalable is leveraged to not only address user
    association at a cluster level but also to take global view of the network when
    user migrations result in transfer to cells outside the cluster being considered.
    GNNs are constructed in a layered architecture where in each layer, the GNN updates
    the representation of each node as follows: 1) aggregating features from the node''s
    immediate neighbors only connected via the edges, 2) combining the node''s features
    with those aggregated from their neighbors via a *permutation invariant* process.
    The update rule of the l th layer at the node u is given by [\[11\]](#page-5-7)


    $$\mathcal{B}\_u^{(l)} = \text{AGGREGATE}^{(l)}\left(\left\{\mathbf{h}\_v^{(l-1)},
    \forall v \in \mathcal{N}(u)\right\}\right), \quad (\mathbf{5})$$


    $$\mathbf{h}\_{u}^{(l)} = \text{COMP}\mathbb{I}\text{NE}^{(l)}\left(\left\{\mathbf{h}\_{u}^{(l-1)},
    \mathcal{B}\_{u}^{(l)}\right\}\right),\tag{6}$$


    where h (l) <sup>u</sup> denotes the feature vector aggregated by the node u from
    its neighbors at the l th layer, N (u) is the set of neighboring nodes to the
    node u, and β (l) u represents the feature vectors of node u at the l th layer.
    AGGREGATE(l) (·) and COMBINE(l) (·) are used to denote the aggregation and combining
    functions of the l th layer which can have various forms depending on the application.
    For example, a common variant of GraphSAGE [\[12\]](#page-5-8) performs an element-wise
    mean as AGGREGATE(l) (·), followed by concatenation with h (l−1) <sup>u</sup>
    , a linear layer and a ReLU as COMBINE(l) (·).


    Many popular GNN architectures weigh all neighbors v ∈ N (u) with equal importance
    (e.g., mean or max-pooling as AGGREGATE) [\[11\]](#page-5-7) which is sometimes
    a limitation due to the relative importance of nodes. Graph attention network
    (GAT) [\[11\]](#page-5-7) addresses this by producing node representations computed
    through a learned weighted average of the representations of all neighbors. A
    scoring function ρ : R <sup>d</sup> × R <sup>d</sup> → R computes a score for
    every edge euv ∈ E, indicating the importance of the features of the neighbor
    u to the node v:


    $$\rho\_{uv}^{(l)} \triangleq \mathbf{LeakyReLU}\_{\delta} \left( \mathbf{a}^{(l)^T}
    \cdot \left[ \mathbf{W}^{(l)} \mathbf{h}\_u^{(l-1)} || \mathbf{W}^{(l)} \mathbf{h}\_v^{(l-1)}
    \right] \right), \tag{7}$$


    <span id="page-2-0"></span>where α(l) ∈ R 2d ′ <sup>l</sup> , W(l) ∈ R d ′ <sup>l</sup>×d<sup>l</sup>
    are learnable parameters, LeakyReLU<sup>δ</sup> is a leaky ReLU non-linear function
    with negative slope δ, and (·) T is a transposition operation. These attention
    scores are normalized across all neighbors u ∈ N (v) using softmax, and the attention
    function is defined as


    $$\alpha\_{uv}^{l} \triangleq \mathbb{Softmax}\left(\rho\_{uv}^{(l)}\right) =
    \frac{\exp(\rho\_{uv}^{(l)})}{\sum\_{j \in \mathcal{N}(u)} \exp(\rho\_{uj}^{(l)})}.\tag{8}$$


    <span id="page-2-1"></span>In our model, a GAT computes a weighted average of
    the transformed features of the neighbor nodes (followed by a nonlinear function
    σ) as the new representation of node u, using the normalized attention coefficients:


    $$\mathbf{h}\_u^{(l)} = \sigma \left( \alpha\_{uu}^l \mathbf{W}^{(l)} \mathbf{h}\_u^{(l-1)}
    + \sum\_{v \in \mathcal{N}(u)} \alpha\_{uv}^l \mathbf{W}^{(l)} \mathbf{h}\_v^{(l-1)}
    \right), \tag{9}$$


    where h (l) <sup>u</sup> ∈ R dl is the dl-sized vector of embeddings generated
    by the l th layer of GAT. Once h (L) <sup>u</sup> , ∀u is obtained after passing
    through L layers of GAT, they are passed through a fully connected neural network
    layer followed by a Softmax operation to find the cell association matrix S i.e.,


    $$\mathbf{S} = \text{Softmax}\left(\sigma\left(\mathbf{H}^{(L)}\mathbf{Q} + \mathbf{B}\right)\right),\tag{10}$$


    where Q ∈ R <sup>d</sup>L×<sup>K</sup> and B ∈ R <sup>K</sup>×<sup>N</sup> are
    the learnable weight and bias matrices, respectively.


    #### *C. Loss Function and GNN Training for NES*


    Denoting by θ the set of parameters used to characterize the GNN, PNW can be reformulated
    to define the loss function as:


    $$\mathcal{L}(\boldsymbol{\theta}) \triangleq P\_{NW}^{\boldsymbol{\theta}}(\boldsymbol{\mu})
    + \underbrace{\lambda\_1 \left[K - \text{Tr}\left(\mathbf{S}(\boldsymbol{\theta})\mathbf{S}^T(\boldsymbol{\theta})\right)\right]}\_{T\_1}
    + \underbrace{\lambda\_2 |\hat{\mathbf{p}}(\boldsymbol{\theta})|\_2}\_{T\_2},
    \tag{11}$$


    where λ<sup>1</sup> and λ<sup>2</sup> are the regularizing coefficients (i.e.,
    hyperparameters) used to control the training of GNN, and Tr(·) is a trace operation.
    P θ NW (µ) is the power consumption parameterized by θ, that we aim to minimize.
    T<sup>1</sup> is the regularizing term that is meant to find the optimal UA. In
    particular, it ensures that each UE is associated with the BS of *only one* cell.
    Since |sk|<sup>1</sup> = 1, and using the fact that |sk|<sup>2</sup> ≤ |sk|<sup>1</sup>
    = 1, with equality only when *one* entry of s<sup>k</sup> is 1 and the rest of
    them to be 0, we argue that Tr S(θ)S T (θ) ≤ K. The equality occurs when in each
    row of S(θ), only a single entry is 1 while the rest are of entries being 0. Note
    that with T1, we intend to associate each UE to a BS in *only one* cell via minimizing
    the gap between Tr S(θ)S T (θ) and K. T<sup>2</sup> in [\(11\)](#page-3-0) is
    meant to control the distribution of PRBs (i.e., PRB utilization) across the BS
    in all cells, in a way that best minimizes L(θ). As we show via the simulation
    results the effect of T<sup>2</sup> is highly depend of the operating bandwidth.
    We obtain θ by training the GNN using an unsupervised loss function given in [\(11\)](#page-3-0)
    via the back propagation technique.


    #### V. SIMULATION RESULTS


    This section provides the simulation results of the proposed GNN-based approach
    to NES-optimized UA.


    #### *A. Data Generation and Simulation Parameters*


    The simulation scenario is composed of N = 7 cells with BS at fixed locations
    with an inter-site distance of 1 km. The region of coverage is an area of size
    3 km × 3 km with the UEs being uniformly distributed. Each cell site is equipped
    with the NTx = 4 transmit antennas with the height of hTx = 25 m. The UEs are
    equipped with single antenna and hTx = 1.5 m. The downlink communication between
    each UE and BS is 5G NR compliant and is based on clustered delay line (CDL) channel
    model in an Urban Macro (UMa) environment with f<sup>c</sup> = 3.5 GHz and Wsc
    = 30 KHz. . We consider no UE mobility and therefore there is no Doppler shift
    in the channel.


    The cell sites are operated with a frequency reuse factor of 1/3. As a benchmark,
    we compare the performance of the proposed GNN-based technique with: I) RSRP:
    UA is based on the generally-used wideband RSRP, II) Genie-aided subband SINR
    (GA-SubSINR): UA is performed based on the best sub-band SINR measured per-PRB
    basis to account for the frequency selectivity of the wireless channel.


    #### *B. Network Architecture and Training*


    The GNN is constructed with 2 layers of GAT, and in the first layer, the GAT takes
    the network graph G with d = 21 dimensional node features and maps the features
    into an embedding space of size 512. After the first layer of GAT, we obtain the
    nodes'' representation H(1) in an embedding space. In the second layer of GAT,
    H(1) is mapped to another embedding space of size 512, namely H(2). A linear transformation
    is applied to H(2) with a bias, followed by a non-linear function σ(·), to map
    the embedded features to a vector of size 7. These features are passed through
    the Softmax(·) operation to find S. We consider ReLU as an activation function
    σ(·) and δ = 0.2. The entire implementation is done with PyTorch.


    <span id="page-3-0"></span>The size of data set is 10000. To form the training
    and testing portions, we first collect the nodes'' features followed by scaling,
    normalizing and adding a self-loop for each node. A homogenous graph is then constructed
    for each network realization. The data set is then shuffled and split such that
    80% is used for training and 20% for testing. ADAM optimizer is used to train
    the GNN with Learning rate = 5e −5 , and β = (0.9, 0.999) in 5000 epochs.


    ### *C. NES Performance Gain vs Bandwidth*


    The heatmap in Fig. [2](#page-4-0) provides an example of the UE association using
    GNN and compared it with GA-SubSINR as a benchmark. This example is configured
    for 50 UEs in the vicinity of a 7-cell, and 20 (MHz) of bandwidth. We observe
    that the connectivity in only few UEs are different namely the UE index of 28,
    33, and 38, which are highlighted in dash yellow horizontal lines. Such UE re-association
    has led to the cell site 3 and 4 to be completely shut down as their UEs are migrated
    to the neighboring cells. The corresponding graphical representation is provided
    in Fig. [3.](#page-4-1) It can be seen that while the connectivity remains largely
    the same the GNN based approach provides about a 21% improvement in NES. This
    can witness significant benefits if we determine cell sites that can be switched
    off based on traffic load experienced. We will provide more detailed analysis
    on this later in this section.


    The GNN performance gain (in percentage) with respect to GA-SubSINR and RSRP-based
    techniques are depicted in Fig. [4](#page-5-9) and Fig. [5,](#page-5-10) respectively,
    with increasing cell bandwidth. While the NES improvement compared to a legacy
    RSRP-based UA approach is fairly evident (ranging between 60−90% according to
    Fig. [5\)](#page-5-10), the benefits are seen to diminish as more PRB resources
    become available with higher cell


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>Fig. 2. An example of user association for 50 UEs across
    7 cells and 20 (MHz) bandwidth: (a) SINR matrix, the values indicates the strength
    of SINR at each UE location for each BS. (b) The UE association based on the GNN
    technique. The (k, n) entry indicates the association of k th UE to the BS in
    the n th cells. (c) The UE association based on the best SINR.


    ![](_page_4_Figure_2.jpeg)


    <span id="page-4-1"></span>


    Fig. 3. The graph representation of the UE association of the example in Fig.
    [2.](#page-4-0) (a) The graph corresponding to the GNN technique. (b) The graph
    corresponding to the best SINR. Each color represents the UEs connected to the
    BS in the same cell.


    bandwidth. In Fig. [4,](#page-5-9) the GNN performance gain is in a more nominal
    range of 10 − 25%, which is still significant considering the GA-SubSINR approach
    has perfect channel information on a per sub-band basis. For a fixed bandwidth,
    the GNN-based technique performs better for small number of UEs rather large number
    of UEs. This can be explained by noting that for a fixed bandwidth (i.e., fixed
    PRB resources), the cell would have a relatively higher PRB utilization ratio
    (i.e., η) to serve the associated UEs. Such an increase in η will lead to a relatively
    higher NES and therefore smaller performance gain.


    We also observe that, for a fixed number of UEs, the performance gain is not consistent
    across different cell bandwidths. In particular, for large bandwidth, the GNN-based
    technique provides the greatest performance gain. Due to the availability of relatively
    large PRB resources in large bandwidth, the overall PRB usage for each cell site
    remains relatively small which results in lower NES to service the UEs. Note however
    that, in extreme scenarios, where a large number of UEs are served within the
    small bandwidth, the GNN-based technique might fall short compared to what GA-
    SubSINR has to offer. As we show later via simulations, this can be addressed,
    through regularization terms, to keep the per-BS PRB utilization low.


    #### *D. Effect of Regularization*


    In Fig. [6,](#page-5-11) we explore the effect of T<sup>1</sup> and T<sup>2</sup>
    in training of GNN. In this figure, we plot the power consumption vs λ2/λ<sup>1</sup>
    for K = 150, W = [20, 80] MHz and compare the prerformance of GNN-based technique
    to the GA-SubSINR as a benchmark. By varying λ2/λ1, we study the tradeoff between
    the cell selection given by T<sup>1</sup> in L(θ) and the network-level PRB distribution
    among the cell sites provided by T2. For larger cell bandwidths, the GNN-based
    technique outperforms GA-SubSINR for all ranges of λ2/λ1. This is attributed to
    the availability of enough PRB resources to serve all UEs. For medium to large
    λ2/λ<sup>1</sup> the GNN-based technique and GA-SubSINR perform the *same* for
    both small and large bandwidth range. Similarly for very small λ2/λ1, GNN perform
    slightly better than GA-SubSINR. We note that, when λ2/λ<sup>1</sup> = 0, the
    GNN falls short compared to GA-SubSINR for W = 20 (MHz). This can be explained
    by noting that the term T<sup>2</sup> is ignored, the GNN may associate the UEs
    to BS irrespective of the PRB distributions among the cell sites, which may overwhelm
    certain BSs. At smaller bandwidth, this directly translates to higher power consumption.
    In contrast, for large bandwidth, there are enough PRB resources to accommodate
    all 150 UEs without incurring further increase in power consumption. In both scenarios,
    the best performance gain is achieved when there is a balance between T<sup>1</sup>
    and T2.


    To further examine the effect of T<sup>1</sup> and T2, we plot the number cell
    switch offs vs λ2/λ<sup>1</sup> in Fig. [7.](#page-5-12) We observe that for small
    λ2/λ1, where we tend to ignore T1, a relatively large number of cell sites are
    switched off. In these cases, UEs are migrated to the neighbouring cells, incurring
    higher traffic loads. Compared to small bandwidth, larger cell bandwidths provide
    a better performance due to availability of enough PRBs to accommodate the same
    number of UEs. It is worth mentioning that, as λ2/λ<sup>1</sup> increases, the
    rate of cell switch off falls more rapidly for large bandwidth, due to the availability
    of more PRBs to serve the UEs in neighboring cells. For large λ2/λ1, T<sup>2</sup>
    becomes dominant, implying that PRB distribution among the cell sites are more
    even. Similar to GA-SubSINR, this leads to no cell switch off.


    ## VI. CONCLUSION


    In this work we propose the use of GNNs to optimize user association from a NES
    perspective whereby the wireless network elements are modeled as nodes of a graph
    with the edges depicting connectivity of UEs with various base stations in its
    RF neighborhood. We formulate an optimization objective to improve the NES while
    maintaining a loadbalanced network that prevents creation of network hotspots
    and automatically determines cell sites that may be switched off to transition
    the network to a more NES-optimal state. We also describe the architecture of
    our GNN-based approach along with the simulation scenarios constructed to do a
    robust


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-9"></span>Fig. 4. GNN performance gain (%) vs W, Performance
    gain with respect to GA-SubSINR.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-11"></span>Fig. 6. GNN performance gain (%) with respect to GA-SubSINR
    vs λ2/λ<sup>1</sup> for W = [20, 80] (MHz) and K = 150.


    evaluation. Comparisons with a legacy UA approach using RSRP shows significant
    improvement utilizing a GNN-based NES approach while it is shown to be competitive
    with a genie-aided approach where a per-PRB SINR metric is deemed available as
    an idealistic case. In future work, we will analayze the impact on QoS of the
    proposed NES policies and derive approaches that minimize impact.


    #### REFERENCES


    - <span id="page-5-0"></span>[1] Shunqing Zhang, Shugong Xu, Geoffrey Ye Li, and
    Ender Ayanoglu, "First 20 years of green radios," *IEEE Trans. Green Commun. and
    Net.*, vol. 4, pp. 1–15, 2020.

    - <span id="page-5-1"></span>[2] M. Lee, G. Yu, H. Dai, and G. Ye Li, "Graph neural
    networks meet wireless communications: Motivation, applications, and future directions,"
    *IEEE Wireless Commun.*, vol. 29, no. 5, pp. 12–19, 2022.

    - [3] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, "A comprehensive
    survey on graph neural networks," *IEEE Trans. Neural Net. and Learn. Syst.*,
    vol. 32, pp. 4–24, 2021.

    - [4] M. Eisen and A. Ribeiro, "Optimal wireless resource allocation with random
    edge graph neural networks," *IEEE Trans. Signal Process.*, vol. 68, pp. 2977–2991,
    2020.


    ![](_page_5_Figure_10.jpeg)


    <span id="page-5-10"></span>Fig. 5. GNN performance gain (%) vs W, Performance
    gain with respect to best RSRP.


    ![](_page_5_Figure_12.jpeg)


    <span id="page-5-12"></span>Fig. 7. GNN-based Cell switched off rate vs λ2/λ1,
    for W = [20, 80] (MHz) and K = 150.


    - [5] Yifei Shen, Yuanming Shi, and et.al., "Graph neural networks for scalable
    radio resource management: Architecture design and theoretical analysis," *IEEE
    J. Sel. Areas Commun.*, vol. 39, no. 1, pp. 101–115, 2021.

    - <span id="page-5-2"></span>[6] Shiwen He, Shaowen Xiong, and et.al. Ou, "An
    overview on the application of graph neural networks in wireless networks," *IEEE
    Open J. Commun.s Society*, vol. 2, pp. 2547–2565, 2021.

    - <span id="page-5-3"></span>[7] Gunther Auer, Vito Giannini, and et.al., "How
    much energy is needed to run a wireless network?," *IEEE Wireless Commun.*, vol.
    18, no. 5, pp. 40–49, 2011.

    - <span id="page-5-4"></span>[8] M. M. Aftab Hossain and Cicek et.al. Cavdar,
    "Energy saving game for massive MIMO: Coping with daily load variation," *IEEE
    Trans. Vehicular Tech.*, vol. 67, no. 3, pp. 2301–2313, 2018.

    - <span id="page-5-5"></span>[9] M. Lee, G. Yu, and H. Dai, "Decentralized inference
    with graph neural networks in wireless communication systems," *IEEE Trans. Mobile
    Comput.*, vol. 22, no. 5, pp. 2582–2598, 2023.

    - <span id="page-5-6"></span>[10] Mingjie Feng, Shiwen Mao, and Tao Jiang, "Base
    station ON-OFF switching in 5G wireless networks: Approaches and challenges,"
    *IEEE Wireless Commun.*, vol. 24, no. 4, pp. 46–54, 2017.

    - <span id="page-5-7"></span>[11] P. Velickovi ˇ c, G. Cucurull, and et.al., "Graph
    attention networks," ´ *Int. Conf. Learning Representations*, 2018.

    - <span id="page-5-8"></span>[12] William L. Hamilton, Rex Ying, and Jure Leskovec,
    "Inductive representation learning on large graphs," *Proc.31st Int. Conf. Neural
    Inf. Proces. Sys.*, 2017.'
- title: "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n\
    \  Miss Latency"
  abstract: 'Caching is crucial for system performance, but the delayed hit phenomenon,

    where requests queue during lengthy fetches after a cache miss, significantly

    degrades user-perceived latency in modern high-throughput systems. While prior

    works address delayed hits by estimating aggregate delay, they universally

    assume deterministic fetch latencies. This paper tackles the more realistic,

    yet unexplored, scenario where fetch latencies are stochastic. We present, to

    our knowledge, the first theoretical analysis of delayed hits under this

    condition, deriving analytical expressions for both the mean and variance of

    the aggregate delay assuming exponentially distributed fetch latency.

    Leveraging these insights, we develop a novel variance-aware ranking function

    tailored for this stochastic setting to guide cache eviction decisions more

    effectively. The simulations on synthetic and real-world datasets demonstrate

    that our proposed algorithm significantly reduces overall latency compared to

    state-of-the-art delayed-hit strategies, achieving a $3\%-30\%$ reduction on

    synthetic datasets and approximately $1\%-7\%$ reduction on real-world traces.'
  url: http://arxiv.org/abs/2505.15531v1
  keywords: ''
  document: '# Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic
    Miss Latency


    Bowen Jiang, Chaofan Ma Shanghai Jiao Tong University


    May 22, 2025


    #### Abstract


    Caching is crucial for system performance, but the delayed hit phenomenon, where
    requests queue during lengthy fetches after a cache miss, significantly degrades
    user-perceived latency in modern high-throughput systems. While prior works address
    delayed hits by estimating aggregate delay, they universally assume deterministic
    fetch latencies. This paper tackles the more realistic, yet unexplored, scenario
    where fetch latencies are stochastic. We present, to our knowledge, the first
    theoretical analysis of delayed hits under this condition, deriving analytical
    expressions for both the mean and variance of the aggregate delay assuming exponentially
    distributed fetch latency. Leveraging these insights, we develop a novel variance-aware
    ranking function tailored for this stochastic setting to guide cache eviction
    decisions more effectively. The simulations on synthetic and real-world datasets
    demonstrate that our proposed algorithm significantly reduces overall latency
    compared to state-of-the-art delayed-hit strategies, achieving a 3% −30% reduction
    on synthetic datasets and approximately 1% − 7% reduction on real-world traces.


    ## 1 Introduction


    Caching is a pivotal technique pervasively employed across diverse computing and
    networking systems, such as Content Delivery Networks (CDNs) [\[1\]](#page-7-0)
    and Mobile Computing environments [\[2\]](#page-8-0), to enhance system performance
    and user experience. Serving requests directly from the cache—a hit—yields low
    latency, whereas a cache miss necessitates fetching data from slower tiers (e.g.,
    remote servers), thereby incurring significant delays. Traditional caching strategies
    span a wide spectrum: from classic heuristics such as Least Recently Used (LRU)
    [\[3\]](#page-8-1), First-In-First-Out (FIFO) [\[5\]](#page-8-2), and Least Frequently
    Used (LFU) [\[4\]](#page-8-3); to sophisticated methods including Adaptive Replacement
    Cache (ARC) [\[6\]](#page-8-4), ADAPTSIZE [\[8\]](#page-8-5), and Least Hit Density
    (LHD) [\[7\]](#page-8-6); and extend to contemporary learning-based approaches
    [\[9,](#page-8-7) [10,](#page-8-8) [11\]](#page-8-9). These diverse algorithms
    have predominantly focused on maximizing the cache hit ratio. Implicitly or explicitly,
    much of this foundational work has operated under the assumption that the fetch
    latency upon a miss is negligible, particularly when compared to request inter-arrival
    times.


    However, in modern high-throughput systems, this negligible-latency assumption
    often breaks down. Fetch latencies from remote servers can exceed 100ms, while
    average request inter-arrival times may plummet to 1µs [\[12\]](#page-8-10). Consequently,
    when an object miss occurs, subsequent requests for that same object arriving
    during the fetch interval cannot be served immediately; they must wait until the
    fetch completes. These waiting requests are termed delayed hits. With an evolving
    ratio between system throughputs and latencies, the impact of delayed hits on
    overall user-perceived latency becomes increasingly significant. Prior works addressing
    delayed hits, such as MAD [\[12\]](#page-8-10), LAC [\[13\]](#page-8-11), VA-CDH
    [\[16\]](#page-8-12), and CALA [\[14\]](#page-8-13), primarily focus on estimating
    the aggregate delay defined as the sum of the initial miss latency and subsequent
    delayed hit latencies. Specifically, MAD [\[12\]](#page-8-10) calculates the historical
    average aggregate delay (AggDelay), assuming all prior requests are missed. LAC
    [\[13\]](#page-8-11) derives an analytical expression for the mean aggregate delay
    under Poisson arrivals. Viewing average estimates as potentially imprecise and
    simple upper bounds as too conservative, CALA [\[14\]](#page-8-13) balances these
    by modeling aggregate delay as a weighted sum of AggDelay and the square of miss
    latency. VA-CDH proposes a novel ranking function considering the variance of
    aggregate delay [\[16\]](#page-8-12). We note that all the previous works are
    typically under the assumption of deterministic miss latency.


    However, the assumption of deterministic fetch latency often deviates from reality,
    where network conditions and server load introduce significant stochasticity into
    fetch times. This paper tackles the delayed hit caching problem under this more
    realistic, yet challenging, condition of stochastic fetch latencies. Such randomness
    further increases the complexity and variability of the aggregate delay, making
    its variance a critical factor to consider alongside its mean for effective latency
    optimization. Building upon the variance-aware principles introduced in [\[16\]](#page-8-12),
    we provide a rigorous theoretical analysis deriving the exact mean and variance
    of aggregate delay under stochastic (specifically, exponentially distributed)
    miss latency. Based on this analysis, we develop the ranking function tailored
    for this stochastic miss latency to guide cache eviction decisions effectively.
    In summary, our main contributions are as follows:


    - We are the first work to model and analyze the delayed hit caching problem under
    the more realistic condition of stochastic miss latency. All previous works related
    to delayed hits assume that the miss latency is deterministic.

    - We derive exact analytical expressions for the mean and variance of the aggregate
    delay that occurs following a cache miss with stochastic fetch times. Leveraging
    this theoretical analysis, we obtain the variance-aware ranking function to guide
    cache evictions.

    - We demonstrate through simulations on synthetic and real-world datasets that
    our proposed algorithm significantly reduces overall latency compared to state-of-the-art
    delayed-hit caching strategies, achieving a 3% −30% reduction average on synthetic
    datasets and 1% −7% reduction approximately on real-world traces.


    ## 2 Problem Definition and Motivation


    ### 2.1 Problem Formulation


    We consider a cache of fixed capacity C operating over a discrete time horizon
    T, serving requests R<sup>t</sup> for objects i drawn from a universe of N types
    at time t. Every object i is associated with a size si , constrained such that
    max<sup>i</sup> s<sup>i</sup> < C. Assuming that the arrival process for each
    object i follows a Poisson process with a rate parameter λ<sup>i</sup> . Cache
    hits yield zero latency. Cache misses initiate a fetch process from a remote server,
    taking a random time Z<sup>i</sup> , and we model Z<sup>i</sup> with a probability
    density function(PDF) g(z). For analytical clarity, we specifically assume Z<sup>i</sup>
    follows an exponential distribution characterized by a rate parameter µ<sup>i</sup>
    (where the mean fetch time is E[Z<sup>i</sup> ] = 1/µ<sup>i</sup> , denoted as
    zi), thus g(z) = µie −µiz for z > 0. Any requests for object i arriving at time
    t ′ during its fetch interval t ′ ∈ (t, t + Z<sup>i</sup> ] become delayed hits,
    incurring latency equal to the remaining fetch time Z<sup>i</sup> − (t ′− t).
    Upon fetch completion, an eviction policy may evict cached objects to accommodate
    the newly fetched object, aiming to minimize the sum of latencies experienced
    by all requests throughout the entire time horizon T.


    Remark 1 In the analysis of stochastic miss latency, the exponential distribution,
    with parameter µi, is adopted as the model. Its primary advantage lies in its
    mathematical tractability, greatly facilitating theoretical analysis and derivations
    within complex systems. The concise mathematical form of the exponential distribution,
    coupled with its unique memoryless property, enables the derivation of exact analytical
    expressions for key statistics of aggregate delay, such as its mean and variance.
    This provides an analytical foundation for understanding the impact of randomness
    on aggregate delay. While acknowledging that actual miss latency may exhibit more
    complex behaviors (e.g., potential heavy-tailed phenomena) than the exponential
    distribution can capture, the exponential distribution, as a widely applied and
    fundamental probabilistic model, offers the necessary simplification and analytical
    feasibility to investigate this challenging problem and elucidate the underlying
    mechanisms of key factors.


    ### 2.2 The Aggregate Delay and Motivation


    The aggregate delay is the initial miss latency plus the sum of latencies incurred
    by subsequent requests for the same object arriving during its fetch process,
    which is defined as follows at time t,


    $$D\_i = Z\_i + \sum\_{t < t'' \le t + Z\_i} (t + Z\_i - t'') \mathbb{I}(R\_{t''}
    = i), \tag{1}$$


    where I(·) is the indicator function and R<sup>t</sup> ′ denotes the request at
    time t ′ . However, since D<sup>i</sup> depends on future arrivals within the
    fetch interval (t, t + Z<sup>i</sup> ], its exact value is unknown when the miss
    occurs. Estimating this aggregate delay is therefore a central challenge addressed
    by prior works [\[12,](#page-8-10) [13,](#page-8-11) [14,](#page-8-13) [15\]](#page-8-14).
    A key limitation of prior methods addressing delayed hits is their focus on estimating
    only the average aggregate delay, often neglecting its inherent variability. While
    the importance of variance has been noted in [\[16\]](#page-8-12), a clear illustration
    of its impact is beneficial. To demonstrate the necessity of considering the variance
    of aggregate delay, we present a toy example comparing two caching policies in
    Fig[.1.](#page-2-0) We consider a cache of size 1, holding either object A or
    B, both with a fetch latency of z = 4ms, and the request sequences are AAABAAABBBBAABBB.
    The initial cache is empty. Policy 1 (Mean-based) prioritizes caching the object
    with the higher observed mean aggregate delay, while Policy 2 (Mean-Variance-based)
    prioritizes storing objects with a large sum of the mean and standard deviation
    of the aggregate latency.


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Figure 1: A toy example to show the importance of variance
    in cache decision.


    Policy 1: At time t = 1, a request for A results in a cache miss with a latency
    of 4, causing subsequent requests at t = 2 and t = 3 to be delayed hits with latencies
    of 3 and 2, respectively. Following this, at t = 4, a request for B incurs a miss
    with a latency of 4. The object A is retrieved and cached at t = 5, leading to
    zero-latency hits for requests from t = 5 to t = 7. When B is retrieved at t =
    8, a comparison is made: A''s average aggregate delay (accumulated from t = 1
    − 3) is 9, while B''s (from the t = 4 miss) is 4. Since A''s average latency is
    higher, A is retained in the cache. Consequently, requests for B from t = 8 to
    t = 11 result in a miss followed by delayed hits, incurring latencies of 4, 3,
    2, and 1. At t = 12, B is retrieved again; its new average latency becomes (10
    + 4)/2 = 7, while A''s remains 9. As A''s average is still higher, A is kept,
    resulting in hits with zero latency at t = 12 and t = 13. Subsequently, requests
    for B from t = 14 to t = 17 trigger another miss sequence with latencies of 4,
    3, 2, and 1, respectively. Therefore, the total latency across this entire sequence
    under Policy 1 amounts to 33.


    Policy 2: The cache behavior matches Policy 1 until t = 12. At this point, a new
    rule based on the sum of average aggregate delay and standard deviation determines
    retention. A''s sum is 9 (avg 9, std 0), while B''s is 10 (avg 7, std 3). Since
    10 > 9, B is cached, causing A to miss at t = 12 (latency 4) and have a delayed
    hit at t = 13 (latency 3). B subsequently hits with zero latency at t = 14 − 15.
    When A''s data arrives at t = 16, the sums are re-evaluated: A is now 9 (avg 8,
    std 1), still less than B''s 10. B remains cached, resulting in zero-latency hits
    at t = 16 − 17. This modified approach leads to a total latency of 30. This example
    demonstrates that considering variance leads to a better caching decision and
    reduces the overall latency.


    ## 3 Characteristics of Aggregate Delay


    As previously demonstrated, incorporating both the mean and variance of aggregate
    delay into caching decisions enables more effective latency reduction. Motivated
    by this finding, we analyze the statistical properties including the distribution
    and variability of aggregate delay under the assumption of stochastic miss delays
    in this section.


    #### 3.1 The distribution of aggregate delay


    Recall the PDF of the random miss latency is g(z) and its mean value is µ<sup>i</sup>
    = 1/z<sup>i</sup> . Because the object i arrives according to a Poisson process
    with rate λ<sup>i</sup> , thus the probability that the number of arrivals N<sup>z</sup>
    <sup>i</sup> during the fetch period of length z<sup>i</sup> euqals k is:


    $$P(N\_i^z = k) = \frac{e^{-\lambda\_i z\_i} (\lambda\_i z\_i)^k}{k!}, \quad k
    = 0, 1, 2, \dots \tag{2}$$


    Based on Theorem 1 in [\[16\]](#page-8-12), the PDF of D<sup>i</sup> can be obtained
    by integrating the conditional PDF


    <span id="page-3-0"></span>f(D<sup>i</sup> |Z<sup>i</sup> = z) over all possible
    values of the miss latency z, weighted by g(z):


    $$\begin{split} f(D\_i) &= \int\_0^\infty f(D\_i | Z\_i = z) g(z) dz \\ &= \int\_0^\infty
    \left[ e^{-\lambda\_i z} \delta(D\_i - z) \right] \mu\_i e^{-\mu\_i z} dz + \\
    &\int\_0^\infty \left[ \sum\_{k=1}^\infty \frac{e^{-\lambda\_i z} (\lambda\_i
    z)^k}{k!} f\_{D\_i | k, z}(D\_i) \right] \mu\_i e^{-\mu\_i z} dz, \end{split}
    \tag{3}$$


    where fDi|k,z(Di) = <sup>1</sup> z <sup>k</sup>(k−1)! P⌊Di/z⌋−<sup>1</sup> <sup>j</sup>=0
    (−1)<sup>j</sup> k j (D<sup>i</sup> − (j + 1)z) k−1 , λ<sup>i</sup> is the arrival
    rate for object i, and k is the total arrivals for object i during the fetch process.
    Using the property of the Dirac δ function R ϕ(z)δ(z − a)dz = ϕ(a) , the first
    part in [\(3\)](#page-3-0) can be calculated as follows,


    $$\int\_0^\infty \left[ e^{-\lambda\_i z} \delta(D\_i - z) \right] \mu\_i e^{-\mu\_i
    z} dz = \mu\_i e^{-(\lambda\_i + \mu\_i)D\_i}.\tag{4}$$


    Analytically solving the second integral in [\(3\)](#page-3-0) presents major
    difficulties. Notably, the upper limit ⌊Di/z⌋ − 1 of the inner summation depends
    directly on the integration variable z, preventing straightforward integration
    as the number of terms changes. Furthermore, the effective integration range z
    ∈ [Di/(k + 1), D<sup>i</sup> ] varies with the summation index k, complicating
    the overall integration process.


    Remark 2 (Hardness). Although we can formulate a formal integral expression for
    the PDF of the aggregate delay D<sup>i</sup> using the Law of Total Probability
    when the delay Z<sup>i</sup> follows an exponential distribution, obtaining its
    closed-form analytical solution for the general case (k ≥ 1) appears to be intractable
    due to the complexity of the resulting integral. Potential future directions to
    address these challenges include employing numerical integration techniques to
    approximate the integral''s value for specific parameters, likely after truncating
    the infinite series over k. Alternatively, analyzing the system using Laplace
    transforms (LD<sup>i</sup> (s) = E[e −sD<sup>i</sup> ]) might offer a more structured
    approach [\[17\]](#page-8-15), potentially simplifying the calculation of moments
    or enabling numerical inversion to approximate the probability density function.


    #### 3.2 The mean and variance of aggregate delay


    The analysis of the mean and variance of aggregate delay under stochastic miss
    latency builds upon the corresponding results for deterministic miss latency.
    Therefore, we first present the mean and variance of aggregate delay for the deterministic
    scenario, as detailed in Theorem [1.](#page-3-1)


    <span id="page-3-1"></span>Theorem 1 (in [\[16\]](#page-8-12)) Assume object i
    arrives according to a Poisson process with rate λ<sup>i</sup> and the miss latency
    is zi. The mean and variance of D<sup>i</sup> are:


    $$E[D\_i] = z\_i \left( 1 + \frac{\lambda\_i z\_i}{2} \right), \text{Var}(D\_i)
    = \frac{z\_i^3 \lambda\_i}{3} \tag{5}$$


    Under stochastic miss latency, the mean and variance of aggregate delays are shown
    in Theorem [2.](#page-3-2)


    <span id="page-3-2"></span>Theorem 2 Let the arrivals for object i follow a Poisson
    process with rate λ<sup>i</sup> . Let the miss latency Zi , follow an exponential
    distribution with rate µ<sup>i</sup> = 1/z<sup>i</sup> , i.e., Z<sup>i</sup> ∼
    Exp(µi). Then, the mean and variance of the aggregated delay D<sup>i</sup> are
    given by:


    $$E[D\_i] = z\_i + \lambda\_i z\_i^2 \tag{6}$$


    $$Var(D\_i) = z\_i^2 + 6\lambda\_i z\_i^3 + 5\lambda\_i^2 z\_i^4 \tag{7}$$


    Proof: The derivation proceeds by conditioning on the miss latency Z<sup>i</sup>
    ∼ Exp(µi), z<sup>i</sup> = 1/µ<sup>i</sup> and applying the Law of Total Expectation
    for the mean and the Law of Total Variance for the variance. We first derive the
    mean E[D<sup>i</sup> ]. By the law of total expectation: E[D<sup>i</sup> ] = E[E[D<sup>i</sup>
    |Z<sup>i</sup> ]].


    Based on Theorem [1,](#page-3-1) we have:


    <span id="page-3-3"></span>

    $$E[D\_i|Z\_i] = Z\_i(1 + \lambda\_i Z\_i/2) \tag{8}$$


    Using Equation [\(8\)](#page-3-3) and E[D<sup>i</sup> ] = E[E[D<sup>i</sup> |Z<sup>i</sup>
    ]], we have:


    $$E[D\_i] = E[Z\_i(1 + \lambda\_i Z\_i/2)] = E[Z\_i + \lambda\_i Z\_i^2/2] \tag{9}$$


    Using the linearity of expectation:


    $$E[D\_i] = E[Z\_i] + \frac{\lambda\_i}{2} E[Z\_i^2]$$


    For Z<sup>i</sup> ∼ Exp(µi), we know that E[Z<sup>i</sup> ] = 1/µ<sup>i</sup>
    and V ar(Zi) = 1/µ<sup>2</sup> i . Thus, E[Z 2 i ] = V ar(Zi) + (E[Z<sup>i</sup>
    ])<sup>2</sup> = 1/µ<sup>2</sup> <sup>i</sup> + (1/µi) <sup>2</sup> = 2/µ<sup>2</sup>
    i . Substituting these values yields:


    $$E[D\_i] = \frac{1}{\mu\_i} + \frac{\lambda\_i}{2} \left(\frac{2}{\mu\_i^2}\right)
    = \frac{1}{\mu\_i} + \frac{\lambda\_i}{\mu\_i^2} = z\_i + \lambda\_i z\_i^2. \tag{10}$$


    Next, we derive the variance V ar(Di). By the law of total variance:


    $$Var(D\_i) = E[Var(D\_i|Z\_i)] + Var(E[D\_i|Z\_i])$$


    We need to compute two terms: E[V ar(D<sup>i</sup> |Zi)] and V ar(E[D<sup>i</sup>
    |Z<sup>i</sup> ]).


    Computing V ar(D<sup>i</sup> |Z<sup>i</sup> = z): Based on Theorem [1,](#page-3-1)
    we have:


    $$\operatorname{Var}(D\_i|Z\_i=z) = \frac{1}{3}\lambda\_i z^3. \tag{11}$$


    Therefore, E[V ar(D<sup>i</sup> |Zi)] = E - 1 3 λiZ 3 i = λi <sup>3</sup> E[Z
    3 i ]. For Z<sup>i</sup> ∼ Exp(µi), E[Z n i ] = n!/µ<sup>n</sup> i . Thus, E[Z
    3 i ] = 3!/µ<sup>3</sup> <sup>i</sup> = 6/µ<sup>3</sup> i .


    $$E[Var(D\_i|Z\_i)] = \frac{\lambda\_i}{3} \frac{6}{\mu\_i^3} = \frac{2\lambda\_i}{\mu\_i^3}$$


    Computing the second term of the law of total variance: V ar(E[D<sup>i</sup> |Z<sup>i</sup>
    ]) From Equation [\(8\)](#page-3-3), we know that:


    $$\operatorname{Var}(E[D\_i|Z\_i]) = \operatorname{Var}(Z\_i(1+\lambda\_i Z\_i/2))
    = \operatorname{Var}(Z\_i + \frac{\lambda\_i}{2}Z\_i^2)$$


    Let Y = E[D<sup>i</sup> | Z<sup>i</sup> ]. Using V ar(Y ) = E[Y 2 ] − (E[Y ])<sup>2</sup>
    , we know from the mean calculation that E[Y ] = E[Z<sup>i</sup> + λi 2 Z 2 i
    ] = E[D<sup>i</sup> ] = <sup>1</sup> µi + λi µ<sup>2</sup> i . Therefore, we can
    obtain


    $$\begin{aligned} E[Y^2] &= E[(Z\_i + \frac{\lambda\_i}{2} Z\_i^2)^2] \\ &= E[Z\_i^2
    + \lambda\_i Z\_i^3 + \frac{\lambda\_i^2}{4} Z\_i^4] \\ &= E[Z\_i^2] + \lambda\_i
    E[Z\_i^3] + \frac{\lambda\_i^2}{4} E[Z\_i^4] \\ &= \frac{2}{\mu\_i^2} + \frac{6\lambda\_i}{\mu\_i^3}
    + \frac{6\lambda\_i^2}{\mu\_i^4}. \end{aligned} \tag{12}$$


    Thus, we have,


    $$\begin{split} Var(E[D\_i|Z\_i]) &= \left(\frac{2}{\mu\_i^2} + \frac{6\lambda\_i}{\mu\_i^3}
    + \frac{6\lambda\_i^2}{\mu\_i^4}\right) - \left(\frac{1}{\mu\_i} + \frac{\lambda\_i}{\mu\_i^2}\right)^2
    \\ &= \frac{1}{\mu\_i^2} + \frac{4\lambda\_i}{\mu\_i^3} + \frac{5\lambda\_i^2}{\mu\_i^4}
    \end{split} \tag{13}$$


    Now we can compute the total variance V ar(Di),


    $$\begin{split} Var(D\_i) &= E[Var(D\_i|Z\_i)] + Var(E[D\_i|Z\_i]) \\ &= \frac{2\lambda\_i}{\mu\_i^3}
    + \left(\frac{1}{\mu\_i^2} + \frac{4\lambda\_i}{\mu\_i^3} + \frac{5\lambda\_i^2}{\mu\_i^4}\right)
    \\ &= \frac{1}{\mu\_i^2} + \frac{6\lambda\_i}{\mu\_i^3} + \frac{5\lambda\_i^2}{\mu\_i^4}
    \\ &= z\_i^2 + 6\lambda\_i z\_i^3 + 5\lambda\_i^2 z\_i^4. \end{split} \tag{14}$$


    ■


    Remark 3 (Parameter dependency). Theorem [2](#page-3-2) considers the aggregate
    delay where miss latency, Z<sup>i</sup> , follows an exponential distribution
    with mean z<sup>i</sup> . The mean aggregate delay grows faster with the average
    latency z<sup>i</sup> (due to the z 2 i term) and linearly with arrival rate λ<sup>i</sup>
    . Variance increases dramatically, involving higher powers of both z<sup>i</sup>
    and λ<sup>i</sup> (up to λ 2 i z 4 i ), indicating high sensitivity to these parameters.
    Intuitively, variability now arises from two sources: the randomness of the latency
    Zi itself and the random arrivals interacting with this random latency. This dual
    randomness causes both mean and especially variance to react much more strongly
    to changes in system load λ<sup>i</sup> and average latency z<sup>i</sup> .


    ## 4 Our algorithm


    Our algorithm builds upon the VA-CDH framework introduced in [\[16\]](#page-8-12),
    which aims to minimize latency in caching with delayed hits under deterministic
    miss latency. A core principle of VA-CDH is incorporating the variability of aggregate
    delay into eviction decisions.


    To achieve this, it employs a novel variance-aware ranking function to evaluate
    the aggregate delay of object i:


    <span id="page-5-0"></span>

    $$f\_i = \frac{E[D\_i] + \omega \sigma [D\_i]}{R\_i s\_i},\tag{15}$$


    where E[D<sup>i</sup> ] and σ[D<sup>i</sup> ] are the estimated mean and standard
    deviation of the aggregate delay D<sup>i</sup> , respectively. R<sup>i</sup> is
    the estimated residual time until the next request for i, s<sup>i</sup> is its
    size, and ω > 0 is a hyperparameter controlling the sensitivity to aggregate delay
    variability. A higher score f<sup>i</sup> indicates a higher priority for keeping
    object i in the cache.


    In this paper, we specifically address the scenario where the fetch latency Z<sup>i</sup>
    upon a miss is stochastic, modeled as an exponential distribution with mean z<sup>i</sup>
    = 1/µ<sup>i</sup> . Based on the derived mean and standard deviation of the aggregate
    delay as presented in Theorem [2,](#page-3-2) the ranking function [\(15\)](#page-5-0)
    takes the specific form:


    <span id="page-5-1"></span>

    $$f\_i = \frac{(z\_i + \lambda\_i z\_i^2) + \omega\sqrt{z\_i^2 + 6\lambda\_i z\_i^3
    + 5\lambda\_i^2 z\_i^4}}{R\_i s\_i},\tag{16}$$


    where the terms in the numerator represent the analytical E[D<sup>i</sup> ] and
    ωσ[D<sup>i</sup> ] respectively. Our algorithm maintains online estimates for
    the residual time R<sup>i</sup> (using LRU) and the arrival rate λ<sup>i</sup>
    (using the inverse of the mean inter-arrival time) similar to [\[16\]](#page-8-12)
    . These parameters are estimated from recent request history within a sliding
    window of size S to handle non-stationarity efficiently, avoiding the overhead
    of tracking the entire history. When an eviction is necessary, our algorithm calculates
    the rank f<sup>i</sup> for each cached object using its estimated R<sup>i</sup>
    , λ<sup>i</sup> , and the specialized ranking function [\(16\)](#page-5-1). Objects
    yielding the lowest rank scores are then evicted to make space for the newly fetched
    object.


    ## 5 Simulation Results


    #### 5.1 Methodology


    We consider the baseline algorithms, including LRU [\[3\]](#page-8-1), LAC [\[13\]](#page-8-11),
    VA-CDH [\[16\]](#page-8-12), CALA [\[14\]](#page-8-13), LHD[\[7\]](#page-8-6),
    LRB[\[9\]](#page-8-7), ADAPTSIZE [\[8\]](#page-8-5), LHDMAD [\[12\]](#page-8-10)
    and LRUMAD[\[12\]](#page-8-10). We note that all of these baselines assume that
    the delay after a miss is a deterministic constant.


    The performance of algorithm A is measured by the latency improvement relative
    to LRU[\[14\]](#page-8-13), which is defined as follows,


    $$\text{Latency Impiprocal of A} = \frac{\text{Latency(LRU)} - \text{Latency(A)}}{\text{Latency(LRU)}},\tag{17}$$


    where Latency(LRU) is the total latency of LRU algorithm, Latency(A) is the total
    latency of algorithm A. In subsection [5.2](#page-5-2) and [5.3,](#page-6-0) we
    set S = 10K and ω = 1.


    #### <span id="page-5-2"></span>5.2 Synthetic dataset


    The performance of the our algorithm is first evaluated using a synthetic dataset
    comprising 100,000 requests for 100 unique objects. Object popularity in this
    dataset follows a Zipf distribution, and object


    <span id="page-6-1"></span>![](_page_6_Figure_0.jpeg)


    Figure 2: Comparison of latency improvement between our algorithm and SOTAs under
    the synthetic dataset(C = 500MB).


    sizes are integers selected uniformly from the range [1MB, 100MB]. We simulate
    under two distinct arrival scenarios: one adhering to a Poisson process and another
    modeled by a Pareto distribution. The latency incurred upon a cache miss is modeled
    as a constant delay L plus a component proportional to the object size. Our algorithm
    can improve the performance by approximately 3% − 30% on two different datasets
    as shown in Fig[.2.](#page-6-1) Importantly, this performance advantage persists
    even when arrivals follow the Pareto distribution, deviating from the Poisson
    model assumption used in our theoretical analysis.


    #### <span id="page-6-2"></span><span id="page-6-0"></span>5.3 Real world traces


    ![](_page_6_Figure_4.jpeg)


    Figure 3: The content popularity and average interval-time distributions of the
    four real-world traces.


    We further validate our algorithm on four real-world traces: Wiki2018 [\[9\]](#page-8-7),
    Wiki2019 [\[9\]](#page-8-7), Cloud [\[18\]](#page-8-16), and YouTube [\[19\]](#page-8-17).
    We present the object popularity and average inter-arrival time distributions
    of these four traces in Fig. [3.](#page-6-2) Using a 256 GB cache and various
    fetch latency settings, Fig. [5](#page-7-1) compares the latency improvement of
    our algorithm relative to LRU against state-of-the-art (SOTA) algorithms.


    Our algorithm consistently outperforms these SOTA algorithms, achieving approximate
    latency reductions of 1%−5% on Wiki2018, 1%−4% on Wiki2019, 1%−2% on Cloud, and
    3%−7% on YouTube. This performance advantage is primarily driven by our ranking
    function''s effective incorporation of miss latency randomness to prioritize evictions.


    <span id="page-7-2"></span>![](_page_7_Figure_0.jpeg)


    Figure 4: The impact of hyperparameters on latency improvement.


    <span id="page-7-1"></span>![](_page_7_Figure_2.jpeg)


    Figure 5: Comparison of latency improvement between our algorithm and SOTAs using
    a 256GB cache.


    ### 5.4 Sensitivity Analysis


    We perform a sensitivity analysis, presented in Fig. [4,](#page-7-2) to assess
    our algorithm''s performance dependency on its key hyperparameters: sliding window
    size S and variance control parameter ω. Both require careful tuning due to inherent
    trade-offs – ω balancing mean versus variance influence, and S managing the compromise
    between historical learning effectiveness and responsiveness to changing request
    patterns. The analysis involves systematically varying ω (at S = 10K) and S (at
    ω = 1). The delay L in miss latency is 5ms. Across these varied settings, our
    algorithm consistently outperforms current STOA approaches, confirming its operational
    robustness.


    ## 6 Conclusion


    In this paper, we address the delayed hits in caching systems, specifically tackling
    the often-overlooked scenario of stochastic fetch latencies. We provide the theoretical
    analysis under stochastic fetch latency, deriving exact expressions for the mean
    and variance of aggregate delay. Based on these findings, we have the variance-aware
    ranking function designed to guide eviction decisions by considering both the
    mean aggregate delay and its variability. The simulations validate our approach,
    demonstrating significant overall latency reductions compared to state-of-the-art
    delayed-hit algorithms.


    ## References


    <span id="page-7-0"></span>[1] Vakali A, Pallis G. Content delivery networks:
    Status and trends[J]. IEEE Internet Computing, 2003, 7(6): 68-74.


    - <span id="page-8-0"></span>[2] Jacob B, Wang D, Ng S. Memory systems: cache,
    DRAM, disk[M]. Morgan Kaufmann, 2010.

    - <span id="page-8-1"></span>[3] Puzak T R. Analysis of cache replacement-algorithms[M].
    University of Massachusetts Amherst, 1985.

    - <span id="page-8-3"></span>[4] Karakostas G, Serpanos D. Practical LFU implementation
    for web caching[J]. Technical Report TR-622-00, 2000.

    - <span id="page-8-2"></span>[5] Chrobak M, Noga J. LRU is better than FIFO[J].
    Algorithmica, 1999, 23: 180-185.

    - <span id="page-8-4"></span>[6] Megiddo N, Modha D S. ARC: A Self-Tuning, low
    overhead replacement cache[C]//2nd USENIX Conference on File and Storage Technologies
    (FAST 03). 2003.

    - <span id="page-8-6"></span>[7] Beckmann N, Chen H, Cidon A. LHD: Improving cache
    hit rate by maximizing hit density[C]//15th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 18). 2018: 389-403.

    - <span id="page-8-5"></span>[8] Berger D S, Sitaraman R K, Harchol-Balter M.
    ADAPTSIZE: Orchestrating the Hot Object Memory Cache in a Content Delivery Network[C]//14th
    USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 2017:
    483-498.

    - <span id="page-8-7"></span>[9] Song Z, Berger D S, Li K, et al. Learning relaxed
    belady for content distribution network caching[C]//17th USENIX Symposium on Networked
    Systems Design and Implementation (NSDI 20). 2020: 529-544.

    - <span id="page-8-8"></span>[10] Chen Q, Wang W, Chen W, et al. Cache-enabled
    multicast content pushing with structured deep learning[J]. IEEE Journal on Selected
    Areas in Communications, 2021, 39(7): 2135-2149.

    - <span id="page-8-9"></span>[11] Lei L, You L, Dai G, et al. A deep learning
    approach for optimizing content delivering in cacheenabled HetNet[C]//2017 international
    symposium on wireless communication systems (ISWCS). IEEE, 2017.

    - <span id="page-8-10"></span>[12] Atre N, Sherry J, Wang W, et al. Caching with
    delayed hits[C]//Proceedings of the Annual conference of the ACM Special Interest
    Group on Data Communication on the applications, technologies, architectures,
    and protocols for computer communication. 2020.

    - <span id="page-8-11"></span>[13] Yan G, Li J. Towards latency awareness for
    content delivery network caching[C]//2022 USENIX Annual Technical Conference (USENIX
    ATC 22). 2022: 789-804.

    - <span id="page-8-13"></span>[14] Zhang C, Tan H, Li G, et al. Online file caching
    in latency-sensitive systems with delayed hits and bypassing[C]//IEEE INFOCOM
    2022-IEEE Conference on Computer Communications. IEEE, 2022.

    - <span id="page-8-14"></span>[15] Tan H, Wang Y, Zhang C, et al. Asymptotically
    Tight Approximation for Online File Caching With Delayed Hits and Bypassing[J].
    IEEE Transactions on Networking, 2025.

    - <span id="page-8-12"></span>[16] Bowen Jiang, Chaofan Ma and Duo Wang. VA-CDH:
    A Variance-Aware Method to Optimize Latency for Caching with Delayed Hits. arXiv:2504.20335
    [cs.NI], 2025.

    - <span id="page-8-15"></span>[17] Schiff J L. The Laplace transform: theory and
    applications[M]. Springer Science and Business Media, 1999.

    - <span id="page-8-16"></span>[18] Waldspurger C A, Park N, Garthwaite A, et al.
    Efficient MRC construction with SHARDS[C]//13th USENIX Conference on File and
    Storage Technologies (FAST 15). 2015: 95- 110.

    - <span id="page-8-17"></span>[19] Zink M, Suh K, Gu Y, et al. Watch global, cache
    local: YouTube network traffic at a campus network: measurements and implications[C]//Multimedia
    Computing and Networking 2008. SPIE, 2008.'
