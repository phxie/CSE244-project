papers:
- title: 'X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs'
  abstract: 'LLM-based multi-agent systems (MAS) extend the capabilities of single
    LLMs by

    enabling cooperation among multiple specialized agents. However, most existing

    MAS frameworks rely on a single LLM to drive all agents, constraining the

    system''s intelligence to the limit of that model. This paper explores the

    paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by

    diverse LLMs, elevating the system''s potential to the collective intelligence

    of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to

    evaluate the performance of various LLMs across different domains and

    MAS-related functions. As an extensive empirical study, we assess 27 LLMs

    across 5 domains (encompassing 21 test sets) and 5 functions, conducting over

    1.7 million evaluations to identify optimal model selections for each

    domain-function combination. Building on these findings, we demonstrate that

    transitioning from homogeneous to heterogeneous LLM-driven MAS can

    significantly enhance system performance without requiring structural redesign.

    Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration

    yields up to 8.4\% performance improvement on the MATH dataset. In a mixed

    chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable

    47\% performance boost on the AIME dataset. Our results underscore the

    transformative potential of heterogeneous LLMs in MAS, highlighting a promising

    avenue for advancing scalable, collaborative AI systems.'
  url: http://arxiv.org/abs/2505.16997v1
  keywords: ''
  document: '# X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs


    Rui Ye1,\* Xiangrui Liu1,\* Qimin Wu<sup>1</sup> Xianghe Pang<sup>1</sup> Zhenfei
    Yin2,3 Lei Bai<sup>4</sup> Siheng Chen1,†


    <sup>1</sup> Shanghai Jiao Tong University <sup>2</sup> University of Oxford <sup>3</sup>
    The University of Sydney <sup>4</sup> Shanghai AI Laboratory \* Equal Contributions
    † Corresponding Author X-MAS:<https://github.com/MASWorks/X-MAS>


    # Abstract


    LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
    enabling cooperation among multiple specialized agents. However, most existing
    MAS frameworks rely on a single LLM to drive all agents, constraining the system''s
    intelligence to the limit of that model. This paper explores the paradigm of heterogeneous
    LLM-driven MAS (X-MAS), where agents are powered by diverse LLMs, elevating the
    system''s potential to the collective intelligence of diverse LLMs. We introduce
    X-MAS-Bench, a comprehensive testbed designed to evaluate the performance of various
    LLMs across different domains and MAS-related functions. As an extensive empirical
    study, we assess 27 LLMs across 5 domains (encompassing 21 test sets) and 5 functions,
    conducting over 1.7 million evaluations to identify optimal model selections for
    each domain-function combination. Building on these findings, we demonstrate that
    transitioning from homogeneous to heterogeneous LLM-driven MAS can significantly
    enhance system performance without requiring structural redesign. Specifically,
    in a chatbot-only MAS scenario, the heterogeneous configuration yields up to 8.4%
    performance improvement on the MATH dataset. In a mixed chatbot-reasoner scenario,
    the heterogeneous MAS could achieve a remarkable 47% performance boost on the
    AIME dataset. Our results underscore the transformative potential of heterogeneous
    LLMs in MAS, highlighting a promising avenue for advancing scalable, collaborative
    AI systems.


    # 1 Introduction


    Large language models (LLMs) such as GPT [\[1\]](#page-9-0), Gemini [\[2\]](#page-9-1),
    Qwen [\[3\]](#page-9-2), have been applied across various domains. However, despite
    their remarkable capabilities, LLMs often struggle with multifaceted, complex,
    and real-world problems due to inherent limitations such as hallucinations [\[4,](#page-9-3)
    [5\]](#page-9-4).


    In response to these limitations, LLM-based multi-agent systems (MAS) have emerged
    as a promising solution [\[6,](#page-9-5) [7,](#page-9-6) [8\]](#page-9-7). MAS
    involves the collaboration of multiple agents, each specialized in specific functions,
    to address problems more effectively than a single model could. his paradigm has
    been successfully applied across various scenarios, including software development
    [\[7,](#page-9-6) [9\]](#page-9-8), mathematics [\[10,](#page-9-9) [11\]](#page-9-10),
    and scientific discovery [\[12,](#page-9-11) [13\]](#page-9-12). For instance,
    ChatDev [\[7\]](#page-9-6), MetaGPT [\[9\]](#page-9-8), and EvoMAC [\[14\]](#page-9-13)
    utilize multiple coding agents (e.g., coders and testers) to improve software
    programming, while AI co-scientist [\[8\]](#page-9-7) employs a MAS to enhance
    biomedical and scientific research.


    Despite notable progress, most existing MAS frameworks rely on a single LLM to
    drive all agents [\[9,](#page-9-8) [7,](#page-9-6) [11,](#page-9-10) [6,](#page-9-5)
    [14,](#page-9-13) [15,](#page-9-14) [16\]](#page-9-15). This manner inherently
    limits the system''s intelligence to that of the underlying model. For example,
    if a single LLM produces fundamental errors in certain facts, these mistakes are
    unlikely to be corrected through the collaboration of agents powered by the same
    model. Inspired


    Preprint. Under review.


    ![](_page_1_Figure_0.jpeg)


    <span id="page-1-0"></span>Figure 1: Overview of our X-MAS-Bench and X-MAS-Design.
    X-MAS-Bench assesses the capabilities of LLMs in MAS while X-MAS-Design focuses
    on transitioning a homogeneous MAS to a heterogeneous one, gaining from the observations
    in X-MAS-Bench. Experiments on chatbot-only and mixed chatbot-reasoner scenarios
    evidently show the benefits of heterogeneous MAS.


    by the advantages of diversity in collective intelligence [\[17,](#page-9-16)
    [18,](#page-9-17) [19\]](#page-10-0), this paper explores MAS with heterogeneous
    LLMs (X-MAS), pushing the system''s capabilities beyond its previous limit to
    harness the collective potential of LLMs trained on diverse corpora or by different
    teams [\[20,](#page-10-1) [21,](#page-10-2) [22\]](#page-10-3).


    To provide a comprehensive evaluation of LLMs in MAS, we introduce X-MAS-Bench,
    a testbed designed to assess the performance of various LLMs across different
    MAS-related functions and domains. Specifically, we consider 5 representative
    functions of agents in MAS, including questionanswering [\[15,](#page-9-14) [9\]](#page-9-8),
    revise [\[14,](#page-9-13) [23\]](#page-10-4), aggregation [\[24,](#page-10-5)
    [25\]](#page-10-6), planning [\[26,](#page-10-7) [10\]](#page-9-9), and evaluation
    [\[16,](#page-9-15) [7\]](#page-9-6); as well as 5 common domains, including mathematics,
    coding, science, medicine, and finance—spanning 21 test sets. Each function is
    assessed under controlled experimental conditions. For example, when assessing
    aggregation, each query is input into several pre-defined LLMs, whose outputs
    are concatenated to be aggregated by the examined LLM. The aggregated responses
    of various LLMs are then evaluated and compared. Finally, we assess 27 LLMs across
    these 5 functions and 5 domains, conducting over 1.7 million evaluations to identify
    the optimal model selections for each domainfunction combination. Our findings
    include that (1) no single LLM excels across all scenarios, (2) a single LLM could
    have significant performance variation across functions and domains, (3) different
    LLMs may show large performance disparities within the same function and domain,
    (4) smaller LLMs can sometimes outperform larger ones, highlighting the potential
    advantages of employing heterogeneous LLMs in MAS. These results provide valuable
    insights for researchers and practitioners in selecting the most appropriate LLMs
    for their specific MAS applications.


    Building on these observations, we explore the effects of transitioning from homogeneous
    to heterogeneous LLM-driven MAS (X-MAS-Design). As a proof of concept, given the
    implementation of a MAS method, we simply assign agents with appropriate LLMs
    (taking seconds) by referring observations in X-MAS-Bench. To validate our idea,
    we examine three existing MAS frameworks—LLM-Debate [\[15\]](#page-9-14), AgentVerse
    [\[16\]](#page-9-15), and DyLAN [\[11\]](#page-9-10)—as well as a prototype MAS
    designed by us, which incorporates all five functions in one system. Our analysis
    covers five domains with no sample overlap compared to X-MAS-Bench. In a chatbot-only
    scenario, we observe consistent improvements in performance for heterogeneous
    MAS over homogeneous configurations, achieving up to a 8.4% performance gain on
    the MATH [\[27\]](#page-10-8) benchmark. Interestingly, while reasoner-only MAS
    often underperforms relative to chatbot-only systems, combining chatbot and reasoner
    in a heterogeneous MAS leads to significant performance improvements. Specifically,
    in the competition-level AIME-2024 benchmark, AgentVerse [\[16\]](#page-9-15)
    is improved from 20% to 50%, and DyLAN [\[11\]](#page-9-10) improved from 40%
    to 63%. Our further experiments reveal that increasing the number of candidate
    LLMs for heterogeneous MAS results in a monotonic performance improvement, reinforcing
    the value of LLM diversity in MAS. Based on our work, future research could explore
    more nuanced strategies for selecting and integrating LLMs in heterogeneous MAS;
    investigate the scalability and adaptability of heterogeneous MAS across different
    industries and other complex tasks.


    Our contributions are as follows:


    1. X-MAS-Bench: We assess 27 LLMs across 5 MAS-related functions and 5 domains,
    conducting over 1.7 million evaluations to identify diverse optimal model selections
    for each domain-function combination. These observations could benefit researchers
    and practitioners in building MAS.


    - 2. X-MAS-Design: Based on findings in X-MAS-Bench, we propose to transition
    existing MAS methods from homogeneous to heterogeneous LLM-driven MAS. We conduct
    extensive experiments, showing that heterogeneous MAS consistently outperforms
    homogeneous MAS.

    - 3. Open Source: We release all data, code, and evaluation results to facilitate
    future MAS research.


    # 2 Related Work


    LLM-based MAS. LLM-based multi-agent systems (MAS) leverage multiple LLM-based
    agents to collaborate for better task solving than single LLM [\[16,](#page-9-15)
    [9,](#page-9-8) [28,](#page-10-9) [6\]](#page-9-5). ChatDev [\[7\]](#page-9-6),
    MetaGPT [\[9\]](#page-9-8), and EvoMAC [\[14\]](#page-9-13) use multiple coding
    agents (e.g., coders and testers) for software programming; while MACM [\[10\]](#page-9-9)
    applies math agents for mathematics. Focusing on general tasks, debate-based methods
    [\[15,](#page-9-14) [25\]](#page-10-6) enable multiple experts in debating for
    better solutions; AgentVerse [\[16\]](#page-9-15) and DyLAN [\[11\]](#page-9-10)
    dynamically adjust the agent team for task solving; while MAS-GPT [\[6\]](#page-9-5)
    trains an LLM for generating MAS. However, all of these methods rely on a single
    LLM to drive all agents, which inherently limits the system''s intelligence to
    that of the underlying LLM. This paper proposes to push the limit by harnessing
    the collective intelligence of heterogeneous LLMs from different sources.


    Heterogeneous LLMs. In a general context of LLMs, there are several works related
    to using heterogeneous LLMs [\[29,](#page-10-10) [30\]](#page-10-11). LLM-Blender
    [\[31\]](#page-10-12) trains a model for ensembling outputs from multiple LLMs.
    MoA [\[32\]](#page-10-13) and ReConcile [\[33\]](#page-10-14) enable multiple
    LLMs for discussion, however, involving all candidate LLMs without considering
    their appropriateness. MASRouter [\[34\]](#page-10-15) manually selects several
    candidate LLMs for MAS and is optimized for their specific framework. In contrast,
    our paper systematically assess the capabilities of LLMs across several MAS-related
    functions and domains, aiming to universally benefit the design of heterogeneous
    MAS for various MAS methods.


    Benchmarking LLMs. Many works benchmark the capabilities of LLMs in various domains
    (such as math [\[27\]](#page-10-8), coding [\[35\]](#page-10-16), science [\[36\]](#page-11-0),
    medicine [\[37\]](#page-11-1), and finance [\[38\]](#page-11-2)) and functions
    (such as planning [\[39\]](#page-11-3) and evaluation [\[40\]](#page-11-4)). However,
    our paper for the first time benchmarks LLMs for MAS, which assesses the capabilities
    of LLMs across 25 function-domain perspectives related to MAS.


    # 3 X-MAS-Bench: Evaluating LLMs for MAS


    X-MAS-Bench is a testbed designed to assess the performance of various LLMs across
    different MAS-related functions and domains. Specifically, we consider 5 representative
    functions of agents in MAS—question-answering [\[15,](#page-9-14) [9\]](#page-9-8),
    revise [\[14,](#page-9-13) [23\]](#page-10-4), aggregation [\[24,](#page-10-5)
    [25\]](#page-10-6), planning [\[26,](#page-10-7) [10\]](#page-9-9), and evaluation
    [\[16,](#page-9-15) [7\]](#page-9-6). Orthogonally, we investigate behaviors in
    5 domains, including mathematics, coding, science, medicine, and finance—spanning
    21 test sets. Each function is assessed under controlled experimental conditions.
    In this section, we demonstrate the details of experimental conditions in Section
    [3.1](#page-2-0) and experimental results in Section [3.2.](#page-3-0)


    ## <span id="page-2-0"></span>3.1 Benchmarking MAS-Related Functions


    To systematically assess LLM capabilities in a multi-agent context, we decompose
    MAS behaviors into five representative and commonly-used agent functions: question-answering,
    revise, aggregation, planning, and evaluation. For each function, we define a
    standardized prompt protocol. In all cases, we carefully control experimental
    conditions such that the only varying factor is the LLM under evaluation. Below
    we detail the designed assessment of each function.


    Question-answering. The question-answering (QA) function measures a LLM''s ability
    to comprehend a question and produce a correct answer in free-text form. This
    function is fundamental to all MAS methods such as LLM-Debate [\[15\]](#page-9-14),
    where each agent first independently answers the query, and to MetaGPT [\[9\]](#page-9-8),
    in which an agent supplies initial draft responses in first stage of the processing.
    In QA assessment, LLM receives a sampled query from any test dataset (e.g., MATH
    [\[27\]](#page-10-8)) as input and returns an answer in free-text format. The
    outputs are compared with ground-truth answers, where the resulted accuracy will
    be the indicator to assess the QA ability.


    Revise. The function of revise assesses an agent''s capacity to revise an initial
    answer, which is potentially flawed, to produce a corrected answer. This function
    commonly exists in workflows requiring iterative refinement, such as the update
    agent in EvoMAC [\[14\]](#page-9-13) and the refiner in Self-Refine [\[23\]](#page-10-4).
    In


    assessment of revise, given a prompt formatted with a sampled query and a corresponding
    answer generated by a pre-defined LLM, the examined LLM is asked to provide a
    final complete answer by reasoning and revising over the provided query and answer.
    In this case, all examined LLMs are provided with the same prompt (i.e., sourced
    from the same query and same pre-defined LLM), therefore ensuring fair comparisons
    among examined LLMs. Similarly, the revised outputs are compared with ground-truth
    answers and the resulting accuracy denotes the revise capability.


    Aggregation. Aggregation refers to the capability of combining multiple candidate
    answers into a coherent, correct, and even improved final answer. It is a key
    mechanism in MAS that leverages multiple answering paths, such as convergent agents
    in MacNet [\[24\]](#page-10-5), and extractive mode of the judge in MAD [\[25\]](#page-10-6).
    To assess aggregation, for each query, we collect candidate responses from a fixed
    set of pre-defined LLMs (set to 3 in this case). The examined LLM is provided
    with the query and these candidate responses in a fixed concatenated format, and
    is asked to synthesize the final answer. Importantly, all candidate responses
    and their order remain identical across all models, ensuring consistency in the
    prompts and allowing for fair comparisons. The aggregated answers are then scored
    against the ground-truth answers using accuracy as the metric.


    Planning. Planning involves decomposing a task into sub-tasks and assigning appropriate
    roles to agents along a workflow to solve the problem collaboratively. This function
    is critical in systems like MACM [\[10\]](#page-9-9) and MapCoder [\[26\]](#page-10-7),
    where a thinker or a planning agent defines the entire agentic workflow. In planning
    assessment, the task of the examined LLM is to provide a suitable plan to the
    sampled query by giving the role descriptions of the agents required to answer
    the query together with the workflow, whose output should follow a pre-defined
    format for subsequent string extraction. Subsequently, the ordered role descriptions
    and the number of roles are extracted. Next, the corresponding number (i.e., role
    number) of candidate LLMs are activated for action according to the role description
    and workflow arrangement. Input prompts and candidate LLMs are kept fixed across
    all examined LLMs. The overall system performance—evaluated by final task accuracy—serves
    as the proxy for planning capability.


    Evaluation. Evaluation measures an agent''s ability to critically assess the quality
    or correctness of other agents'' outputs. This function is used in MAS to filter
    faulty reasoning, select better solutions, determine early stopping, or guide
    further actions—commonly seen in frameworks like AgentVerse [\[16\]](#page-9-15)
    and ChatDev [\[7\]](#page-9-6). In our assessment, each examined LLM is presented
    with a query and an answer generated by a pre-defined LLM. The examined LLM is
    instructed to determine whether the provided answer correctly address the query.
    The candidate answer and evaluation instruction remain constant across LLMs to
    ensure fair comparison. Unlike the evaluation in previous functions, the judgment
    of the examined LLM is compared against ground-truth correctness.


    #### <span id="page-3-0"></span>3.2 Experiments in Evaluating LLMs across Functions
    and Domains


    Following the above definitions of functions, this section assesses the capabilities
    of various LLMs in different functions and domains, aiming at demonstrating the
    landscape of LLMs for MAS. The reported results are expected to demonstrate the
    potential of leveraging heterogeneous LLMs for MAS and facilitate future researchers
    in choosing appropriate LLMs for their MAS.


    Experimental setups. We examine 27 LLMs, covering 20 chatbots (i.e., instructed
    LLMs) and 7 reasoners (i.e., reasoning LLMs). Among the 20 chatbots, we consider
    general chatbots trained by different companies such as Llama [\[21\]](#page-10-2),
    Qwen [\[20\]](#page-10-1), Mistral [\[41,](#page-11-5) [42\]](#page-11-6), and
    domain-specific chatbots including mathematics [\[22\]](#page-10-3), coding [\[43\]](#page-11-7),
    science [\[44,](#page-11-8) [45\]](#page-11-9), medicine [\[46,](#page-11-10)
    [47\]](#page-11-11), and finance [\[46,](#page-11-10) [47\]](#page-11-11). The
    reasoners include LLMs from DeepSeek [\[48\]](#page-11-12), Qwen [\[49\]](#page-11-13),
    and others [\[50\]](#page-11-14). We set each model''s maximum token limit to
    its own capacity, 8192 tokens maxed, with a temperature of 0.5 by default. Specially,
    all LLMs instantiated within the planning workflow are executed with their temperature
    fixed at 0 to guarantee as the planning involves format-following. Our datasets
    cover domains including mathematics [\[27,](#page-10-8) [51,](#page-11-15) [52,](#page-11-16)
    [53,](#page-11-17) [54,](#page-11-18) [55\]](#page-11-19), coding [\[56,](#page-12-0)
    [57,](#page-12-1) [58\]](#page-12-2), science [\[36,](#page-11-0) [59,](#page-12-3)
    [60,](#page-12-4) [61\]](#page-12-5), medicine [\[62,](#page-12-6) [63,](#page-12-7)
    [64\]](#page-12-8), and finance [\[65,](#page-12-9) [66,](#page-12-10) [67\]](#page-12-11),
    where each dataset is randomly sampled up to 500 examples without replacement;
    see more details in Section [C.](#page-13-0)


    No single LLM excels across all scenarios. We plot the size-performance values
    of each evaluated chatbot LLM across 25 function-domain combinations in Figure
    [2](#page-4-0) and report the summary of top-3 LLMs for each combination in Table
    [1;](#page-5-0) see results of all LLMs in Figure [5](#page-17-0) and Table [5.](#page-18-0)
    From these results, we see that (1) No single LLM excels universally across all
    scenarios. A heterogeneous


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-0"></span>Figure 2: Benchmarking chatbot LLMs on 5 MAS-related
    functions and 5 domains. We see that no single LLM excels across all scenarios,
    indicating the potential advantages of employing heterogeneous LLMs in MAS. All
    evaluation results will be open-sourced for future research.


    MAS can capitalize on these differences by assigning scenario-specialized models
    (e.g., Llama3- OpenBioLLM for medicine) to specific agents, maximizing collective
    intelligence. (2) LLMs exhibit varied performance across MAS-related functions,
    reinforcing the value of heterogeneity.


    A single LLM could have significant performance variation across domains and functions.
    Individual LLMs exhibit substantial performance disparities when evaluated across
    different domains and functions, underscoring the limitations of relying on a
    single model in a homogeneous MAS. For instance, in Figure [2,](#page-4-0) Qwen2.5-7B
    performs exceptionally well for revising in coding domain; while dropping to a
    mid-tier level for revising in medicine domain and planning in coding domain.


    There are large performance disparities across LLMs within the same domain and
    function. For the function of revise or the domain of coding, we observe diverse
    behaviors on the examined LLMs, as shown by disperse scatters in Figure [2](#page-4-0)
    (second row and second column).


    Smaller LLMs can outperform larger ones in niche scenarios. While larger models
    like Qwen2.5- 72B-Instruct and Llama-3.1-70B-Instruct often lead, smaller models
    occasionally excel in specific function-domain pairs. For example, in revise-coding
    pair, Qwen2.5-7B-Instruct (79.2) outperforms Qwen2.5-72B-Instruct (77.3); while
    in aggregation-finance and evaluation-finance pairs, Qwen2.5- 14B achieves the
    best performance among all models. This indicates that heterogeneous MAS can incorporate
    smaller, specialized models to optimize performance and computational efficiency,
    reducing reliance on resource-intensive large models while maintaining or improving
    outcomes.


    Low-performing models highlight the risk of homogeneous MAS. Some models consistently
    underperform across domains and functions (e.g., BioMedGPT-LM-7B and SciPhi-Mistral-7B-32k).
    <span id="page-5-0"></span>Table 1: Summary of top-3 LLMs for each function-domain
    combination (chatbot-only scenario). All the assessed LLMs are instructed models
    (e.g., Qwen2.5-32B denotes Qwen2.5-32B-Instruct.). We see that no single LLM excels
    across all scenarios. Meanwhile, the top models are not always those with the
    largest sizes, indicating the potential of improving both performance and cost.


    | Function    | Rank   | Mathematics                                                                |
    Coding                                                                 | Science                                                                       |
    Medicine                                                                   | Finance                                                               |

    |-------------|--------|----------------------------------------------------------------------------|------------------------------------------------------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------|-----------------------------------------------------------------------|

    | QA          | 5      | Qwen2.5-32B (69.2)                                                         |
    Qwen2.5-32B (80.3)                                                     | Qwen2.5-72B
    (60.7)                                                            | Qwen2.5-72B
    (70.4)                                                         | Qwen2.5-72B (74.0)                                                    |

    |             | 5      | Qwen2.5-72B (68.8)                                                         |
    Qwen2.5-72B (77.1)                                                     | Qwen2.5-32B
    (60.0)                                                            | Llama3-OpenBioLLM-70B
    (69.7)                                               | Qwen2.5-32B (71.0)                                                    |

    |             | 5      | Qwen2.5-Math-72B (68.2)                                                    |
    Qwen2.5-Coder-14B (72.3)                                               | Qwen2.5-Math-72B
    (57.1)                                                       | Llama-3.1-70B (69.6)                                                       |
    Qwen2.5-Coder-32B (70.3)                                              |

    | Revise      | 5      | Qwen2.5-Coder-32B (68.4)                                                   |
    Qwen2.5-7B (79.2)                                                      | Qwen2.5-72B
    (60.6)                                                            | Llama-3.1-70B
    (71.0)                                                       | Qwen2.5-72B (70.9)                                                    |

    |             | 5      | Qwen2.5-14B (68.4)                                                         |
    Qwen2.5-Coder-32B (77.7)                                               | Qwen2.5-32B
    (60.2)                                                            | Qwen2.5-72B
    (69.3)                                                         | Llama-3.1-70B
    (70.1)                                                  |

    |             | 5      | Qwen2.5-32B (68.2)                                                         |
    Qwen2.5-72B (77.3)                                                     | Qwen2.5-Math-72B
    (60.2)                                                       | Qwen2.5-Math-72B
    (68.1)                                                    | Qwen2.5-32B (70.1)                                                    |

    | Aggregation | 5      | Llama-3.1-70B (77.4)                                                       |
    Qwen2.5-72B (85.5)                                                     | Llama-3.1-70B
    (67.3)                                                          | Llama3-OpenBioLLM-70B
    (73.4)                                               | Qwen2.5-14B (73.6)                                                    |

    |             | 5      | Qwen2.5-Coder-32B (77.1)                                                   |
    Mistral-Small-3.1-24B (80.2)                                           | Qwen2.5-32B
    (66.7)                                                            | Qwen2.5-7B
    (72.7)                                                          | Mistral-Small-3.1-24B
    (73.2)                                          |

    |             | 5      | Qwen2.5-14B (76.2)                                                         |
    Qwen2.5-Coder-32B (78.4)                                               | Qwen2.5-Coder-32B
    (66.5)                                                      | Llama-3.1-70B (72.7)                                                       |
    Qwen2.5-7B (72.8)                                                     |

    | Planning    | 5<br>5 | Qwen2.5-14B (65.0)<br>5 Mistral-Small-3.1-24B (65.0)<br>Qwen2.5-32B
    (64.7) | Llama-3.1-70B (71.0)<br>Qwen2.5-14B (70.5)<br>Qwen2.5-32B (70.1)       |
    Qwen2.5-Coder-7B (55.5)<br>Qwen2.5-32B (55.3)<br>Mistral-Small-3.1-24B (55.1)
    | Qwen2.5-Coder-14B (65.4)<br>Qwen2.5-7B (65.3)<br>Qwen2.5-32B (65.2)        |
    Qwen2.5-72B (64.7)<br>Qwen2.5-Coder-14B (63.6)<br>Qwen2.5-14B (63.2)  |

    | Evaluation  | 5<br>5 | Qwen2.5-32B (79.0)<br>Qwen2.5-14B (78.1)<br>5 Mistral-Small-3.1-24B
    (77.9) | Qwen2.5-14B (55.4)<br>Qwen2.5-Coder-32B (54.7)<br>Llama-3.1-70B (53.8)
    | Llama-3.1-70B (67.9)<br>Mistral-Small-3.1-24B (66.1)<br>Qwen2.5-32B (65.3)    |
    Llama-3.1-70B (70.5)<br>Qwen2.5-72B (69.4)<br>Mistral-Small-3.1-24B (68.7) | Llama-3.1-70B
    (72.6)<br>Qwen2.5-14B (72.6)<br>Qwen2.5-Math-72B (72.3) |


    A homogeneous MAS relying on such models would be severely limited, whereas a
    heterogeneous setup can mitigate this by integrating appropriate and high-performing
    LLMs.


    Consistent high performers enable robust heterogeneous configurations. Models
    like Qwen-2.5- 32B-Instruct, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct
    frequently rank among the top across domains and functions (e.g., 80.3 in QA-coding,
    79.0 in evaluation-math for Qwen-2.5-32B-Instruct). These models can serve as
    reliable anchors in a heterogeneous MAS, complemented by specialized models for
    niche tasks (e.g., Llama3-OpenBioLLM-70B in medicine), ensuring robust and scalable
    performance improvements.


    # 4 X-MAS-Design: Leveraging Diversity for MAS


    Based on the findings in X-MAS-Bench (Section [3.2\)](#page-3-0), we explore the
    effects of transitioning from homogeneous to heterogeneous LLM-driven MAS (X-MAS-Design).
    We show how a homogeneous MAS is transformed into a heterogeneous MAS in Section
    [4.1.](#page-5-1) We provide experimental results in a chatbot-only scenario (Section
    [4.2\)](#page-6-0) and a mixed chatbot-reasoner scenario (Section [4.3\)](#page-7-0).


    ### <span id="page-5-1"></span>4.1 Transitioning from Homogeneous to Heterogeneous
    LLM-driven MAS


    Transitioning existing MAS methods. As a proof of concept, we aim to show that
    a simple manual modification of the LLM configurations can enhance the performance
    of MAS without any structural improvement. For each target MAS method (e.g., AgentVerse
    [\[16\]](#page-9-15), LLM-Debate [\[15\]](#page-9-14)), we retain the original
    agent roles and interaction topology but substitute the single homogeneous LLM
    with several appropriate LLMs for the agents. Concretely, for each domain-function
    pair in the original design (e.g., the evaluator for coding in AgentVerse), we
    replace the uniform LLM driver with the top performer in the pool of available
    models based on observations from X-MAS-Bench (Section [3.2\)](#page-3-0). By
    preserving the method''s interaction logic and prompt templates, we ensure that
    any performance gains stem solely from LLM heterogeneity rather than modifications
    of workflow. Please note that this modification is efficient as it only takes
    human researchers less than one minute to accomplish and could be automated even
    if we replace humans with LLMs with limited sizes (e.g., 7B [\[20\]](#page-10-1)).


    X-MAS-Proto. In addition to adapting existing MAS methods to heterogeneous ones,
    we implement X-MAS-Proto, a prototype MAS that explicitly implements all five
    functions (QA, revise, aggregation, planning, evaluation) in a single pipeline,
    serving as a proper object for investigation. The system (see the MAS in Figure
    [1\)](#page-1-0) first invokes a planning agent to generate several different
    high-level ideas to the question; next, multiple QA agents concurrently answer
    the query based on its corresponding ideas while one of the answers will be evaluated
    and revised to obtain a potentially better answer; finally, an aggregation agent
    synthesizes across answers to get the final solution. With X-MAS-Proto, we could
    straightforwardly assign appropriate LLMs for different functional agents, aiming
    to clearly demonstrate the benefits of LLM heterogeneity in MAS.


    <span id="page-6-1"></span>Table 2: Transitioning from homogeneous to heterogeneous
    LLM-driven MAS (X-MAS-Design). There are four considered MAS methods and four
    candidate models. X-MAS-Design consistently achieves top performances across 5
    domains (3 are relatively out-of-domain for candidate LLMs).


    | MAS Method      | LLM               | Math  | Coding | Science | Medicine |
    Finance | Average |

    |-----------------|-------------------|-------|--------|---------|----------|---------|---------|

    |                 | Qwen2.5-Math-7B   | 2.40  | 3.21   | 0.40    | 6.00     |
    5.33    | 3.47    |

    |                 | Qwen2.5-Coder-32B | 75.20 | 72.69  | 32.00   | 47.60    |
    64.00   | 58.30   |

    | AgentVerse [16] | Qwen2.5-32B       | 83.20 | 76.31  | 34.00   | 50.40    |
    74.67   | 63.72   |

    |                 | Mistral-3.1-24B   | 66.80 | 62.25  | 31.20   | 40.00    |
    65.33   | 55.12   |

    |                 | X-MAS-Design      | 88.40 | 77.51  | 41.20   | 51.20    |
    72.00   | 66.06   |

    |                 | Qwen2.5-Math-7B   | 79.20 | 40.96  | 29.60   | 35.20    |
    30.67   | 43.13   |

    |                 | Qwen2.5-Coder-32B | 82.40 | 78.71  | 34.40   | 46.80    |
    68.00   | 62.06   |

    | LLM-Debate [15] | Qwen2.5-32B       | 85.20 | 75.50  | 32.80   | 50.80    |
    77.33   | 64.33   |

    |                 | Mistral-3.1-24B   | 76.80 | 66.67  | 33.60   | 52.00    |
    66.67   | 59.15   |

    |                 | X-MAS-Design      | 88.40 | 79.92  | 39.20   | 51.60    |
    77.33   | 67.29   |

    |                 | Qwen2.5-Math-7B   | 0.00  | 13.25  | 15.20   | 13.20    |
    5.33    | 9.40    |

    |                 | Qwen2.5-Coder-32B | 77.20 | 78.31  | 34.80   | 41.60    |
    61.33   | 58.65   |

    | DyLAN [11]      | Qwen2.5-32B       | 81.60 | 74.70  | 38.00   | 46.00    |
    73.33   | 62.73   |

    |                 | Mistral-3.1-24B   | 75.20 | 61.85  | 32.80   | 41.60    |
    72.00   | 56.69   |

    |                 | X-MAS-Design      | 88.80 | 78.71  | 38.80   | 47.20    |
    76.00   | 65.90   |

    |                 | Qwen2.5-Math-7B   | 10.40 | 12.85  | 2.00    | 10.80    |
    5.33    | 8.28    |

    |                 | Qwen2.5-Coder-32B | 82.00 | 76.71  | 33.60   | 46.80    |
    58.67   | 59.56   |

    | X-MAS-Proto     | Qwen2.5-32B       | 82.00 | 69.88  | 31.20   | 45.60    |
    72.00   | 60.14   |

    |                 | Mistral-3.1-24B   | 78.80 | 63.05  | 34.40   | 46.40    |
    72.00   | 58.93   |

    |                 | X-MAS-Design      | 90.40 | 78.71  | 40.00   | 46.80    |
    73.33   | 65.85   |


    #### <span id="page-6-0"></span>4.2 Experiments in Chatbot-only Scenarios


    Experimental setups. We experiment on X-MAS-Proto and three existing MAS methods
    including AgentVerse [\[16\]](#page-9-15), LLM-Debate [\[15\]](#page-9-14), and
    DyLAN [\[11\]](#page-9-10). Considering performances and efficiencies, we select
    four candidate chatbot LLMs: Qwen-2.5-32B, Mistral-Small-3.1-24B, Qwen-2.5-Coder-32B,
    and Qwen-2.5-Math-7B. We test MAS on a *held-out* test splits of MATH-500, MBPP,
    SciBench, PubMedQA, and FinanceBench, covering the examined 5 domains. See model
    selection in Section [D.](#page-13-1)


    Consistent performance gains of X-MAS-Design over homogeneous MAS. Table [2](#page-6-1)
    reports the performance comparisons of the homogeneous and heterogeneous versions
    of four MAS methods, where four LLMs are selected as candidates. The table demonstrates
    that X-MAS-Design, the heterogeneous MAS configuration, consistently outperforms
    all homogeneous configurations on average for four methods. In DyLAN, X-MAS-Design
    achieves an average performance of 65.90, surpassing the best homogeneous model
    (Qwen2.5-32B, 62.73) by 3 points. There are only two outlier cases—LLM-Debate
    in medicine and Agentverse in finance—likely due to the candidate LLMs not including
    specialized models for these particular domains. These results validate the X-MAS-Bench
    findings, which identified optimal model selections for domain-function combinations.
    By leveraging diverse and appropriate LLMs, X-MAS-Design harnesses collective
    intelligence, leading to superior performance without requiring structural changes
    to existing MAS methods.


    Method-agnostic benefits of heterogeneity. The performance improvements of X-MAS-Design
    are consistent across all four MAS methods, despite their differing architectures
    and philosophies. This method-agnostic nature of X-MAS-Design''s improvements
    highlights its versatility, providing strong evidence of our core idea in advocating
    X-MAS.


    X-MAS-Design could leverage the strengths of weak models to offset their weaknesses.
    Homogeneous configurations show significant variability in performance across
    domains, with certain models underperforming in specific areas. For example, Qwen2.5-Math-7B
    performs poorly in most domains (e.g., 2.40 in Math, 0.40 in Science for AgentVerse),
    indicating its limited generalizability. Even stronger models like Qwen2.5-32B
    and Mistral-3.1-24B show weaknesses, such as Mistral-3.1- 24B''s 31.2 in Science
    (AgentVerse) or Qwen2.5-32B''s 31.2 in Science (X-MAS-Proto). In contrast, X-MAS-Design
    consistently achieves balanced performance. That is, X-MAS-Design mitigates the
    limitations of individual LLMs by combining their strengths, indicating the benefits
    of collective intelligence and that our X-MAS-Bench provides helpful guidance
    for the design of X-MAS.


    | MAS Method      | LLM          | Math  | Coding | Science | Medicine | Finance
    | Average |

    |-----------------|--------------|-------|--------|---------|----------|---------|---------|

    |                 | Chatbot      | 20.00 | 75.50  | 37.60   | 47.20    | 72.00   |
    50.46   |

    | AgentVerse [16] | Reasoner     | 0.00  | 11.65  | 5.60    | 44.40    | 21.33   |
    16.60   |

    |                 | X-MAS-Design | 50.00 | 77.91  | 40.00   | 52.40    | 78.67   |
    59.80   |

    |                 | Chatbot      | 16.67 | 74.70  | 35.60   | 49.20    | 73.33   |
    49.90   |

    | LLM-Debate [15] | Reasoner     | 26.67 | 79.12  | 41.60   | 50.00    | 72.00   |
    53.88   |

    |                 | X-MAS-Design | 56.67 | 81.12  | 44.40   | 54.40    | 80.00   |
    63.32   |

    |                 | Chatbot      | 20.00 | 74.70  | 34.00   | 44.00    | 70.76   |
    48.67   |

    | DyLAN [11]      | Reasoner     | 40.00 | 76.31  | 42.40   | 45.60    | 68.00   |
    54.46   |

    |                 | X-MAS-Design | 63.33 | 80.32  | 42.80   | 46.80    | 76.00   |
    61.85   |

    |                 | Chatbot      | 23.33 | 72.69  | 34.80   | 44.80    | 68.00   |
    48.72   |

    | X-MAS-Proto     | Reasoner     | 0.00  | 71.49  | 23.20   | 49.20    | 56.00   |
    39.98   |

    |                 | X-MAS-Design | 70.00 | 79.12  | 47.20   | 52.80    | 76.00   |
    65.02   |


    <span id="page-7-1"></span>Table 3: Effectiveness of X-MAS-Design in mixing chatbots
    and reasoners. While reasoner-based homogeneous MAS performs worse than chatbot-based
    homogeneous MAS, incorporating chatbots and reasoners into heterogeneous MAS contributing
    to large performance improvement.


    #### <span id="page-7-0"></span>4.3 Experiments in Mixed Chatbot-Reasoner Scenarios


    Experimental setups. The examined MAS methods follow that in Section [4.2.](#page-6-0)
    As chatbots and reasoners exhibit different behaviors, we consider two candidate
    LLMs: Qwen-2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B. These methods are
    tested on AIME-2024 and *held-out* splits of MBPP, SciBench, PubMedQA and FinanceBench,
    covering the five examined domains. We also test the methods on *entirely new*
    (compared to X-MAS-Bench) test sets: AIME-2025 [\[68\]](#page-12-12) (the latest
    AIME math competition) and MATH-MAS [\[69\]](#page-12-13) (multi-step). See model
    selection in Section [E.](#page-16-0)


    Mixing chatbots and reasoners in X-MAS-Design achieves superior performance across
    domains and MAS methods. In Table [3,](#page-7-1) we explore the potential of
    mixing chatbot and reasoner LLMs in X-MAS-Design. From the table, we see that
    (1) X-MAS-Design, combining chatbot and reasoner agents powered by heterogeneous
    LLMs, consistently outperforms both standalone chatbot and reasoner configurations
    across all five domains. (2) Standalone chatbot and reasoner configurations show
    complementary strengths and weaknesses. The heterogeneous X-MAS-Design mitigates
    individual role limitations by combining chatbot and reasoner strengths, as guided
    by X-MAS-Bench''s 1.7 million evaluations. This synergy enables robust performance
    across diverse domains.


    Mixing chatbots and reasoners leads to dramatic improvements in math domain (AIME).
    We additionally evaluate homogeneous and heterogeneous MAS on two entirely new
    benchmarks: AIME-2025 and MATH-MAS in Table [4.](#page-7-2) From Table [3](#page-7-1)
    and [4,](#page-7-2) we see that in math domain (i.e., AIME-2024, AIME-2025, MATH-MAS),
    X-MAS-Design contributes to substantial performance boosts. Notably, for X-MAS-Proto,
    X-MAS-Design scores 70% in AIME-2024, a 46.67%-point gain over


    <span id="page-7-2"></span>


    | Table 4: | Examination on entirely new           |  |  |

    |----------|---------------------------------------|--|--|

    |          | benchmarks. X-MAS-Design achieves sig |  |  |

    |          | nificantly best performance.          |  |  |


    | Benchmark    |       | AIME-25 MATH-M |

    |--------------|-------|----------------|

    | Chatbot      | 13.33 | 14.18          |

    | Reasoner     | 10.00 | 5.97           |

    | X-MAS-Design | 46.67 | 48.13          |


    the second-best homogeneous MAS, indicating the potential of X-MAS in reasoning-intensive
    tasks. Meanwhile, X-MAS-Design outperforms the second-best chatbot-based homogeneous
    MAS by 33% and 34% on the challenging AIME-2025 and MATH-MAS, respectively, indicating
    the generalization of our core idea. In the era where reasoning models prevail,
    our experiments point out a potential direction: further scaling compute with
    X-MAS that mixes chatbots and reasoners.


    #### 4.4 Ablation Study


    Increasing the number of candidate models enhances the performance of X-MAS-Design.
    Following the setup in Section [4.2,](#page-6-0) we conduct experiments with X-MAS-Proto
    on three domains (math, coding, and science) by tuning the number of candidate
    models. We use the full split for larger sample numbers. From Figure [3,](#page-8-0)
    we observe that (1) X-MAS-Design consistently outperforms homogeneous MAS (i.e.,
    1 candidate model), indicating the benefits of X-MAS. (2) With the number


    ![](_page_8_Figure_0.jpeg)


    <span id="page-8-0"></span>Figure 3: Diversity for the win. Experiments are conducted
    with X-MAS-Proto on three domains. Increasing the number of candidate models generally
    enhances the system performance, strongly indicating the benefits of LLM heterogeneity
    for MAS.


    of candidate models increases, we can generally observe an increase of performance.
    One exception is in the science domain, which can be attributed that the added
    model from 2 to 3 is not closely related to science. This curve strongly indicates
    the benefits of including diverse LLMs in MAS.


    Arbitrary model selection could lead to suboptimal performance: X-MAS-Bench offers
    critical observations to guide the design of X-MAS. To verify the effectiveness
    of the observations from X-MAS-Bench, we compare X-MAS with LLM selection guided
    by X-MAS-Bench to X-MAS with arbitrary selection. We follow the setup in Section
    [2,](#page-6-1) where we experiment on X-MAS-Proto on MATH-500. We arbitrarily
    determine 5 reasonable sets of configurations for designing X-MAS (see details
    in Section [D.2\)](#page-15-0), denoted by blue bars in Figure [4.](#page-8-1)
    Homogeneous MAS driven by three different LLMs is denoted by red bars. From the
    figure, we see that (1) X-MAS-Design, which is designed based on observations
    from X-MAS-Bench, significantly performs the best. (2) Among those 5 X-MAS


    ![](_page_8_Figure_4.jpeg)


    <span id="page-8-1"></span>Figure 4: Comparing X-MAS with LLM selection guided
    by X-MAS-Bench and arbitrary selection. X-MAS-Design, which is guided by X-MAS-Bench,
    significantly performs the best.


    without X-MAS-Bench''s guidance, 3 of them achieve slightly better performance
    than homogeneous MAS, while 1 performs slightly worse than the best homogeneous
    MAS and 1 even performs significantly worst (only 24.8%). This indicates that
    appropriate LLM selection is critical for ensuring the performance of X-MAS and
    that results in X-MAS-Bench can provide valuable insights.


    # 5 Conclusions


    This paper advocates building LLM-based MAS with heterogeneous LLMs. We introduce
    X-MAS-Bench, a comprehensive testbed designed to assess the capabilities of various
    LLMs in supporting for MAS. We provide a systematic empirical study, which assesses
    27 LLMs (both chatbots and reasoners, both genralists and specialists) across
    5 representative MAS-related functions and 5 common domains, highlighting the
    potential of employing heterogeneous LLMs in MAS. Based on the insights from X-MAS-Bench,
    we examine the effects of transitioning from homogeneous to heterogeneous LLMdriven
    MAS (X-MAS-Design). Our experiments operating on 4 MAS methods demonstrate that
    the performance of MAS can be significantly and consistently improved by leveraging
    heterogeneous MAS without any structural re-design, strongly supporting our advocacy.
    See limitations in Section [A.](#page-13-2)


    Our work highlights an intriguing direction that leverages the collective intelligence
    of heterogeneous LLMs to achieve higher-level intelligence without additional
    training. Looking ahead, future research could explore areas such as automated
    or dynamic model selection, the impact of further scaling model candidates, optimizing
    the synergy between LLM selection and MAS, achieving strong performance with weaker
    agents, and training agents specifically suited for MAS.


    # References


    - <span id="page-9-0"></span>[1] OpenAI. Gpt-4 technical report. *arXiv preprint
    arXiv:2303.08774*, 2023.

    - <span id="page-9-1"></span>[2] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan
    Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan,
    Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context. *arXiv preprint arXiv:2403.05530*, 2024.

    - <span id="page-9-2"></span>[3] An Yang, Baosong Yang, Beichen Zhang, Binyuan
    Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et
    al. Qwen2. 5 technical report. *arXiv preprint arXiv:2412.15115*, 2024.

    - <span id="page-9-3"></span>[4] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao
    Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren''s
    song in the ai ocean: a survey on hallucination in large language models. *arXiv
    preprint arXiv:2309.01219*, 2023.

    - <span id="page-9-4"></span>[5] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
    Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
    Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation. In *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pages 12076–12100, 2023.

    - <span id="page-9-5"></span>[6] Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei
    Yin, Jing Shao, and Siheng Chen. MAS-GPT: Training LLMs to build LLM-based multi-agent
    systems. In *Workshop on Reasoning and Planning for Large Language Models*, 2025.

    - <span id="page-9-6"></span>[7] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen,
    Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev:
    Communicative agents for software development. In *Proceedings of the 62nd Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 15174–15186, 2024.

    - <span id="page-9-7"></span>[8] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin,
    Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran
    Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. *arXiv preprint arXiv:2502.18864*,
    2025.

    - <span id="page-9-8"></span>[9] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu
    Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,
    Zijuan Lin, et al. Metagpt: Meta programming for a multi-agent collaborative framework.
    In *The Twelfth International Conference on Learning Representations*, 2024.

    - <span id="page-9-9"></span>[10] Bin Lei, Yi Zhang, Shan Zuo, Ali Payani, and
    Caiwen Ding. Macm: Utilizing a multi-agent system for condition mining in solving
    complex mathematical problems. In *The Thirty-eighth Annual Conference on Neural
    Information Processing Systems*, 2024.

    - <span id="page-9-10"></span>[11] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu,
    and Diyi Yang. A dynamic llm-powered agent network for task-oriented agent collaboration.
    In *First Conference on Language Modeling*, 2024.

    - <span id="page-9-11"></span>[12] Daniil A Boiko, Robert MacKnight, Ben Kline,
    and Gabe Gomes. Autonomous chemical research with large language models. *Nature*,
    624(7992):570–578, 2023.

    - <span id="page-9-12"></span>[13] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob
    Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated
    open-ended scientific discovery. *arXiv preprint arXiv:2408.06292*, 2024.

    - <span id="page-9-13"></span>[14] Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui
    Liu, Zijie Yu, Yuchen Hou, Shuo Tang, and Siheng Chen. Self-evolving multi-agent
    networks for software development. In *The Thirteenth International Conference
    on Learning Representations*, 2025.

    - <span id="page-9-14"></span>[15] Yilun Du, Shuang Li, Antonio Torralba, Joshua
    B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language
    models through multiagent debate. In *Forty-first International Conference on
    Machine Learning*, 2024.

    - <span id="page-9-15"></span>[16] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng
    Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian,
    et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent
    behaviors. In *The Twelfth International Conference on Learning Representations*,
    2024.

    - <span id="page-9-16"></span>[17] Lu Hong and Scott E Page. Groups of diverse
    problem solvers can outperform groups of high-ability problem solvers. *Proceedings
    of the National Academy of Sciences*, 101(46):16385–16389, 2004.

    - <span id="page-9-17"></span>[18] Maria Kozhevnikov, Carol Evans, and Stephen
    M Kosslyn. Cognitive style as environmentally sensitive individual differences
    in cognition: A modern synthesis and applications in education, business, and
    management. *Psychological science in the public interest*, 15(1):3–33, 2014.

    - <span id="page-10-0"></span>[19] Ishani Aggarwal, Anita Williams Woolley, Christopher
    F Chabris, and Thomas W Malone. Cognitive diversity, collective intelligence,
    and learning in teams. *Proceedings of Collective Intelligence*, 1(3.1):3–3, 2015.

    - <span id="page-10-1"></span>[20] An Yang, Baosong Yang, Beichen Zhang, Binyuan
    Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan
    Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren
    Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,
    Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,
    Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong
    Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. *arXiv preprint
    arXiv:2412.15115*, 2024.

    - <span id="page-10-2"></span>[21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
    Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy
    Yang, Angela Fan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*,
    2024.

    - <span id="page-10-3"></span>[22] An Yang, Beichen Zhang, Binyuan Hui, Bofei
    Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang
    Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via
    self-improvement. *arXiv preprint arXiv:2409.12122*, 2024.

    - <span id="page-10-4"></span>[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
    Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye,
    Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. *Advances
    in Neural Information Processing Systems*, 36, 2024.

    - <span id="page-10-5"></span>[24] Chen Qian, Zihao Xie, YiFei Wang, Wei Liu,
    Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan
    Liu, and Maosong Sun. Scaling large language model-based multi-agent collaboration.
    In *The Thirteenth International Conference on Learning Representations*, 2025.

    - <span id="page-10-6"></span>[25] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing
    Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging
    divergent thinking in large language models through multi-agent debate. In Yaser
    Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, *Proceedings of the 2024
    Conference on Empirical Methods in Natural Language Processing*, pages 17889–17904,
    Miami, Florida, USA, November 2024. Association for Computational Linguistics.

    - <span id="page-10-7"></span>[26] Md Ashraful Islam, Mohammed Eunus Ali, and
    Md Rizwan Parvez. Mapcoder: Multi-agent code generation for competitive problem
    solving. In *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 4912–4944, 2024.

    - <span id="page-10-8"></span>[27] Dan Hendrycks, Collin Burns, Saurav Kadavath,
    Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring
    mathematical problem solving with the math dataset. *NeurIPS*, 2021.

    - <span id="page-10-9"></span>[28] Shengran Hu, Cong Lu, and Jeff Clune. Automated
    design of agentic systems. In *The Thirteenth International Conference on Learning
    Representations*, 2025.

    - <span id="page-10-10"></span>[29] Lingjiao Chen, Matei Zaharia, and James Zou.
    Less is more: Using multiple LLMs for applications with lower costs. In *Workshop
    on Efficient Systems for Foundation Models @ ICML2023*, 2023.

    - <span id="page-10-11"></span>[30] Saranya Venkatraman, Nafis Irtiza Tripto,
    and Dongwon Lee. Collabstory: Multi-llm collaborative story generation and authorship
    analysis. *arXiv preprint arXiv:2406.12665*, 2024.

    - <span id="page-10-12"></span>[31] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin.
    Llm-blender: Ensembling large language models with pairwise ranking and generative
    fusion. In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 14165–14178, 2023.

    - <span id="page-10-13"></span>[32] Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce
    Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities.
    In *The Thirteenth International Conference on Learning Representations*, 2025.

    - <span id="page-10-14"></span>[33] Justin Chen, Swarnadeep Saha, and Mohit Bansal.
    Reconcile: Round-table conference improves reasoning via consensus among diverse
    llms. In *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 7066–7085, 2024.

    - <span id="page-10-15"></span>[34] Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng
    Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. Masrouter: Learning to route llms for
    multi-agent systems. *arXiv preprint arXiv:2502.11133*, 2025.

    - <span id="page-10-16"></span>[35] Carlos E Jimenez, John Yang, Alexander Wettig,
    Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language
    models resolve real-world github issues? In *The Twelfth International Conference
    on Learning Representations*, 2024.

    - <span id="page-11-0"></span>[36] David Rein, Betty Li Hou, Asa Cooper Stickland,
    Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel
    R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. *arXiv preprint arXiv:2311.12022*,
    2023.

    - <span id="page-11-1"></span>[37] OpenAI. Introducing healthbench. <https://openai.com/index/healthbench/>,
    2025. Accessed: 2025-05-15.

    - <span id="page-11-2"></span>[38] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao
    Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: a large language
    model, instruction data and evaluation benchmark for finance. In *Proceedings
    of the 37th International Conference on Neural Information Processing Systems*,
    pages 33469–33484, 2023.

    - <span id="page-11-3"></span>[39] Karthik Valmeekam, Matthew Marquez, Alberto
    Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark
    for evaluating large language models on planning and reasoning about change. *Advances
    in Neural Information Processing Systems*, 36:38975–38987, 2023.

    - <span id="page-11-4"></span>[40] Sijun Tan, Siyuan Zhuang, Kyle Montgomery,
    William Yuan Tang, Alejandro Cuadron, Chenguang Wang, Raluca Popa, and Ion Stoica.
    Judgebench: A benchmark for evaluating LLM-based judges. In *The Thirteenth International
    Conference on Learning Representations*, 2025.

    - <span id="page-11-5"></span>[41] Mistral. Mistral-7b-instruct-v0.3. [https://huggingface.co/mistralai/](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
    [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3),
    2024. Accessed: 2025-05-15.

    - <span id="page-11-6"></span>[42] Mistral. Mistral-small-3.1-24b-instruct-2503.
    [https://huggingface.co/mistralai/](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503)
    [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503),
    2025. Accessed: 2025-05-15.

    - <span id="page-11-7"></span>[43] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang,
    Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al.
    Qwen2. 5-coder technical report. *arXiv preprint arXiv:2409.12186*, 2024.

    - <span id="page-11-8"></span>[44] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao
    Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, et al. Developing chemdfm
    as a large language foundation model for chemistry. *Cell Reports Physical Science*,
    6(4), 2025.

    - <span id="page-11-9"></span>[45] SciPhi. Sciphi-mistral-7b-32k. <https://huggingface.co/SciPhi/SciPhi-Mistral-7B-32k>,
    2023. Accessed: 2025-05-15.

    - <span id="page-11-10"></span>[46] Duxiaoman-DI. Llama3-xuanyuan3-70b-chat. [https://huggingface.co/Duxiaoman-DI/](https://huggingface.co/Duxiaoman-DI/Llama3-XuanYuan3-70B-Chat)
    [Llama3-XuanYuan3-70B-Chat](https://huggingface.co/Duxiaoman-DI/Llama3-XuanYuan3-70B-Chat),
    2024. Accessed: 2025-05-15.

    - <span id="page-11-11"></span>[47] SYSU-MUCFC-FinTech-Research-Center. Zhilu-2-8b-instruct.
    [https://huggingface.co/](https://huggingface.co/SYSU-MUCFC-FinTech-Research-Center/ZhiLu-2-8B-Instruct)
    [SYSU-MUCFC-FinTech-Research-Center/ZhiLu-2-8B-Instruct](https://huggingface.co/SYSU-MUCFC-FinTech-Research-Center/ZhiLu-2-8B-Instruct),
    2024. Accessed: 2025-05-15.

    - <span id="page-11-12"></span>[48] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
    Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.
    Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
    *arXiv preprint arXiv:2501.12948*, 2025.

    - <span id="page-11-13"></span>[49] Qwen Team. Qwq-32b: Embracing the power of
    reinforcement learning, March 2025.

    - <span id="page-11-14"></span>[50] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai,
    January 2025.

    - <span id="page-11-15"></span>[51] Wang Ling, Dani Yogatama, Chris Dyer, and
    Phil Blunsom. Program induction by rationale generation: Learning to solve and
    explain algebraic word problems. In *Proceedings of the 55th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    158–167, 2017.

    - <span id="page-11-16"></span>[52] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
    Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided
    language models. In *International Conference on Machine Learning*, pages 10764–10799.
    PMLR, 2023.

    - <span id="page-11-17"></span>[53] Maxwell-Jia. Aime-2024. [https://huggingface.co/datasets/Maxwell-Jia/AIME\\_2024](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024),
    2024. Accessed: 2025-05-15.

    - <span id="page-11-18"></span>[54] Dan Hendrycks, Collin Burns, Steven Basart,
    Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*,
    2021.

    - <span id="page-11-19"></span>[55] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng
    Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan
    Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu
    Chen. MMLU-pro: A more robust and challenging multi-task language understanding
    benchmark. In *The Thirty-eight Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track*, 2024.

    - <span id="page-12-0"></span>[56] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
    Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,
    Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained
    on code. *arXiv preprint arXiv:2107.03374*, 2021.

    - <span id="page-12-1"></span>[57] Jacob Austin, Augustus Odena, Maxwell Nye,
    Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael
    Terry, Quoc Le, et al. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*, 2021.

    - <span id="page-12-2"></span>[58] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang,
    and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous
    evaluation of large language models for code generation. In *Thirty-seventh Conference
    on Neural Information Processing Systems*, 2023.

    - <span id="page-12-3"></span>[59] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu,
    Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and
    Wei Wang. Scibench: Evaluating college-level scientific problemsolving abilities
    of large language models. In *Forty-first International Conference on Machine
    Learning*, 2024.

    - <span id="page-12-4"></span>[60] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma,
    Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: A multi-level large language
    model evaluation benchmark for scientific research. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 38, pages 19053–19061, 2024.

    - <span id="page-12-5"></span>[61] Kehua Feng, Keyan Ding, Weijie Wang, Xiang
    Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen.
    Sciknoweval: Evaluating multi-level scientific knowledge of large language models.
    *arXiv preprint arXiv:2406.09098*, 2024.

    - <span id="page-12-6"></span>[62] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan
    Sankarasubbu. Medmcqa: A large-scale multisubject multi-choice dataset for medical
    domain question answering. In *Conference on health, inference, and learning*,
    pages 248–260. PMLR, 2022.

    - <span id="page-12-7"></span>[63] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung
    Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a
    large-scale open domain question answering dataset from medical exams. *Applied
    Sciences*, 11(14):6421, 2021.

    - <span id="page-12-8"></span>[64] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William
    Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 2567–2577, 2019.

    - <span id="page-12-9"></span>[65] Pranab Islam, Anand Kannappan, Douwe Kiela,
    Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: A new benchmark
    for financial question answering. *arXiv preprint arXiv:2311.11944*, 2023.

    - <span id="page-12-10"></span>[66] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena
    Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan
    R Routledge, et al. Finqa: A dataset of numerical reasoning over financial data.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 3697–3711, 2021.

    - <span id="page-12-11"></span>[67] P. Malo, A. Sinha, P. Korhonen, J. Wallenius,
    and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic
    texts. *Journal of the Association for Information Science and Technology*, 65,
    2014.

    - <span id="page-12-12"></span>[68] OpenCompass. Aime2025. <https://huggingface.co/datasets/opencompass/AIME2025>,
    2024. Accessed: 2025-05-15.

    - <span id="page-12-13"></span>[69] Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei
    Yin, and Lei Bai. Reso: A reward-driven self-organizing llm-based multi-agent
    system for reasoning tasks. *arXiv preprint arXiv:2503.02390*, 2025.

    - <span id="page-12-14"></span>[70] Mistral. Codestral-22b-v0.1. <https://huggingface.co/mistralai/Codestral-22B-v0.1>,
    2024. Accessed: 2025-05-15.

    - <span id="page-12-15"></span>[71] Malaikannan Sankarasubbu Ankit Pal. Openbiollms:
    Advancing open-source large language models for healthcare and life sciences.
    <https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B>, 2024.

    - <span id="page-12-16"></span>[72] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang,
    Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained
    transformer for biomedicine. *arXiv preprint arXiv:2308.09442*, 2023.


    # <span id="page-13-2"></span>A Limitations


    Despite being the most comprehensive evaluation of LLMs for MAS, there are still
    LLMs that have not been included yet. When transitioning from homogeneous to heterogeneous
    MAS, we currently rely on manual modification as a proof of concept. Despite that
    the modification is quite simple, it is worthwhile to explore automated solutions.


    # B Broader Impacts


    This paper introduces X-MAS-Bench, aiming at assessing the capabilities of LLMs
    when being incorporated in MAS. The assessed results and the corresponding findings
    could serve the community, facilitating appropriate model selections during the
    design of MAS. Our X-MAS-Design aims to transition an existing homogeneous MAS
    to a heterogeneous one. Similar concept could be extended to many existing MAS,
    making the overall system perform better.


    The potential negative impacts of our approach mirror those associated with LLMs
    including risks of misuse. However, these issues are intrinsic to LLM usage in
    general.


    # <span id="page-13-0"></span>C Experimental Setups


    We examine 27 LLMs, covering 20 chatbots (i.e., instructed LLMs) and 7 reasoners
    (i.e., reasoning LLMs). Among the 20 chatbots, we consider general chatbots trained
    by different companies: Llama (Llama-3.1-8/70B-Instruct [\[21\]](#page-10-2)),
    Qwen (Qwen2.5-7/14/32/72B-Instruct [\[20\]](#page-10-1), Mistral (Mistral-7B-Instruct-v0.3
    [\[41\]](#page-11-5), Mistral-Small-3.1-24B-Instruct-2503 [\[42\]](#page-11-6));
    we also include domain-specific chatbots including mathematics (Qwen2.5-Math-7/72B-Instruct
    [\[22\]](#page-10-3)), coding (Qwen2.5-Coder-7/14/32B-Instruct [\[43\]](#page-11-7),
    Codestral-22B-v0.1 [\[70\]](#page-12-14)), science (ChemDFM-v1.5-8B [\[44\]](#page-11-8),
    SciPhi-Mistral-7B-32k [\[45\]](#page-11-9)), medicine (Llama3-OpenBioLLM-70B [\[71\]](#page-12-15)
    and BioMedGPT-LM-7B [\[72\]](#page-12-16)), and finance (Llama3-XuanYuan3-70B-Chat
    [\[46\]](#page-11-10) and ZhiLu-2-8B-Instruct [\[47\]](#page-11-11)) The reasoners
    include LLMs from DeepSeek (DeepSeek-R1-Distill-Llama-8/70B and DeepSeek-R1-Distill-Qwen-7/14/32B
    [\[48\]](#page-11-12)), Qwen (QwQ-32B [\[49\]](#page-11-13)) , other (OpenThinker-32B
    [\[50\]](#page-11-14)) LLMs. We set each model''s maximum token limit to its own
    capacity, 8192 tokens maxed, with a temperature of 0.5 by default. Specially,
    all LLMs instantiated within the planning workflow are executed with their temperature
    fixed at 0 to guarantee as the planning involves format-following. Our datasets
    cover domains including mathematics (AIME-2024 [\[53\]](#page-11-17), AQUA-RAT
    [\[51\]](#page-11-15), GSM-Hard [\[52\]](#page-11-16), MATH [\[27\]](#page-10-8),
    MMLU-Math [\[54\]](#page-11-18), MMLU-Pro-Math [\[55\]](#page-11-19)), coding
    (HumanEval [\[56\]](#page-12-0), HumanEval-Plus [\[58\]](#page-12-2), MBPP [\[57\]](#page-12-1),
    MBPP-Plus, MMLU-Coding, MMLU-Pro-coding), science (GPQA-Main [\[36\]](#page-11-0),
    GPQA-Diamond, SciBench [\[59\]](#page-12-3), SciEval [\[60\]](#page-12-4), SciKnowEval
    [\[61\]](#page-12-5), MMLU-Sci, MMLU-Pro-Sci), medicine (MedM-CQA [\[62\]](#page-12-6),
    MedQA [\[63\]](#page-12-7), PubMedQA [\[64\]](#page-12-8), MMLU-Med, MMLU-Pro-Med),
    and finance (FinanceBench [\[65\]](#page-12-9), FinQA [\[66\]](#page-12-10), FPB
    [\[67\]](#page-12-11), MMLU-Finan, MMLU-Pro-Finan), where each dataset is randomly
    sampled up to 500 examples without replacement (except for SciKnowEval, from which
    we draw 800 instances to ensure sufficient coverage of its specialized tasks).


    # <span id="page-13-1"></span>D Experiments on X-MAS in Chatbot-Only Scenarios


    # D.1 Experiments setups of X-MAS-Design in Chatbot-Only Scenarios


    The available LLMs are Qwen-2.5-32B-Instruct, Qwen-2.5-Coder-32B-Instruct, Qwen-2.5-Math-7B-Instruct
    and Mistral-Small-3.1-24B-Instruct-2503.


    # D.1.1 Agentverse


    Mathematics. The role assigner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-Coder-32B-Instruct
    and Qwen-2.5-Math-7B-Instruct, the critic is Qwen-2.5-Coder-32B-Instruct, the
    evaluator is Qwen-2.5-32B-Instruct.


    Coding. The role assigner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-32B-Instruct,
    the critic is Qwen-2.5-Coder-32B-Instruct, the evaluator is Qwen-2.5-Coder-32B-Instruct.


    Science. The role assigner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-32B-Instruct,
    the critic is Qwen-2.5-32B-Instruct, the evaluator is Mistral-Small-3.1-24B-Instruct-2503.


    Medicine. The role assigner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-32B-Instruct,
    the critic is Mistral-Small-3.1-24B-Instruct-2503, the evaluator is Mistral-Small-3.1-24B-Instruct-2503.


    Finance. The role assigner is Mistral-Small-3.1-24B-Instruct-2503, the solver
    is Qwen-2.5-Coder-32B-Instruct, the critic is Qwen-2.5-32B-Instruct, the evaluator
    is Mistral-Small-3.1-24B-Instruct-2503.


    #### D.1.2 LLM-Debate


    Mathematics. The debate agent is Qwen-2.5-Coder-32B-Instruct and Qwen-2.5-Math-7B-Instruct,
    the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Coding. The debate agent is Qwen-2.5-Coder-32B-Instruct, the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Science. The debate agent is Qwen-2.5-32B-Instruct, the aggregator is Qwen-2.5-32B-Instruct.


    Medicine. The debate agent is Qwen-2.5-32B-Instruct, the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Finance. The debate agent is Qwen-2.5-Coder-32B-Instruct, the aggregator is Mistral-Small-3.1-
    24B-Instruct-2503.


    #### D.1.3 DyLAN


    Mathematics. The node agent is Qwen-2.5-Coder-32B-Instruct and Qwen-2.5-Math-7B-Instruct,
    the ranker is Mistral-Small-3.1-24B-Instruct-2503.


    Coding. The node agent is Qwen-2.5-Coder-32B-Instruct, the ranker is Mistral-Small-3.1-24B-Instruct-2503.


    Science. The node agent is Qwen-2.5-32B-Instruct, the ranker is Qwen-2.5-32B-Instruct.


    Medicine. The node agent is Mistral-Small-3.1-24B-Instruct-2503, the ranker is
    Mistral-Small-3.1- 24B-Instruct-2503.


    Finance. The node agent is Qwen-2.5-Coder-32B-Instruct, the ranker is Mistral-Small-3.1-24B-Instruct-2503.


    #### D.1.4 X-MAS-Proto


    Mathematics. The planner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-Coder-32B-Instruct
    and Qwen-2.5-Math-7B-Instruct, the reviser is Qwen-2.5-Coder-32B-Instruct, the
    evaluator is Qwen-2.5-32B-Instruct, the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Coding. The planner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-Coder-32B-Instruct,
    the reviser is Qwen-2.5-Coder-32B-Instruct, the evaluator is Qwen-2.5-32B-Instruct,
    the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Science. The planner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-32B-Instruct,
    the reviser is Qwen-2.5-32B-Instruct, the evaluator is Mistral-Small-3.1-24B-Instruct-2503,
    the aggregator is Qwen-2.5-32B-Instruct.


    Medicine. The planner is Qwen-2.5-32B-Instruct, the solver is Qwen-2.5-32B-Instruct,
    the reviser is Mistral-Small-3.1-24B-Instruct-2503, the evaluator is Mistral-Small-3.1-24B-Instruct-2503,
    the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    Finance. The planner is Mistral-Small-3.1-24B-Instruct-2503, the solver is Qwen-2.5-Coder-32B-Instruct,
    the reviser is Qwen-2.5-32B-Instruct, the evaluator is Mistral-Small-3.1-24B-Instruct-2503,
    the aggregator is Mistral-Small-3.1-24B-Instruct-2503.


    #### <span id="page-15-0"></span>D.2 Experimental Setups of X-MAS with Non-X-MAS-Bench-Guided
    Model Selections


    We arbitrarily determine five reasonable manually designed model configurations
    to examine the robustness and performance sensitivity of the X-MAS-Design under
    diverse agent choices. These configurations are constructed without referring
    to the X-MAS-Bench, and are denoted as X-MAS1 through X-MAS5. Each configuration
    includes distinct combinations of planner, solver, evaluator, reviser, and aggregator
    roles. For comparison, we also include the original X-MAS-Design configuration
    guided by X-MAS-Bench selection.


    The X-MAS-Bench-guided configuration, referred to as X-MAS-Design in chatbot-only
    scenarios, adopts the following models for each agent role:


    - Planner: Qwen-2.5-32B-Instruct

    - Solver: Qwen-2.5-Coder-32B-Instruct

    - Evaluator: Qwen-2.5-32B-Instruct

    - Reviser: Qwen-2.5-Coder-32B-Instruct

    - Aggregator: Mistral-Small-3.1-24B-Instruct-2503


    This configuration reflects a well-balanced assignment with domain-specialized
    solvers (e.g., math) and stronger general-purpose planning and evaluation agents.


    In contrast, the five alternative configurations (X-MAS1 to X-MAS5) are constructed
    based on general instruction-tuned LLMs without prior empirical optimization.
    These setups are:


    #### X-MAS1


    - Planner: Mistral-Small-3.1-24B-Instruct-2503

    - Solver: Qwen-2.5-Math-7B-Instruct

    - Evaluator: Qwen-2.5-Coder-32B-Instruct

    - Reviser: Qwen-2.5-Math-7B-Instruct

    - Aggregator: Qwen-2.5-32B-Instruct


    #### X-MAS2


    - Planner: Mistral-Small-3.1-24B-Instruct-2503

    - Solver: Qwen-2.5-Coder-32B-Instruct

    - Evaluator: Qwen-2.5-Math-7B-Instruct

    - Reviser: Qwen-2.5-Coder-32B-Instruct

    - Aggregator: Qwen-2.5-32B-Instruct


    # X-MAS3


    - Planner: Qwen-2.5-Math-7B-Instruct

    - Solver: Mistral-Small-3.1-24B-Instruct-2503

    - Evaluator: Qwen-2.5-32B-Instruct

    - Reviser: Mistral-Small-3.1-24B-Instruct-2503

    - Aggregator: Qwen-2.5-Coder-32B-Instruct


    # X-MAS4


    - Planner: Qwen-2.5-Coder-32B-Instruct

    - Solver: Qwen-2.5-32B-Instruct

    - Evaluator: Qwen-2.5-Math-7B-Instruct

    - Reviser: Qwen-2.5-32B-Instruct

    - Aggregator: Mistral-Small-3.1-24B-Instruct-2503


    #### X-MAS5


    - Planner: Qwen-2.5-32B-Instruct

    - Solver: Qwen-2.5-Coder-32B-Instruct

    - Evaluator: Mistral-Small-3.1-24B-Instruct-2503

    - Reviser: Qwen-2.5-Coder-32B-Instruct

    - Aggregator: Qwen-2.5-Math-7B-Instruct


    All configurations are evaluated on the MATH-500 subset following the X-MAS-Proto
    scheme. The goal of this analysis is to understand the effect of heterogeneous
    agent assignments on final multiagent performance, as well as to validate the
    necessity and advantages of X-MAS-Bench-guided agent selection. These baselines
    also serve to demonstrate the variance among manually configured pipelines in
    the absence of systematic design guidance.


    # <span id="page-16-0"></span>E Experiments on X-MAS in Mixed Chatbot-Reasoner
    Scenarios


    ### E.1 Model Selections


    The available LLMs are Qwen-2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B.


    #### E.1.1 Agentverse


    Mathematics. The role assigner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the critic is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B.


    Coding. The role assigner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the critic is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B.


    Science. The role assigner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the critic is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B.


    Medicine. The role assigner is Qwen-2.5-72B-Instruct, the solver is Qwen-2.5-72B-Instruct,
    the critic is Qwen-2.5-72B-Instruct, the evaluator is DeepSeek-R1-Distill-Qwen-32B.


    Finance. The role assigner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the critic is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B.


    # E.1.2 LLM-Debate


    Mathematics. The debate agent is DeepSeek-R1-Distill-Qwen-32B, the aggregator
    is DeepSeek-R1- Distill-Qwen-32B.


    Coding. The debate agent is DeepSeek-R1-Distill-Qwen-32B, the aggregator is Qwen-2.5-72B-Instruct.


    Science. The debate agent is DeepSeek-R1-Distill-Qwen-32B, the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    Medicine. The debate agent is Qwen-2.5-72B-Instruct, the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    Finance. The debate agent is DeepSeek-R1-Distill-Qwen-32B, the aggregator is DeepSeek-R1-
    Distill-Qwen-32B.


    # E.1.3 DyLAN


    Mathematics.The node agent is DeepSeek-R1-Distill-Qwen-32B, the ranker is Qwen-2.5-72B-Instruct.


    Coding. The node agent is DeepSeek-R1-Distill-Qwen-32B, the ranker is Qwen-2.5-72B-Instruct.


    Science. The node agent is DeepSeek-R1-Distill-Qwen-32B, the ranker is Qwen-2.5-72B-Instruct.


    Medicine. The node agent is Qwen-2.5-72B-Instruct, the ranker is Qwen-2.5-72B-Instruct.


    ![](_page_17_Figure_0.jpeg)


    <span id="page-17-0"></span>Figure 5: Benchmarking LLMs on 5 MAS-related functions
    and 5 domains.


    Finance. The node agent is DeepSeek-R1-Distill-Qwen-32B, the ranker is Qwen-2.5-72B-Instruct.


    #### E.1.4 X-MAS-Proto


    Mathematics. The planner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the reviser is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B,
    the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    Coding. The planner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the reviser is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B,
    the aggregator is Qwen-2.5-72B-Instruct.


    Science. The planner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the reviser is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B,
    the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    Medicine. The planner is Qwen-2.5-72B-Instruct, the solver is Qwen-2.5-72B-Instruct,
    the reviser is Qwen-2.5-72B-Instruct, the evaluator is DeepSeek-R1-Distill-Qwen-32B,
    the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    Finance. The planner is Qwen-2.5-72B-Instruct, the solver is DeepSeek-R1-Distill-Qwen-32B,
    the reviser is DeepSeek-R1-Distill-Qwen-32B, the evaluator is DeepSeek-R1-Distill-Qwen-32B,
    the aggregator is DeepSeek-R1-Distill-Qwen-32B.


    <span id="page-18-0"></span>Table 5: Top-3 Models per Function and Domain (reasoner
    and chatbot)


    | Function   | Rank        | Mathematics                                                              |
    Coding                                                                                                                                                                                                                                                 |
    Science                                                             | Medicine                                                                                                                             |
    Finance                                                                                                                            |

    |------------|-------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|

    | QA         | 5           | QwQ-32B (80.5)<br>5 DeepSeek-R1-Distill-Qwen-32B
    (79.0)                  | DeepSeek-R1-Distill-Qwen-14B (82.0)<br>Qwen2.5-32B (80.3)<br>5
    DeepSeek-R1-Distill-Qwen-14B (78.8) DeepSeek-R1-Distill-Qwen-32B (80.0) DeepSeek-R1-Distill-Qwen-32B
    (68.3)                                                                             |
    QwQ-32B (69.4)<br>DeepSeek-R1-Distill-Llama-70B (69.4)              | QwQ-32B
    (73.8)<br>Qwen2.5-72B (70.4)                                                                                                 |
    DeepSeek-R1-Distill-Llama-70B (75.1) DeepSeek-R1-Distill-Qwen-32B (74.8)<br>QwQ-32B
    (74.6)<br>DeepSeek-R1-Distill-Llama-70B (74.3) |

    | Revise     | 5           | QwQ-32B (78.6)<br>5 DeepSeek-R1-Distill-Qwen-32B
    (77.8)                  | DeepSeek-R1-Distill-Llama-70B (81.7)<br>5 DeepSeek-R1-Distill-Llama-70B
    (78.2) DeepSeek-R1-Distill-Qwen-32B (81.0) DeepSeek-R1-Distill-Llama-70B (66.3)
    DeepSeek-R1-Distill-Llama-70B (66.3) DeepSeek-R1-Distill-Llama-70B (73.9)<br>Qwen2.5-7B
    (79.2) | QwQ-32B (67.0)                                                      |
    Llama-3.1-70B (71.0)<br>DeepSeek-R1-Distill-Qwen-32B (65.9) DeepSeek-R1-Distill-Llama-70B
    (70.7) DeepSeek-R1-Distill-Qwen-32B (73.5) | QwQ-32B (76.6)                                                                                                                     |

    |            | 5           | QwQ-32B (83.2)<br>Aggregation 5 DeepSeek-R1-Distill-Qwen-32B
    (82.2)      | Qwen2.5-72B (85.5)<br>QwQ-32B (84.2)<br>5 DeepSeek-R1-Distill-Qwen-14B
    (81.2) DeepSeek-R1-Distill-Llama-70B (83.1) DeepSeek-R1-Distill-Qwen-32B (70.3)
    DeepSeek-R1-Distill-Llama-70B (73.6)                                                            |
    QwQ-32B (71.3)                                                      | DeepSeek-R1-Distill-Llama-70B
    (71.7) DeepSeek-R1-Distill-Llama-8B (74.1) DeepSeek-R1-Distill-Qwen-32B (76.4)<br>QwQ-32B
    (73.8)       | DeepSeek-R1-Distill-Llama-70B (76.4)<br>QwQ-32B (74.6)                                                                             |

    | Planning   | 5<br>5<br>5 | Qwen2.5-14B (65.0)<br>Mistral-Small-3.1-24B (65.0)<br>Qwen2.5-32B
    (64.7) | Llama-3.1-70B (71.0)<br>Qwen2.5-14B (70.5)<br>Qwen2.5-32B (70.1)                                                                                                                                                                                       |
    Qwen2.5-Coder-7B (56.1)<br>Qwen2.5-32B (55.6)<br>Qwen2.5-72B (55.6) | Qwen2.5-Coder-14B
    (65.4)<br>Qwen2.5-7B (65.3)<br>Qwen2.5-32B (65.2)                                                                  |
    Qwen2.5-72B (64.7)<br>Qwen2.5-Coder-14B (63.6)<br>Qwen2.5-14B (63.2)                                                               |

    | Evaluation | 5<br>5      | QwQ-32B (84.2)<br>OpenThinker-32B (83.3)                                 |
    5 DeepSeek-R1-Distill-Llama-70B (85.9) DeepSeek-R1-Distill-Qwen-32B (56.2) DeepSeek-R1-Distill-Llama-70B
    (70.9)<br>Qwen2.5-14B (55.4)<br>QwQ-32B (55.3)                                                                                                |
    OpenThinker-32B (69.0)                                              | Llama-3.1-70B
    (70.5)<br>DeepSeek-R1-Distill-Qwen-32B (69.1) DeepSeek-R1-Distill-Llama-70B (70.2)                                     |
    OpenThinker-32B (76.6)<br>QwQ-32B (73.8)<br>DeepSeek-R1-Distill-Qwen-14B (69.8)
    DeepSeek-R1-Distill-Llama-70B (73.1)               |'
- title: "Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine"
  abstract: 'Large Language Models (LLMs) show promise in biomedicine but lack true
    causal

    understanding, relying instead on correlations. This paper envisions causal LLM

    agents that integrate multimodal data (text, images, genomics, etc.) and

    perform intervention-based reasoning to infer cause-and-effect. Addressing this

    requires overcoming key challenges: designing safe, controllable agentic

    frameworks; developing rigorous benchmarks for causal evaluation; integrating

    heterogeneous data sources; and synergistically combining LLMs with structured

    knowledge (KGs) and formal causal inference tools. Such agents could unlock

    transformative opportunities, including accelerating drug discovery through

    automated hypothesis generation and simulation, enabling personalized medicine

    through patient-specific causal models. This research agenda aims to foster

    interdisciplinary efforts, bridging causal concepts and foundation models to

    develop reliable AI partners for biomedical progress.'
  url: http://arxiv.org/abs/2505.16982v1
  keywords: ''
  document: '# 1. Introduction


    Large Language Models (LLMs) have demonstrated unprecedented flexibility across
    many tasks in medicine, from answering clinical questions to interpreting biomedical
    literature [\[10\]](#page-4-0). However, these models primarily learn correlations
    in data rather than true causal relationships [\[8\]](#page-4-1). Correlation
    is not causation – a mantra especially critical in biomedical research [\[11\]](#page-4-2)where
    understanding why something works is often more important than what correlates
    with an outcome. Standalone LLMs hallucinate, rely on outdated data, and lack
    causal understanding, limiting their use in medicine [\[15\]](#page-5-0). While
    LLMs excel at processing text and capturing domain knowledge implicitly, even
    generating plausible causal arguments [\[9\]](#page-4-3), distinguishing cause
    from effect or performing robust counterfactual reasoning remains a challenge.
    They may exhibit ''causal blindness'', struggling to infer causality directly
    from complex biological data [\[22\]](#page-5-1). Meanwhile, multimodal foundation
    models are emerging that can interpret combinations of text, imaging, genomics,
    and other data, producing rich outputs like free-text explanations or image annotations
    [\[14\]](#page-5-2). The convergence of multi-domain data and LLM capabilities
    presents a new opportunity to create causality-aware LLM agents for biomedicine.
    Such agents could integrate multimodal biomedical data and reason like scientists,
    envisioning interventions and predicting outcomes. Recent work has explored using
    LLMs within agentic frameworks for hypothesis generation [\[4\]](#page-4-4), interacting
    with tools [\[17,](#page-5-3) [20\]](#page-5-4), structuring unstructured real-world
    data (RWD) like clinical notes [\[7\]](#page-4-5), automating causal discovery
    workflows [\[20\]](#page-5-4), converting narrative text into analyzable graph
    structures [\[13\]](#page-5-5), and even engaging users conversationally [\[21\]](#page-5-6).
    However, achieving true personalization and reliability requires moving beyond
    the capabilities of LLMs alone. Integrating structured knowledge sources like
    knowledge graphs (KGs) offers a path to ground LLM reasoning and enhance explainability,
    though KGs themselves can be static [\[15\]](#page-5-0). Furthermore, genuine
    causal understanding necessitates incorporating formal causal discovery and inference
    methods, such as Mendelian Randomization (MR) or techniques handling confounding
    in observational data [\[7,](#page-4-5) [20,](#page-5-4) [22\]](#page-5-1), potentially
    applied to individual patient data for personalized insights [\[8,](#page-4-1)
    [21\]](#page-5-6). This position paper outlines a vision for causal LLM agents,
    discussing key challenges to be overcome, including the synergistic integration
    of LLMs, KGs, causal methods, and multi-modal data, and highlighting


    <sup>\*</sup>Corresponding Author: Yuwen Zhang at zhangyu@missouri.edu.


    <span id="page-1-0"></span>potential opportunities across several biomedical domains,
    such as automated causal knowledge discovery and Real-World Evidence (RWE) generation,
    as shown in Figure 1.


    # 2. Challenges in Enabling Causal Reasoning


    #### 2.1. Agentic Framework Design and Control


    Building an agentic LLM framework for biomedicine means allowing the model to
    autonomously perform tasks such as proposing experiments, retrieving literature,
    performing analyses, or making clinical suggestions, rather than just passively
    answering questions. Designing such autonomy raises serious safety and control
    challenges. In biomedical contexts, an agent''s actions or recommendations can
    directly impact experiments or patient care, so strict oversight and safeguards
    are essential. For example, if a causal LLM agent was connected to a lab automation
    system, a literature database like PubMed, a GWAS data repository like OpenGWAS,
    or a clinical decision support tool, we must ensure it cannot execute harmful
    interventions or retrieve and process information incorrectly. Frameworks like
    MRAgent demonstrate how LLMs can control complex workflows involving multiple
    external tools and data sources for tasks like automated causal discovery using
    Mendelian Randomization. Mechanisms for human-in-theloop control, as explored
    in frameworks like MatAgent, permission gating for high-stakes actions, and alignment
    with ethical guidelines are critical. Furthermore, agents should be bounded in
    their goals – an "aligned" agent that understands the limits of its authority.
    Designing a framework where the LLM''s agentic behavior can be audited and constrained
    is an open challenge. Integrating structured knowledge, such as KGs, can enhance
    transparency and provide clearer evidence for the agent''s decisions, potentially
    increasing trust. LLMs themselves might be used to convert narrative clinical
    text, like psychiatric case formulations, into graph-based representations suitable
    for analysis, although the reliability and validity of such conversions require
    careful assessment. Researchers have argued that AI agents should be scrutinized
    like scientists, undergoing rigorous peer review of their plans and reasoning.
    Achieving the right balance between an agent''s autonomy for efficiency and scale,
    and reliable control and traceability for safety and trust, will be pivotal for
    biomedical LLM agents.


    #### 2.2. Benchmarking, Evaluation, and Reproducibility


    Evaluating a causal LLM agent''s performance poses unique difficulties. Traditional
    metrics are insufficient to gauge causal reasoning quality or decision-making
    safety. New benchmarks and evaluation strategies are needed to test specific causal
    capabilities. This includes assessing the ability to generate text corresponding
    to correct causal argu-


    ![](_page_1_Figure_6.jpeg)


    Figure 1. Schematic of major challenges in causal reasoning.


    ments, reason about counterfactuals, and identify necessary and sufficient causes
    in vignettes [\[9\]](#page-4-3). Performance on formal causal tasks, like pairwise
    causal discovery or full graph generation, needs evaluation using established
    benchmarks (for example, Tubingen pairs) and domain-specific graphs, ¨ including
    tests for generalization to novel datasets created after LLM training cutoffs
    [\[9\]](#page-4-3). Alternative evaluation methods are emerging. Structured output
    evaluation, such as having LLMs generate KGs from medical concepts [\[16\]](#page-5-7)
    or causal graphs from narrative text [\[13\]](#page-5-5), allows comparison against
    human experts or ground truth using graph similarity metrics (for example, node
    semantic similarity, Jaccard edge similarity). Task-specific performance evaluation
    in biomedical applications, like identifying cancer genes [\[22\]](#page-5-1)
    or assessing causality in pharmacovigilance [\[18\]](#page-5-8), often reveals
    the superiority of domain-specific models. For agentic systems, evaluating the
    entire workflow execution, comparing against human expert performance on the same
    multistep tasks [\[20\]](#page-5-4), provides a holistic assessment. Comprehensive
    clinical evaluation frameworks, such as CLEVER [\[10\]](#page-4-0), assess LLMs
    across multiple clinical dimensions (comprehension, reasoning, decision support,
    risk management) using expert ratings, including performance across different
    data distributions (ID, OOD, rare diseases) and comparisons against physicians.
    Multi-agent evaluation, using ensembles or debating frameworks to critique hypotheses
    [\[2,](#page-4-6) [3\]](#page-4-7), offers another layer of validation. However,
    standardized protocols are lacking, and real-world testing remains limited [\[5\]](#page-4-8).
    Reproducibility is also a concern due to LLM stochasticity and updates [\[6\]](#page-4-9).
    While CoT aids auditabil<span id="page-2-0"></span>ity [\[16,](#page-5-7) [20\]](#page-5-4),
    ensuring verifiable outputs is challenging. Addressing LLM failure modes like
    hallucination, bias, and sensitivity to prompts requires systematic stress-testing
    and robust, multi-faceted evaluation methodologies that combine these diverse
    approaches [\[9\]](#page-4-3).


    #### 2.3. Causal Inference and Multimodal Integration Difficulties


    Current LLMs, even very powerful ones, lack a true grasp of formal causal inference
    [\[9\]](#page-4-3). While they excel at knowledgebased causal discovery (for example,
    inferring A→B from variable names) and generating counterfactual arguments from
    text, they inherently struggle with distinguishing cause from correlation in observational
    data [\[19\]](#page-5-9), understanding interventions rigorously, and applying
    formal causal methods without external guidance [\[8\]](#page-4-1). Their textual
    knowledge does not imply understanding of formal mathematical and causal logic.
    Several strategies aim to bridge this gap by integrating LLMs with more formal
    approaches and diverse data types. One key strategy involves integrating causal
    tools and methods via agentic frameworks. LLM agents can be designed to invoke
    external libraries or APIs that implement specific causal algorithms. For instance,
    the "Causal Agent" framework demonstrates using tools for standard causal discovery
    (like the PC algorithm) and inference on tabular data [\[8\]](#page-4-1). Domain-specific
    examples include MRAgent, which automates Mendelian Randomization workflows by
    calling tools to query PubMed, fetch GWAS data from OpenGWAS, and execute MR analyses
    using packages like TwoSampleMR [\[20\]](#page-5-4). Similarly, the ICGI framework
    combines LLM prompting for knowledge retrieval with data-driven causal feature
    selection using Debiased Machine Learning (DML) on omics data to identify potential
    cancer genes [\[22\]](#page-5-1). Another important direction is the synergistic
    integration of LLMs and Knowledge Graphs (KGs). KGs ground LLM reasoning, improve
    explainability, and reduce hallucination [\[15\]](#page-5-0). LLMs help build
    and update KGs by extracting new relationships from text or data. This bi-directional
    interaction creates a more robust and dynamic knowledge system, addressing the
    static nature of KGs and the ungrounded nature of LLMs [\[15\]](#page-5-0). Furthermore,
    effectively handling multimodal data is crucial for biomedical applications. Agents
    must process text, images, structured data, and logs. [\[17\]](#page-5-3). LLMs
    help structure clinical EMR text to enable scalable RWE generation [\[7,](#page-4-5)
    [10\]](#page-4-0). Advanced frameworks like MATMCD explore explicitly integrating
    multiple modalities, such as using retrieved text or logs alongside statistical
    causal graphs, employing agents for data augmentation and constraint generation
    to improve causal discovery [\[7\]](#page-4-5). As discussed, narrative-to-graph
    conversion is a promising but still unvalidated use case [\[13\]](#page-5-5).
    Building these capabilities relies heavily on the availability of large-scale,
    multimodal biomedical datasets for training


    ![](_page_2_Figure_4.jpeg)


    Figure 2. Fully-automated cycle of causal LLM agent workflow in different biomedicine
    applications.


    more capable foundation models [\[12\]](#page-4-10). Crucially, domainspecific
    pre-training and fine-tuning consistently improve model performance on specialized
    medical and causal tasks compared to general-purpose models [\[10,](#page-4-0)
    [18\]](#page-5-8).


    # 3. Opportunities and Applications


    As depicted in Figure 2, despite the significant challenges, a future causal LLM
    agent offers exciting opportunities to transform biomedical research and healthcare.
    By combining the broad knowledge and pattern recognition of foundation models
    with principled causal reasoning, such agents could unlock novel applications
    in several domains.


    #### 3.1. Drug Discovery and Biomedical Research


    Causally-aware LLM agents can function as "AI scientists," accelerating discovery
    by automating knowledge synthesis, hypothesis generation, and analysis. A key
    application is automated causal knowledge discovery. Agents like MRAgent demonstrate
    the ability to autonomously scan literature, identify potential exposure-outcome
    pairs for a given disease, check for prior causal analyses (like Mendelian Randomization),
    retrieve relevant genetic data (OpenG-WAS), execute causal inference analyses
    (using tools like TwoSampleMR), and generate comprehensive reports. This significantly
    speeds up the process of uncovering potential causal links from existing knowledge
    and data [\[20\]](#page-5-4). Building on this, AI-driven hypothesis generation
    can be enhanced. Agents can synthesize information from literature (potentially
    via RAG), structured databases, and exper<span id="page-3-0"></span>imental data
    to propose testable causal hypotheses. LLMs'' strength in generating causal arguments
    based on their vast training data can bootstrap this process [\[2,](#page-4-6)
    [4\]](#page-4-4). Furthermore, causal agents can aid in target identification
    and validation. By integrating LLM-driven causal reasoning with data-driven causal
    discovery methods applied to omics data (like DML-CGI), potential causal genes
    or pathways implicated in diseases like cancer can be identified and prioritized,
    complete with explanations for their proposed roles [\[22\]](#page-5-1). Finally,
    automated experiment analysis, such as using vision-based agents to detect drug-cell
    interactions in microscopy images without task-specific training, can streamline
    experimental workflows. Collectively, these capabilities promise to shorten research
    cycles, prioritize experiments based on causal plausibility, and uncover novel
    therapeutic strategies [\[1\]](#page-4-11).


    #### 3.2. Personalized Medicine and Treatment Decisions


    Causality-aware LLM agents can personalize medicine by tailoring decisions to
    individual causal factors, beyond population averages. A core capability is identifying
    personal causal drivers. By applying causal discovery and inference techniques,
    potentially orchestrated by an LLM agent invoking specialized tools, to an individual''s
    longitudinal multi-modal data (from wearables, EHRs, genomics), patient-specific
    causal links between factors like diet, activity, sleep, and health outcomes can
    be uncovered. TChat-Diet uses N-of-1 analysis to estimate personalized nutrient
    effects [\[8,](#page-4-1) [21\]](#page-5-6). Once personal causal factors are
    identified (for example, a specific mutation driving a cancer subtype), the agent
    can tailor interventions and suggest targeted therapies or lifestyle changes addressing
    the root cause for that individual [\[22\]](#page-5-1). Crucially, LLMs can explain
    recommendations using patient-specific causal models, improving transparency.
    This moves beyond simply matching patients to guidelines towards dynamically stratifying
    them into causal subgroups [\[6\]](#page-4-9). LLMs also show promise in diagnostic
    assistance. Advanced medical LLMs like Med-Found can generate differential diagnoses
    and outline reasoning steps, demonstrating effectiveness even for rare diseases
    [\[10\]](#page-4-0). Integrating causal reasoning capabilities could further enhance
    diagnostic accuracy by helping to distinguish causes from symptoms or comorbidities.
    Linking multimodal data by causal relationships can further improve personalized
    care.


    #### 3.3. Public Health and Epidemiology


    Automated causal discovery techniques, potentially driven by LLM agents like MRAgent,
    can systematically scan literature and population-level data (like GWAS) to hypothesize
    and test potential causal links between environmental exposures, genetic factors,
    and public health outcomes [\[20\]](#page-5-4). These capabilities can lead to
    more informed, data-driven public health policies and interventions that are grounded
    in an understanding of causal mechanisms.


    # 4. Discussion


    Causal LLM agents are a key step toward reliable, impactful AI in science and
    biomedicine. This requires moving beyond the correlation-based capabilities of
    current LLMs towards systems that genuinely understand and manipulate cause-and-effect
    relationships. Achieving this necessitates a synergistic approach, moving beyond
    standalone LLMs to integrate their strengths with formal causal methodologies,
    structured knowledge, and dynamic interaction with external tools. Key research
    directions emerge from the challenges and opportunities discussed. Firstly, synergistic
    integration appears paramount. The best path combines LLMs with KGs for grounding
    and formal tools (e.g., MR, DML) for reasoning. Agentic frameworks are essential
    for orchestrating these components, enabling LLMs to intelligently invoke external
    tools, query KGs, and process diverse data inputs. Secondly, we must effectively
    leverage LLM strengths while mitigating their weaknesses. LLMs excel at inferring
    relationships, generating arguments, and extracting causal structures from clinical
    narratives. These abilities can automate knowledge synthesis and augment human
    expertise. However, their struggles with formal causal inference from data, susceptibility
    to hallucination, and lack of inherent grounding necessitate integration with
    verifiable knowledge sources (KGs) and formal causal methods. Robust evaluation
    and alignment techniques are crucial for ensuring safety and reliability. Thirdly,
    progress relies heavily on the data ecosystem. This includes access to largescale,
    diverse, multimodal biomedical datasets and, critically, the ability to structure
    and integrate Real-World Data (RWD), especially the vast amounts of unstructured
    text in EMRs. Domain-specific pre-training and fine-tuning of LLMs have proven
    vital for achieving high performance on specialized medical and causal tasks.
    Fourthly, robust evaluation methodologies must be developed and standardized.
    This goes beyond typical NLP metrics to include specific assessments of causal
    reasoning, performance across different data distributions and disease rarities,
    evaluation of agentic task completion against expert benchmarks, and assessment
    of clinical utility via structured human evaluation frameworks. Finally, the applications
    driving this research are transformative. Key opportunities include automating
    causal discovery from literature and data, generating reliable Real-World Evidence
    from EMRs, enabling personalized medicine through N-of-1 causal analysis, and
    providing explainable diagnostic and decision support.


    # 5. Conclusion


    The development of causal LLM agents for biomedicine requires a shift from standalone
    models to sophisticated, integrated systems. These agents, empowered by specialized
    tools, grounded by structured knowledge, and guided by formal causal principles,
    hold the potential to become invaluable partners for researchers and clinicians.
    Realizing this vision demands continued interdisciplinary collaboration to tackle
    the complex challenges of integration, evaluation, validation, and safe, ethical
    deployment.


    # References


    - <span id="page-4-11"></span>[1] Adib Bazgir and Yuwen Zhang. Drug discovery
    agent: An automated vision detection system for drug-cell interactions. In *1st
    CVPR Workshop on Computer Vision For Drug Discovery (CVDD): Where are we and What
    is Beyond?*, 2025. [4](#page-3-0)

    - <span id="page-4-6"></span>[2] Adib Bazgir, Yuwen Zhang, et al. Proteinhypothesis:
    A physics-aware chain of multi-agent rag llm for hypothesis generation in protein
    science. In *Towards Agentic AI for Science: Hypothesis Generation, Comprehension,
    Quantification, and Validation*, 2025. [2,](#page-1-0) [4](#page-3-0)

    - <span id="page-4-7"></span>[3] Adib Bazgir, Yuwen Zhang, et al. Matagent: A
    human-inthe-loop multi-agent llm framework for accelerating the material science
    discovery cycle. In *AI for Accelerated Materials Design-ICLR 2025*, 2025. [2](#page-1-0)

    - <span id="page-4-4"></span>[4] Adib Bazgir, Yuwen Zhang, et al. Agentichypothesis:
    A survey on hypothesis generation using llm systems. In *Towards Agentic AI for
    Science: Hypothesis Generation, Comprehension, Quantification, and Validation*,
    2025. [1,](#page-0-0) [4](#page-3-0)

    - <span id="page-4-8"></span>[5] Suhana Bedi, Yutong Liu, Lucy Orr-Ewing, Dev
    Dash, Sanmi Koyejo, Alison Callahan, Jason A Fries, Michael Wornow, Akshay Swaminathan,
    Lisa Soleymani Lehmann, et al. Testing and evaluation of health care applications
    of large language models: a systematic review. *JAMA*, 2024. [2](#page-1-0)

    - <span id="page-4-9"></span>[6] Shanghua Gao, Ada Fang, Yepeng Huang, Valentina
    Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic,
    and Marinka Zitnik. Empowering biomedical discovery with ai agents. *Cell*, 187(22):6125–
    6151, 2024. [2,](#page-1-0) [4](#page-3-0)

    - <span id="page-4-5"></span>[7] Javier Gonzalez, Cliff Wong, Zelalem Gero, Jass
    Bagga, ´ Risa Ueno, Isabel Chien, Eduard Oravkin, Emre Kiciman, Aditya Nori, Roshanthi
    Weerasinghe, et al. Trialscope: a unifying causal framework for scaling real-world
    evidence generation with biomedical language models. *arXiv preprint arXiv:2311.01301*,
    2023. [1,](#page-0-0) [3](#page-2-0)

    - <span id="page-4-1"></span>[8] Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye,
    and Fei Wu. Causal agent based on large language model. *arXiv preprint arXiv:2408.06849*,
    2024. [1,](#page-0-0) [3,](#page-2-0) [4](#page-3-0)

    - <span id="page-4-3"></span>[9] Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao
    Tan. Causal reasoning and large language models: Opening a new frontier for causality.
    *Transactions on Machine Learning Research*, 2023. [1,](#page-0-0) [2,](#page-1-0)
    [3](#page-2-0)

    - <span id="page-4-0"></span>[10] Xiaohong Liu, Hao Liu, Guoxing Yang, Zeyu Jiang,
    Shuguang Cui, Zhaoze Zhang, Huan Wang, Liyuan Tao, Yongchang Sun, Zhu Song, et
    al. A generalist medical language model for disease diagnosis assistance. *Nature
    Medicine*, pages 1–11, 2025. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0) [4](#page-3-0)

    - <span id="page-4-2"></span>[11] Sebastian Lobentanzer, Pablo Rodriguez-Mier,
    Stefan Bauer, and Julio Saez-Rodriguez. Molecular causality in the advent of foundation
    models. *Molecular Systems Biology*, 20(8): 848–858, 2024. [1](#page-0-0)

    - <span id="page-4-10"></span>[12] Alejandro Lozano, Min Woo Sun, James Burgess,
    Jeffrey J Nirschl, Christopher Polzak, Yuhui Zhang, Liangyu Chen, Jeffrey Gu,
    Ivan Lopez, Josiah Aklilu, et al. A large-scale vision-language dataset derived
    from open scientific literature to advance biomedical generalist ai. *arXiv preprint
    arXiv:2503.22727*, 2025. [3](#page-2-0)

    - <span id="page-5-5"></span>[13] Chris Ludlow. Investigating the capability of
    large language models to identify causal relations in psychiatric case studies:
    A methodological proof of concept for the analysis of psychological case formulations.
    *LLMs and Psychiatric Cases*, 2025. [1,](#page-0-0) [2,](#page-1-0) [3](#page-2-0)

    - <span id="page-5-2"></span>[14] Michael Moor, Oishi Banerjee, Zahra Shakeri
    Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar.
    Foundation models for generalist medical artificial intelligence. *Nature*, 616(7956):259–265,
    2023. [1](#page-0-0)

    - <span id="page-5-0"></span>[15] Chaelim Park, Hayoung Lee, Seonghee Lee, and
    Okran Jeong. Synergistic joint model of knowledge graph and llm for enhancing
    xai-based clinical decision support systems. *Mathematics*, 13(6):949, 2025. [1,](#page-0-0)
    [3](#page-2-0)

    - <span id="page-5-7"></span>[16] Gabriel R Rosenbaum, Lavender Yao Jiang, Ivaxi
    Sheth, Jaden Stryker, Anton Alyakin, Daniel Alexander Alber, Nicolas K Goff, Young
    Joon Fred Kwon, John Markert, Mustafa Nasir-Moin, et al. Medg-krp: Medical graph
    knowledge representation probing. *arXiv preprint arXiv:2412.10982*, 2024. [2,](#page-1-0)
    [3](#page-2-0)

    - <span id="page-5-3"></span>[17] ChengAo Shen, Zhengzhang Chen, Dongsheng Luo,
    Dongkuan Xu, Haifeng Chen, and Jingchao Ni. Exploring multi-modal integration
    with tool-augmented llm agents for precise causal discovery. *arXiv preprint arXiv:2412.13667*,
    2024. [1,](#page-0-0) [3](#page-2-0)

    - <span id="page-5-8"></span>[18] Xingqiao Wang, Xiaowei Xu, Zhichao Liu, and
    Weida Tong. Bidirectional encoder representations from transformers-like large
    language models in patient safety and pharmacovigilance: A comprehensive assessment
    of causal inference implications. *Experimental Biology and Medicine*, 248(21):
    1908–1917, 2023. [2,](#page-1-0) [3](#page-2-0)

    - <span id="page-5-9"></span>[19] Anpeng Wu, Kun Kuang, Minqin Zhu, Yingrong Wang,
    Yujia Zheng, Kairong Han, Baohong Li, Guangyi Chen, Fei Wu, and Kun Zhang. Causality
    for large language models. *arXiv preprint arXiv:2410.15319*, 2024. [3](#page-2-0)

    - <span id="page-5-4"></span>[20] Wei Xu, Gang Luo, Weiyu Meng, Xiaobing Zhai,
    Keli Zheng, Ji Wu, Yanrong Li, Abao Xing, Junrong Li, Zhifan Li, et al. Mragent:
    an llm-based automated agent for causal knowledge discovery in disease via mendelian
    randomization. *Briefings in Bioinformatics*, 26(2):bbaf140, 2025. [1,](#page-0-0)
    [2,](#page-1-0) [3,](#page-2-0) [4](#page-3-0)

    - <span id="page-5-6"></span>[21] Zhongqi Yang, Elahe Khatibi, Nitish Nagesh,
    Mahyar Abbasian, Iman Azimi, Ramesh Jain, and Amir M Rahmani. Chatdiet: Empowering
    personalized nutrition-oriented food recommender chatbots through an llm-augmented
    framework. *Smart Health*, 32:100465, 2024. [1,](#page-0-0) [4](#page-3-0)

    - <span id="page-5-1"></span>[22] Haolong Zeng, Chaoyi Yin, Chunyang Chai, Yuezhu
    Wang, Qi Dai, and Huiyan Sun. Cancer gene identification through integrating causal
    prompting large language model with omics data–driven causal inference. *Briefings
    in Bioinformatics*, 26(2):624, 2025. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0)
    [4](#page-3-0)'
- title: "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n\
    \  Design"
  abstract: 'Single-agent LLMs hit hard limits--finite context, role overload, and
    brittle

    domain transfer. Conventional multi-agent fixes soften those edges yet expose

    fresh pains: ill-posed decompositions, fuzzy contracts, and verification

    overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a

    framework that converts domain priors into an algorithmic blueprint hierarchy,

    in which tasks are recursively split into typed, controller-mediated subtasks,

    each solved zero-shot or with the lightest viable boost (e.g.,

    chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch

    theorem, KtR trades the chase for a universal prompt for disciplined

    decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents

    raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a

    single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a

    six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,

    versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation

    thus turns modest models into reliable collaborators--no ever-larger monoliths

    required.'
  url: http://arxiv.org/abs/2505.16979v1
  keywords: ''
  document: "## Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\
    \ Design\n\nZhenkun Li 1 \\* † University of South Florida zhenkun@usf.edu\n\n\
    Shuhang Lin Rutgers University shuhang.lin@rutgers.edu\n\n### Abstract\n\nSingle\
    \ -agent LLMs hit hard limits—finite context, role overload, brittle domain transfer.\
    \ Conventional multi -agent fixes soften those edges yet expose fresh pains: ill\
    \ -posed decompositions, fuzzy contracts, and verification overhead that blunts\
    \ the gains. We therefore present Know-The-Ropes (KtR), a framework that converts\
    \ domain priors into an algorithmic blueprint hierarchy: tasks are recursively\
    \ split into typed, controller-mediated subtasks, each solved zero-shot or with\
    \ the lightest viable boost (chain-of-thought, micro-tune, self-check). Grounded\
    \ in the No -Free -Lunch theorem, KtR trades the chase for a universal prompt\
    \ for disciplined decomposition. On a Knapsack benchmark (3–8 items) three GPT\
    \ -4o -mini agents raise accuracy from 3% zero -shot to 95% on size -5 instances\
    \ after patching a single bottleneck agent. On the tougher Task -Assignment suite\
    \ (6–15 jobs) a six -agent o3 -mini blueprint hits 100% up to size 10 and ≥84%\
    \ on sizes 13–15, versus ≤11% zero -shot. Algorithm -aware decomposition plus\
    \ targeted augmentation thus turns modest models into reliable collaborators—no\
    \ ever-larger monoliths required.\n\n### 1 Introduction\n\nIndividual large language\
    \ model (LLM) agents typically excel in the specific domain they are optimized\
    \ for [\\(Thirunavukarasu et al.,](#page-16-0) [2023;](#page-16-0) [Kasneci](#page-15-0)\
    \ [et al.](#page-15-0) , [2023](#page-15-0) ; [Wu et al.](#page-16-1) , [2023b\\\
    )](#page-16-1), yet they struggle to achieve universal versatile [\\(Zhang et\
    \ al.](#page-16-2) , [2024](#page-16-2) ; [Xu et al.](#page-16-3) , [2024\\)](#page-16-3).\
    \ A natural antidote is division of labor—splitting tasks into specialized agents\
    \ that negotiate a joint answer—and early frameworks like Mixture-of-Agents indeed\
    \ show that a wellorchestrated team can outperform its best member [\\(Wang et\
    \ al.,](#page-16-4) [2024;](#page-16-4) [Guo et al.,](#page-15-1) [2024;](#page-15-1)\
    \ [Bo et al.,](#page-15-2) [2024\\)](#page-15-2).\n\nLingyao Li∗† University of\
    \ South Florida lingyaol@usf.edu\n\nYongfeng Zhang Rutgers University yongfeng.zhang@rutgers.edu\n\
    \nYet large-scale audits have cooled that optimism. Firstly, each problem needs\
    \ a well-designed prompt with significant manual effort. Even though some multi-agent\
    \ frameworks seems to achieve satisfying results, when evaluation leakage and\
    \ prompt over-fitting are removed, the headline boosts of naïve agent swarms slip\
    \ to single-digit percentages—and can even turn negative once a task needs more\
    \ than two or three coordination rounds [\\(Pan](#page-15-3) [et al.](#page-15-3)\
    \ , [2025](#page-15-3) ; [Zhu et al.](#page-16-5) , [2025\\)](#page-16-5). Post-mortems\
    \ trace the shortfall to a recurring pattern: ill-posed decompositions propagate\
    \ ambiguity, loose role definitions create blind spots or duplication, verification\
    \ layers either invoke brittle chain-of-thought heuristics or blow up the token\
    \ budget, and every extra message compounds latency and cost almost quadratically\
    \ with the number of rounds [\\(Ye](#page-16-6) , [2025](#page-16-6) ; [Shu et\
    \ al.](#page-16-7) , [2024\\)](#page-16-7). In short, simply throwing more \"\
    brains\" at a problem does not guarantee progress; sustainable gains demand disciplined,\
    \ principled systems engineering—the very gap our work aims to close.\n\nKnow-The-Ropes\
    \ (KtR) converts domain priors into an algorithmic hierarchy: recursively split\
    \ a task until each leaf fits the base LLM's zeroshot reach or the lightest augmentation\
    \ (chain-ofthought [\\(Wei et al.,](#page-16-8) [2022\\)](#page-16-8), small fine-tune,\
    \ selfcheck loops, etc). Typed I/O contracts, enforced by a lightweight controller,\
    \ isolate agents and prevent cross-talk, context bloat, or silent overwrites.\
    \ Multi-agent system(MAS) construction thus resembles classic pipeline tuning—pinpoint\
    \ the bottleneck, refine the split, and attach the cheapest fix that works.\n\n\
    Theory justifies the approach. The No-Free-Lunch theorem [\\(Wolpert and Macready,](#page-16-9)\
    \ [1997;](#page-16-9) [Wolpert](#page-16-10) , [2021\\)](#page-16-10) dictates\
    \ that no single agent or orchestration rule prevails across all problem distributions—there\
    \ is no silver bullet. Robustness must therefore flow from exploiting domain structure,\
    \ not from ever-larger prompt templates. KtR\n\n<sup>\\*</sup>Equal contribution\
    \ as first authors.\n\n<sup>†</sup>Corresponding authors.\n\nmeets this mandate\
    \ by re-using classical algorithms whose behavior is already well understood and\
    \ optimized.\n\nOur empirical preview includes the following:\n\n• Knapsack Problem\
    \ (proof of concept with a lightweight base model). On 3–8-item instances GPT-4o-mini\
    \ scores only 60 % → 0 % zero-shot, and a blunt task-level fine-tune barely helps.\
    \ Applying KtR yields a three-agent blueprint; fine-tuning just the \"trimmer\"\
    \ on 1 200 examples lifts accuracy to 95 %–70 %, showing that KtR can turn a compact,\
    \ low-capacity model into a high-performing multi-agent system.\n\n• Task-Assignment\
    \ Problem (proof of scalability with a stronger base model). With o3 mini, a six-agent\
    \ KtR blueprint tackles sizes 6–15. Splitting one bottleneck agent into two finer\
    \ leaves drives those leaves to 100 % and 97 % accuracy and raises overall system\
    \ accuracy to ≥ 84 % across all sizes—demonstrating that KtR's gains grow with\
    \ base-model capacity and that the knapsack results are not an isolated success.\n\
    \nOur contributions lie in two aspects.\n\n• KtR framework. By formalizing blueprint\
    \ hierarchies, we apply NFL-grounded algorithmic design that bypasses documented\
    \ MAS pitfalls, streamlines the assembly pipeline, and relieves performance bottlenecks.\n\
    \n• Empirical validation. Across two canonical optimization problems, KtR transforms\
    \ modest base models into systems that match—or exceed—their fine-tuned counterparts\
    \ while using orders-of-magnitude less specialized data.\n\n### 2 Related Work\n\
    \nMulti-Agent Systems (MAS) have been widely employed to enhance the capabilities\
    \ of LLMs to tackle complex tasks [\\(Qiu et al.,](#page-16-11) [2024;](#page-16-11)\
    \ [Yan et al.,](#page-16-12) [2024;](#page-16-12) [Ma et al.,](#page-15-4) [2024;](#page-15-4)\
    \ [Lin et al.,](#page-15-5) [2024;](#page-15-5) [Hua et al.,](#page-15-6) [2023;](#page-15-6)\
    \ [Yu et al.,](#page-16-13) [2024\\)](#page-16-13). This is because MAS typically\
    \ distribute tasks across agents that collaborate to achieve a common goal, thereby\
    \ improving both efficiency and adaptability. Recent frameworks like CAMEL [\\\
    (Li et al.,](#page-15-7) [2023\\)](#page-15-7) enable role-based cooperative dialogues\
    \ by assigning agents distinct personas, while AutoGen [\\(Wu et al.,](#page-16-14)\
    \ [2023a\\)](#page-16-14) and MetaGPT [\\(Hong et al.,](#page-15-8) [2023\\)](#page-15-8)\
    \ orchestrate multirole agent teams through structured conversation loops and\
    \ predefined workflows. In math optimization, OR-LLM-Agent can translate naturallanguage\
    \ problem descriptions into formal Gurobi\n\nmodels—achieving an 85% correct-solution\
    \ rate on real-world benchmarks [\\(Zhang and Luo,](#page-16-15) [2025\\)](#page-16-15).\n\
    \nDespite this excitement, studies show that simply scaling up to LLM-based MAS\
    \ often yields only marginal gains over single-agent baselines [\\(Pan et al.,](#page-15-3)\
    \ [2025\\)](#page-15-3). LLM agents still struggle with context management and\
    \ consistency, meaning that elaborate multi-agent prompts can fail to realize\
    \ the intended collaboration [\\(Bo et al.,](#page-15-2) [2024\\)](#page-15-2).\
    \ A recent systematic audit of popular MAS frameworks has identified 14 distinct\
    \ failure modes [\\(Cemri et al.,](#page-15-9) [2025\\)](#page-15-9), which can\
    \ be grouped into three categories, including flawed design (e.g., ambiguous role\
    \ definition), inter-agent misalignment (e.g., communication failures), and quality\
    \ control (e.g., no reliable check mechanism).\n\nTo address these challenges,\
    \ researchers have proposed multiple strategies to make LLM-based MAS more reliable\
    \ [\\(Zhu et al.,](#page-16-5) [2025;](#page-16-5) [Tran et al.,](#page-16-16)\
    \ [2025\\)](#page-16-16). A key strategy is improving the agent interaction structure\
    \ [\\(Zhu et al.,](#page-16-5) [2025\\)](#page-16-5). For example, the AgentDropout\
    \ framework proposes a dynamic agent-pruning strategy, which seeks to discard\
    \ less critical actors during training [\\(Wang et al.,](#page-16-17) [2025\\\
    )](#page-16-17). Another effective strategy is incorporating feedback and verification\
    \ loops [\\(Hong et al.,](#page-15-8) [2023\\)](#page-15-8). A recent study shows\
    \ that frameworks with strong role specialization and iterative feedback mechanisms\
    \ tend to outperform those without these features [\\(Anonymous,](#page-15-10)\
    \ [2025\\)](#page-15-10). In addition, systematic evaluations suggest that the\
    \ communication topology matters: a well-designed protocol between agents can\
    \ significantly improve collective performance on complex tasks [\\(Zhu et al.,](#page-16-5)\
    \ [2025\\)](#page-16-5).\n\nWhile existing multi-agent frameworks and strategies\
    \ have demonstrated notable progress [\\(Li](#page-15-11) [et al.,](#page-15-11)\
    \ [2025;](#page-15-11) [Hong et al.,](#page-15-8) [2023;](#page-15-8) [Zhu et\
    \ al.,](#page-16-5) [2025\\)](#page-16-5), they still face challenges in dynamic\
    \ role reallocation and efficient inter-agent communication. These limitations\
    \ become especially pronounced when addressing complex tasks, such as NP-hard\
    \ optimization problems. To bridge this gap, our work proposes a heuristic strategy\
    \ that embeds domain-specific rules and algorithms directly into agent coordination.\
    \ This approach enables on-thefly role adaptation and task decomposition, particularly\
    \ in math optimization contexts where conventional MAS frameworks often struggle.\n\
    \n### 3 Methodology\n\n### 3.1 Framework Design—Know the Ropes\n\nWe propose the\
    \ heuristic framework \"Know the Ropes.\" This framework offers a structured methodology\
    \ for designing specialized MAS leveraging LLMs. This heuristic focuses on translating\
    \ known, effective procedures or algorithms into a coherent multi-agent architecture.\
    \ As presented in Figure [1,](#page-3-0) the core idea is to decompose a complex\
    \ overall task into its fundamental computational stages. Each stage is then mapped\
    \ to a wellformulated sub-task, designed to be tractable for an individual agent.\
    \ These specialized agents are subsequently orchestrated to mirror the data/control\
    \ flow of the original procedure, which can effectively embed problem-solving\
    \ logic into the multiagent system. The following definitions formalize the components\
    \ of this framework.\n\nDefinition 3.1. *A* well-formulated task *is a tuple*\
    \ T = (I, O, R)*, consisting of the following:*\n\n• *Input domain* I*: an unambiguous\
    \ description of all admissible inputs*\n\n• *Output co-domain* O *an unambiguous\
    \ description of all admissible outputs*\n\n• *Requirement relation* R ⊂ I × O*:\
    \ a relation such that for each input* x ∈ I *it defines explicitly the subset*\
    \ R(x) ⊂ O *as the set of outputs that are considered correct.*\n\nDefinition\
    \ 3.2. *A* workflow blueprint B = (T ,P) *consisting of the following.*\n\n• *A\
    \ finite set of well-formulated tasks* T = {T1, · · · , Tn}*.*\n\n• *An orchestration\
    \ protocol* P *that specifies:*\n\n− *The control-flow graph that determines when\
    \ each* T<sup>i</sup> *is invoked.*\n\n− *The data-dependency edges that map outputs\
    \ of some tasks to inputs of others.*\n\n− *Any global invariants, error-handling\
    \ rules, or communication channels required to realize the end-to-end objective\
    \ of* B*.*\n\nDefinition 3.3. *Given a workflow blueprint* B = (T ,P)*, a* decomposition\
    \ *selects a task* T ∈ T *and replace it with a sub-blueprint* B<sup>T</sup> =\
    \ (T<sup>T</sup> ,P<sup>T</sup> ) *such that the following hold.*\n\n• *Each task*\
    \ T ′ ∈ T<sup>T</sup> *is strictly simpler than* T*.*\n\n• *The composite protocol*\
    \ P ′ *, obtained by embedding* P<sup>T</sup> *in place of* T *inside* P*, preserves\
    \ all external interface of* T*.*\n\n*The result of the decomposition is a new\
    \ blueprint* S ′ = (T ′ ,P ′ )*, where* T ′ = (T \\{T}) ∪ T<sup>T</sup> *. We\
    \ record this process as*\n\n$$\\mathcal{B} \\stackrel{D}{\\leadsto} \\mathcal{B'}\
    \ $$\n\n<span id=\"page-2-0\"></span>Definition 3.4. *Let* M *be a set of LLM\
    \ models. A well-formulated task* T *is said to be* M-tractable *if a model inside*\
    \ M *satisfies the requirement relation* R<sup>T</sup> *with high, empirically\
    \ verified accuracy, after optional augmentations (e.g., chain-of-thought prompting,\
    \ tool calls, self-reflection loops, or finetuning).*\n\nDefinition 3.5. *Given\
    \ a set of LLM models* M *and a blueprint* B*, an* M-tractable hierarchy *is a\
    \ sequence of decompositions*\n\n$$\\mathcal{B} \\stackrel{D\\_1}{\\leadsto} \\\
    mathcal{B}\\_1 \\stackrel{D\\_2}{\\leadsto} \\dots \\stackrel{D\\_n}{\\leadsto}\
    \ \\mathcal{B}\\_n$$\n\n*such that each task in the terminal blueprint* B<sup>n</sup>\
    \ *is* M*-tractable in the sense of Definition [3.4.](#page-2-0)*\n\nDefinition\
    \ 3.6. *Given a set of LLM models* M *and an* M*-tractable blueprint* B = (T ,P)*,\
    \ a* system instantiation *is to instantiate* B *into a MAS in the following way.*\n\
    \n• *We create one agent* A<sup>i</sup> *per tractable task* T<sup>i</sup> ∈ T\
    \ *, bundling any necessary augmentations with the agent.*\n\n• *We implement\
    \ the orchestration protocol* P *as message-passing or function calls among agents,\
    \ preserving data-dependencies and control flow.*\n\n<span id=\"page-2-1\"></span>Algorithm\
    \ 1 Know-The-Ropes (KtR) Pseudo-code\n\n```\n1: procedure KNOWTHEROPES(T,M)\n\
    2: B ← CREATEBLUEPRINT({T}, trivial_protocol)\n3: while ∃ U ∈ B.tasks & ¬MTRACTABLE(U,M)\
    \ do\n4: U\n         ∗ ← CHOOSETASKTODECOMPOSE(U)\n5: Bsub ← DESIGNSUBBLUEPRINT(U\n\
    \                                   ∗\n                                    )\n\
    6: B ← EMBEDSUBBLUEPRINT(B, U∗\n                                    , Bsub)\n\
    7: end while\n8: MAS ← INSTANTIATESYSTEM()\n9: for all V ∈ B.tasks do\n10: aug\
    \ ← SELECTAUGMENTATIONS(V,M)\n11: agent ← CREATEAGENT(V,M, aug)\n12: MAS.AddAgent(agent)\n\
    13: end for\n14: IMPLEMENTPROTOCOL(MAS, B.protocol)\n15: return MAS\n16: end procedure\n\
    ```\nOur method. For a given (well-formulated) task T and a given set of LLM models\
    \ M that will be used to solve the task, we perform the following. (See Algorithm\
    \ [1](#page-2-1) and Figure [1\\)](#page-3-0)\n\n• Define the initial blueprint\
    \ B = (T ,P), where T = {T} and P is trivial.\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\
    \nFigure 1: Illustration of the Know-The-Ropes (KtR) strategy: heuristic, prior-guided\
    \ decomposition of a complex task into sub-tasks, each instantiated as a coordinated\
    \ LLM agent within a multi-agent architecture.\n\n• Guided by domain heuristics,\
    \ prior knowledge, and experiments on specific tasks, we construct an M-tractable\
    \ hierarchy\n\n$$\\mathcal{B} \\stackrel{D\\_1}{\\leadsto} \\mathcal{B}\\_1 \\\
    stackrel{D\\_2}{\\leadsto} \\dots \\stackrel{D\\_n}{\\leadsto} \\mathcal{B}\\\
    _n$$\n\n• Materialize the terminal blueprint B<sup>n</sup> as a MAS MAS(Bn,M)\
    \ that targets the initial task T.\n\nThis three-step procedure—atomic blueprint,\
    \ tractable hierarchy construction, and system instantiation—provides a principled\
    \ pathway from an arbitrarily complex task to a deployable multiagent solution\
    \ whose correctness hinges on model capabilities that have been explicitly validated.\
    \ To demonstrate the practical application and efficacy of the \"Know the Ropes\"\
    \ framework in creating such specialized MAS, we present two case studies. In\
    \ each case, a well-understood algorithmic solution to a complex problem is decomposed\
    \ into an M-tractable hierarchy and instantiated as a MAS.\n\n### 4 Experiment\
    \ Design\n\n### 4.1 Proof-of-Concept: 0/1 Knapsack Problem (KSP)\n\nTo furnish\
    \ a clear proof-of-concept for KtR, we start with the classical NP-hard Knapsack\
    \ Problem (KSP)—a staple in resource allocation, logistics, and investment planning.\
    \ By deliberately using the lightweight, general-purpose GPT-4o-mini as every\
    \ agent's backbone, we establish a modest baseline that lets us highlight how\
    \ KtR's multi-agent choreography amplifies a small model's capability well beyond\
    \ its solo limits.\n\nBelow we only present a mathematical formulation of the\
    \ problem, while a more detailed explanation of the problem as well as the algorithmic\
    \ solution can be found in Appendix [B.](#page-9-0)\n\n#### 4.1.1 Problem Formulation\n\
    \nFor a Knapsack problem of size N, its input involves a weight vector ⃗w = (w1,\
    \ · · · , w<sup>N</sup> ), a value vector ⃗v = (v1, · · · , v<sup>N</sup> ), and\
    \ a capacity W. To formulate the goal, we introduce the state vector ⃗x = (x1,\
    \ · · · , x<sup>N</sup> ) ∈ {0, 1} <sup>N</sup> , i.e., all its entries take value\
    \ in {0, 1}. Then the objective of the problem is to find the following value\n\
    \n$$Z = \\max\\_{\\vec{x} \\in \\{0, 1\\}^N \\\\ \\vec{x} \\cdot \\vec{w} \\le\
    \ W} \\vec{x} \\cdot \\vec{v}.$$\n\n### 4.1.2 KtR Multi-Agent Design\n\nFollowing\
    \ KtR heuristic, the iterative dynamic programming solution for the KSP as in\
    \ Appendix [B.2](#page-9-1) is decomposed into tasks for three specialized agents,\
    \ as presented below. Prompts designed each individual agent are attached in Appendix\
    \ [D.](#page-11-0)\n\nSystem Controller: Controller initialize a set of feasible\
    \ states S<sup>0</sup> = {(0, 0)} and controls a look on k from 1 to N. For each\
    \ k, it sends Sk−<sup>1</sup> and (wk, vk) to Worker Agent, and then send the\
    \ result plus W to the Trimmer Agent. The Controller then take the union of the\
    \ output of Trimmer Agent and Sk−<sup>1</sup> to obtain Sk. Once all items are\
    \ processed, the Controller invokes the Reporter Agent for final result.\n\nWorker\
    \ Agent: This agent is responsible for the iterative state expansion by the following\
    \ formula:\n\n$$S\\_{add} = \\{ (w + w\\_k, v + v\\_k) \\text{ for all } (w, v)\
    \ \\in S\\_{k-1} \\}.$$\n\nTrimmer Agent: This agent performs the trimming task\
    \ according to the following formula:\n\n$$S\\_{trmmed} = \\{(w, v) \\in S\\_{add}\
    \ \\mid w \\le W\\}.$$\n\nReporter Agent: This agent executes the solution report.\
    \ It find the element with maximal value within the final state set S<sup>N</sup>\
    \ .\n\n### 4.2 Proof of scalability—Task Assignment Problem (TAP)\n\nBuilding\
    \ on the previous section—where KtR already stretched the capabilities of the\
    \ compact GPT-4o-mini on the Knapsack baseline—we now test KtR's scalability.\
    \ We upgrade the backbone to the larger o3-mini and tackle the more demanding\
    \ Task-Assignment Problem (TAP), demonstrating that the framework's performance\
    \ rises in lockstep with the underlying model's capacity.\n\nAgain, below we only\
    \ present a mathematical formulation and details are referred to Appendix [B.](#page-9-0)\n\
    \n#### 4.2.1 Problem Formulation\n\nFor a Task assignment problem of size N, its\
    \ input involves an N × N cost matrix C = (Cij )N×<sup>N</sup> . We use S<sup>N</sup>\
    \ to denote the set of permutations of n elements, or equivalently, the set of\
    \ bijections from the set {1, 2, · · · , N} to itself. The objective of the problem\
    \ is to find the following value:\n\n$$Z = \\max\\_{\\sigma \\in \\mathfrak{S}\\\
    _N} \\left( \\sum\\_{i=1}^N C\\_{i\\sigma(i)} \\right).$$\n\n#### 4.2.2 KtR Multi-Agent\
    \ Design\n\nAlgorithm from Appendix [B.4](#page-10-0) now maps to a MAS under\
    \ our KtR methodology. As explained in Section [5.2.2,](#page-5-0) based on test\
    \ results of the agentic tasks and heuristic argument, we further decompose the\
    \ tasks from the original system design to further improve the performance. The\
    \ final system design contains six agents described below. Let N be the size of\
    \ the problem and C be the original cost matrix.\n\nRow Reducer: This agent reduces\
    \ rows of the matrix C to obtain C ′ .\n\nColumn Reducer: This agent further reduces\
    \ the columns of C ′ to obtain C ′′ .\n\nMatcher: This agent finds a maximal collection\
    \ of zeroes in the reduced matrix C ′′, such that no two zeroes share same row\
    \ or column. Let L be the number of zeroes in the maximal collection.\n\nPainter:\
    \ When L < N, with input from Mather, Painter is prompted for find a minimal collection\
    \ of rows and columns covering all zeroes.\n\nNormalizer: Receiving input from\
    \ Painter, Normalizer creates more zeroes outside of the selected rows and columns\
    \ to obtain an updated matrix C ′′′ .\n\nReporter: When L = N, Report sums up\
    \ values of entries of the original cost matrix C corresponding to the maximal\
    \ collection of zeroes found by Matcher, and report this sum as the final solution\
    \ to the problem.\n\nThe System Controller arranges task for Row Reducer and Column\
    \ Reducer linearly, then controls a loop: Matcher first find a set of zeroes and\
    \ the Controller checks if the number of zeroes L equals the problem size N, the\
    \ size of the problem. If so, the loop is broken and the positions of zeroes is\
    \ sent to the Reporter for final output. Otherwise, Painters are called in to\
    \ find a minimal collections of lines to cover the zeroes and Normalizer follows\
    \ to create more zeroes. Then the Controller iterates the loop and send the updated\
    \ matrix to Matcher.\n\n### 5 Experiment Result\n\nOur experimental protocol unfolds\
    \ in two stages. First, we run a uniform benchmark across a suite of baseline\
    \ models—including several GPT and Llama variants—to fix a reference point for\
    \ each task. The second stage then splits by objective: For KSP we deliver a proof\
    \ of concept, while for TAP we provide a proof of scalability. For ground truth,\
    \ we use python code to randomly generate problems, and then use the Google OR-Tools\
    \ [\\(Perron](#page-16-18) [and Furnon,](#page-16-18) [2022\\)](#page-16-18) as\
    \ in Appendix [C](#page-10-1) to generate solutions to compare with.\n\n#### 5.1\
    \ Experiment Result for KSP\n\n#### 5.1.1 Baseline LLM Performance\n\nFigure [2](#page-5-1)\
    \ shows the baseline LLM performance across multiple difficulty levels. The accuracy\
    \ across difficulty levels (from 3 to 8 items) in the KSP scenario reveals substantial\
    \ performance variation among the tested LLMs. Among those, the GPT-o3-mini, as\
    \ a reasoning model, consistently demonstrates superior accuracy. Model GPT-4.1\
    \ outperforming its smaller counterparts, namely GPT-4.1-mini and its primer GPT-4o-mini.\
    \ Other\n\nLLMs, including Claude-haiku, Llama, and Qwen series also show performance\
    \ degradation, with higher variability particularly evident at greater difficulty\
    \ levels. Meanwhile, the performance of final KtR multi-agent system is also drawn\
    \ in Figure [2.](#page-5-1) The comparison shows that KtR substantially boosts\
    \ performance, validating its effectiveness.\n\n<span id=\"page-5-1\"></span>![](_page_5_Figure_1.jpeg)\n\
    \nFigure 2: KSP baseline performance from single LLMs as well as the KtR mulit-agent\
    \ system.\n\n#### 5.1.2 Multi-Agent Performance\n\nBased on Figure [2,](#page-5-1)\
    \ GPT-4o-mini exhibits a pronounced performance decline beginning at instances\
    \ of 4 items, underscoring its limited scalability to more complex scenarios;\
    \ thus, we select it as the baseline model for our KtR framework design. Figure\
    \ [3](#page-6-0) further illustrates the resulting multi-agent system along with\
    \ the experimental outcomes derived from our proposed strategy.\n\nSingle LLM\
    \ performance. We establish two baseline performances for GPT-4o-mini acting as\
    \ a single agent to solve the KSP. First, the zeroshot GPT-4o-mini is directly\
    \ prompted with KSP instances. As Figure [3B](#page-6-0) s shows, its accuracy\
    \ decreases from 60% for 3 items to 0% (8 items). Second, we evaluate a fine-tuned\
    \ GPT-4o-mini (standalone). Figure [3C](#page-6-0) indicates that fine-tuning\
    \ offers some improvement over the zero-shot, but still as low as 3% for 8-item\
    \ KSP.\n\nStandard MAS performance. Following our KtR heuristic, we map the algorithm\
    \ for KSP into a MAS design, illustrated in Figure [3F](#page-6-0). Initially,\
    \ each agent is driven by the standard, non-fine-tuned GPT-4o-mini. The performance\
    \ of this standard MAS is presented in Figure [3F](#page-6-0). Its performance\
    \ descreases from 18% for 3 items to 4% for 8 items. This initial result implies\
    \ that without augmenting the agents' abilities, the MAS does not effectively\
    \ handle the task.\n\nWe profile each agent in isolation (Figure [3E](#page-6-0))\
    \ and uncover a single choke point: Trimmer. Its accuracy collapses as the feasible-state\
    \ set S<sup>k</sup> (cf. Section [B.2\\)](#page-9-1) grows—54 % for 1–8 states,\
    \ 24 % for 9–16, 7 % for 17–24, and just 5 % for 25–32. Because the algorithm\
    \ loops once per state, even small per-iteration errors compound, and this cascading\
    \ inaccuracy ultimately sinks the entire run.\n\nAugmented MAS performance. To\
    \ eliminate the bottleneck, we fine-tune the Trimmer's GPT-4o-mini backbone (Figure\
    \ [3G](#page-6-0), highlighted as 'Augmented Trimmer'). Accuracy leapt to 95 %\
    \ for 1–8 feasible states, 89 % for 9–16, 81 % for 17–24, and 67 % for 25–32 (Figure\
    \ [3H](#page-6-0)). Adding a lightweight self-check—prompting the model to audit\
    \ its own answer—preserved or marginally improved these gains. Replacing the bottleneck\
    \ with the fine-tuned Trimmer lifts end-to-end KSP accuracy to nearsaturation\
    \ across sizes (Figure [3I](#page-6-0)): 95 % for 3-item instances, 90 % for 4,\
    \ 95 % for 5, 85 % for 6, 76 % for 7, and 70 % for 8. A single targeted upgrade\
    \ thus turns KtR into a consistently high-performing solver as the problem scales.\n\
    \n#### 5.2 Experiment result on TAP\n\n#### 5.2.1 Baseline LLM Performance\n\n\
    Figure [4](#page-6-1) illustrates the baseline performance of multiple LLMs on\
    \ the TAP task across multiple difficulty levels (from 3 to 8 tasks). The results\
    \ reveal marked differences in model capabilities. The only reasoning model, GPT-4o-mini,\
    \ consistently outperforms all others, exhibiting strong accuracy at lower difficulty\
    \ levels, though its performance declines as task complexity increases. In contrast,\
    \ GPT-4.1 demonstrates moderate but stable accuracy across all difficulty levels,\
    \ surpassing its mini-sized counterparts. Other models, including Claude-3.5-Haiku,\
    \ Qwen2.5, and Llma-3 variants, show intermediate performance with variability.\n\
    \nWe observe that single-agent models (e.g., GPT-3-mini, GPT-4-mini, GPT-4.1)\
    \ drop to 30-50% accuracy at TAP levels 7-8, while KtR MAS maintains steady performance\
    \ near 100%, even surpassing reasoning models, demonstrating its exceptional robustness\
    \ and generalization capabilities.\n\n#### <span id=\"page-5-0\"></span>5.2.2\
    \ Multi-Agent Performance\n\nBased on Figure [4,](#page-6-1) GPT-o3-mini consistently\
    \ outperforms other LLMs across all evaluated tasks, making it our choice for\
    \ subsequent experiments. Our goal is to assess the scalability of our proposed\
    \ strategy and investigate how its performance evolves as task difficulty increases.\
    \ Figure [5](#page-7-0) illustrates the MAS design and corresponding ex-\n\n<span\
    \ id=\"page-6-0\"></span>![](_page_6_Figure_0.jpeg)\n\nFigure 3: KSP evaluation\
    \ of the KtR stategy. B: Zero-shot accuracy of the baseline model. C: Zero-shot\
    \ accuracy after a light, task-specific fine-tune of the same model. D & G: Blueprints\
    \ of the MAS without (D) and with (G) augmentations. E: Per-agent accuracies before\
    \ augmentation, revealing the system's bottleneck. H: Boost delivered by two targeted\
    \ augmentations—task-level fine-tuning and self-check prompting—applied to the\
    \ bottleneck agent. F & I: Corresponding test accuracies for the two blueprints.\n\
    \n<span id=\"page-6-1\"></span>![](_page_6_Figure_2.jpeg)\n\nFigure 4: TAP baseline\
    \ performance from single LLMs as well as the KtR mulit-agent system.\n\nperimental\
    \ outcomes obtained using our heuristicbased approach.\n\nSingle LLM performance.\
    \ Again, we evaluate the baseline performance of using o3-mini as a single agent.\
    \ The o3-mini model achieves a relatively high performance (83%) but decays quickly\
    \ as in Figure [5B](#page-7-0): 37% on problems of size 9, 21% on problems of\
    \ size 12 and finally is reduced to 3% for problems of size 15.\n\nAgent performance\
    \ and further decomposition. Guided by the Hungarian algorithm [\\(Kuhn,](#page-15-12)\
    \ [1955\\)](#page-15-12), our first KtR blueprint mapped each step to a single\
    \ agent; Step 3 from Appendix [B.4](#page-10-0) relied on\n\na lone Cover Seeker\
    \ rather than the later \"Matcher + Painter\" pair. This baseline already scored\
    \ 98 % (size 6), 88 % (size 9), 78 % (size 12), and 56 % (size 15), validating\
    \ the approach.\n\nWe then stress-tested each agent on two bands, with matrix\
    \ sizes 6-10 and 11-15, to pinpoint weaknesses. One-shot agents were flawless:\
    \ Row Reducer and Reporter reached 100 % on both bands, and Column Reducer hit\
    \ 100 % / 92 %. Normalizer held 99 % / 94 %, but Cover Seeker fell to 97 % / 84\
    \ %. Because Zero Seeker operates inside the main loop, its errors accumulate,\
    \ making it the clear bottleneck for larger TAP instances.\n\nWe then perform\
    \ a further decomposition of Step 3 in Section [B.2](#page-9-1) in a two-step\
    \ process: Step 3.1. Finding a maximal collection of zero-entries, such that no\
    \ two share a same row or column; Step 3.2. Finding a minimal collection of rows\
    \ and columns covering all zero-entries.\n\nWe believe this decomposition is helpful\
    \ due to the following reasons. First, by a mathematical argument, the size of\
    \ collections from sub-tasks 3.1 and 3.2 match. Second, a heuristic argument indicating\
    \ that knowing the maximal collection of zeroes simplifies the task to find minimal\
    \ collection\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure\
    \ 5: TAP evaluation of the KtR strategy. B: Zero-shot accuracy of the baseline\
    \ model. D: Initial blueprint derived from the Hungarian algorithm; its end-to-end\
    \ accuracy is shown in C. F: Per-agent accuracies within this blueprint, prompting\
    \ the finer decomposition outlined in E. I: Side-by-side comparison of per-agent\
    \ accuracies before and after decomposition. G: Final, decomposed blueprint, whose\
    \ overall accuracy appears in H.\n\nof rows and columns. Last, the original Step\
    \ 5 can then simply use the positions of the zeroes from Step 3.1, once optimal\
    \ check passes. Note, this also explains why we prefer a further decomposition\
    \ rather than fine-tuning the original agent, as a further decomposition improves\
    \ the system flow as well.\n\nEmpirical pays off, as shown in Figure [5I](#page-7-0),\
    \ Matcher reaches 100 % accuracy on both difficulty bands, while Painter climbs\
    \ to 98 % and 97 %—a sharp jump from the original Cover Seeker's 97 % and 84 %.\n\
    \nFinal MAS performance. Leveraging the refined decomposition, we deploy a six-agent\
    \ system (Figure [5H](#page-7-0)) that solves size 6–10 instances almost flawlessly:\
    \ almost 100 % accuracy versus 83 – 27 % for o3-mini zero-shot. It sustains high\
    \ performance on size-11–15 tasks (95 %, 97 %, 90 %, 93 %, 84 %); even the dip\
    \ at size 15 far exceeds the 3 % zero-shot baseline, highlighting the substantial\
    \ capacity gain of our multi-agent architecture.\n\n### 6 Discussion\n\nAcross\
    \ two case studies, we show that disciplined decomposition combined with diagnosis-driven\
    \ augmentation can raise low-capacity LLMs into dependable problem-solvers—and\
    \ the gains grow further as the underlying model improves.\n\nProof-of-Concept\
    \ on KSP. Baseline GPT-4omini accuracy collapsed as instance size increased, and\
    \ a naïve three-agent blueprint offered little relief. Agent-level profiling singled\
    \ out Trimmer as the lone bottleneck. Fine-tuning that agent alone—1 200 step-by-step\
    \ examples, no changes elsewhere—lifted end-to-end accuracy from ≤ 18 % to ≥ 70\
    \ % across sizes 3–8, peaking at 95 % on size 3. These results validate KtR's\
    \ \"identify → isolate → augment\" cycle: a targeted upgrade can rescue a small\
    \ general-purpose model when both single-agent and untargeted multi-agent baselines\
    \ fail.\n\nProof-of-Scalability on TAP. Repeating the procedure with the stronger\
    \ o3-mini model, we began with a five-agent design. Direct testing pinpointed\
    \ a composite planning task that capped overall performance. KtR prescribes recursive\
    \ refinement: we split that task into two simpler, typed sub-tasks, each amenable\
    \ to the base model. Accuracy on the difficult sizes (11-15) rose from as low\
    \ as 11 % single-agent to ≥ 84 % multi-agent, while sizes 6- 10 reached 100 %.\
    \ Thus, as model capacity scales,\n\nKtR continues to amplify it rather than saturate.\n\
    \nBeyond these demonstrations, we would like to also outline several research\
    \ threads that would deepen the approach in the future:\n\n• Model Portfolio Allocation.\
    \ Many leaves sit well below the flagship model's frontier; mixing lighter or\
    \ domain-tuned LLMs would slash cost while keeping accuracy, letting the controller\
    \ pick \"just-enough\" capacity per task.\n\n• Complexity–Capacity Estimation.\
    \ Replace rule-of-thumb splits with a principled score that (i) quantifies task\
    \ difficulty and (ii) predicts an LLM's post-augmentation capacity, enabling data-driven\
    \ decompositions.\n\n• End-to-End Automation. With the above metrics, the ultimate\
    \ goal is to design a system that automates the whole KtR methodology: decomposing\
    \ the tasks, evaluate capacities, and assemble the multi-agent system for solution.\n\
    \n### 7 Limitations\n\nDespite demonstrating sizable performance gains, our study\
    \ has several limitations that should guide future work.\n\nNarrow task scope.\
    \ We evaluate KtR on two canonical optimization problems (Knapsack and Task-Assignment).\
    \ While chosen to illustrate proof-of-concept and proof-of-scalability, these\
    \ tasks have well-structured objective functions and small action spaces; generalizing\
    \ to opendomain reasoning or multi-modal settings remains untested.\n\nSynthetic\
    \ data & idealized inputs. All problem instances are randomly generated and fully\
    \ specified. Real-world inputs (noisy, partially observed, or adversarial) could\
    \ degrade both decomposition quality and agent reliability.\n\nCost and latency\
    \ trade-offs. Although the peragent inference cost is trending downward, we do\
    \ not quantify absolute wall-clock latency, energy consumption, or controller\
    \ overhead for large agent swarms.\n\nBottleneck identification heuristic. We\
    \ locate bottleneck subtasks via held-out accuracy screens; this assumes the availability\
    \ of inexpensive groundtruth labels. Automated bottleneck detection without labels\
    \ is an open problem.\n\n### 8 Ethical considerations\n\nAmplifying decision-making\
    \ power by scaling agent crowds may exacerbate existing biases present in the\
    \ base models; we have not run a bias or fairness audit. Addressing these limitations—particularly\
    \ expanding to less structured domains and benchmarking real-world resource usage—will\
    \ be essential to establish the broader utility and safety of the KtR framework.\n\
    \n### A Appendix: Weighted No Free Lunch Theorem\n\nIn this section, we present\
    \ a weighted version of the No-Free-Lunch theorem. As the motivation, current\
    \ approaches to MAS design can often result in overly general solutions that may\
    \ exhibit suboptimal performance on specific and complex tasks. This sub-optimality\
    \ arises partially from a lack of domain-specific inductive bias. To formalize\
    \ this, we present a weighted variant of the No Free Lunch (NFL) theorem. The\
    \ following demonstration, leveraging a weighted variant of the No Free Lunch\
    \ theorem, quantitatively illustrates how inductive bias tailored to the target\
    \ domain enhances performance.\n\nThat is, we present a formal proof showing that,\
    \ under a non-uniform prior concentrated on a problem-specific subset of functions,\
    \ a specialized learning algorithm achieves strictly lower expected risk than\
    \ a general-purpose algorithm.\n\nNote the No-Free-Lunch theorem has been known\
    \ to research community for more than two decades. Here what we present is a modification\
    \ of the standard statement to better fit for our discussion on the MAS. As we\
    \ don't find in literature the precise version of the NFL theorem as we stated\
    \ below, we also present a proof for self-containedness. We do not claim any originality\
    \ of the theorem and the proof.\n\nTheorem A.1 (Weighted NFL). *Let* X *be a finite\
    \ input domain,* Y *a finite label set, and* F = Y X *the set of all functions*\
    \ f : X → Y *. Consider*\n\n- *a general algorithm* A<sup>0</sup> *with constant\
    \ expected loss* ε<sup>0</sup> *on every* f ∈ F*,*\n- *a specialized algorithm*\
    \ A′ *satisfying*\n\n$$L(h\\_{A'},f) \\le \\begin{cases} \\varepsilon\\_1, & f\
    \ \\in \\mathcal{F}',\\\\ \\varepsilon\\_2, & f \\notin \\mathcal{F}', \\end{cases}$$\n\
    \n*where* ε<sup>1</sup> < ε<sup>0</sup> < ε2*, and*\n\n• *a prior* P *with* P(f\
    \ ∈ F′ ) = p *and* P(f /∈ F ′ ) = 1 − p*.*\n\n$$\\boldsymbol{\\mu}$$\n\n$$p >\
    \ \\frac{\\varepsilon\\_0 - \\varepsilon\\_2}{\\varepsilon\\_1 - \\varepsilon\\\
    _2},$$\n\n*then the expected risk of* A′ *is strictly lower than that of* A0*,\
    \ i.e.*\n\n$$R(A') < R(A\\_0).$$\n\n*Proof.* By definition,\n\n$$R(A\\_0) = \\\
    mathbb{E}\\_{f \\sim P}[L(h\\_{A\\_0}, f)] = \\varepsilon\\_0.$$\n\nand\n\n$$R(A')\
    \ = \\mathbb{E}\\_{f \\sim P}[L(h\\_{A'}, f)] = p\\,\\varepsilon\\_1 + (1 - p)\\\
    ,\\varepsilon\\_2.$$\n\nHence\n\n$$\\begin{aligned} R(A') < R(A\\_0) \\Longleftrightarrow\
    \ p \\, \\varepsilon\\_1 + (1 - p) \\, \\varepsilon\\_2 < \\varepsilon\\_0 \\\\\
    \ \\Longleftrightarrow p(\\varepsilon\\_1 - \\varepsilon\\_2) > \\varepsilon\\\
    _0 - \\varepsilon\\_2, \\end{aligned}$$\n\nwhich rearranges to\n\n$$p > \\frac{\\\
    varepsilon\\_0 - \\varepsilon\\_2}{\\varepsilon\\_1 - \\varepsilon\\_2}.$$\n\n\
    This completes the proof.\n\n## <span id=\"page-9-0\"></span>B Appendix: KSP and\
    \ TAP description\n\nIn this appendix, we provide details about the KSP and TAP,\
    \ including their problem description and algorithm based on which we design our\
    \ MAS.\n\n### B.1 KSP Problem Formulation\n\nThe usual input of KSP involves a\
    \ set of N items, whose items are characterized by pairs (w<sup>i</sup> , vi)\
    \ of weights w<sup>i</sup> and values v<sup>i</sup> , as well as a capacity value\
    \ W. The goal of KSP is to find a subset of items such that the total weight does\
    \ not exceed the given capacity while the total value is maximized. Mathematically,\
    \ we record information of items by two vectors, both of dimension N: a weight\
    \ vector ⃗w = (w1, · · · , w<sup>N</sup> ) and a value vector ⃗v = (v1, · · ·\
    \ , v<sup>N</sup> ). We also introduce the set of state-vectors {0, 1} <sup>N</sup>\
    \ , whose elements are vectors ⃗x = (x1, · · · , x<sup>N</sup> ) where entries\
    \ x<sup>i</sup> takes values between 0 and 1, indicating whether an item is chosen\
    \ in a subset or not:\n\n$$x\\_i = \\begin{cases} 1 & \\text{item } i \\text{\
    \ is chosen} \\\\ 0 & \\text{item } i \\text{ is excluded} \\end{cases}$$\n\n\
    Thus state vectors controls which items is in the chosen subset, and the inner\
    \ product of ⃗x with ⃗w\n\nand ⃗v then compute the total weight and total value\
    \ for the given subset, respectively.\n\nGiven a weight vectors ⃗w, a value vector\
    \ ⃗v, and the capacity constraint W, the objective of the Knapsack problem then\
    \ can be formulated as finding the following (optimal) value:\n\n$$Z = \\max\\\
    _{\\vec{x} \\in \\{0, 1\\}^N \\atop \\vec{x} \\cdot \\vec{w} \\le W} \\vec{x}\
    \ \\cdot \\vec{v}. \\tag{1}$$\n\nHere the maximal value is taken over all state\
    \ vectors (or equivalently, all subsets of items) satisfying the constraint that\
    \ the total weight ⃗x · ⃗w not exceeding the capacity W.\n\nThis version of the\
    \ problem, where each item can either be fully included or not at all, is commonly\
    \ known as the 0/1 KSP.\n\n### <span id=\"page-9-1\"></span>B.2 KSP Problem Solution\n\
    \nA classic approach to the Knapsack Problem iteratively enumerates all feasible\
    \ states—a dynamicprogramming strategy first introduced by Bellman [\\(Bellman,](#page-15-13)\
    \ [1957\\)](#page-15-13). A feasible state can be defined by a pair (current\\\
    _weight, current\\_value) representing the accumulated weight and value of a set\
    \ of items selected so far, such that current\\_weight ≤ W. We can describe the\
    \ algorithm in the form of mathematical induction. We start with the initial set\
    \ of feasible states S<sup>0</sup> = {(0, 0)}, representing an empty set of chosen\
    \ items. We then add items in to form a set S<sup>k</sup> from Sk−<sup>1</sup>\
    \ inductively, with capacity being aware: for each k, assuming that Sk−<sup>1</sup>\
    \ has been constructed, then we add the pair (wk, vk) to all items in Sk−<sup>1</sup>\
    \ to form a new set Sadd:\n\n$$S\\_{add} = \\{ (w + w\\_k, v + v\\_k) \\text{\
    \ for all } (w, v) \\in S\\_{k-1} \\}.$$\n\nThen, we trim the set according to\
    \ the capacity:\n\n$$S\\_{trmmed} = \\{(w, v) \\in S\\_{add} \\mid w \\le W\\\
    }.$$\n\nNote this also removes all repetitive states in the set. Finally we take\
    \ the union of the two intermediate sets to create Sk:\n\n$$S\\_k = S\\_{k-1}\
    \ \\cup S\\_{trmmed} \\cdot$$\n\nThe inductive step terminate when we have run\
    \ through all items and obtaining the final set S<sup>N</sup> , we pick the element\
    \ in S<sup>N</sup> with maximal value, as the solution to the KSP. Explicitly,\n\
    \n$$Z = \\max\\_{(w,v)\\in S\\_N} v$$\n\n#### B.3 TAP Problem Formulation\n\n\
    TAP seeks to optimally assign a set of N resources (agents or workers) to N tasks,\
    \ where each potential assignment incurs a specific cost. With the constraint\
    \ that each resource can only be assigned to one unique task, the objective of\
    \ TAP is to find an assignment covering all tasks that minimizes the total cost.\
    \ The resource-task specific cost is recorded in an N × N matrix C, where the\
    \ entry Cij represents the cost associated with assigning resource i to task j,\
    \ for i, j ∈ {1, 2, . . . , N}.\n\nTo formally define the problem, we introduce\
    \ a set S<sup>n</sup> which can be described in either one of the following three\
    \ equivalent ways:\n\n• The group of automorphisms of the set N = {1, 2, · · ·\
    \ , N}.\n\n• The set (or group) of bijections from the set N to itself.\n\n• The\
    \ set of all permutations involving N elements.\n\nNote elements in S<sup>N</sup>\
    \ convey the idea that each resource is assigned to a unique task.\n\nNow, given\
    \ the N × N cost matrix C, the objective of the TAP is to find the following (optimized)\
    \ value\n\n$$Z = \\max\\_{\\sigma \\in \\mathfrak{S}\\_N} \\left( \\sum\\_{i=1}^N\
    \ C\\_{i\\sigma(i)} \\right).$$\n\nNote when we treat σ ∈ S<sup>N</sup> as a (bijective)\
    \ map from N = {1, 2, · · · , N} to itself, the notation Ciσ(i) represents the\
    \ entry on the i-th row and σ(i) th column of the cost matrix C.\n\n#### <span\
    \ id=\"page-10-0\"></span>B.4 TAP Problem Solution\n\nThe typical solution for\
    \ TAP is using the Hungarian algorithm [\\(Kuhn,](#page-15-12) [1955\\)](#page-15-12),\
    \ which provides a polynomial-time method to find the objective value Z. We summarize\
    \ the algorithm as follows.\n\nStep 1. Row Reduction. For each row, we find the\
    \ minimal element in the row and subtract it from all entries in the row, creating\
    \ at least one zero on each row. Mathematically, starting from the original cost\
    \ matrix CN×<sup>N</sup> , we create a new reduced matrix C ′ such that for i,\
    \ j ∈ {1, 2, · · · , N}, we have\n\n$$C'\\_{ij} = C\\_{ij} - \\min\\_{1 \\le k\
    \ \\le N} C\\_{ik}$$\n\nStep 2. Column Reduction. Similarly, we further reduce\
    \ C ′ to C ′′ as follows: For each column, we find the minimal element in the\
    \ column and subtract it from all entries in the column, guaranteeing\n\nat least\
    \ one zero on each column. Mathematically, take\n\n$$C\\_{ij}^{\\prime\\prime}\
    \ = C\\_{ij}^{\\prime} - \\min\\_{1 \\le k \\le N} C\\_{kj}$$\n\nStep 3. Find\
    \ covering lines. We then find a smallest collection of rows and columns to cover\
    \ all zeroes. Here smallest is the sense of the number of elements in the collection\
    \ (of rows and column), over all possible such collections. If the size of this\
    \ minimal collection, denoted by L, coincides with N, the size of the problem,\
    \ then we skip Step 4 to enter the final stage of the algorithm.\n\nStep 4. Matrix\
    \ Improvement. However, if L < N, we need an improvement for the matrix C ′′ before\
    \ looping back to Step 3: given the minimal collection of rows and columns from\
    \ Step 3, we find the minimal value of all entries that are not covered, and then\
    \ subtract this minimal value from all uncovered entries of C ′′, and then add\
    \ this minimal value to all entries of C ′′ that are covered twice, i.e., by both\
    \ rows and columns. Let C ′′′ be the resulting matrix.\n\nStep 5. Assignment Identification.\
    \ Once the condition L = N is met, the final step is to identify the optimal assignment.\
    \ This involves selecting a set of N independent zeros from the current matrix\
    \ C ′′′, such that no two selected zeros share the same row or column. Each selected\
    \ zero at position (i, j) corresponds to assigning agent i to task j. The total\
    \ cost of this optimal assignment is then calculated by summing the costs from\
    \ the original cost matrix C corresponding to these selected zero positions.\n\
    \nA non-trivial fact guaranteed by the Hungarian algorithm is that, in Step 5\
    \ the collection of zeroes might not be unique, while different collections are\
    \ deemed to result in the same total summation of corresponding entries in the\
    \ original cost matrix C.\n\n### <span id=\"page-10-1\"></span>C Ground-truth\
    \ Data Preparation\n\nWe utilize Google OR-Tools [\\(Perron and Furnon,](#page-16-18)\
    \ [2022\\)](#page-16-18) to generate optimal solutions—serving as ground-truth\
    \ datasets—for both problem scenarios. OR-Tools is a widely adopted open-source\
    \ software suite developed by Google for solving combinatorial optimization problems.\
    \ It is renowned for its efficiency and reliability in addressing NPhard challenges\
    \ through advanced optimization algorithms. The suite is distributed under the\
    \ permissive Apache License 2.0, allowing unrestricted use, modification, and\
    \ distribution [\\(Perron and Furnon,](#page-16-18) [2022\\)](#page-16-18).\n\n\
    For KSP, we generate random instances by assigning weights and values to items\
    \ along with a maximum capacity constraint. Optimal solutions are then computed\
    \ using OR-Tools' dynamic programming approach. For TAP, we similarly generate\
    \ random cost matrices that represent the cost of assigning workers to tasks.\
    \ Optimal assignments are obtained using the Hungarian algorithm as implemented\
    \ in OR-Tools, which efficiently minimizes the total assignment cost.\n\n## <span\
    \ id=\"page-11-0\"></span>D Appendix: Prompt gallery\n\nNote that all prompts\
    \ we presented in the following, except for the self-check prompt for Trimmer\
    \ Agent in KSP problem, are the system prompt for agents. The user prompt will\
    \ only contain the precise problem to be handled by the agent in the form specified\
    \ by the prompt.\n\n## D.1 KSP prompts\n\n## D.1.1 Prompt for zero shot\n\nYou\
    \ are an expert in the field of Knapsack Problem.\n\nYou are given a Knapsack\
    \ Problem in the json format, of the following form:\n\n```\n{\n  \"id\" : str,\n\
    \  \"items\" : list of pairs of integers,\n  \"capacity\" : int\n}\n```\nEach\
    \ pair in the list is a pair of integers of the form [weight, value], i.e., the\
    \ first entry is the weight and the second entry is the value.\n\nYour task is\
    \ to solve the Knapsack Problem and provide the optimal solution. That is, you\
    \ need to find a subset of the pairs that maximizes the total value, subject to\
    \ the constraint that the total weight of the subset is less than or equal to\
    \ the capacity.\n\nPlease think step by step when solving the problem.\n\nYou\
    \ need to return the optimal solution in the following json format:\n\n```\n{\n\
    \  \"max_value\" : int,\n}\n```\nPlease only return the json format, nothing else.\n\
    \n# D.1.2 Prompt for Worker Agent\n\nYou are a key member of a multi−agent team\
    \ collaboratively solving the Knapsack Problem. Your specific role is the Worker,\
    \ responsible for performing mathematical computations for the team.\n\nYou will\
    \ receive input in the following JSON format:\n\n{\"c\\_list\": [[int, int], ...],\
    \ \"s\\_item\": [int, int]}\n\nEach pair within 'c\\_list' contains two integers.\n\
    \n## Your task is to:\n\n− Add 's\\_item' to each pair in 'c\\_list' entry−wise.\
    \ For instance, if a pair in 'c\\_list' is '[2, 5]' and 's\\_item' is '[3, 4]',\
    \ the result should be '[2+3, 5+4] = [5, 9]'.\n\nTo ensure accuracy:\n\n- − Proceed\
    \ systematically, applying step−by−step reasoning.\n- − Carefully perform each\
    \ addition individually for all pairs provided in the list.\n\nYour response must\
    \ strictly follow this JSON format:\n\n{\"n\\_list\": [[int, int], ...]}\n\nReturn\
    \ only the specified JSON object without any additional commentary or text.\n\n\
    # D.1.3 Prompt for Trimmer Agent\n\nYou are a key member of a multi−agent team\
    \ collaboratively solving the Knapsack Problem. Your specific role is the Trimmer,\
    \ responsible for trimming the list based on the given capacity constraint.\n\n\
    You will receive input in the following JSON format:\n\n{\"n\\_list\": [[int,\
    \ int], ...], \"capacity\": int}\n\nEach pair within 'n\\_list' contains two integers:\
    \ the first integer represents the weight, and the second integer represents the\
    \ value.\n\nYour task is to:\n\n− Carefully analyze each pair in the provided\
    \ list.\n\n- − Remove all pairs whose weight (the first integer ) strictly exceeds\
    \ the specified capacity.\n- − If identical pairs appear multiple times, retain\
    \ only one instance of each.\n\nTo ensure accuracy:\n\n- − Proceed systematically,\
    \ applying step−by−step reasoning.\n- − Verify each pair carefully against the\
    \ capacity constraint.\n\nYour response must strictly follow this JSON format:\n\
    \n{\"t\\_list\": [[int, int], ...]}\n\nReturn only the specified JSON object without\
    \ any additional commentary or text.\n\n### D.1.4 Prompt for Reporter Agent\n\n\
    You are a key member of a multi−agent team collaboratively solving the Knapsack\
    \ Problem. Your specific role is the Reporter, responsible for determining and\
    \ clearly reporting the final answer based on the provided information.\n\nYou\
    \ will receive input in the following JSON format:\n\n{\"c\\_list\": [[int, int],\
    \ ...]}\n\n- Each pair within 'c\\_list' contains two integers: the first integer\
    \ represents the weight, and the second integer represents the value.\n- Your\
    \ task is to carefully analyze this list, identify the pair with the maximal value\
    \ (the second integer in each pair), and report only that maximal value. If the\
    \ list is empty, then report the maximal value as 0.\n\nTo ensure accuracy:\n\n\
    - − Proceed systematically, applying step−by−step reasoning.\n- − Carefully examine\
    \ every pair in the provided list.\n\nYour response must strictly follow this\
    \ JSON format: {\"max\\_value\": int}\n\nReturn only the JSON object as specified\
    \ above, without any additional commentary or text.\n\n## D.1.5 Self-check prompt\
    \ for Trimmer Agent\n\nTo better fulfill your task, please conduct a double check\
    \ on the result you just provided. If your answer is already correct, please confirm\
    \ by copying the last output.\n\nWhen double check, please pay attention to the\
    \ following typical types of mistakes:\n\n- In particular, please check if you\
    \ made any typical mistakes as listed below:\n- 1. If you added in a pair that\
    \ is not in the original n\\_list.\n- 2. If there is still a pair in the t\\_list\
    \ that still exceeds the capacity.\n- 3. If there is a pair in n\\_list that does\
    \ not exceed the capacity but is not in the t\\_list.\n- If you found any errors,\
    \ please create a corrected answer.\n\n## D.2 TAP prompts\n\n{\n\n}\n\n## D.2.1\
    \ Prompt for zero shot\n\n- You are an expert in solving the Assignment Problem.\
    \ In the assignment problem, there are n workers and n jobs. Each worker has a\
    \ cost of assigning to each job. Each worker can only be assigned to one job.\
    \ Your task is to find the optimal assignment of workers to jobs that minimizes\
    \ the total cost.\n- You are given the problem in the following json format:\n\
    \n```\n\"id\" : str,\n\"cost_matrix\" : list of lists of integers\n```\nThe cost\
    \ matrix is a square matrix of size n x n, where n is the number of workers and\
    \ jobs, in the form of a nested list [[int, int, ...], [int, int, ...], ...].\
    \ The (i, j)th entry of the matrix represents the cost of assigning the ith worker\
    \ to the jth job.\n\nIn either case, please follow the format requirement of the\
    \ output.\n\nYour task is to find the optimal assignment of workers to jobs that\
    \ minimizes the total cost.\n\n```\nPlease think step by step when solving the\n\
    \    problem.\n```\n\n```\nYou need to return the optimal assignment in the\n\
    \    following json format:\n```\n\n```\n{\n  \"optimal_cost\" : int\n}\n```\n\
    Please only return the json format, nothing else.\n\n## D.2.2 Prompt for Row Reducer\
    \ Agent\n\n```\nYou are given a matrix in the following json\n    format:\n```\n\
    \n```\n{\n  \"matrix\" : list of lists of integers\n}\n```\n\n```\nThe matrix\
    \ is in the form of a nested list [[int, int,\n      ...], [int, int, ...], ...].\n\
    ```\n\n```\nYour task is to reduce the matrix by subtracting\n    the minimum\
    \ value of each row from all the\n    elements in that row.\n```\n\n```\nPlease\
    \ think step by step when solving the\n    problem:\n```\nStep 0: Work on one\
    \ row at a time.\n\n- Step 1: Find the minimum value of the row.\n- Step 2: Subtract\
    \ the minimum value of the row from all the elements in that row.\n- Step 3: Return\
    \ the reduced matrix in the following json format:\n\n{\"reduced\\_matrix\" :\
    \ list of lists of integers}\n\nPlease only return the json format, nothing else.\n\
    \n# D.2.3 Prompt for Column Reducer Agent\n\nYou are given a matrix in the following\
    \ json format:\n\n```\n{\n  \"matrix\" : list of lists of integers\n}\n```\nThe\
    \ matrix is in the form of a nested list [[int, int, ...], [int, int, ...], ...].\n\
    \nYour task is to reduce the matrix by subtracting the minimum value of each column\
    \ from all the elements in that column.\n\nPlease think step by step when solving\
    \ the problem: Step 0: Work on one column at a time. Step 1: Find the minimum\
    \ value of the column. Step 2: Subtract the minimum value of the column from all\
    \ the elements in that column. Step 3: Return the reduced matrix in the following\
    \ json format:\n\n{\"reduced\\_matrix\" : list of lists of integers}\n\nPlease\
    \ only return the json format, nothing else.\n\n# D.2.4 Prompt for Cover Seeker\
    \ Agent\n\n```\nYou are given a problem in the following json\n    format:\n```\n\
    { \"matrix\" : list of lists of integers }\n\nThe matrix is in the form of a nested\
    \ list [[int, int, ...], [int, int, ...], ...].\n\n```\nYour task is to find a\
    \ smallest collection of rows\n    and columns of the matrix, such that any\n\
    \    zeroes in the matrix is contained in a chosen\n    row or column. Small means\
    \ the sum of the\n    sizes of the row and column collections is\n    the smallest\
    \ possible.\n```\n\n```\nPlease think step by step when solving the\n    problem,\
    \ and return your response in the\n    following json format:\n```\n\n```\n{\"\
    collum_collection\" : [int, int, ...], \"\n     row_collection\" : [int, int,\
    \ ...]}\n```\nThe integers in the collum\\_collection and row\\_collection are\
    \ the indices of the rows and columns that you choose.\n\nPlease only return the\
    \ json format, nothing else.\n\n### D.2.5 Prompt for Matcher Agent\n\nYou are\
    \ given a matrix in the following json format:\n\n```\n{\n  \"matrix\" : list\
    \ of lists of integers\n}\n```\n- The matrix is in the form of a nested list [[int,\
    \ int, ...], [int, int, ...], ...].\n- Your task is to find the largest collection\
    \ of zeroes in the matrix, such that no two zeroes are in the same row or column.\n\
    - Please think step by step when solving the problem, and return your response\
    \ in the following json format:\n\n{\"largest\\_collection\" : [[int, int], [int,\
    \ int], ...]}\n\nThe list of pairs of integers is in the form of [[ row\\_index,\
    \ column\\_index], [row\\_index, column\\_index], ...].\n\nPlease only return\
    \ the json format, nothing else.\n\n## D.2.6 Prompt for Painter Agent\n\n{\n\n\
    }\n\nYou are given a problem in the following json format:\n\n\"matrix\" : list\
    \ of lists of integers \"collection\" : list of lists of integers\n\n- The matrix\
    \ is in the form of a nested list [[int, int, ...], [int, int, ...], ...].\n-\
    \ The collection is in the form of a nested list [[int, int], [int, int], ...].\n\
    - Your task is to find a smallest collection of rows and columns of the matrix,\
    \ such that any zeroes in the matrix is contained in a chosen row or column. Small\
    \ means the sum of the sizes of the row and column collections is the smallest\
    \ possible.\n- To assist you, you are provided with a collection of zeroes in\
    \ the input json format. The\n\ncollection contains the positions of a maximal\
    \ collection of zeroes in the matrix, such that no two zeroes are in the same\
    \ row or column.\n\n- Please use this collection of zeroes to find the rows and\
    \ columns as desired. More precisely, you should first choose one row or column\
    \ for each zero in the collection, such that the chosen rows and columns cover\
    \ as much of the zeroes in the matrix as possible. Then add in more rows or columns\
    \ if needed.\n- Please think step by step when solving the problem, and return\
    \ your response in the following json format:\n\n{\"collum\\_collection\" : [int,\
    \ int, ...], \" row\\_collection\" : [int, int, ...]}\n\nThe integers in the collum\\\
    _collection and row\\_collection are the indices of the rows and columns that\
    \ you choose.\n\nPlease only return the json format, nothing else.\n\n## D.2.7\
    \ Prompt for Normalizer Agent\n\nYou are given a problem in the following json\
    \ format:\n\n\"matrix\" : list of lists of integers \"collumn\\_collection\" :\
    \ list of integers \"row\\_collection\" : list of integers\n\n}\n\n{\n\nThe matrix\
    \ is in the form of a nested list [[int, int, ...], [int, int, ...], ...].\n\n\
    The collumn\\_collection and row\\_collection are the indices of some selected\
    \ rows and columns that covers all the zeroes in the matrix.\n\nYour task is the\
    \ following:\n\n- 1. Find the minimal value in the matrix that is not covered\
    \ by the selected rows and columns.\n- 2. If this value is 0, return the original\
    \ matrix.\n- 3. If this value is not 0, subtract this value from all uncovered\
    \ entries in the matrix.\n- 4. For the entries that covered by both a selected\
    \ row and a selected column, add this value to\n\nthe entries.\n\n- 5. For the\
    \ entries that are covered by a selected row or column, but not both, do nothing.\n\
    - 6. Please return the updated matrix in the following json format:\n\n{\"normalized\\\
    _matrix\" : list of lists of integers}\n\nPlease only return the json format,\
    \ nothing else.\n\n#### D.2.8 Prompt for Reporter Agent\n\nYou are given a problem\
    \ in the following json format:\n\n{ \"matrix\" : list of lists of integers \"\
    collection\" : list of lists of integers }\n\n- The matrix is in the form of a\
    \ nested list [[int, int, ...], [int, int, ...], ...].\n- The collection contains\
    \ a set of entries of the matrix in the form of [[row\\_index, column\\_index],\
    \ [row\\_index, column\\_index], ...].\n\nYour task is the following:\n\n- 1.\
    \ Sum up the values of all the entries in the collection.\n- 2. Return the total\
    \ value in the following json format:\n\n{\"total\\_value\" : int}\n\nPlease only\
    \ return the json format, nothing else.\n\n### References\n\n- <span id=\"page-15-10\"\
    ></span>Anonymous. 2025. [Code in harmony: Evaluating multi](https://openreview.net/forum?id=URUMBfrHFy)[agent\
    \ frameworks.](https://openreview.net/forum?id=URUMBfrHFy) In *Submitted to CS598\
    \ LLM Agent 2025 Workshop*. Under review.\n- <span id=\"page-15-13\"></span>Richard\
    \ Bellman. 1957. *Dynamic Programming*. Princeton University Press, Princeton,\
    \ NJ, USA.\n- <span id=\"page-15-2\"></span>Xiaohe Bo, Zeyu Zhang, Quanyu Dai,\
    \ Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. 2024. Reflective multi-agent\
    \ collaboration based on large language models. *Advances in Neural Information\
    \ Processing Systems*, 37:138595–138631.\n- <span id=\"page-15-9\"></span>Mert\
    \ Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh\
    \ Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, and\
    \ 1 others. 2025. Why do\n\nmulti-agent llm systems fail? *arXiv preprint arXiv:2503.13657*.\n\
    \n- <span id=\"page-15-1\"></span>Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi\
    \ Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024.\
    \ Large language model based multi-agents: A survey of progress and challenges.\
    \ *arXiv preprint arXiv:2402.01680*.\n- <span id=\"page-15-8\"></span>Sirui Hong,\
    \ Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\
    \ Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, and 1 others. 2023. Metagpt: Meta\
    \ programming for multi-agent collaborative framework. *arXiv preprint arXiv:2308.00352*,\
    \ 3(4):6.\n- <span id=\"page-15-6\"></span>Wenyue Hua, Lizhou Fan, Lingyao Li,\
    \ Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. 2023.\
    \ War and peace (waragent): Large language model-based multi-agent simulation\
    \ of world wars. *arXiv preprint arXiv:2311.17227*.\n- <span id=\"page-15-0\"\
    ></span>Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna\
    \ Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier,\
    \ and 1 others. 2023. Chatgpt for good? on opportunities and challenges of large\
    \ language models for education. *Learning and individual differences*, 103:102274.\n\
    - <span id=\"page-15-12\"></span>Harold W. Kuhn. 1955. [The hungarian method for\
    \ the](https://doi.org/10.1002/nav.3800020109) [assignment problem.](https://doi.org/10.1002/nav.3800020109)\
    \ *Naval Research Logistics Quarterly*, 2(1-2):83–97.\n- <span id=\"page-15-7\"\
    ></span>Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\
    \ Ghanem. 2023. Camel: Communicative agents for\" mind\" exploration of large\
    \ language model society. *Advances in Neural Information Processing Systems*,\
    \ 36:51991–52008.\n- <span id=\"page-15-11\"></span>Hang Li, Tianlong Xu, Ethan\
    \ Chang, and Qingsong Wen. 2025. Knowledge tagging with large language model based\
    \ multi-agent system. In *Proceedings of the AAAI Conference on Artificial Intelligence*,\
    \ volume 39, pages 28775–28782.\n- <span id=\"page-15-5\"></span>Shuhang Lin,\
    \ Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu\
    \ Jin, Jiebo Luo, and Yongfeng Zhang. 2024. [BattleAgent:](https://doi.org/10.18653/v1/2024.emnlp-demo.18)\
    \ [Multi-modal dynamic emulation on historical battles](https://doi.org/10.18653/v1/2024.emnlp-demo.18)\
    \ [to complement historical analysis.](https://doi.org/10.18653/v1/2024.emnlp-demo.18)\
    \ In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language\
    \ Processing: System Demonstrations*, pages 172–181, Miami, Florida, USA. Association\
    \ for Computational Linguistics.\n- <span id=\"page-15-4\"></span>Hao Ma, Tianyi\
    \ Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. 2024. Coevolving\
    \ with the other you: Fine-tuning llm with sequential cooperative multi-agent\
    \ reinforcement learning. *Advances in Neural Information Processing Systems*,\
    \ 37:15497–15525.\n- <span id=\"page-15-3\"></span>Melissa Z Pan, Mert Cemri,\
    \ Lakshya A Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer,\
    \ Aditya Parameswaran, Kannan Ramchandran, Dan\n\nKlein, and 1 others. 2025. Why\
    \ do multiagent systems fail? In *ICLR 2025 Workshop on Building Trust in Language\
    \ Models and Applications*.\n\n- <span id=\"page-16-18\"></span>Laurent Perron\
    \ and Vincent Furnon. 2022. Google OR-Tools. [https://developers.google.com/](https://developers.google.com/optimization)\
    \ [optimization](https://developers.google.com/optimization).\n- <span id=\"page-16-11\"\
    ></span>Jianing Qiu, Kyle Lam, Guohao Li, Amish Acharya, Tien Yin Wong, Ara Darzi,\
    \ Wu Yuan, and Eric J Topol. 2024. Llm-based agentic systems in medicine and healthcare.\
    \ *Nature Machine Intelligence*, 6(12):1418–1420.\n- <span id=\"page-16-7\"></span>Raphael\
    \ Shu, Nilaksh Das, Michelle Yuan, Monica Sunkara, and Yi Zhang. 2024. Towards\
    \ effective genai multi-agent collaboration: Design and evaluation for enterprise\
    \ applications. *arXiv preprint arXiv:2412.05449*.\n- <span id=\"page-16-0\"></span>Arun\
    \ James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,\
    \ Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models in medicine.\
    \ *Nature medicine*, 29(8):1930– 1940.\n- <span id=\"page-16-16\"></span>Khanh-Tung\
    \ Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O'Sullivan, and Hoang\
    \ D Nguyen. 2025. Multi-agent collaboration mechanisms: A survey of llms. *arXiv\
    \ preprint arXiv:2501.06322*.\n- <span id=\"page-16-4\"></span>Junlin Wang, Jue\
    \ Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024. Mixture-of-agents enhances\
    \ large language model capabilities. *arXiv preprint arXiv:2406.04692*.\n- <span\
    \ id=\"page-16-17\"></span>Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao\
    \ Zhang, Jie Liu, and Min Zhang. 2025. Agentdropout: Dynamic agent elimination\
    \ for tokenefficient and high-performance llm-based multi-agent collaboration.\
    \ *arXiv preprint arXiv:2503.18891*.\n- <span id=\"page-16-8\"></span>Jason Wei,\
    \ Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\
    \ Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large\
    \ language models. *Advances in neural information processing systems*, 35:24824–\
    \ 24837.\n- <span id=\"page-16-10\"></span>David H Wolpert. 2021. What is important\
    \ about the no free lunch theorems? In *Black box optimization, machine learning,\
    \ and no-free lunch theorems*, pages 373–388. Springer.\n- <span id=\"page-16-9\"\
    ></span>David H Wolpert and William G Macready. 1997. No free lunch theorems for\
    \ optimization. *IEEE transactions on evolutionary computation*, 1(1):67–82.\n\
    - <span id=\"page-16-14\"></span>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran\
    \ Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu,\
    \ and 1 others. 2023a. Autogen: Enabling next-gen llm applications via multi-agent\
    \ conversation. *arXiv preprint arXiv:2308.08155*.\n- <span id=\"page-16-1\"></span>Shijie\
    \ Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann,\
    \ Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023b. Bloomberggpt:\
    \ A large language model for finance. *arXiv preprint arXiv:2303.17564*.\n- <span\
    \ id=\"page-16-3\"></span>Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.\
    \ Hallucination is inevitable: An innate limitation of large language models.\
    \ *arXiv preprint arXiv:2401.11817*.\n- <span id=\"page-16-12\"></span>Yuwei Yan,\
    \ Qingbin Zeng, Zhiheng Zheng, Jingzhe Yuan, Jie Feng, Jun Zhang, Fengli Xu, and\
    \ Yong Li. 2024. Opencity: A scalable platform to simulate urban activities with\
    \ massive llm agents. *arXiv preprint arXiv:2410.21286*.\n- <span id=\"page-16-6\"\
    ></span>Ye Ye. 2025. Task memory engine (tme): Enhancing state awareness for multi-step\
    \ llm agent tasks. *arXiv preprint arXiv:2504.08525*.\n- <span id=\"page-16-13\"\
    ></span>Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi,\
    \ Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, and 1 others. 2024. Aipatient:\
    \ Simulating patients with ehrs and llm powered agentic workflow. *arXiv preprint\
    \ arXiv:2409.18924*.\n- <span id=\"page-16-15\"></span>Bowen Zhang and Pengcheng\
    \ Luo. 2025. Or-llm-agent: Automating modeling and solving of operations research\
    \ optimization problem with reasoning large language model. *arXiv preprint arXiv:2503.10009*.\n\
    - <span id=\"page-16-2\"></span>Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister,\
    \ Rui Zhang, and Sercan Arik. 2024. Chain of agents: Large language models collaborating\
    \ on long-context tasks. *Advances in Neural Information Processing Systems*,\
    \ 37:132208–132237.\n- <span id=\"page-16-5\"></span>Kunlun Zhu, Hongyi Du, Zhaochen\
    \ Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru\
    \ Tang, Heng Ji, and 1 others. 2025. Multiagentbench: Evaluating the collaboration\
    \ and competition of llm agents. *arXiv preprint arXiv:2503.01935*."
- title: "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n \
    \ Generation"
  abstract: 'Grammar plays a critical role in natural language processing and text/code

    generation by enabling the definition of syntax, the creation of parsers, and

    guiding structured outputs. Although large language models (LLMs) demonstrate

    impressive capabilities across domains, their ability to infer and generate

    grammars has not yet been thoroughly explored. In this paper, we aim to study

    and improve the ability of LLMs for few-shot grammar generation, where grammars

    are inferred from sets of a small number of positive and negative examples and

    generated in Backus-Naur Form. To explore this, we introduced a novel dataset

    comprising 540 structured grammar generation challenges, devised 6 metrics, and

    evaluated 8 various LLMs against it. Our findings reveal that existing LLMs

    perform sub-optimally in grammar generation. To address this, we propose an

    LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar

    generation. HyGenar achieves substantial improvements in both the syntactic and

    semantic correctness of generated grammars across LLMs.'
  url: http://arxiv.org/abs/2505.16978v1
  keywords: ''
  document: '# HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar
    Generation


    Weizhi Tang, Yixuan Li, Chris Sypherd, Elizabeth Polgreen, Vaishak Belle University
    of Edinburgh


    > [Weizhi.Tang@ed.ac.uk](mailto:email@domain) , [yixuan.li.cs@ed.ac.uk](mailto:email@domain)
    , [c.n.sypherd@sms.ed.ac.uk](mailto:email@domain) , [elizabeth.polgreen@ed.ac.uk](mailto:email@domain)
    , [vbelle@ed.ac.uk](mailto:email@domain)


    ### Abstract


    Grammar plays a critical role in natural language processing and text/code generation
    by enabling the definition of syntax, the creation of parsers, and guiding structured
    outputs. Although large language models (LLMs) demonstrate impressive capabilities
    across domains, their ability to infer and generate grammars has not yet been
    thoroughly explored. In this paper, we aim to study and improve the ability of
    LLMs for few-shot grammar generation, where grammars are inferred from sets of
    a small number of positive and negative examples and generated in Backus-Naur
    Form. To explore this, we introduced a novel dataset comprising 540 structured
    grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs
    against it. Our findings reveal that existing LLMs perform sub-optimally in grammar
    generation. To address this, we propose an LLMdriven hybrid genetic algorithm,
    namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements
    in both the syntactic and semantic correctness of generated grammars across LLMs
    [1](#page-0-0) . fibonacci(9)


    ### 1 Introduction


    Grammar inference, also known as grammar induction, consists of inferring a grammar
    from a set of examples [\(Horning](#page-10-0) , [1969](#page-10-0) ; [De la Higuera](#page-9-0)
    , [2010](#page-9-0) ; [Stevenson and Cordy](#page-12-0) , [2014b](#page-12-0)
    ; [D''Ulizia et al.](#page-9-1) , [2011\)](#page-9-1). It has been studied and
    used in various fields, such as natural language processing, where it can reduce
    the effort required to generate syntactic or semantic models automatically [\(Kai
    et al.](#page-10-1) , [2024](#page-10-1) ; [D''ulizia et al.](#page-9-2) , [2011\)](#page-9-2),
    and software engineering, where inferred grammars can guide reverse engineering
    and automated parser generation [\(Steven](#page-12-1)[son and Cordy,](#page-12-1)
    [2014a\)](#page-12-1). By relying on characteristic examples [\(De la Higuera](#page-9-0)
    , [2010\)](#page-9-0), grammar infer-


    <span id="page-0-1"></span>![](_page_0_Figure_8.jpeg)


    Figure 1: Given a small set of positive and negative examples, LLMs should infer
    and generate a grammar that accepts all positives and rejects all negatives.


    ence enables automated discovery of underlying syntactic or structural patterns.


    Backus-Naur Form (BNF) is used to define the grammar of formal languages [\(Chomsky](#page-9-3)
    , [1956](#page-9-3) ; [Backus](#page-8-0) , [1959](#page-8-0) ; [Backus et al.](#page-8-1)
    , [1960\)](#page-8-1), typically Context-Free Grammars (CGFs) [\(Chomsky](#page-9-3)
    , [1956](#page-9-3) ; [Aho](#page-8-2) , [2007\)](#page-8-2). It has also been
    used in various ways such as in parser generators like ANTLR4 [\(Parr](#page-12-2)
    , [2013\)](#page-12-2) and Yacc [\(Johnson and Hill](#page-10-2) , [1978\)](#page-10-2)
    or in constraining output structure of Large Language Models (LLMs) [\(Willard
    and Louf](#page-12-3) , [2023](#page-12-3) ; [Beurer-](#page-8-3)[Kellner et al.,](#page-8-3)
    [2024\)](#page-8-3).


    Although LLMs have exhibited remarkable capabilities across diverse domains [\(Gaur
    and Saunshi,](#page-9-4) [2023](#page-9-4) ; [Imani et al.](#page-10-3) , [2023](#page-10-3)
    ; [Pan et al.](#page-12-4) , [2023](#page-12-4) ; [Tang and](#page-12-5) [Belle](#page-12-5)
    , [2024](#page-12-5) ; [Li et al.](#page-11-0) , [2024](#page-11-0) ; [Jiang et
    al.](#page-10-4) , [2024a\)](#page-10-4), their capacity for grammar inference,
    and partic-


    <span id="page-0-0"></span><sup>1</sup>The code is open-source and available at
    [https://](https://github.com/RutaTang/HyGenar) [github.com/RutaTang/HyGenar](https://github.com/RutaTang/HyGenar)
    .


    ularly for generating grammars in BNF, has not yet been well explored. This paper
    focuses on investigating and improving the ability of LLMs to infer a CFG and
    generate it in BNF from a given set of positive and negative examples. A correctly
    generated grammar should accept all positives and reject all negatives. Typically,
    grammar inference requires a large set of characteristic examples that can uniquely
    identify a CFG [\(De la Higuera,](#page-9-0) [2010\)](#page-9-0). However, we
    instead explore whether LLMs can infer a CFG from fewer examples without imposing
    any constraints on the examples, based on their experience and knowledge acquired
    through training on large-scale corpora. We refer to this process as "few-shot
    grammar generation", emphasizing both the grammar inference of CFGs with fewer
    examples and their generation in BNF, and we use "grammar" to denote a CFG represented
    in BNF. An example is shown in Figure [1.](#page-0-1)


    To explore the capacity of LLMs for few-shot grammar generation, we first construct
    a dedicated dataset of 540 challenges, each with only 3 positive and 3 negative
    examples, and use it to evaluate the performance of 8 LLMs, encompassing both
    open- and closed-source models of varying parameter sizes, including those specialized
    for code generation. We then propose an LLM-driven hybrid genetic algorithm, namely
    HyGenar, which adapts the principles of genetic algorithms with the integration
    of LLM-driven population initialization and mutation. We devised and adopted 6
    metrics to comprehensively evaluate and analyze their performance from the perspectives
    of syntax and semantic correctness, over-fitting, overgeneralization, and utility.
    The results show that, while most LLMs demonstrate unsatisfactory performance,
    our proposed algorithm significantly enhances the grammar generation ability across
    most of the evaluated LLMs.


    We summarize the main contributions of this paper as follows:


    - 1. We constructed a dedicated dataset of 540 challenges for few-shot grammar
    generation and comprehensively evaluated 8 LLMs;

    - 2. We designed 6 metrics for measuring the ability of LLMs in this task and
    performed an extensive analysis based on them;

    - 3. We proposed a novel method, HyGenar, to improve the grammar generation performance
    of


    LLMs and showed that it achieves significant improvements across LLMs.


    ### 2 Background


    ### 2.1 Context-Free Grammar


    A *context-free grammar* (CFG) consists of terminals, non-terminals, a start symbol,
    and production rules [\(Hopcroft et al.,](#page-10-5) [2001;](#page-10-5) [Aho,](#page-8-2)
    [2007\)](#page-8-2). It can be formally defined as a quadruple G = (V, Σ, Π, S),
    where V is a finite set of non-terminal symbols, Σ is the set of terminals, Σ
    ∩ V = ∅, Π is a finite set of production rules, Π ⊆ V ×(V ∪Σ)<sup>∗</sup> , and
    S ∈ V is the start symbol of the grammar. Elements of (V ∪ Σ)<sup>∗</sup> are
    known as sentential forms.


    The language generated by G, denoted L(G), comprises all strings derivable from
    S using the rules in Π: L(G) = {σ ∈ Σ ∗ | S <sup>∗</sup>→ σ}. For α, β ∈ (V ∪Σ)<sup>∗</sup>
    , we say α directly derives β in one step as α→β, and define α <sup>∗</sup>→ β
    as α deriving β in zero or more steps if there exists a finite sequence of γ0,
    . . . , γ<sup>n</sup> ∈ (V ∪ Σ)<sup>∗</sup> where n ≥ 0, such that α = γ<sup>0</sup>
    → γ<sup>1</sup> → · · · → γ<sup>n</sup> = β.


    ### <span id="page-1-0"></span>2.2 Backus-Naur Form


    Backus–Naur Form (BNF) is a notation used to define the grammar of formal languages,
    typically CFG [\(Chomsky,](#page-9-3) [1956;](#page-9-3) [Backus,](#page-8-0)
    [1959;](#page-8-0) [Backus et al.,](#page-8-1) [1960\)](#page-8-1). In this paper,
    LLM generates grammars in BNF.


    A context-free grammar G = (V, Σ, Π, S) is given in BNF notation as a list of
    grouped production rules, where each rule group is written:


    $$ ::= \alpha\_1|\alpha\_2|\dots|$$


    where v<sup>i</sup> enclosed between the pair "< >" is a nonterminal symbol in
    V , and α1, α<sup>2</sup> . . . ∈ (Σ ∪ V ) ∗ is the list of sentential forms that
    can be derived in one step from v<sup>i</sup> . This represents the set of all
    production rules with v<sup>i</sup> on the left-hand side, i.e., v<sup>i</sup>
    → α1, v<sup>i</sup> → α2, . . ..


    We extend the definition of a CFG to define a grammar in BNF to be G = (V, Σ,
    Π, S, R), where V, Σ, Π, and S are defined as before. R is a set of sets of production
    rules, denoted {r1, . . . rn}. Each r<sup>i</sup> ∈ R is a set of production rules
    where the left-hand side of all rules is the i th non-terminal symbol v<sup>i</sup>
    , i.e., r<sup>i</sup> is (v<sup>i</sup> × (Σ ∪ V ) ∗ ) ∩ Π. Since each non-terminal
    symbol has a corresponding rule set, n = |V | = |R|. The language of the BNF grammar
    G is denoted by L(G) and is equal to the language of the original CFG, L(G).


    We say that a grammar G is a *valid* grammar, and that valid(G) evaluates to true,
    if it has a correct BNF syntax, and if R is as defined above, and if all nonterminal
    symbols have at least one rule in their corresponding rule set.


    #### 2.3 Grammar Inference


    Grammar inference aims to learn grammar automatically from a set of examples [\(Horning,](#page-10-0)
    [1969;](#page-10-0) [De la Higuera,](#page-9-0) [2010;](#page-9-0) [Stevenson
    and Cordy,](#page-12-0) [2014b;](#page-12-0) [D''Ulizia et al.,](#page-9-1) [2011\)](#page-9-1).
    In this paper, we focus on inferring a CFG, in BNF notation, from a small set
    of positive and negative examples.


    Given a set of positive examples P and negative examples N , consisting of strings
    that must be, respectively, accepted and rejected, the objective is to infer a
    BNF grammar G. The generated G should satisfy P ⊆ L(G) and N ∩ L(G) = ∅, which
    ensures G accepts all positive examples and rejects all negative examples.


    ### 3 Related Work


    #### 3.1 Grammar Generation


    Grammar inference has been widely studied and applied across various fields, such
    as natural language processing [\(Kai et al.,](#page-10-1) [2024;](#page-10-1)
    [D''Ulizia et al.,](#page-9-1) [2011\)](#page-9-1), bio-informatics [\(De la Higuera,](#page-9-0)
    [2010\)](#page-9-0), pattern recognition [\(Pedro et al.,](#page-12-6) [2013;](#page-12-6)
    [Richetin and](#page-12-7) [Vernadat,](#page-12-7) [1984;](#page-12-7) [De la
    Higuera,](#page-9-0) [2010\)](#page-9-0), and software engineering [\(Schröder
    and Cito,](#page-12-8) [2022;](#page-12-8) [Steven](#page-12-0)[son and Cordy,](#page-12-0)
    [2014b\)](#page-12-0). Previous works have also proposed various approaches for
    grammar inference [\(Rodrigues and Lopes,](#page-12-9) [2007;](#page-12-9) [Cohen
    et al.,](#page-9-5) [2017;](#page-9-5) [Li et al.,](#page-11-1) [2023;](#page-11-1)
    [D''ulizia et al.,](#page-9-2) [2011;](#page-9-2) [Chen,](#page-8-4) [1995\)](#page-8-4).
    However, few works are directly related to exploring the ability of LLMs for few-shot
    grammar generation, which, to reiterate, is to infer grammars from a small set
    of positives and negatives while generating them in BNF.


    #### 3.2 Code Generation


    LLMs demonstrate the ability of code generation [\(Jiang et al.,](#page-10-6)
    [2024b;](#page-10-6) [Huang et al.,](#page-10-7) [2023;](#page-10-7) [De](#page-9-6)[haerne
    et al.,](#page-9-6) [2022\)](#page-9-6), with various approaches proposed to improve
    it [\(Shinn et al.,](#page-12-10) [2023;](#page-12-10) [Madaan](#page-11-2) [et
    al.,](#page-11-2) [2023;](#page-11-2) [Huang et al.,](#page-10-7) [2023;](#page-10-7)
    [Jiang et al.,](#page-10-8) [2023b;](#page-10-8) [Chen et al.,](#page-8-5) [2023\)](#page-8-5).
    We consider grammar generation to share notable similarities with code generation,
    since in grammar generation it is not only required to infer grammars but also
    to generate grammars in BNF. Thus, following a similar approach to Reflexion [\(Shinn
    et al.,](#page-12-10) [2023\)](#page-12-10) and Self-Refine [\(Madaan et al.,](#page-11-2)
    [2023\)](#page-11-2) to enhance code generation, we propose a method as one of
    the baselines for evaluation.


    ### 4 Grammar Generation Ability of LLMs


    In this section, we describe the dedicated dataset constructed to evaluate the
    ability of LLMs in grammar generation, introduce 6 metrics we use in evaluation,
    explain experiments, and analyze results in detail. We detail each in the following
    subsections.


    #### 4.1 Dataset


    To evaluate the capacity of LLMs for grammar generation, we present a dedicated
    dataset.


    During dataset construction, for each k ∈ {1, 2, . . . , 9}, we prompted GPT-4o
    [\(OpenAI et al.,](#page-11-3) [2024\)](#page-11-3) to generate 10 distinct reference
    grammars, where each reference grammar Gref has k nonterminal symbols and thus
    |R| = k. This gives a total of 90 reference grammars. We used Gref to prompt GPT-4o
    to produce 6 different challenges with each challenge consisting of a set of positives
    P and negatives N where |P| = 3 and |N | = 3, in a way that P ⊆ L(Gref ) and N
    ∩ L(Gref ) = ∅. However, GPT-4o often failed to produce challenges with valid
    reference grammars, positives, and negatives as k increased. We manually corrected
    erroneous reference grammars, positives, and negatives by using a BNF parser which
    takes a grammar and outputs whether a grammar is in valid BNF, and whether positives
    are accepted, and negatives are rejected[2](#page-2-0) .


    Following this process, we obtained a dataset of 540 challenges, each consisting
    of 3 positives and 3 negatives. Figure [1](#page-0-1) shows an example challenge
    and a corresponding solution.


    #### 4.2 Metrics


    Let C be a set of N challenges where each is a tuple (Gref ,P, N , G<sup>∗</sup>
    ) consisting of a reference grammar, and a set of positive examples and negative
    examples, respectively, and the corresponding candidate grammar G<sup>∗</sup>
    generated by an LLM. We evaluate the quality of generated grammars using 6 key
    metrics:


    Syntax Correctness (SX ) The syntax correctness metric SX quantifies the proportion
    of guesses


    <span id="page-2-0"></span><sup>2</sup>Refer to Appendix [A](#page-13-0) for the
    details of dataset construction.


    that conform to the valid BNF syntax defined in Section [2.2.](#page-1-0) We define
    an indicator function as:


    $$\mathbb{I}\_{SX}(G^\*) = \begin{cases} 1 & \text{if } valid(G^\*), \\ 0 & \text{otherwise.}
    \end{cases}$$


    SX(C) is defined as: <sup>1</sup> N P<sup>N</sup> <sup>i</sup>=1 <sup>I</sup>SX(G<sup>∗</sup>
    i ).


    Semantic Correctness (SE) SE captures the proportion of guesses that are semantically
    correct. We define an indicator function as follows, noting that if G<sup>∗</sup>
    is not in valid BNF, then L(G<sup>∗</sup> ) = ∅:


    $$\mathbb{I}\_{SE}(G^\*,\mathcal{P},\mathcal{N}) = \begin{cases} 1 & \text{if
    } \mathcal{P} \subseteq \mathcal{L}(G^\*) \land \\ & \mathcal{N} \cap \mathcal{L}(G^\*)
    = \emptyset \\ 0 & \text{otherwise}. \end{cases}$$


    SE(C) is given by: <sup>1</sup> N P<sup>N</sup> <sup>i</sup>=1 <sup>I</sup>SE(G<sup>∗</sup>
    i ,P<sup>i</sup> , Ni).


    Estimating Grammar Quality Given a set of positive and negative examples, there
    are many possible valid and semantically correct solutions that are undesirable.
    For instance, the following grammar would be an undesirable solution for Figure
    [1.](#page-0-1) Here the L(G<sup>∗</sup> ) = |P|, and is overfitted to the examples:


    $$\mathsf{stmt}> \colon \mathsf{z} \mathrel{\mathop{:}} \mathtt{add(\{1,2,3\}''''
    \mid \mathsf{"merge(\{x,y\}'''' \mid )} \mathsf{z} \mathrel{\mathop{:}} \mathtt{add(\{2\}''''
    \mid )}$$


    Equally undesirable is a grammar that overgeneralizes from the examples, and defines
    a significantly larger language than the reference grammar, because this is highly
    likely to contain invalid strings. There are no standard metrics to measure over-fitting
    or over-generalization in grammar generation, so we devise 4 metrics based around
    the number of production rules used in parsing the positive examples.


    First, let Π<sup>P</sup> ⊆ Π be the set of production rules that are used in the
    left-most derivations of all positive examples in P. That is, the set of rules
    in Π which occur in a sequence of rules S → α<sup>1</sup> → . . . → α<sup>n</sup>
    → p where p ∈ P, and all rules expand the left-most non-terminal in α1, . . .
    , αn.


    We report metrics across only *solved* challenges, i.e., a challenge where G<sup>∗</sup>
    is syntactically and semantically correct. Our four metrics are defined as follows[3](#page-3-0)
    :


    • Diff (C), calculates the difference between the number of production rules in
    G<sup>∗</sup> used in parsing the positive examples and the number of production
    rules used by Gref for a given challenge, i.e., |Π ref P | − |Π<sup>∗</sup> P
    |. A grammar that uses substantially fewer production rules has probably overfitted
    to the examples, and a grammar that uses substantially more production rules may
    have over-generalized. We report the average of Diff across all solved challenges
    as Diff <sup>⋄</sup> .


    - OF estimates over-fitting by counting the percentage of solved challenges on
    which G<sup>∗</sup> uses fewer than half the number of production rules used by
    Gref , i.e., the number of times |Π ref P | − |Π<sup>∗</sup> P | > |Π ref P |
    2 .

    - OG estimates over-generalization by counting the percentage of solved challenges
    on which G<sup>∗</sup> uses more than half the number of production rules used
    by Gref , i.e., |Π ref P | − |Π<sup>∗</sup> P | < − |Π ref P | 2 .

    - TU calculates the proportion of production rules that are used in parsing the
    positive examples for a given challenge, i.e., <sup>|</sup>Π<sup>∗</sup> P | <sup>Π</sup><sup>∗</sup>
    , indicating the utility. We report the average of TU across all solved challenges
    as T U<sup>⋄</sup> .


    #### 4.3 Baselines


    To establish baselines, we adopted two approaches, Direct Prompting (DP) and Optimization
    of the BNF Parser for LLM-Friendly Feedback (OPF).


    In DP, we directly prompted LLMs with positive and negative examples, asking them
    to produce grammars that accept all positives and reject all negatives[4](#page-3-1)
    .


    In OPF, inspired by Reflexion [\(Shinn et al.,](#page-12-10) [2023\)](#page-12-10)
    and Self-Refine [\(Madaan et al.,](#page-11-2) [2023\)](#page-11-2), we further
    optimized the BNF parser by enabling it to provide more LLM-friendly error messages
    as feedback, aiming to improve the performance of grammar generation for LLMs[5](#page-3-2)
    .


    #### 4.4 Experiment Settings


    For a comprehensive evaluation, we selected a total of 8 LLMs, ensuring a diverse
    selection of both open- and closed-source LLMs, along with LLMs with varying parameter
    sizes and LLMs specifically designed for code generation.


    Specifically, we selected two closed-source LLMs, *GPT-4o* [\(OpenAI et al.,](#page-11-3)
    [2024\)](#page-11-3) and *GPT-3.5-Turbo* [\(Brown et al.,](#page-8-6) [2020a\)](#page-8-6).
    For the other 6


    <span id="page-3-0"></span><sup>3</sup>Refer to Appendix [B](#page-13-1) for the
    details of formal definitions.


    <span id="page-3-1"></span><sup>4</sup>Refer to Appendix [C](#page-14-0) for the
    details of DP.


    <span id="page-3-2"></span><sup>5</sup>Refer to Appendix [D](#page-14-1) for the
    details of OPF.


    open-source LLMs, we note them in the notation: {*model\_name*}*:*{*parameter\_size*}*-*{*model\_type*}.


    We selected the following LLMs: *Llama3:70b-Instruct* [\(Grattafiori et al.,](#page-9-7)
    [2024\)](#page-9-7), *Qwen:72b-Chat* [\(Bai et al.,](#page-8-7) [2023\)](#page-8-7),
    *Gemma2:27b-Instruct* [\(Team et al.,](#page-12-11) [2024\)](#page-12-11), *Mistral:7b-Instruct*
    [\(Jiang et al.,](#page-10-9) [2023a\)](#page-10-9), *Codestral:22b* [\(MistralAI,](#page-11-4)
    [2024\)](#page-11-4), and *Starcoder2:15b-Instruct* [\(Lozhkov et al.,](#page-11-5)
    [2024\)](#page-11-5). Among the selected LLMs, *Starcoder2:15b-Instruct* and *Codestral:22b*
    are LLMs for code generation.


    For the DP baseline, we set the temperature to 0 and the maximum token to 2000.


    For the OPF baseline[6](#page-4-0) , we set the temperature to 0.3, the maximum
    token to 2000, and the maximum number of turns, *max\_turns*, to 5.


    #### 4.5 Results & Analysis


    The results of SX and SE for the 8 LLMs are presented in Table [1.](#page-5-0)
    We observed that *GPT-4o*, *GPT-3.5-Turbo*, and *Gemma2:27b-Instruct* achieve
    relatively high SX , while the other LLMs generally fare worse in DP baseline.
    Compared to the DP baseline, applying OPF leads to a significant enhancement in
    SX for both *Mistral:7b-Instruct* and *Codestral:22b*, yielding an 18% improvement
    for each of them, suggesting that parser feedback can help increase SX . Nevertheless,
    for most LLMs, OPF yields only slight gains in SX . Furthermore, it is worth noting
    that for *Starcoder2:15b-Instruct*, SX decreases by 16% after applying OPF. This
    indicates that feedback from the parser can sometimes lead to performance degradation
    if LLMs fail to interpret the feedback correctly.


    Although several LLMs attain high SX , their SE remains low in DP, with an average
    of 39%. For example, *GPT-3.5-Turbo* has 94% SX but only 37% SE. With OPF, LLMs
    like *Mistral:7b-Instruct* and *Codestral:22b* achieve notable SE improvement
    with enhanced SX , illustrating that improved SX can positively influence SE.
    However, for most LLMs, OPF yields only marginal SE gains, particularly in the
    case that their SX is already high and OPF fails to contribute significant SX
    improvement.


    Noticeably, although OPF helps improve SX , a significant gap still persists between
    SX and SE for most LLMs. For example, even after OPF, *GPT-3.5-Turbo* maintains
    a 57% gap between SX and SE, and *Starcoder2:15b-Instruct* has a 40% gap. Since
    the ultimate objective is to improve SE, a more advanced approach is needed to
    address this limitation.


    Furthermore, as other four metrics shown in Table [2,](#page-7-0) on average,
    for most of LLMs, the lower Diff <sup>⋄</sup> , OF, and OG indicate negligible
    over-fitting or over-generalization issues, and the higher TU <sup>⋄</sup> means
    LLMs are not predisposed to generate irrelevant production rules. Nevertheless,
    as the number of production rules increases, Diff <sup>⋄</sup> and OF exhibit
    slight increases, indicating a tendency toward mild over-fitting, while no significant
    overgeneralization issues are observed [7](#page-4-1) .


    In addition to the quantitative metrics, we also conducted a qualitative analysis
    by examining the generated grammars. We compared the grammars containing syntactic
    errors generated in the DP but corrected after the OPF. We primarily found four
    significant issues causing lower SX: unsupported symbols such as injecting quantifiers
    like "\*" and "?" or character classes like "[a-z]", erroneously introduced and
    misplaced brackets such as wrapping two terminals with round and square brackets,
    failing to wrap non-terminal with angle brackets, and omission the separators
    "|" between sentential forms. While the the first two issues are rampant across
    most LLMs, the third issue is mainly found in *Mistral:7b-Instruct* and the last
    one is sporadic. For most LLMs, OPF can occasionally mitigate these issues, but
    insignificantly. Furthermore, we also observed LLMs show an ability to recognize
    keywords in the examples, treating them as complete terminals rather than decomposing
    them into multiple terminals as individual characters. For example, they treat
    "if" and "SELECT" as complete terminals rather than splitting them into multiple
    characters. This may be benefited from the common sense acquired by LLMs from
    the corpus [\(Brown et al.,](#page-8-8) [2020b\)](#page-8-8).


    ### <span id="page-4-2"></span>5 LLM-Driven Hybrid Genetic Algorithm


    The results of the baselines demonstrate the unsatisfactory performance of LLMs
    in grammar generation. To address this, we propose an LLM-driven hybrid genetic
    algorithm, namely HyGenar, a novel algorithm inspired by the concept of genetic
    algorithms. The following sections detail HyGenar and elaborate on our experiment
    settings, results, and analysis.


    <span id="page-4-0"></span><sup>6</sup>Refer to Appendix [D](#page-14-1) for the
    reason of setting the temperature greater than 0 for OPF.


    <span id="page-4-1"></span><sup>7</sup>Refer to Appendix [F.3](#page-15-0) and
    [F.4](#page-17-0) for more details.


    <span id="page-5-0"></span>


    | Models                  | Syntax Correctness (SX<br>) |        |             |
    Semantics Correctness(SE) |        |             |

    |-------------------------|-----------------------------|--------|-------------|---------------------------|--------|-------------|

    |                         | DP                          | OPF    | HyGenar     |
    DP                        | OPF    | HyGenar     |

    | GPT-4o                  | 93                          | 97 ↑4  | 96 ↑3<br>↓1
    | 84                        | 85 ↑1  | 93 ↑9<br>↑8 |

    | GPT-3.5-Turbo           | 94                          | 95 ↑1  | 99 ↑5<br>↑4
    | 37                        | 38 ↑1  | 61 ↑24 ↑23  |

    | Llama3:70b-Instruct     | 57                          | 61 ↑4  | 75 ↑18 ↑14  |
    41                        | 42 ↑1  | 61 ↑20 ↑19  |

    | Qwen:72b-Chat           | 47                          | 49 ↑2  | 76 ↑29 ↑27  |
    20                        | 21 ↑1  | 38 ↑18 ↑17  |

    | Mistral:7b-Instruct     | 1                           | 19 ↑18 | 1 − ↓18     |
    0                         | 8 ↑8   | 1 ↑1 ↓7     |

    | Gemma2:27b-Instruct     | 91                          | 92 ↑1  | 98 ↑7 ↑6    |
    56                        | 57 ↑1  | 79 ↑23 ↑22  |

    | Starcoder2:15b-Instruct | 76                          | 60 ↓16 | 98 ↑22 ↑38  |
    30                        | 20 ↓10 | 44 ↑14 ↑24  |

    | Codestral:22b           | 53                          | 71 ↑18 | 80 ↑27 ↑9   |
    44                        | 52 ↑8  | 67 ↑23 ↑15  |


    Table 1: The results of syntax and semantic correctness for LLMs grammar generation
    are presented as percentages (%). For each LLM, the best syntax and semantic correctness
    are highlighted with bold font. Blue arrows ↑↓ represent performance differences
    relative to the DP baseline, while red arrows ↑↓ indicate differences relative
    to the OPF baseline.


    #### 5.1 Methodology


    HyGenar consists of four main components: Fitness, Selection, Crossover, and Mutation.
    It begins by prompting an LLM to generate an initial population of candidate grammars
    from positive and negative examples. In each generation, the Fitness function
    scores each candidate and the Select function chooses a subset of the population.
    To form a new population, the Cross function operates on two randomly selected
    candidates from this subset to generate a new candidate, which is then modified
    by the Mutate function and added to the new population, until the maximum population
    size is reached. The new population then advances to the next generation. We provide
    pseudocode in Algorithm [1](#page-16-0) in Appendix [E.](#page-15-1) We detail
    each component as follows:


    Fitness Given a generated grammar G<sup>∗</sup> , if it is syntactically incorrect,
    it is assigned a score of −1. For a valid grammar, we define two indicator functions:


    $$\begin{aligned} \mathbb{I}\_{\mathcal{A}}(G^\*,p) &= \begin{cases} 1 & \text{if
    } p \in \mathcal{L}(G^\*), \\ 0 & \text{otherwise.} \end{cases} \\ \mathbb{I}\_{\mathcal{R}}(G^\*,n)
    &= \begin{cases} 1 & \text{if } n \notin \mathcal{L}(G^\*), \\ 0 & \text{otherwise.}
    \end{cases} \end{aligned}$$


    The Fitness function Fitness(G<sup>∗</sup> ,P, N ) is then defined as:


    $$\sum\_{p\_i \in \mathcal{P}} \mathbb{I}\_{\mathcal{A}}(G^\*, p\_i) + \sum\_{n\_i
    \in \mathcal{N}} \mathbb{I}\_{\mathcal{R}}(G^\*, n\_i).$$


    Selection Let G = {G<sup>∗</sup> 1 , G<sup>∗</sup> 2 , . . . , G<sup>∗</sup> k
    } be a population of candidate grammars. Each G<sup>∗</sup> <sup>i</sup> <sup>∈</sup>
    <sup>G</sup> is


    assigned a fitness score s<sup>i</sup> ∈ S by the Fitness function, where S =
    [s1, s2, . . . , sk] is a sequence of their corresponding scores. We define the
    Select function as:


    $$\text{Select}(\mathbb{G}, S) = \{G^\*\_{\sigma(1)}, G^\*\_{\sigma(2)}, \dots,
    G^\*\_{\sigma(\frac{k}{2})}\},$$


    where σ : {1, 2, . . . , k} → {1, 2, . . . , k} is a permutation of the indices
    such that: sσ(1) ≥ sσ(2) ≥ · · · ≥ sσ(k) . Hence, half of the candidates from
    the population are selected in decreasing order of their fitness scores.


    Crossover The crossover function splices the production rules from two grammars
    together, at a randomly chosen splicing point. Let G<sup>∗</sup> a and G<sup>∗</sup>
    b be two candidate grammars, with rules R<sup>∗</sup> a and R<sup>∗</sup> b respectively.
    If both R are empty, we return one grammar at random. If one grammar has a nonempty
    R, we return that grammar. Otherwise, we apply the crossover function with probability
    ρ (the crossover rate).


    The crossover operation works in the following way. Let ℓ = min(|R<sup>∗</sup>
    a |, |R<sup>∗</sup> b |). First sample a crossover point w ∼ Uniform({1, 2, .
    . . , ℓ}). Let R∗ <sup>a</sup> = r a 1 , r<sup>a</sup> 2 . . . be the rules from
    G<sup>∗</sup> a and R<sup>∗</sup> b be the rules from G<sup>∗</sup> b . We take
    the first w − 1 rule sets from R<sup>∗</sup> a and then prefix these to the last
    n−w rule sets from R<sup>∗</sup> b , where n = |R<sup>∗</sup> b |. This generates
    a new rule set R′ = {r 1 a , . . . , rw−<sup>1</sup> a , r<sup>w</sup> b , . .
    . r<sup>n</sup> b }.


    Crossover returns a new grammar G′ = (V ′ , Σ ′ , Π′ , Sa, R′ ), where V ′ and
    Σ ′ are all nonterminal and terminal symbols in R′ , Π′ is all rules in R′ , and
    S<sup>a</sup> is the start symbol from G<sup>∗</sup> a .


    Mutation We use two mutation methods: mutation by LLM, and local mutation. We
    chose


    whether to mutate a grammar at all, with probability µ (the mutation rate). If
    the grammar has no production rules, we apply the LLM mutation. Otherwise, we
    apply local mutation with a probability 0.5 and LLM mutation otherwise.


    LLM-Driven Mutation The LLM-driven mutation uses G<sup>∗</sup> , P, and N to prompt[8](#page-6-0)
    an LLM to produce a new grammar. In this approach, we expect that, with the knowledge
    and experience obtained by training on vast corpora, LLMs can heuristically provide
    more novel and dramatic modifications such as introducing new terminals or nonterminals,
    adding or removing production rules, or reshaping the structure of grammars, which
    is hard to approach with local mutation.


    Local Mutation The local mutation is designed to produce incremental, targeted
    alterations to the grammar while preserving the majority of its original form.
    It is less flexible than LLM-driven mutation and it is unable to introduce new
    non-terminals or to drastically restructure grammars. However, it can not only
    potentially find a grammar candidate but also provide insights for LLM-driven
    mutation. Given a grammar G<sup>∗</sup> , with a set of sets of production rules
    R<sup>∗</sup> , local mutation comprises the following steps:


    - Rule set selection We sample an integer i ∼ Uniform({1, . . . , |R|}). This
    index i chooses the rule set r<sup>i</sup> ∈ R<sup>∗</sup> that will be mutated.

    - Shuffle The Shuffle mutation shuffles the order of symbols on the right-hand
    side of a production rule. That is, each rule is a mapping from a non-terminal
    v to a sequence of non-terminal and terminal symbols (V ∪ Σ)<sup>∗</sup> , and
    we shuffle this sequence randomly. For example, the rule set: <e> ::= <e> "\*"
    <e> | <e> "/" <e> with infix operators, may be shuffled to <e> ::= "\*" <e> <e>
    | "/" <e> <e>, switching to prefix operators.


    Shuffle is applied to all production rules in r<sup>i</sup> .


    • Space Insertion The SpaceInsert mutation inserts a randomly chosen number of
    whitespace terminal symbols "␣" into the righthand side of a production rule.
    Given a rule v − i → α, Shuffle chooses the number of whitespace terminals to
    be inserted by


    sampling I ∼ Uniform(0, |α|), where |α| is the number of symbols in α. Each space
    is inserted before or after a randomly chosen symbol in α. As an example, the
    rule set: <s> ::= <noun> <verb> may be changed to <s> ::= <noun> "␣" <verb>.


    For each production rule in r<sup>i</sup> , we randomly decide whether SpaceInsert
    should be applied to that rule with a low probability[9](#page-6-1) .


    The Shuffle alteration heuristic is motivated by two key insights. First, shuffling
    the right-hand side of production rules may yield a grammar that accepts more
    positive examples and rejects more negative ones. Second, although Shuffle may
    rarely yield a better candidate for a complex target grammar, the new variant
    of the grammar produced from Shuffle is expected to provide alternative perspectives
    and new insights for LLMs to help generate subsequent grammars in future generations.


    The SpaceInsert alteration was introduced because some LLMs tended to omit explicit
    space symbols between symbols in an alternative, resulting in degraded grammar
    generation, even if they were prompted to pay attention to space inclusion[10](#page-6-2)
    . We expect that incorporating SpaceInsert will offer insights for LLMs of the
    explicit inclusion of spaces to thereby enhance performance.


    #### 5.2 Experiment Settings


    In addition to a set of positive and negative examples and an LLM, HyGenar takes
    four parameters: *population size* (grammars per generation), *generations* (number
    of evolution iterations), *crossover rate* (probability of crossover), *mutation
    rate* (probability of mutation). In our experiments, we set these to 10, 5, 0.7,
    and 0.3, respectively.


    We selected the same 8 LLMs as the DP and OPF baselines, setting maximum tokens
    to 2000 and temperature to 0.7. In HyGenar, a nonzero temperature is necessary
    for diversity. To ensure it does not significantly impact results and show its
    robustness, we further repeat the experiments 5 times with *GPT-3.5-Turbo* and
    *GPT-4o*.


    #### 5.3 Results & Analysis


    As shown in Table [1,](#page-5-0) HyGenar substantially boosts SX for most LLMs.
    For example, *Qwen:72b-Instruct* gains 29% over DP and 27% over OPF. Even for
    LLMs already improved by OPF, such


    <span id="page-6-0"></span><sup>8</sup>Refer to Prompt Template [2](#page-17-1)
    in Appendix [E](#page-15-1) for the prompt we designed for LLM-driven mutation.


    <span id="page-6-1"></span><sup>9</sup>We fix it to 0.1.


    <span id="page-6-2"></span><sup>10</sup>Refer to Prompt Template [1](#page-17-2)
    in Appendix [C](#page-14-0) for details.


    <span id="page-7-0"></span>


    | Methods | ⋄<br>Diff | OF   | OG   | ⋄<br>TU |

    |---------|-----------|------|------|---------|

    | DP      | 1.12      | 3.83 | 0.63 | 88.74   |

    | OPF     | 1.10      | 4.72 | 1.31 | 90.76   |

    | HyGenar | 1.19      | 4.44 | 0.92 | 91.27   |


    Table 2: The averages of Diff <sup>⋄</sup> , OF(%), OG(%), and TU <sup>⋄</sup>
    (%) across all LLMs.


    as *Codestral:22b* with an 18% improvement, HyGenar adds an additional 9%. Meanwhile,
    *Starcoder2:15b-Instruct*, which experiences a 16% drop under OPF, achieves a
    22% improvement compared to DP and a 38% improvement over OPF, with HyGenar. On
    average, it improves SX 13.88% compared to DP and 9.88% over OPF.


    While enhancing SX is essential, the ultimate objective is to improve SE. As shown
    in Table [1,](#page-5-0) our method significantly boosts SE for all LLMs except
    *Mistral:7b-Instruct*. For example, with Hy-Genar, *GPT-4o* rises from 84% with
    DP and 85% with OPF to 93%, and *GPT-3.5-Turbo*, noted for low semantic accuracy,
    increases by 24% over DP and 23% over OPF. Notably, although the contribution
    from the enhancement of SX is essential to SE, HyGenar does not rely solely on
    enhancing SX to achieve significant improvement of SE, as five LLMs demonstrated
    higher SE increases than their SX . Across the selected LLMs, the average SE improvement
    is 16.5% compared to DP and 15.13% compared to OPF.


    We further analyzed the performance as the number of non-terminals and production
    rules increases. For non-terminals, we partition the dataset into 3 groups: C<sup>1</sup>
    (1–3 non-terminals), C<sup>2</sup> (4-6 nonterminals), and C<sup>3</sup> (7-9
    non-terminals). For production rules, we split the dataset into another 3 groups:
    P<sup>1</sup> (1–6 production rules), P<sup>2</sup> (6-15 production rules), and
    P<sup>3</sup> (greater than 16 production rules). We observed that the performance
    of LLMs decreases as the number of non-terminals and production rules increases.
    Nevertheless, HyGenar still substantially improves both SX and SE[11](#page-7-1)
    .


    Furthermore, as shown in Table [2,](#page-7-0) OF does not significantly increase
    in HyGenar, indicating that the substantial improvements observed in both the
    SX and SE for HyGenar are not attributed to overfitting[12](#page-7-2) .


    Moreover, we also conducted a qualitative anal-


    ysis of how HyGenar improves SX and SE. For SX, HyGenar significantly reduces
    the issues of unsupported symbols injection and misplaced brackets. However, it
    fails to address the issue of unwrapped non-terminals, which is also the issue
    mainly happened in *Mistral:7b-Instruct*. Unlike OPF, which benefits from more
    explicit syntax error feedback, our approach lacks such direct syntax corrective
    guidance, meaning that if an LLM inherently struggles to generate syntactically
    correct grammars, our method may fail to produce valid candidates and process
    evolution, thereby lowering both SX and SE. Nevertheless, as long as at least
    a few candidates are generated in correct syntax, HyGenar can optimize their generations
    during the evolutionary process to mitigate the aforementioned issues and improve
    SX . For SE, attributing the significant improvement is complex. However, we still
    observed two phenomena. First, after applying HyGenar, terminals that were not
    previously considered in the grammars generated by DP or OPF have been introduced.
    Second, the semantic errors that were caused by the absence of space terminals
    in DP or OPF have been alleviated.


    Due to the relatively high temperature of 0.7 used for HyGenar, we repeated the
    experiments 5 times with *GPT-4o* and *GPT-3.5-Turbo*[13](#page-7-3) to ensure
    robustness. The averages of SX for *GPT-4o* and *GPT-3.5-Turbo* are 95.8% and
    98.6%, with standard deviations 0.4% and 0.49% respectively, while the averages
    of SE are 93.2% and 61.6% with standard deviations 0.4% and 0.49% respectively.
    These results indicate that setting the temperature to 0.7 has a negligible impact
    on performance and show the robustness of HyGenar.


    ### 6 Conclusion


    To explore the few-shot grammar generation ability of LLMs, we constructed a dedicated
    dataset consisting of 540 challenges, devised and adopted 6 metrics, and evaluated
    8 various LLMs. Due to their unsatisfactory performance, we introduced HyGenar,
    an LLM-driven hybrid genetic algorithm for grammar generation. Our results indicate
    that HyGenar significantly enhances both syntax and semantic correctness compared
    to the two baselines. We believe this work provides valuable insights into LLM-based
    grammar generation and highlights the potential of LLM-driven hybrid genetic algorithms
    in this domain.


    <span id="page-7-2"></span><span id="page-7-1"></span><sup>11</sup>Refer to Tables
    [4](#page-19-0) and [5](#page-20-0) in Appendix [F.1](#page-15-2) and [F.2](#page-15-3)
    for details. <sup>12</sup>Refer to Tables [6,](#page-21-0) [7,](#page-22-0) and
    [8](#page-23-0) in Appendix [F.3](#page-15-0) for the more details.


    <span id="page-7-3"></span><sup>13</sup>Refer to Table [3](#page-18-0) in Appendix
    [F.5](#page-17-3) for details.


    ### 7 Limitations


    We discuss several limitations and concerns in this work, revealing potential
    challenges, constraints, and confusion.


    First, although the results indicate that *GPT-4o* exhibits remarkable SX and
    SE, it is important to note that these results may be attributable to the use
    of GPT-4o during dataset construction. Nonetheless, even though *GPT-4o* already
    demonstrates excellent performance, HyGenar can still enhance it significantly.


    Second, as demonstrated, our method does not outperform OPF for *Mistral:7b-Instruct*
    in SX and SE due to its inherent failure to generate syntactically correct grammars.
    Nevertheless, our approach yields significant SX and SE improvements for all other
    LLMs. We also propose to combine syntactical feedback and HyGenar to mitigate
    this limitation and further improve the performance.


    Third, one may argue that given any finite set of positive and negative examples,
    it could always be possible to construct a regular grammar rather than a CFG that
    can accept all positives and reject all negatives. However, such an approach may
    function more like a classifier rather than a grammar and may lack applicability
    in subsequent tasks, such as constructing an abstract syntax tree.


    Finally, in this work, we primarily focus on LLM-based few-shot grammar generation
    without comparing algorithms that are not LLM-based. The reasons behind this are
    that most algorithms require a large set of characteristic examples to uniquely
    determine the target grammar [\(De la Higuera,](#page-9-0) [2010\)](#page-9-0).
    Instead, we do not impose such constraints on our example set and hypothesize
    that the experience and knowledge acquired from corpus can enable LLMs to handle
    few-shot grammar generation tasks. Consequently, those algorithms may not be directly
    applicable. In addition, since we focus on the exploration and improvement of
    the ability of LLMs in few-shot grammar generation, we construct two LLM-based
    baselines for fair comparison.


    ### References


    - <span id="page-8-2"></span>2007. *Compilers: principles, techniques, tools*,
    2nd ed edition. Pearson/Addison Wesley, Boston.

    - <span id="page-8-0"></span>John W Backus. 1959. The syntax and the semantics
    of the proposed international algebraic language of the


    zurich acm-gamm conference. In *ICIP Proceedings*, pages 125–132.


    - <span id="page-8-1"></span>John W Backus, Friedrich L Bauer, Julien Green, Charles
    Katz, John McCarthy, Alan J Perlis, Heinz Rutishauser, Klaus Samelson, Bernard
    Vauquois, Joseph Henry Wegstein, et al. 1960. Report on the algorithmic language
    algol 60. *Communications of the ACM*, 3(5):299–311.

    - <span id="page-8-7"></span>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
    Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei
    Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin
    Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu,
    Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,
    Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
    Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren
    Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. [Qwen technical report.](https://arxiv.org/abs/2309.16609)
    *Preprint*, arXiv:2309.16609.

    - <span id="page-8-3"></span>Luca Beurer-Kellner, Marc Fischer, and Martin T.
    Vechev. 2024. [Guiding llms the right way:](https://api.semanticscholar.org/CorpusID:268363645)
    [Fast, non-invasive constrained generation.](https://api.semanticscholar.org/CorpusID:268363645)
    *ArXiv*, abs/2403.06988.

    - <span id="page-8-6"></span>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
    Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
    Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
    Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford,
    Ilya Sutskever, and Dario Amodei. 2020a. [Language models are few-shot learn](https://arxiv.org/abs/2005.14165)[ers.](https://arxiv.org/abs/2005.14165)
    *Preprint*, arXiv:2005.14165.

    - <span id="page-8-8"></span>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
    Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
    Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
    Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
    Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
    Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
    Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners.
    In *Proceedings of the 34th International Conference on Neural Information Processing
    Systems*, NIPS ''20, Red Hook, NY, USA. Curran Associates Inc.

    - <span id="page-8-4"></span>Stanley F Chen. 1995. Bayesian grammar induction
    for language modeling. *arXiv preprint cmp-lg/9504034*.

    - <span id="page-8-5"></span>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
    Denny Zhou. 2023. [Teaching large language models](https://doi.org/10.48550/arXiv.2304.05128)
    [to self-debug.](https://doi.org/10.48550/arXiv.2304.05128) *ArXiv*, abs/2304.05128.

    - <span id="page-9-3"></span>N. Chomsky. 1956. [Three models for the description
    of](https://doi.org/10.1109/TIT.1956.1056813) [language.](https://doi.org/10.1109/TIT.1956.1056813)
    *IRE Transactions on Information Theory*, 2(3):113–124.

    - <span id="page-9-5"></span>Mor Cohen, Avi Caciularu, Idan Rejwan, and Jonathan
    Berant. 2017. [Inducing regular grammars using re](https://api.semanticscholar.org/CorpusID:8704099)[current
    neural networks.](https://api.semanticscholar.org/CorpusID:8704099) *ArXiv*, abs/1710.10453.

    - <span id="page-9-0"></span>Colin De la Higuera. 2010. *Grammatical inference:
    learning automata and grammars*. Cambridge University Press.

    - <span id="page-9-6"></span>Enrique Dehaerne, Bappaditya Dey, Sandip Halder,
    Stefan De Gendt, and Wannes Meert. 2022. [Code gener](https://doi.org/10.1109/ACCESS.2022.3196347)[ation
    using machine learning: A systematic review.](https://doi.org/10.1109/ACCESS.2022.3196347)
    *IEEE Access*, 10:82434–82455.

    - <span id="page-9-2"></span>Arianna D''ulizia, F. Ferri, and P. Grifoni. 2011.
    [A](https://doi.org/10.1007/s10462-010-9199-1) [survey of grammatical inference
    methods for natu](https://doi.org/10.1007/s10462-010-9199-1)[ral language learning.](https://doi.org/10.1007/s10462-010-9199-1)
    *Artificial Intelligence Review*, 36:1–27.

    - <span id="page-9-1"></span>Arianna D''Ulizia, Fernando Ferri, and Patrizia Grifoni.
    2011. [A survey of grammatical inference methods](https://doi.org/10.1007/s10462-010-9199-1)
    [for natural language learning.](https://doi.org/10.1007/s10462-010-9199-1) *Artificial
    Intelligence Review*, 36(1):1–27.

    - <span id="page-9-4"></span>Vedant Gaur and Nikunj Saunshi. 2023. [Reasoning
    in](https://doi.org/10.18653/v1/2023.findings-acl.364) [large language models
    through symbolic math word](https://doi.org/10.18653/v1/2023.findings-acl.364)
    [problems.](https://doi.org/10.18653/v1/2023.findings-acl.364) In *Findings of
    the Association for Computational Linguistics: ACL 2023*, pages 5889–5903, Toronto,
    Canada. Association for Computational Linguistics.

    - <span id="page-9-7"></span>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
    Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur,
    Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,
    Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun
    Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste
    Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak,
    Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
    Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,
    Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv
    Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor
    Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
    Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia
    Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem
    Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol
    Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet,
    Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer
    van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,
    Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo
    Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden
    Alwala, Karthik Prasad,


    Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid
    El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia,
    Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins,
    Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de
    Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin
    Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie
    Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes
    Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier
    Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic,
    Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,
    Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,
    Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit
    Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross
    Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,
    Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan
    Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang,
    Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot,
    Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha,
    Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao,
    Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez,
    Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei
    Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang
    Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle
    Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue
    Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos,
    Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya
    Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg,
    Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus,
    Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,
    Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita
    Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman,
    Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth
    Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
    Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
    Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao
    Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer,
    Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins,
    David Xu, Davide Testuggine, Delia David, Devi Parikh,


    Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling,
    Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan
    Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix
    Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank
    Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer,
    Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan,
    Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb,
    Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim
    Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai
    Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste
    Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy
    Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard,
    Jonathan Mc-Phie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
    U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,
    Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,
    Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng
    Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,
    Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
    Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer,
    Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan,
    Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad
    Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata
    Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
    Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar,
    Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,
    Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,
    Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham
    Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah
    Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin
    Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha
    Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh
    Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy
    Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,
    Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,
    Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng,
    Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal
    Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun
    Zhang, Tim


    Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria
    Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,
    Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang,
    Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun
    Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin
    Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi
    Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,
    Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. [The llama 3 herd](https://arxiv.org/abs/2407.21783)
    [of models.](https://arxiv.org/abs/2407.21783) *Preprint*, arXiv:2407.21783.


    - <span id="page-10-5"></span>John E Hopcroft, Rajeev Motwani, and Jeffrey D Ullman.
    2001. Introduction to automata theory, languages, and computation. *Acm Sigact
    News*, 32(1):60–65.

    - <span id="page-10-0"></span>James Jay Horning. 1969. *A study of grammatical
    inference*. Stanford University.

    - <span id="page-10-7"></span>Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck,
    and Heming Cui. 2023. [Agentcoder: Multi-agent](https://doi.org/10.48550/arXiv.2312.13010)[based
    code generation with iterative testing and opti](https://doi.org/10.48550/arXiv.2312.13010)[misation.](https://doi.org/10.48550/arXiv.2312.13010)
    *ArXiv*, abs/2312.13010.

    - <span id="page-10-3"></span>Shima Imani, Liang Du, and Harsh Shrivastava. 2023.
    [MathPrompter: Mathematical reasoning using large](https://doi.org/10.18653/v1/2023.acl-industry.4)
    [language models.](https://doi.org/10.18653/v1/2023.acl-industry.4) In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    5: Industry Track)*, pages 37– 42, Toronto, Canada. Association for Computational
    Linguistics.

    - <span id="page-10-9"></span>Albert Q. Jiang, Alexandre Sablayrolles, Arthur
    Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,
    Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne
    Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
    and William El Sayed. 2023a. [Mistral 7b.](https://arxiv.org/abs/2310.06825) *Preprint*,
    arXiv:2310.06825.

    - <span id="page-10-4"></span>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim,
    and Sunghun Kim. 2024a. [A survey on large](https://arxiv.org/abs/2406.00515)
    [language models for code generation.](https://arxiv.org/abs/2406.00515) *Preprint*,
    arXiv:2406.00515.

    - <span id="page-10-6"></span>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim,
    and Sunghun Kim. 2024b. [A survey on large language](https://doi.org/10.48550/arXiv.2406.00515)
    [models for code generation.](https://doi.org/10.48550/arXiv.2406.00515) *ArXiv*,
    abs/2406.00515.

    - <span id="page-10-8"></span>Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,
    and Ge Li. 2023b. [Self-planning code generation](https://doi.org/10.1145/3672456)
    [with large language models.](https://doi.org/10.1145/3672456) *ACM Transactions
    on Software Engineering and Methodology*.

    - <span id="page-10-2"></span>S. C. Johnson and Murray Hill. 1978. [Yacc: Yet
    another](https://api.semanticscholar.org/CorpusID:62540186) [compiler-compiler.](https://api.semanticscholar.org/CorpusID:62540186)

    - <span id="page-10-1"></span>Jushi Kai, Shengyuan Hou, Yusheng Huang, and Zhouhan
    Lin. 2024. [Leveraging grammar induction](https://arxiv.org/abs/2410.04878) [for
    language understanding and generation.](https://arxiv.org/abs/2410.04878) *Preprint*,
    arXiv:2410.04878.

    - <span id="page-11-1"></span>Yixuan Li, Federico Mora, Elizabeth Polgreen, and
    Sanjit A Seshia. 2023. Genetic algorithms for searching a matrix of metagrammars
    for synthesis. *arXiv preprint arXiv:2306.00521*.

    - <span id="page-11-0"></span>Yixuan Li, Julian Parsert, and Elizabeth Polgreen.
    2024. [Guiding enumerative program synthesis with large](https://doi.org/10.1007/978-3-031-65630-9_15)
    [language models.](https://doi.org/10.1007/978-3-031-65630-9_15) In *Computer
    Aided Verification*, CAV 2024, pages 280–301, Cham. Springer Nature Switzerland.

    - <span id="page-11-5"></span>Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada,
    Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding
    Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii
    Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan
    Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov,
    Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan
    Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian
    McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn
    Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,
    Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha,
    Leandro von Werra, and Harm de Vries. 2024. [Starcoder 2 and the stack v2: The](https://arxiv.org/abs/2402.19173)
    [next generation.](https://arxiv.org/abs/2402.19173) *Preprint*, arXiv:2402.19173.

    - <span id="page-11-2"></span>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
    Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye,
    Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean
    Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. [Self-refine: Itera](https://openreview.net/forum?id=S37hOerQLB)[tive
    refinement with self-feedback.](https://openreview.net/forum?id=S37hOerQLB) In
    *Thirty-seventh Conference on Neural Information Processing Systems*.

    - <span id="page-11-4"></span>MistralAI. 2024. Codestral. [https://mistral.ai/](https://mistral.ai/news/codestral/)
    [news/codestral/](https://mistral.ai/news/codestral/).

    - <span id="page-11-3"></span>OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve


    Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi,
    Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford,
    Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
    Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene,
    Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
    Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,
    Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga,
    Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin,
    Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
    Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,
    Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros,
    Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis,
    Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee,
    Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie
    Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
    Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer,
    Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan,
    Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko,
    Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,
    Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,
    Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O''Keefe, Jakub Pachocki, Alex
    Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish,
    Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe
    de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael,
    Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris
    Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron
    Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
    Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather
    Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov,
    Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
    Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie
    Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas
    Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
    Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone,
    Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,
    Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael


    Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan
    Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,
    William Zhuk, and Barret Zoph. 2024. [Gpt-4 technical report.](https://arxiv.org/abs/2303.08774)
    *Preprint*, arXiv:2303.08774.


    - <span id="page-12-4"></span>Liangming Pan, Alon Albalak, Xinyi Wang, and William
    Yang Wang. 2023. [Logic-lm: Empower](http://arxiv.org/abs/2305.12295)[ing large
    language models with symbolic solvers](http://arxiv.org/abs/2305.12295) [for faithful
    logical reasoning.](http://arxiv.org/abs/2305.12295) (arXiv:2305.12295). ArXiv:2305.12295
    [cs].

    - <span id="page-12-2"></span>Terence Parr. 2013. *The Definitive ANTLR 4 Reference*,
    2nd edition. Pragmatic Bookshelf.

    - <span id="page-12-6"></span>Ricardo Wandré Dias Pedro, Fátima L. S. Nunes, and
    Ariane Machado-Lima. 2013. [Using grammars for](https://doi.org/10.1145/2543581.2543593)
    [pattern recognition in images.](https://doi.org/10.1145/2543581.2543593) *ACM
    Computing Surveys (CSUR)*, 46:1 – 34.

    - <span id="page-12-7"></span>M. Richetin and F. Vernadat. 1984. [Efficient regular](https://doi.org/10.1016/0031-3203(84)90063-3)
    [grammatical inference for pattern recognition.](https://doi.org/10.1016/0031-3203(84)90063-3)
    *Pattern Recognit.*, 17:245–250.

    - <span id="page-12-9"></span>Ernesto Rodrigues and Heitor Silvério Lopes. 2007.
    [Genetic programming for induction of context-free](https://api.semanticscholar.org/CorpusID:255763)
    [grammars.](https://api.semanticscholar.org/CorpusID:255763) *Seventh International
    Conference on Intelligent Systems Design and Applications (ISDA 2007)*, pages
    297–302.

    - <span id="page-12-8"></span>Michael Schröder and Jürgen Cito. 2022. [Grammars
    for](https://doi.org/10.1145/3510455.3512787) [free: toward grammar inference
    for ad hoc parsers.](https://doi.org/10.1145/3510455.3512787) In *Proceedings
    of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas
    and Emerging Results*, page 41–45, Pittsburgh Pennsylvania. ACM.

    - <span id="page-12-10"></span>Noah Shinn, Federico Cassano, Ashwin Gopinath,
    Karthik R Narasimhan, and Shunyu Yao. 2023. [Re](https://openreview.net/forum?id=vAElhFcKW6)[flexion:
    language agents with verbal reinforcement](https://openreview.net/forum?id=vAElhFcKW6)
    [learning.](https://openreview.net/forum?id=vAElhFcKW6) In *Thirty-seventh Conference
    on Neural Information Processing Systems*.

    - <span id="page-12-1"></span>Andrew Stevenson and J. Cordy. 2014a. [A survey
    of](https://doi.org/10.1016/j.scico.2014.05.008) [grammatical inference in software
    engineering.](https://doi.org/10.1016/j.scico.2014.05.008) *Sci. Comput. Program.*,
    96:444–459.

    - <span id="page-12-0"></span>Andrew Stevenson and James R. Cordy. 2014b. [A sur](https://doi.org/10.1016/j.scico.2014.05.008)[vey
    of grammatical inference in software engineering.](https://doi.org/10.1016/j.scico.2014.05.008)
    *Science of Computer Programming*, 96:444–459.

    - <span id="page-12-5"></span>Weizhi Tang and Vaishak Belle. 2024. Tom-lm: Delegating
    theory of mind reasoning to external symbolic executors in large language models.
    *arXiv preprint arXiv:2404.15515*.

    - <span id="page-12-11"></span>Gemma Team, Morgane Riviere, Shreya Pathak, Pier
    Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard,
    Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen,
    Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton
    Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt
    Hoffman, Shantanu Thakoor, Jean-Bastien Grill,


    Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish,
    Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock,
    Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu,
    Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher
    A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika
    Rogozinska, ´ Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira,
    Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn
    Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh
    Dhand, ´ Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway,
    Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe
    Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju
    yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin
    McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren
    Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini
    Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder
    Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt
    Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan,
    Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao,
    Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez,
    Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko
    Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena
    Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara
    Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause,
    Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan,
    Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas
    Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming
    Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor
    Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli
    Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks,
    Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu,
    Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin,
    Kathleen Kenealy, Robert Dadashi, and Alek Andreev. 2024. [Gemma 2: Improving](https://arxiv.org/abs/2408.00118)
    [open language models at a practical size.](https://arxiv.org/abs/2408.00118)
    *Preprint*, arXiv:2408.00118.


    <span id="page-12-3"></span>Brandon T. Willard and Rémi Louf. 2023. [Effi](https://arxiv.org/abs/2307.09702)[cient
    guided generation for large language models.](https://arxiv.org/abs/2307.09702)
    *Preprint*, arXiv:2307.09702.


    ### <span id="page-13-0"></span>A Dataset Construction


    To evaluate the capacity of LLMs in few-shot grammar generation, we present a
    dedicated dataset. We explain the details of the construction process in this
    section.


    For clear explanation, let Gref = S<sup>K</sup> <sup>k</sup>=1 <sup>G</sup> ref
    k be a set of reference grammars, where G ref k is a set of reference grammars
    in which each reference grammar G ref k having exactly k non-terminals and thus
    its |R| = k. Each set of reference grammars G ref <sup>k</sup> = {G ref k,1 ,
    Gref k,2 , . . . , Gref k,n } contains n reference grammar with k non-terminals.


    Initially, we constructed Gref = S<sup>9</sup> <sup>k</sup>=1 <sup>G</sup> ref
    k . For each number k, we prompted GPT-4o to produce n = 10 reference grammars
    to yield G ref <sup>k</sup> = {G ref k,1 , Gref k,2 , . . . , Gref k,10}, with
    the prompt template demonstrated in Prompt Template [3.](#page-17-1) In Prompt
    Template [3,](#page-17-1) k means the placeholder of the number of non-terminals,
    and n means the number of reference grammars needed to be produced. However, GPT-4o
    failed to consistently generate reference grammars in the correct syntax or with
    the correct number of non-terminals, especially as the number of non-terminals
    increased. To ensure that the generated reference grammars are syntactically correct
    and have the correct number of nonterminals, we used a BNF parser to do verification.
    It takes a Gref and checks whether valid(Gref ) is true and whether it has the
    required number of non-terminals. Any reference grammar that is not valid or has
    a wrong number of non-terminals were manually corrected. For duplicated reference
    grammars, we prompted GPT-4o to generate an alternative. This resulted in 90 reference
    grammars (i.e., |Gref | = | S<sup>9</sup> <sup>k</sup>=1 <sup>G</sup> ref k |
    = 90), which are the reference grammars used to generate positive and negative
    examples subsequently.


    For each reference grammar Gref ∈ Gref , we prompted GPT-4o to generate 6 various
    challenges. For each challenge, GPT-4o is prompted by Prompt Template [6](#page-17-1)
    and Prompt Template [4](#page-17-1) to produce a set of 3 positive examples (P
    ⊆ L(Gref )), and a set of three negative examples (N ∩ L(Gref ) = ∅), respectively.
    In both Prompt Template [6](#page-17-1) and Prompt Template [4,](#page-17-1) m
    means the number of examples needed to be generated, and *reference\_grammar*
    means the given reference grammar by which the generated examples should be accepted
    or rejected. However, we observed that GPT-4o frequently failed to produce valid
    positive and negative examples, leading to either P ̸⊆ L(Gref ) or


    N ∩ L(Gref ) ̸= ∅, or both. The number of failures tends to increase as the number
    of non-terminals of Gref increases. To ensure the correctness of the generated
    challenges, we used a BNF parser to verify whether all given positive examples
    and negative examples can be accepted and rejected, respectively, by their corresponding
    Gref . Erroneous positive and negative examples were manually corrected. Ultimately,
    we obtained a dataset consisting of a total of 540 challenges.


    We visually summarize the dataset construction procedure in Figure [2.](#page-14-2)


    ### <span id="page-13-1"></span>B Grammar Quality Metrics


    This section covers the formal definitions of the grammar quality metrics.


    First define Π<sup>P</sup> ⊆ Π to be the set of production rules that are used
    in the left-most derivations of all positive examples in P. That is, the set of
    rules in Π which occur in a sequence of rules S → α<sup>1</sup> → . . . → α<sup>n</sup>
    → p where p ∈ P, and all rules expand the left-most non-terminal in α1, . . .
    , αn.


    Let Π<sup>∗</sup> be the production rules in G<sup>∗</sup> and Πref be the production
    rules in Gref . Let us define Diff (Gref , G<sup>∗</sup> ) = |Π ref P | − |Π<sup>∗</sup>
    P |. The average difference in production rules used over the whole set of k solved
    challenges, C ′ , is given by:


    $$Diff^\diamond = \frac{1}{k} \sum\_{i=1}^k Diff(G\_i^{ref}, G\_i^\*).$$


    We define two indicator functions, which indicate when a grammar uses substantially
    fewer rules than the reference grammar, and substantially more rules than the
    reference grammar:


    $$\begin{aligned} \mathbb{I}\_{OF}(G^{ref}, G^\*, \mathcal{P}) &= \begin{cases}
    1 & \text{if } |\Pi\_{\mathcal{P}}^{ref}| - |\Pi\_{\mathcal{P}}^\*| \\ & > \frac{|\Pi\_{\mathcal{P}}^{ref}|}{2}
    \\ 0 & \text{otherwise.} \end{cases} \\\\ \mathbb{I}\_{OG}(G^{ref}, G^\*, \mathcal{P})
    &= \begin{cases} 1 & \text{if } |\Pi\_{\mathcal{P}}^{ref}| - |\Pi\_{\mathcal{P}}^\*|
    \\ & < -\frac{|\Pi\_{\mathcal{P}}^{ref}|}{2} \\ 0 & \text{otherwise.} \end{cases}
    \end{aligned}$$


    The metric to estimate whether the generated grammars overfit the examples, is
    then given by


    $$OF(C'') = \frac{1}{k} \sum\_{i=1}^{k} \mathbb{I}\_{OF}(G\_i^{ref}, G\_i^\*,
    \mathcal{P}\_i).$$


    <span id="page-14-2"></span>![](_page_14_Figure_0.jpeg)


    (5) Refine & Correct Figure 2: The Dataset Construction Process: (1) GPT-4o is
    prompted with Prompt Template 3 to generate a set of reference grammars; (2) A
    set of reference grammars Gref = S9 <sup>k</sup>=1 <sup>G</sup> ref k are generated
    by LLMs; (3) A BNF parser is used to check the correctness of each generated reference
    grammar; (4) Erroneous reference grammars are reported to humans; (5) Reported
    reference grammars are modified and corrected manually; (6) GPT-4o is prompted
    with Prompt Template 6 and Prompt Template [4](#page-17-1) to generate challenges
    for each reference grammars; (7) Challenges are generated by LLMs, in which each
    challenge consists of 3 positive and 3 negative examples; (8) A BNF parser is
    used to verify whether positive and negative examples are accepted and rejected
    by their corresponding reference grammar respectively; (9) Erroneous challenges
    are reported to humans; (10) Reported challenges are corrected manually; (11)
    The final dataset consisting of 540 challenges are obtained.


    The metric to estimate whether the generated grammars overgeneralize the examples,
    OG, is given as:


    $$OG(C'') = \frac{1}{k} \sum\_{i=1}^{k} \mathbb{I}\_{OG}(G\_i^{ref}, G\_i^\*,
    \mathcal{P}\_i).$$


    In addition, the TU metric, for a given challenge, to measure the percentage of
    |Π<sup>∗</sup> | taken up by |Π<sup>∗</sup> P |, indicating the utility of G<sup>∗</sup>
    , is given as:


    $$TU(G^\*, \mathcal{P}) = \frac{|\Pi\_{\mathcal{P}}^\*|}{|\Pi^\*|}$$


    for which lower T U indicates a bunch of irrelevant or nonsensical production
    rules of G<sup>∗</sup> while higher T U indicates the opposite. The average utility
    over C ′ is given by:


    $$TU^\diamond = \frac{1}{k} \sum\_{i=1}^k TU(G\_i^\*, \mathcal{P})$$


    ## <span id="page-14-0"></span>C Direct Prompting


    For the DP approach, Prompt Template [1](#page-17-2) is used to prompt LLMs to
    directly generate a grammar with a given set of positive and negative examples.
    In Prompt Template 1, *positive\_examples* and *negative\_examples* are placeholders
    for a set of positive and negative examples. In addition, it also specifies a
    list of requirements LLMs should take care of and obey when generating grammars.


    ## <span id="page-14-1"></span>D Optimization of BNF Parser for Providing LLM-Friendly
    Feedback


    (10) Correct & Refine For the OPF approach, we use the same prompt template used
    in DP, as shown in Prompt Template [1,](#page-17-2) to prompt LLMs to generate
    an initial grammar. Then, Prompt Template 5 is used in the iterations of the feedback
    loop to construct prompts from feedback offered by the BNF parser to LLMs. In
    Prompt Template [5,](#page-17-1) *positive\_examples* and *negative\_examples*
    are the placeholders for a set of positive and negative examples, *bnf\_grammar*
    means the previously generated erroneous grammar, and *parser\_feedback* is the
    placeholder for feedback provided from the BNF parser.


    Furthermore, for each feedback given by the BNF parser, in addition to giving
    essential feedback such as notifying the line number for the place the error occurs,
    we optimize the BNF parser to also provide LLM-friendly feedback to LLMs, such
    as the possible reasons for the error or ways to fix it. We have shown some of
    them in Parser Feedback 1 and Parser Feedback [2.](#page-17-1)


    In addition, OPF includes a parameter called *max\_turns*, which specifies the
    maximum number of feedback iterations. If an LLM can generate a valid grammar
    based on earlier feedback, the algorithm stops early; otherwise, it continues
    until reaching the specified maximum.


    Moreover, it is worth noting that this approach does not aim to follow every component
    of Reflexion or Self-Refine strictly. For instance, it does not


    maintain a long-term context or external memory. Instead, it uses only the most
    recent feedback in each turn to guide self-refinement. Concretely, in each feedback
    iteration, an LLM is provided with the previously generated erroneous grammar,
    corresponding positive and negative examples, and the latest feedback, to produce
    revised grammars. Therefore, since, in each iteration, an LLM may produce similar
    and even the same grammar as the previous ones, especially when they fail to fix
    the previous errors leading to the same feedback provided by the parser, we set
    the temperature to 0.3 to expect to enable LLMs to generate more diverse grammars
    even for encountering the same feedback, to optimize the performance. This approach
    highlights the optimization from the perspective of the parser to provide more
    LLM-friendly feedback. However, due to limited space and the trivial-yetcomplex
    optimization process, for the details of the optimization of the BNF parser, please
    refer to our source code and the comments.


    ## <span id="page-15-1"></span>E LLM-Driven Hybrid Genetic Algorithm


    The pseudocode of HyGenar is presented in Algorithm [1,](#page-16-0) with detailed
    descriptions of its primary functions including Fitness, Select, Cross, and Mutate
    provided in Section [5.](#page-4-2) In Algorithm [1,](#page-16-0) we note Fitness
    function as FITNESS, Select function as SELECT, Cross function as CROSS, and Mutate
    function as MUTATE.


    As shown in Algorithm [1,](#page-16-0) it takes seven parameters: P and N means
    a set of positive and negative examples respectively, k indicates *population
    size*, g represents *generations* which means the number of iterations of evolution,
    ρ means the *crossover rate*, µ means the *mutation rate*, and LLM means an LLM
    which takes a prompt and returns a response. In addition, PROMPTGENERATOR means
    to generate a prompt from the prompt template shown in Prompt Template [1.](#page-17-2)
    MAXFITNESSS-CORE is a constant indicating the highest fitness score any candidate
    grammar can achieve and due to each challenge in the constructed dataset only
    having 3 positive and 3 negative examples, the highest fitness score is 6. We
    thus set MAXFIT-NESSSCORE to 6.


    In addition, for *LLMMut*, we have shown the prompt template used to prompt LLMs
    to mutate a given grammar in Prompt Template [2,](#page-17-1) in which *bnf\_grammar*
    means the placeholder for a candidate grammar while *positive\_examples* and *negative\_examples*
    means a set of positive and negative examples respectively.


    ## F Additional Results


    In addition to the results shown in Table [1,](#page-5-0) we have shown 7 more
    results and discussed them respectively in subsections [F.1,](#page-15-2) [F.2,](#page-15-3)
    [F.3,](#page-15-0) [F.4,](#page-17-0) and [F.5.](#page-17-3)


    ## <span id="page-15-2"></span>F.1 Results for C1, C2, and C<sup>3</sup>


    Table [4](#page-19-0) presents the results categorized into three subsets: C1,
    C2, and C3. The subset C<sup>1</sup> includes challenges where the reference grammars
    have 1 ∼ 3 non-terminals, C<sup>2</sup> are those with 4 ∼ 6 non-terminals, and
    C<sup>3</sup> consists of challenges with 7 ∼ 9 non-terminals. Therefore, it aims
    to demonstrate and analyze the performance of LLMs as the number of non-terminals
    increases.


    As the results are shown in Table [4,](#page-19-0) as the number of non-terminals
    increases, both SX and SE decrease across all LLMs. While DP and OPF exhibit suboptimal
    and unsatisfactory performance, HyGenar consistently demonstrates and contributes
    substantial improvements across most LLMs, even as the number of non-terminals
    increases. For example, in the case of *GPT-4o*, HyGenar increases the SE by 21%
    compared to DP and OPF on C3. Similarly, with *GPT-3.5-Turbo*, compared to DP
    and OPF, HyGenar improves the SE by 30% and 28% on C<sup>2</sup> and 20% and 19%
    on C3, respectively.


    ## <span id="page-15-3"></span>F.2 Results for P1, P2, and P<sup>3</sup>


    Table [5](#page-20-0) demonstrates the results grouped into three subsets: P1,
    P2, and P3. The subset P<sup>1</sup> consists of challenges where the reference
    grammars have 1 ∼ 6 production rules, P<sup>2</sup> includes those with 7 ∼ 15
    production rules, and P<sup>3</sup> consists of challenges with the number of
    production rules greater than 16. Therefore, it aims to show and analyze the performance
    of LLMs as the number of production rules increases.


    Similar to the results of C1, C2, and C3, as the number of production rules increases,
    both SX and SE decrease across all LLMs. Nevertheless, Hy-Genar can still steadily
    improve both SE and SX , even as the number of production rules increases.


    ### <span id="page-15-0"></span>F.3 Results for Diff , OF, and OG Metrics


    To investigate whether LLMs generate grammars in an overfitted manner and whether
    HyGenar improves performance through overfitting, as well as


    #### <span id="page-16-0"></span>Algorithm 1 HyGenar


    |     | 1: procedure GENERATEGRAMMAR(P,<br>N                          | , k, g,
    ρ, µ, LLM)                                              |

    |-----|---------------------------------------------------------------|-----------------------------------------------------------------|

    | 2:  | population<br>←<br>[]                                         | ▷<br>Initialize
    population as an empty list                     |

    | 3:  | G∗<br>←<br>.best<br>null                                      | ▷<br>Keep
    track of overall best grammar                         |

    | 4:  | ← −1<br>f itness.best                                         | ▷<br>Track
    highest fitness found so far                         |

    | 5:  | ←<br>for i<br>1<br>to k<br>do                                 |                                                                 |

    | 6:  | ←<br>prompt<br>PROMPTGENERATOR                                | ▷<br>Use
    prompt template from Prompt Template 1                 |

    | 7:  | G∗ ←<br>LLM(prompt)                                           | ▷<br>Use
    LLM to get an initial candidate                        |

    | 8:  | FITNESS(G∗<br>score<br>←<br>,P,<br>N<br>)                     | ▷<br>Compute
    fitness                                            |

    | 9:  | if score<br>=<br>MAXFITNESSSCORE then                         |                                                                 |

    | 10: | return G∗                                                     | ▷<br>Return
    early if perfect score is achieved                  |

    | 11: | [G∗<br>population<br>←<br>population<br>∥<br>]                | ▷<br>Add
    candidate to population                                |

    | 12: | for i<br>←<br>1<br>to g<br>do                                 |                                                                 |

    | 13: | ←<br>f itnessScores<br>[]                                     |                                                                 |

    | 14: | for G∗ ∈<br>population<br>do                                  |                                                                 |

    | 15: | FITNESS(G∗<br>←<br>,P,<br>N<br>score<br>)                     |                                                                 |

    | 16: | ←<br>f itnessScores<br>f itnessScores                         | [(score,
    G∗<br>∥<br>)]<br>▷<br>Add a tuple of score and grammar |

    | 17: | if score > f itness.best<br>then                              |                                                                 |

    | 18: | f itness.best<br>←<br>score                                   |                                                                 |

    | 19: | G∗<br>G∗<br>.best<br>←                                        |                                                                 |

    | 20: | if f itness.best<br>=<br>MAXFITNESSSCORE then                 |                                                                 |

    | 21: | return G∗<br>.best                                            | ▷<br>Return
    the best grammar found                              |

    | 22: | G<br>= [G∗<br>(score, G∗<br> <br>)<br>∈<br>f itnessScores]    |                                                                 |

    | 23: | (score, G∗<br>S<br>= [score<br> <br>)<br>∈<br>f itnessScores] |                                                                 |

    | 24: | SELECT(G, S)<br>selected<br>←                                 |                                                                 |

    | 25: | population.new<br>←<br>[]                                     |                                                                 |

    | 26: | while  population.new <br>< k<br>do                           |                                                                 |

    | 27: | G∗<br>, G∗<br>b ←<br>RANDOMCHOICE(selected)<br>a              |                                                                 |

    | 28: | G∗ ←<br>CROSS(G∗<br>, G∗<br>, ρ)<br>a<br>b                    |                                                                 |

    | 29: | if UNIFORM(0,<br>1)<br>< µ<br>then                            |                                                                 |

    | 30: | G∗ ←<br>MUTATE(G∗<br>,P,<br>N<br>, LLM)                       |                                                                 |

    | 31: | population.new<br>←<br>population.new                         | [G∗<br>∥<br>]                                                   |

    | 32: | population<br>←<br>population.new<br>return G∗<br>.best       | ▷<br>Proceed
    to the next generation                             |


    to examine whether LLMs and our method produce overly generalized grammars, we
    employed the three evaluation metrics: Diff , OF, OG. The results are presented
    in Table [6,](#page-21-0) [7,](#page-22-0) and [8,](#page-23-0) respectively.
    The notation "N/A" indicates inapplicability. Since these three metrics are only
    applicable when a grammar possesses correct semantics, "N/A" thus signifies that
    no grammar in the evaluation set exhibits correct semantics.


    Through the Diff metric, as shown in Tabel [6,](#page-21-0) we observe that the
    number of production rules used in derivations of the generated grammars and the
    reference grammars does not differ significantly on average. However, as the number
    of production rules in the reference grammar increases, the Diff <sup>⋄</sup>


    exhibits a slight upward trend. The Diff <sup>⋄</sup> is almost always positive
    across most LLMs and methods, which indicates that, in most cases, the number
    of production rules used by the generated grammars is lower than that of the reference
    grammars.


    Furthermore, Table [7](#page-22-0) presents the OF metric. For some models, particularly
    *GPT-3.5-Turbo*, an increasing number of production rules corresponds to a certain
    degree of overfitting. Nevertheless, on average, most LLMs do not exhibit significant
    overfitting. Additionally, we observe that in HyGenar, the OF metric does not
    show a significant difference compared to the baselines, indicating that HyGenar
    does not improve performance through overfitting.


    Additionally, we present the OG metric in Table [8.](#page-23-0) We observed that,
    for some models, such as *Qwen:72b-Instruct*, as the number of production rules
    in the reference grammar increases, they tend to generate overly generalized grammars.
    Nevertheless, on average, most LLMs do not tend to generate overgeneralized grammars.


    ### <span id="page-17-0"></span>F.4 Results for TU Metrics


    To investigate whether LLMs generate irrelevant production rules, we employ the
    TU metric. For example, given the challenge shown in Figure [1,](#page-0-1) a
    generated grammar might be:


    ```

    <stmt> ::= <func> "(" <args> ")"

    <args> ::= <expr> | <expr> "," <args>

    <expr> ::= <char> | <number>

    <func> ::= <char> <func> | <char>

    <char> ::= "a" | ... | "z"

    <number> ::= "0" | ... | "9"

    <hello> ::= "hello"

    <world> ::= "world"

    ```

    in which:


    <hello> ::= "hello" <world> ::= "world"


    are two irrelevant production rules.


    As shown in Table [9,](#page-24-0) on average, across both baselines and in HyGenar,
    TU remains relatively high, indicating that LLMs do not tend to produce irrelevant
    production rules. However, as the number of production rules increases, TU shows
    a tendency of declination. Nevertheless, this does not imply that LLMs generate
    more irrelevant rules. Considering the results from OG, we think this decrease
    may more likely be attributable to the generated grammar becoming more generalized.


    ## <span id="page-17-3"></span>F.5 Results for Robustness Evaluation


    Since HyGenar requires setting the temperature greater than 0 which we set to
    0.7, we repeated 5 independent experiments for both *GPT-4o* and *GPT-3.5-Turbo*
    to ensure the temperature does not affect the results significantly.


    The results are demonstrated in Table [3,](#page-18-0) in which each row means
    the results of one experiment. The averages of syntax correctness of *GPT-4o*
    and *GPT-3.5-Turbo* are 95.8% and 98.6% and the standard deviations are 0.4% and
    0.49%, respectively. The averages of semantic correctness of *GPT-4o* and


    *GPT-3.5-Turbo* are 93.2% and 61.6% and the standard deviations are 0.4% and 0.49%.
    Therefore, it indicates that although we set the temperature to 0.7 in HyGenar,
    the fluctuation of both syntax correctness and semantic correctness are very slight
    and the performance across multiple experiments stays steady. Thus, the results
    demonstrated the robustness of our proposed method, HyGenar.


    <span id="page-17-2"></span>Prompt Template 1: Generate a Grammar Directly with
    a Given Set Positive and Negative Examples


    Given a set of positive and negative examples, generate the Backus–Naur Form (BNF)
    grammar that accepts all positive examples and rejects all negative examples.
    1. Only generate the standard BNF grammar;


    2. The generated BNF grammar MUST accept all positive examples and reject all
    negative examples;


    3. Each terminal symbol MUST be quoted with double quotes and MUST NOT escape
    double quotes or pipeline in terminal symbols;


    4. Pay special attention to whether spaces, line breaks, or other special symbols
    are required between each symbol, and if so, these need to be explicitly specified,
    e.g. <term> ::= "1" "+" "2" can handle "1+2" but not "1 + 2" while <term> ::=
    "1" " " "+" " " "2" can handle "1 + 2" but not "1+2"; 5. The entry point of the
    generated BNF grammar MUST be the non-terminal symbol in the first production
    rule; 6. Only the generated BNF should be wrapped in a pair of triple backtick;
    7. Do NOT output any additional texts, comments, or explanations.


    <span id="page-17-1"></span>===Positive Examples=== {*positive\_examples*} ===Negative
    Examples=== {*negative\_examples*}


    <span id="page-18-0"></span>


    | Experiment    | SX | SE |  |  |  |  |  |

    |---------------|----|----|--|--|--|--|--|

    | GPT-4o        |    |    |  |  |  |  |  |

    | 1st           | 95 | 93 |  |  |  |  |  |

    | 2nd           | 96 | 93 |  |  |  |  |  |

    | 3rd           | 96 | 94 |  |  |  |  |  |

    | 4th           | 96 | 93 |  |  |  |  |  |

    | 5th           | 96 | 93 |  |  |  |  |  |

    | GPT-3.5-Turbo |    |    |  |  |  |  |  |

    | 1st           | 98 | 62 |  |  |  |  |  |

    | 2nd           | 98 | 62 |  |  |  |  |  |

    | 3rd           | 99 | 62 |  |  |  |  |  |

    | 4th           | 99 | 61 |  |  |  |  |  |

    | 5th           | 99 | 61 |  |  |  |  |  |


    Table 3: Results of Syntax and Semantic Correctness for HyGenar with *GPT-4o*
    and *GPT-3.5-Turbo* on Grammar Generation by Conducting 5 Independent Experiments
    (%)


    ### Parser Feedback 1: Invalid Production Rule


    This error is likely due to not satisfying one of the following requirements:


    1. A rule MUST start with a non-terminal definition;


    2. A non-terminal symbol MUST be in angle brackets, e.g. <non-terminal>; 3. A
    non-terminal definition must be


    followed by ''::='' to indicate the start of the right-hand side;


    ## Prompt Template 2: LLM-Driven Mutation


    Modify the following BNF grammar slightly to improve its acceptance of the positive
    examples and rejection of the negative examples.


    ===BNF Grammar=== {*bnf\_grammar*}


    ===Positive Examples=== {*positive\_examples*} ===Negative Examples=== {*negative\_examples*}


    Only output the modified BNF grammar wrapped in triple backticks.


    ## Prompt Template 3: Generate Grammars


    Generate a list of random standard Backus-Naur Form (BNF) grammar with the following
    constraints:


    1. Each generated BNF grammar MUST be SELF-CONTAINED and VALID, which means it
    should be able to recognize a valid string;


    2. Each generated BNF grammar MUST have exactly {k} lines;


    3. Each generated BNF grammar MUST be unique;


    4. Each generated BNF grammar MUST be separated by a newline in addition to the
    linebreak;


    5. For each generated BNF grammar, the entry point MUST be at the first line;


    6. Only generate {n} BNF grammars;


    7. Only output BNF grammars WITHOUT any additional text or code block, like "```".


    ## Prompt Template 4: Generate Negative Examples with a Given Grammar


    Generate a list of negative examples with the following constraints:


    1. Each example MUST be separated by a newline in addition to the linebreak;


    2. Only output examples WITHOUT any additional text or code block, like "```";


    3. Only output {m} examples;


    4. Each example MUST be generated based on the given BNF grammar;


    5. Each example should be greatly related to the given BNF grammar, but ensure
    it is NOT a valid string for the given BNF grammar.


    For example, given the following BNF grammar:


    <term> ::= "0" | "1" | "2" you should output negative examples like: 6


    \*


    9


    Then, the given BNF grammar is: {*reference\_grammar*}


    <span id="page-19-0"></span>


    | Challenge Set           | SXDP | SXOP F | SXHyGenar           | SEDP | SEOP
    F | SEHyGenar  |

    |-------------------------|------|--------|---------------------|------|--------|------------|

    | GPT-4o                  |      |        |                     |      |        |            |

    | C1                      | 100  | 100    | 100                 | 99   | 99     |
    100 ↑1 ↑1  |

    | C2                      | 100  | 100    | 100                 | 93   | 95 ↑2  |
    100 ↑7 ↑5  |

    | C3                      | 79   | 92 ↑13 | 87 ↑8 ↓5            | 59   | 59     |
    80 ↑21 ↑21 |

    | All                     | 93   | 97 ↑4  | 96 ↑3 ↓1            | 84   | 85 ↑1  |
    93 ↑9 ↑8   |

    |                         |      |        | GPT-3.5-Turbo       |      |        |            |

    | C1                      | 98   | 97 ↓1  | 100 ↑2 ↑3           | 72   | 71 ↓1  |
    93 ↑21 ↑22 |

    | C2                      | 98   | 99 ↑1  | 100 ↑2 ↑1           | 28   | 30 ↑2  |
    58 ↑30 ↑28 |

    | C3                      | 84   | 90 ↑6  | 96 ↑12 ↑6           | 11   | 12 ↑1  |
    31 ↑20 ↑19 |

    | All                     | 94   | 95 ↑1  | 99 ↑5 ↑4            | 37   | 38 ↑1  |
    61 ↑24 ↑23 |

    |                         |      |        | Qwen:72b-Chat       |      |        |            |

    | C1                      | 73   | 76 ↑3  | 96 ↑23 ↑20          | 52   | 53 ↑1  |
    76 ↑24 ↑23 |

    | C2                      | 48   | 48     | 77 ↑29 ↑29          | 6    | 8 ↑2   |
    26 ↑20 ↑18 |

    | C3                      | 20   | 23 ↑3  | 56 ↑36 ↑33          | 1    | 2 ↑1   |
    11 ↑10 ↑9  |

    | All                     | 47   | 49 ↑2  | 76 ↑29 ↑27          | 20   | 21 ↑1  |
    38 ↑18 ↑17 |

    |                         |      |        | Llama3:70b-Instruct |      |        |            |

    | C1                      | 88   | 90 ↑2  | 97 ↑9 ↑7            | 78   | 77 ↓1  |
    94 ↑16 ↑17 |

    | C2                      | 54   | 60 ↑6  | 76 ↑22 ↑16          | 31   | 35 ↑4  |
    61 ↑30 ↑26 |

    | C3                      | 28   | 34 ↑6  | 52 ↑24 ↑18          | 15   | 14 ↓1  |
    29 ↑14 ↑15 |

    | All                     | 57   | 61 ↑4  | 75 ↑18 ↑14          | 41   | 42 ↑1  |
    61 ↑20 ↑19 |

    |                         |      |        | Gemma2:27b-Instruct |      |        |            |

    | C1                      | 99   | 100 ↑1 | 100 ↑1              | 91   | 92 ↑1  |
    98 ↑7 ↑6   |

    | C2                      | 97   | 97     | 99 ↑2 ↑2            | 49   | 49     |
    84 ↑35 ↑35 |

    | C3                      | 76   | 79 ↑3  | 93 ↑17 ↑14          | 26   | 29 ↑3  |
    54 ↑28 ↑25 |

    | All                     | 91   | 92 ↑1  | 98 ↑7 ↑6            | 56   | 57 ↑1  |
    79 ↑23 ↑22 |

    |                         |      |        | Mistral:7b-Instruct |      |        |            |

    | C1                      | 1    | 25 ↑24 | 3 ↑2 ↓22            | 0    | 17 ↑17
    | 2 ↑2 ↓15   |

    | C2                      | 1    | 20 ↑19 | 1 ↓19               | 1    | 6 ↑5   |
    0 ↓1 ↓6    |

    | C3                      | 1    | 11 ↑10 | 0 ↓1 ↓11            | 0    | 1 ↑1   |
    0 ↓1       |

    | All                     | 1    | 19 ↑18 | 1 ↓18               | 0    | 8 ↑8   |
    1 ↑1 ↓7    |

    |                         |      |        | Codestral:22b       |      |        |            |

    | C1                      | 82   | 96 ↑14 | 99 ↑17 ↑3           | 82   | 92 ↑10
    | 98 ↑16 ↑6  |

    | C2                      | 53   | 77 ↑24 | 86 ↑33 ↑9           | 36   | 45 ↑9  |
    69 ↑33 ↑24 |

    | C3                      | 23   | 39 ↑16 | 57 ↑34 ↑18          | 15   | 19 ↑4  |
    33 ↑18 ↑14 |

    | All                     | 53   | 71 ↑18 | 80 ↑27 ↑9           | 44   | 52 ↑8  |
    67 ↑23 ↑15 |

    | Starcoder2:15b-Instruct |      |        |                     |      |        |            |

    | C1                      | 97   | 68 ↓29 | 100 ↑3 ↑32          | 67   | 42 ↓25
    | 84 ↑17 ↑42 |

    | C2                      | 73   | 65 ↓8  | 99 ↑26 ↑34          | 14   | 12 ↓2  |
    31 ↑17 ↑19 |

    | C3                      | 58   | 48 ↓10 | 94 ↑36 ↑46          | 11   | 7 ↓4   |
    17 ↑6 ↑10  |

    | All                     | 76   | 60 ↓16 | 98 ↑22 ↑38          | 30   | 20 ↓10
    | 44 ↑14 ↑24 |


    Table 4: Averages of Syntax and Semantic Correctness Grouped in C1, C2, and C<sup>3</sup>
    (%)


    <span id="page-20-0"></span>


    | Challenge Set           | SXDP | SXOP F | SXHyGenar           | SEDP | SEOP
    F | SEHyGenar  |

    |-------------------------|------|--------|---------------------|------|--------|------------|

    |                         |      |        | GPT-4o              |      |        |            |

    | P1                      | 100  | 100    | 100                 | 99   | 99     |
    100 ↑1 ↑1  |

    | P2                      | 100  | 100    | 100                 | 96   | 95 ↓1  |
    100 ↑4 ↑5  |

    | P3                      | 81   | 93 ↑12 | 89 ↑8 ↓4            | 62   | 64 ↑2  |
    82 ↑20 ↑18 |

    | All                     | 93   | 97 ↑4  | 96 ↑3 ↓1            | 84   | 85 ↑1  |
    93 ↑9 ↑8   |

    |                         |      |        | GPT-3.5-Turbo       |      |        |            |

    | P1                      | 98   | 97 ↓1  | 100 ↑2 ↑3           | 69   | 67 ↓2  |
    93 ↑24 ↑26 |

    | P2                      | 99   | 100 ↑1 | 100 ↑1              | 18   | 22 ↑4  |
    42 ↑24 ↑20 |

    | P3                      | 86   | 90 ↑4  | 96 ↑10 ↑6           | 23   | 24 ↑1  |
    46 ↑23 ↑22 |

    | All                     | 94   | 95 ↑1  | 99 ↑5 ↑4            | 37   | 38 ↑1  |
    61 ↑24 ↑23 |

    |                         |      |        | Qwen:72b-Chat       |      |        |            |

    | P1                      | 72   | 74 ↑2  | 96 ↑24 ↑22          | 42   | 43 ↑1  |
    69 ↑27 ↑26 |

    | P2                      | 51   | 51     | 76 ↑25 ↑25          | 12   | 13 ↑1  |
    24 ↑12 ↑11 |

    | P3                      | 21   | 25 ↑4  | 59 ↑38 ↑34          | 6    | 7 ↑1   |
    21 ↑15 ↑14 |

    | All                     | 47   | 49 ↑2  | 76 ↑29 ↑27          | 20   | 21 ↑1  |
    38 ↑18 ↑17 |

    |                         |      |        | Llama3:70b-Instruct |      |        |            |

    | P1                      | 86   | 90 ↑4  | 97 ↑11 ↑7           | 73   | 73     |
    92 ↑19 ↑19 |

    | P2                      | 63   | 69 ↑6  | 86 ↑23 ↑17          | 38   | 40 ↑2  |
    67 ↑29 ↑27 |

    | P3                      | 26   | 30 ↑4  | 47 ↑21 ↑17          | 15   | 15     |
    29 ↑14 ↑14 |

    | All                     | 57   | 61 ↑4  | 75 ↑18 ↑14          | 41   | 42 ↑1  |
    61 ↑20 ↑19 |

    |                         |      |        | Gemma2:27b-Instruct |      |        |            |

    | P1                      | 99   | 100 ↑1 | 100 ↑1              | 87   | 88 ↑1  |
    98 ↑11 ↑10 |

    | P2                      | 99   | 99     | 100 ↑1 ↑1           | 48   | 47 ↓1  |
    84 ↑36 ↑37 |

    | P3                      | 77   | 80 ↑3  | 94 ↑17 ↑14          | 33   | 37 ↑4  |
    58 ↑25 ↑21 |

    | All                     | 91   | 92 ↑1  | 98 ↑7 ↑6            | 56   | 57 ↑1  |
    79 ↑23 ↑22 |

    |                         |      |        | Mistral:7b-Instruct |      |        |            |

    | P1                      | 2    | 26 ↑24 | 3 ↑2 ↓23            | 1    | 15 ↑14
    | 2 ↑1 ↓13   |

    | P2                      | 0    | 19 ↑19 | 1 ↑1 ↓18            | 0    | 5 ↑5   |
    0 ↓5       |

    | P3                      | 0    | 13 ↑13 | 0 ↓13               | 0    | 3 ↑3   |
    0 ↓3       |

    | All                     | 1    | 19 ↑18 | 1 ↓18               | 0    | 8 ↑8   |
    1 ↑1 ↓7    |

    |                         |      |        | Codestral:22b       |      |        |            |

    | P1                      | 82   | 97 ↑15 | 99 ↑17 ↑2           | 79   | 89 ↑10
    | 97 ↑18 ↑8  |

    | P2                      | 51   | 73 ↑22 | 80 ↑29 ↑7           | 35   | 38 ↑3  |
    66 ↑31 ↑28 |

    | P3                      | 29   | 47 ↑18 | 64 ↑35 ↑17          | 21   | 30 ↑9  |
    41 ↑20 ↑11 |

    | All                     | 53   | 71 ↑18 | 80 ↑27 ↑9           | 44   | 52 ↑8  |
    67 ↑23 ↑15 |

    | Starcoder2:15b-Instruct |      |        |                     |      |        |            |

    | P1                      | 96   | 68 ↓28 | 100 ↑4 ↑32          | 54   | 34 ↓20
    | 74 ↑20 ↑40 |

    | P2                      | 65   | 59 ↓6  | 99 ↑34 ↑40          | 16   | 10 ↓6  |
    26 ↑10 ↑16 |

    | P3                      | 67   | 55 ↓12 | 95 ↑28 ↑40          | 20   | 15 ↓5  |
    31 ↑11 ↑16 |

    | All                     | 76   | 60 ↓16 | 98 ↑22 ↑38          | 30   | 20 ↓10
    | 44 ↑14 ↑24 |


    Table 5: Averages of Syntax and Semantic Correctness Grouped in P1, P2, and P<sup>3</sup>
    (%)


    <span id="page-21-0"></span>


    | Challenge Set | ⋄<br>Diff<br>DP | ⋄<br>Diff<br>OP F       | ⋄<br>Diff<br>HyGenar
    |  |

    |---------------|-----------------|-------------------------|----------------------|--|

    |               |                 | GPT-4o                  |                      |  |

    | P1            | 0.22            | 0.22                    | 0.21                 |  |

    | P2            | 1.38            | 1.32                    | 1.18                 |  |

    | P3            | 4.37            | 3.95                    | 3.36                 |  |

    | All           | 1.76            | 1.63                    | 1.65                 |  |

    |               |                 | GPT-3.5-Turbo           |                      |  |

    | P1            | 0.42            | 0.37                    | 0.27                 |  |

    | P2            | 2.43            | 2.34                    | 2.21                 |  |

    | P3            | 3.38            | 3.42                    | 4.30                 |  |

    | All           | 1.40            | 1.43                    | 1.81                 |  |

    |               |                 | Qwen:72b-Chat           |                      |  |

    | P1            | 0.12            | 0.08                    | -0.02                |  |

    | P2            | 0.32            | 0.38                    | 0.38                 |  |

    | P3            | 2.33            | 3.00                    | 3.02                 |  |

    | All           | 0.04            | 0.50                    | 0.68                 |  |

    |               |                 | Llama3:70b-Instruct     |                      |  |

    | P1            | 0.39            | 0.38                    | 0.44                 |  |

    | P2            | 1.56            | 1.62                    | 1.50                 |  |

    | P3            | 2.52            | 2.42                    | 2.85                 |  |

    | All           | 1.00            | 1.00                    | 1.21                 |  |

    |               |                 | Gemma2:27b-Instruct     |                      |  |

    | P1            | 0.55            | 0.54                    | 0.59                 |  |

    | P2            | 1.77            | 1.74                    | 1.50                 |  |

    | P3            | 4.03            | 4.27                    | 4.40                 |  |

    | All           | 1.64            | 1.74                    | 1.93                 |  |

    |               |                 | Mistral:7b-Instruct     |                      |  |

    | P1            | 1.00            | -0.59                   | 0.00                 |  |

    | P2            | N/A             | 1.88                    | N/A                  |  |

    | P3            | N/A             | 3.17                    | N/A                  |  |

    | All           | 1.00            | 0.44                    | 0.00                 |  |

    |               |                 | Codestral:22b           |                      |  |

    | P1            | 0.18            | 0.16                    | 0.10                 |  |

    | P2            | 1.26            | 1.17                    | 0.75                 |  |

    | P3            | 1.86            | 2.34                    | 2.96                 |  |

    | All           | 0.72            | 0.85                    | 0.94                 |  |

    |               |                 | Starcoder2:15b-Instruct |                      |  |

    | P1            | 0.39            | 0.41                    | 0.20                 |  |

    | P2            | 0.40            | 0.62                    | 0.71                 |  |

    | P3            | 2.90            | 3.13                    | 4.11                 |  |

    | All           | 1.02            | 1.22                    | 1.32                 |  |

    |               |                 |                         |                      |  |


    Table 6: Averages of Diff <sup>⋄</sup> Grouped in P1, P2, and P<sup>3</sup>


    <span id="page-22-0"></span>


    | Challenge Set | OF<br>DP | OF<br>OP F              | OF<br>HyGenar |

    |---------------|----------|-------------------------|---------------|

    |               |          | GPT-4o                  |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 4.03     | 0.68                    | 1.28          |

    | P3            | 13.49    | 11.54                   | 11.31         |

    | All           | 5.08     | 3.50                    | 4.17          |

    |               |          | GPT-3.5-Turbo           |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 35.71    | 34.29                   | 24.24         |

    | P3            | 17.02    | 16.67                   | 24.47         |

    | All           | 9.05     | 9.85                    | 11.89         |

    |               |          | Qwen:72b-Chat           |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 0.00     | 0.00                    | 0.00          |

    | P3            | 25.00    | 21.43                   | 19.05         |

    | All           | 2.8      | 2.65                    | 3.92          |

    |               |          | Llama3:70b-Instruct     |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 1.67     | 4.76                    | 0.95          |

    | P3            | 6.45     | 6.45                    | 10.17         |

    | All           | 1.35     | 2.21                    | 2.12          |

    |               |          | Gemma2:27b-Instruct     |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 9.33     | 9.59                    | 6.11          |

    | P3            | 16.18    | 16.00                   | 14.41         |

    | All           | 6.00     | 6.19                    | 5.88          |

    |               |          | Mistral:7b-Instruct     |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | N/A      | 12.05                   | N/A           |

    | P3            | N/A      | 33.33                   | N/A           |

    | All           | 0        | 7.32                    | 0             |

    |               |          | Codestral:22b           |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 0.00     | 0.00                    | 0.00          |

    | P3            | 4.76     | 6.56                    | 10.84         |

    | All           | 0.84     | 1.42                    | 2.49          |

    |               |          | Starcoder2:15b-Instruct |               |

    | P1            | 0.00     | 0.00                    | 0.00          |

    | P2            | 16.00    | 6.25                    | 4.88          |

    | P3            | 12.20    | 12.90                   | 15.87         |

    | All           | 5.49     | 4.63                    | 5.04          |


    Table 7: Averages of OF Grouped in P1, P2, and P<sup>3</sup> (%)


    <span id="page-23-0"></span>


    | Challenge Set | OGDP  | OGOP F                  | OGHyGenar |

    |---------------|-------|-------------------------|-----------|

    |               |       | GPT-4o                  |           |

    | P1            | 0.00  | 0.00                    | 1.11      |

    | P2            | 0.00  | 0.00                    | 0.00      |

    | P3            | 0.00  | 0.00                    | 0.60      |

    | All           | 0.00  | 0.00                    | 0.60      |

    |               |       | GPT-3.5-Turbo           |           |

    | P1            | 0.00  | 0.00                    | 1.19      |

    | P2            | 0.00  | 0.00                    | 0.00      |

    | P3            | 2.13  | 0.00                    | 1.06      |

    | All           | 0.50  | 0.00                    | 0.91      |

    |               |       | Qwen:72b-Chat           |           |

    | P1            | 1.32  | 2.56                    | 3.20      |

    | P2            | 0.00  | 0.00                    | 2.70      |

    | P3            | 8.33  | 0.00                    | 0.00      |

    | All           | 1.87  | 1.77                    | 2.45      |

    |               |       | Llama3:70b-Instruct     |           |

    | P1            | 0.00  | 0.00                    | 0.00      |

    | P2            | 0.00  | 0.00                    | 0.00      |

    | P3            | 0.00  | 0.00                    | 0.00      |

    | All           | 0.00  | 0.00                    | 0.00      |

    |               |       | Gemma2:27b-Instruct     |           |

    | P1            | 0.00  | 0.00                    | 0.00      |

    | P2            | 0.00  | 1.37                    | 1.53      |

    | P3            | 0.00  | 0.00                    | 0.00      |

    | All           | 0.00  | 0.33                    | 0.47      |

    |               |       | Mistral:7b-Instruct     |           |

    | P1            | 0.00  | 11.11                   | 0.00      |

    | P2            | N/A   | 0.00                    | N/A       |

    | P3            | N/A   | 0.00                    | N/A       |

    | All           | 0.00  | 7.32                    | 0.00      |

    |               |       | Codestral:22b           |           |

    | P1            | 0.70  | 1.25                    | 1.14      |

    | P2            | 1.85  | 1.67                    | 0.97      |

    | P3            | 0.00  | 0.00                    | 0.00      |

    | All           | 0.84  | 1.07                    | 0.83      |

    |               |       | Starcoder2:15b-Instruct |           |

    | P1            | 0.00  | 0.00                    | 2.24      |

    | P2            | 12.00 | 0.00                    | 4.88      |

    | P3            | 0.00  | 0.00                    | 0.00      |

    | All           | 1.83  | 0.00                    | 2.10      |


    Table 8: Averages of OG Grouped in P1, P2, and P<sup>3</sup> (%)


    <span id="page-24-0"></span>


    | Challenge Set | ⋄<br>TU<br>DP | ⋄<br>TU<br>OP F         | OG⋄<br>HyGenar |  |

    |---------------|---------------|-------------------------|----------------|--|

    |               |               | GPT-4o                  |                |  |

    | P1            | 100           | 100                     | 99.81          |  |

    | P2            | 99.60         | 99.55                   | 99.67          |  |

    | P3            | 93.04         | 92.80                   | 91.39          |  |

    | All           | 97.93         | 97.81                   | 96.96          |  |

    |               |               | GPT-3.5-Turbo           |                |  |

    | P1            | 99.64         | 99.33                   | 97.93          |  |

    | P2            | 90.91         | 89.71                   | 89.11          |  |

    | P3            | 78.56         | 79.74                   | 77.57          |  |

    | All           | 93.43         | 93.04                   | 90.32          |  |

    |               |               | Qwen:72b-Chat           |                |  |

    | P1            | 92.95         | 94.66                   | 90.47          |  |

    | P2            | 76.56         | 74.29                   | 81.31          |  |

    | P3            | 79.79         | 77.88                   | 77.53          |  |

    | All           | 88.56         | 88.79                   | 86.14          |  |

    |               |               | Llama3:70b-Instruct     |                |  |

    | P1            | 100           | 100                     | 99.80          |  |

    | P2            | 88.48         | 89.06                   | 93.03          |  |

    | P3            | 97.73         | 98.24                   | 87.33          |  |

    | All           | 96.58         | 96.71                   | 95.41          |  |

    |               |               | Gemma2:27b-Instruct     |                |  |

    | P1            | 99.82         | 99.82                   | 99.25          |  |

    | P2            | 92.67         | 94.21                   | 95.80          |  |

    | P3            | 93.06         | 94.02                   | 90.61          |  |

    | All           | 96.50         | 97.07                   | 95.78          |  |

    |               |               | Mistral:7b-Instruct     |                |  |

    | P1            | 50.00         | 68.34                   | 83.33          |  |

    | P2            | N/A           | 64.67                   | N/A            |  |

    | P3            | N/A           | 64.17                   | N/A            |  |

    | All           | 50.00         | 67.01                   | 83.33          |  |

    |               |               | Codestral:22b           |                |  |

    | P1            | 98.83         | 98.11                   | 98.36          |  |

    | P2            | 88.76         | 90.31                   | 89.58          |  |

    | P3            | 87.33         | 85.27                   | 83.37          |  |

    | All           | 94.54         | 93.66                   | 92.41          |  |

    |               |               | Starcoder2:15b-Instruct |                |  |

    | P1            | 97.86         | 96.44                   | 96.64          |  |

    | P2            | 83.29         | 87.57                   | 81.92          |  |

    | P3            | 84.91         | 85.39                   | 80.48          |  |

    | All           | 92.40         | 91.96                   | 89.83          |  |


    Table 9: Averages of T U<sup>⋄</sup> Grouped in P1, P2, and P<sup>3</sup> (%)


    ### Parser Feedback 2: Lack of Alternatives


    This error is likely due to the reason that the right-hand side is not defined
    after ''::=''.


    ## Prompt Template 5: Feedback Prompt in OPF


    Given a set of positive and negative examples, generate the Backus–Naur Form (BNF)
    grammar that accepts all positive examples and rejects all negative examples.
    1. Only generate the standard BNF grammar; 2. The generated BNF grammar MUST accept
    all positive examples and reject all negative examples; 3. Each terminal symbol
    MUST be quoted with double quotes and MUST NOT escape double quotes or pipeline
    in terminal symbols; 4. Pay special attention to whether spaces, line breaks,
    or other special symbols are required between each symbol, and if so, these need
    to be explicitly specified, e.g. <term> ::= "1" "+" "2" can handle "1+2" but not
    "1 + 2" while <term> ::= "1" " " "+" " " "2" can handle "1 + 2" but not "1+2";
    5. The entry point of the generated BNF grammar MUST be the non-terminal symbol
    in the first production rule; 6. Only the generated BNF should be wrapped in a
    pair of triple backtick; 7. Do NOT output any additional texts, comments, or explanations.


    ===Positive Examples=== {*positive\_examples*} ===Negative Examples=== {*negative\_examples*}


    ===Generated BNF=== {*bnf\_grammar*}


    ===Feedback=== The generated BNF grammar has incorrect syntax and please consider
    fixing it by referring to the feedback. Here is the feedback from the BNF parser:
    {*parser\_feedback*}


    ## Prompt Template 6: Generate Positive Examples with a Given Grammar


    Generate a list of positive examples with the following constraints:


    1. Each example MUST be separated by a newline in addition to the linebreak;


    2. Only output examples WITHOUT any additional text or code block, like "```";


    3. Only output {m} examples;


    4. Each example MUST be generated based on the given BNF grammar;


    5. Pay attention to whether the whitespaces are allowed between symbols.


    For example, given the following BNF grammar:


    <term> ::= "0" | "1" | "2" you should output positive examples like: 0


    1


    2


    Then, the given BNF grammar is: {*reference\_grammar*}'
- title: "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n\
    \  Agentic Scenarios"
  abstract: 'Large Language Models (LLMs) have demonstrated advanced capabilities
    in

    real-world agentic applications. Growing research efforts aim to develop

    LLM-based agents to address practical demands, introducing a new challenge:

    agentic scenarios often involve lengthy instructions with complex constraints,

    such as extended system prompts and detailed tool specifications. While

    adherence to such instructions is crucial for agentic applications, whether

    LLMs can reliably follow them remains underexplored. In this paper, we

    introduce AgentIF, the first benchmark for systematically evaluating LLM

    instruction following ability in agentic scenarios. AgentIF features three key

    characteristics: (1) Realistic, constructed from 50 real-world agentic

    applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.

    (3) Complex, averaging 11.9 constraints per instruction, covering diverse

    constraint types, such as tool specifications and condition constraints. To

    construct AgentIF, we collect 707 human-annotated instructions across 50

    agentic tasks from industrial application agents and open-source agentic

    systems. For each instruction, we annotate the associated constraints and

    corresponding evaluation metrics, including code-based evaluation, LLM-based

    evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically

    evaluate existing advanced LLMs. We observe that current models generally

    perform poorly, especially in handling complex constraint structures and tool

    specifications. We further conduct error analysis and analytical experiments on

    instruction length and meta constraints, providing some findings about the

    failure modes of existing LLMs. We have released the code and data to

    facilitate future research.'
  url: http://arxiv.org/abs/2505.16944v1
  keywords: ''
  document: "# AGENTIF: Benchmarking Instruction Following of Large Language Models\
    \ in Agentic Scenarios\n\nYunjia Qi1<sup>∗</sup> , Hao Peng1<sup>∗</sup> , Xiaozhi\
    \ Wang<sup>1</sup> , Amy Xin<sup>1</sup> , Youfeng Liu<sup>2</sup> , Bin Xu<sup>1</sup>\
    \ , Lei Hou<sup>1</sup> , Juanzi Li<sup>1</sup> <sup>1</sup>Tsinghua University\
    \ <sup>2</sup>Zhipu AI {qyj23, peng-h24}@mails.tsinghua.edu.cn\n\n# Abstract\n\
    \nLarge Language Models (LLMs) have demonstrated advanced capabilities in realworld\
    \ agentic applications. Growing research efforts aim to develop LLM-based agents\
    \ to address practical demands, introducing a new challenge: agentic scenarios\
    \ often involve lengthy instructions with complex constraints, such as extended\
    \ system prompts and detailed tool specifications. While adherence to such instructions\
    \ is crucial for agentic applications, whether LLMs can reliably follow them remains\
    \ underexplored. In this paper, we introduce AGENTIF, the first benchmark for\
    \ systematically evaluating LLM instruction following ability in agentic scenarios.\
    \ AGENTIF features three key characteristics: (1) Realistic, constructed from\
    \ 50 real-world agentic applications. (2) Long, averaging 1, 723 words with a\
    \ maximum of 15, 630 words. (3) Complex, averaging 11.9 constraints per instruction,\
    \ covering diverse constraint types, such as tool specifications and condition\
    \ constraints. To construct AGENTIF, we collect 707 human-annotated instructions\
    \ across 50 agentic tasks from industrial application agents and open-source agentic\
    \ systems. For each instruction, we annotate the associated constraints and corresponding\
    \ evaluation metrics, including code-based evaluation, LLM-based evaluation, and\
    \ hybrid code-LLM evaluation. We use AGENTIF to systematically evaluate existing\
    \ advanced LLMs. We observe that current models generally perform poorly, especially\
    \ in handling complex constraint structures and tool specifications. We further\
    \ conduct error analysis and analytical experiments on instruction length and\
    \ meta constraints, providing some findings about the failure modes of existing\
    \ LLMs. We have released the code[2](#page-0-0) and data[3](#page-0-1) to facilitate\
    \ future research.\n\n# 1 Introduction\n\nLarge language models (LLMs) have demonstrated\
    \ strong capabilities in real-world agentic applications [\\[19\\]](#page-10-0).\
    \ Growing studies focus on developing LLM-based agents to address practical demands,\
    \ such as Web Agents [\\[7,](#page-9-0) [8\\]](#page-9-1), Education Agents [\\\
    [4\\]](#page-9-2), GUI Agents [\\[31,](#page-10-1) [32\\]](#page-10-2), and PPT\
    \ Agents [\\[34\\]](#page-11-0). While these agentic scenarios expand the application\
    \ scope of LLMs, they also pose a new challenge: agentic tasks usually involve\
    \ long and complex instructions, such as extended system prompts and detailed\
    \ tool specifications. Correctly following such instructions is a prerequisite\
    \ for solving these tasks and reflects a fundamental capability of LLMs.\n\nHowever,\
    \ whether LLMs can effectively follow instructions in real-world agentic scenarios\
    \ remains underexplored. Previous work on benchmarking instruction following of\
    \ LLMs has mainly focused on\n\n<sup>∗</sup>Equal contribution.\n\n<span id=\"\
    page-0-0\"></span><sup>2</sup><https://github.com/THU-KEG/AgentIF>\n\n<span id=\"\
    page-0-1\"></span><sup>3</sup><https://huggingface.co/datasets/THU-KEG/AgentIF>\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFigure 1: (a) The\
    \ length distribution of instructions across AGENTIF (log-scale). (b) Success\
    \ rates of several representative LLMs on different constraint dimensions (detailed\
    \ descriptions are in § [3.2\\)](#page-3-0).\n\nrelatively short instructions,\
    \ which are usually synthetically generated. For example, the widely used benchmark\
    \ IFEval [\\[36\\]](#page-11-1) is synthetically constructed with various constraint\
    \ types, such as formatting, and has an average instruction length of only 45\
    \ words. Subsequent studies have expanded the instruction scope to include more\
    \ constraint types [\\[14,](#page-9-3) [23,](#page-10-3) [33,](#page-11-2) [29\\\
    ]](#page-10-4), system prompts [\\[22\\]](#page-10-5), multi-turn conversations\
    \ [\\[10\\]](#page-9-4), and action constraints [\\[16\\]](#page-10-6). Nonetheless,\
    \ the instructions in these benchmarks are typically short and usually synthetically\
    \ generated, resulting in a gap from real-world agentic applications. Figure [2](#page-2-0)\
    \ illustrates an instruction from a real-world agentic scenario. We can observe\
    \ that the instruction is long with complex structures and constraint types, such\
    \ as condition constraints, example constraints, and tool specifications, posing\
    \ novel and significant challenges for LLMs. As existing instruction-following\
    \ benchmarks typically lack coverage of such agentic instructions, it is necessary\
    \ to systematically evaluate LLMs' ability to handle them.\n\nConsidering the\
    \ above concerns, we propose AGENTIF, the first benchmark to evaluate instruction\
    \ following of LLMs in real-world agentic scenarios. Specifically, we first collect\
    \ 50 agentic tasks from industrial applications and open-source agentic systems.\
    \ For each task, we manually annotate around 20 user queries, each combined with\
    \ the corresponding agentic system prompt to form an instruction. We then extract\
    \ all the constraints from each instruction and investigate their types. As illustrated\
    \ in Figure [2,](#page-2-0) we classify the constraints into three main types:\
    \ (1) formatting constraints, which specify the structure or presentation of the\
    \ output; (2) semantic constraints, which require semantic understanding to check,\
    \ e.g., language style; (3) tool constraints, which involve adherence to tool\
    \ specifications, e.g., the parameter format of a function. We also classify the\
    \ representation types of these constraints into three types: (1) vanilla, which\
    \ means the constraints is described directly in plain text; (2) condition, where\
    \ constraints are triggered under certain conditions, e.g., \"if the output exceeds\
    \ 100 words, include the keyword *paper*\"; (3) example, which is similar to in-context\
    \ learning [\\[3\\]](#page-9-5), where the model is expected to follow the structure\
    \ shown in examples. Finally, AGENTIF includes 707 high-quality manually annotated\
    \ instructions, with each instruction containing an average of 11.9 constraints.\
    \ These real-world agentic instructions, with extended length (Figure [1\\(](#page-1-0)a))\
    \ and complex constraints, present significant challenges to existing LLMs. For\
    \ the evaluation on AGENTIF, we annotate the evaluation method for each constraint,\
    \ consisting of three paradigms: (1) code verification, which examines constraint\
    \ satisfaction through Python code; (2) LLM verification, which assesses constraint\
    \ satisfaction using large language models; (3) hybrid verification, which uses\
    \ a combination of code and LLM verification. For example, when evaluating the\
    \ constraint \"The abstract should be no less than 100 words\", the method first\
    \ uses an LLM to extract the abstract section, followed by code verification to\
    \ assess word count compliance.\n\nWe conduct comprehensive experiments to evaluate\
    \ current advanced LLMs on AGENTIF. Specifically, we evaluate several representative\
    \ thinking LLMs, non-thinking LLMs, and LLMs optimized for instruction-following.\
    \ As shown in Figure [1\\(](#page-1-0)b), all LLMs perform poorly and even the\
    \ bestperforming model only follows fewer than 30% of the instructions perfectly.\
    \ We further conduct error\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_0.jpeg)\n\
    \nFigure 2: An example instruction of AGENTIF.\n\nanalysis and find that the most\
    \ challenging condition and tool constraints introduce new challenges. We also\
    \ find the performance declines as instruction length increases in AGENTIF. Additionally,\
    \ we identify a novel category of constraints, meta constraints, which reflects\
    \ underlying prioritization issues. In conclusion, advanced LLMs still struggle\
    \ to follow real-world agentic instructions.\n\n# 2 Related Work\n\nInstruction\
    \ following is a fundamental capability of LLMs, referring to following user instructions,\
    \ including task completion and adherence to user requirements. The most widely\
    \ used instructionfollowing benchmark is IFEval [\\[37\\]](#page-11-3), which\
    \ is the first to formalize the task as multi-constraint compliance, such as requirements\
    \ on output length and format. For example, an instruction in IFEval \"Write a\
    \ long email, with at least 500 words. The email must include the keywords 'correlated'\
    \ and 'experiencing\"' includes constraints on output length, required keywords,\
    \ which can be efficiently and accurately verified using Python code. Subsequent\
    \ work has expanded instruction scope in several directions: (1) More constraint\
    \ types [\\[14,](#page-9-3) [23,](#page-10-3) [33,](#page-11-2) [29\\]](#page-10-4),\
    \ which include constraints requiring semantic understanding (e.g., style) and\
    \ adopt LLMs for evaluation. (2) Multilingual [\\[33,](#page-11-2) [9,](#page-9-6)\
    \ [10\\]](#page-9-4). (3) Multi-turn [\\[10,](#page-9-4) [9\\]](#page-9-6), which\
    \ assesses instruction following in multi-turn dialogues, such as requiring the\
    \ model to revise its response in the last round. (4) Code [\\[30\\]](#page-10-7),\
    \ which evaluates instruction-following capability in code generation. Notably,\
    \ there are also two studies closely related to agentic scenarios. SysBench [\\\
    [22\\]](#page-10-5) evaluates compliance with system prompts, which are mined\
    \ from realistic user logs. However, these prompts are typically short and lack\
    \ tool usage, still leaving a gap from realistic agentic scenarios. AgentOrca\
    \ [\\[16\\]](#page-10-6) assesses adherence to operational constraints and routines\
    \ of LLMs. They primarily focus on compliance with function invocation and do\
    \ not involve the complex system prompts or constraint types typical of real-world\
    \ agents. In conclusion, existing instruction-following benchmarks overlook the\
    \ evaluation of instruction compliance in realistic agentic scenarios.\n\nIn realistic\
    \ agentic scenarios, as shown in Figure [2,](#page-2-0) instructions are typically\
    \ long with complex constraint types, structures, and tool specifications. As\
    \ LLM-based agents are increasingly deployed across various domains [\\[7,](#page-9-0)\
    \ [8,](#page-9-1) [4,](#page-9-2) [31,](#page-10-1) [32,](#page-10-2) [34\\]](#page-11-0),\
    \ accurate adherence to agentic instructions becomes essential. To address this\
    \ need, we introduce AGENTIF, the first instruction-following benchmark for agentic\
    \ scenarios. AGENTIF comprises data from real-world industrial applications and\
    \ open-source agentic workflows, with comprehensive human annotations. Each instruction\
    \ in AGENTIF contains 1, 700 tokens and 14 constraints on average, which presents\
    \ significant challenges to current LLMs.\n\n# 3 AGENTIF\n\nThis section presents\
    \ a detailed introduction to AGENTIF, including 4 parts: the constraint taxonomy\
    \ (§ [3.1\\)](#page-3-1), the dataset construction process (§ [3.2\\)](#page-3-0),\
    \ the statistics of AGENTIF (§ [3.3\\)](#page-4-0), and the evaluation protocol\
    \ (§ [3.4\\)](#page-5-0). Figure [3](#page-3-2) illustrates the dataset construction\
    \ and evaluation workflow of AGENTIF.\n\n<span id=\"page-3-2\"></span>![](_page_3_Figure_0.jpeg)\n\
    \nFigure 3: The data construction process and evaluation workflow of AGENTIF.\
    \ The detailed descriptions of different constraint types are presented in § [3.1.](#page-3-1)\n\
    \n# <span id=\"page-3-1\"></span>3.1 Constraint Taxonomy\n\nTo comprehensively\
    \ evaluate LLMs' ability to follow complex instructions in agentic settings, we\
    \ investigate about 100 instructions from real-world scenarios and construct a\
    \ constraint taxonomy, which classify constraints along two dimensions: constraint\
    \ type, such as whether the constraint requires semantic understanding for verification,\
    \ and constraint representation type, such as whether constraint is conditionally\
    \ triggered. More details and examples are provided in Appendix [C.](#page-12-0)\n\
    \nConstraint Type The constraint type refers to the specific evaluation aspect,\
    \ such as format or style. Following prior work [\\[14,](#page-9-3) [29\\]](#page-10-4),\
    \ we categorize constraints into two commonly used types and introduce a new type,\
    \ that is tool constraints, specific to agentic scenarios. Formatting constraints\
    \ specify the structure or presentation of the output. These include requirements\
    \ about syntax (e.g., JSON or Markdown), layout (e.g., bullet points, tables,\
    \ or paragraph length), symbol conventions (e.g., using backticks for filenames),\
    \ and step-by-step formatting (e.g., explaining a principle in three steps). Semantic\
    \ constraints focus on the semantic meaning and informativeness of the content,\
    \ including requirements for depth, completeness (e.g., inclusion of keywords\
    \ or references), and adherence to a specific style or tone. Tool constraints\
    \ are newly introduced specifically for agentic scenarios, requiring the model\
    \ to invoke tools according to given specifications, such as adhering to the correct\
    \ parameter types, avoiding internet access, or restricting tool usage to a predefined\
    \ set of functions.\n\nConstraint Presentation Type The constraint presentation\
    \ type refers to how constraints are presented in text. We categorize this into\
    \ three forms. Vanilla constraints are described explicitly in plain text (e.g.,\
    \ include the keyword *paper*). Conditional constraints are triggered only under\
    \ specific conditions, which may be derived from the input (e.g., responding only\
    \ if certain keywords appear) or from the model's own output behavior (e.g., applying\
    \ markdown rules only when markdown is used). Example constraints are not explicitly\
    \ stated but implied through few-shot examples, like in-context learning [\\[3\\\
    ]](#page-9-5), requiring the model to infer and follow constraints from given\
    \ output examples, which requires analogical reasoning and inductive capabilities.\n\
    \n### <span id=\"page-3-0\"></span>3.2 Dataset Construction\n\nAs shown in Figure\
    \ [3,](#page-3-2) AGENTIF is constructed through a semi-automated pipeline consisting\
    \ of three main steps: agentic instruction collection, constraint extraction,\
    \ constraint evaluation design. More details of the data construction process\
    \ and human annotation are shown in appendix [D.](#page-13-0)\n\nInstruction Collection\
    \ We focus on real-world agentic instructions, constructing our dataset from two\
    \ sources: open-source agentic applications and industrial agent-based frameworks.\
    \ We collect instructions based on two key principles: (1) Realistic: Each instruction\
    \ should reflect practical, real-world agentic tasks. (2) Complex: Instructions\
    \ should involve complex constraints, structures,\n\n<span id=\"page-4-2\"></span>\n\
    \n| Benchmark         | #Inst. | Len.   | #Cons. | Data<br>Resource |        |\
    \ Constraint Type |         | Evaluation Method |           |  |\n|-------------------|--------|--------|--------|------------------|--------|-----------------|---------|-------------------|-----------|--|\n\
    |                   |        |        |        |                  | Tool   | Conditional\
    \     | Example | Code-based        | LLM-based |  |\n| IFEval [37]       | 541\
    \    | 36     | 1.5    | Synthetic        | ✗<br>✗ |                 | ✗     \
    \  | ✓                 | ✗         |  |\n| FollowBench [14]  | 820    | 253  \
    \  | 3.0    | Synthetic        | ✗      | ✗               | ✓       | ✓      \
    \           | ✓         |  |\n| InfoBench [23]    | 500    | 38     | 4.5    |\
    \ Synthetic        | ✗      | ✗               | ✗       | ✗                 |\
    \ ✓         |  |\n| SysBench [22]     | 500    | 521    | 2.4    | Realistic \
    \       | ✗      | ✗               | ✗       | ✗                 | ✓         |\
    \  |\n| ComplexBench [29] | 1, 150 | 448    | 4.2    | Synthetic        | ✗  \
    \    | ✓               | ✗       | ✓                 | ✓         |  |\n| AgentOrca\
    \ [16]    | 663    | 1, 144 | -      | Synthetic        | ✓      | ✓         \
    \      | ✗       | ✓                 | ✗         |  |\n| Multi-IF [10]     | 4,\
    \ 501 | 48     | 7.1    | Synthetic        | ✗      | ✗               | ✗    \
    \   | ✓                 | ✗         |  |\n| AGENTIF (ours)    | 707    | 1, 723\
    \ | 11.9   | Realistic        | ✓      | ✓               | ✓       | ✓       \
    \          | ✓         |  |\n\nTable 1: The statistics of AGENTIF and previous\
    \ instruction-following benchmarks. The statistics include dataset size (#Inst.),\
    \ average instruction length (#Len.), average number of constraints per instruction\
    \ (#Cons.), data resource, constraint types, and supported evaluation methods.\n\
    \nand tool specifications that pose significant challenges for LLMs. Specifically,\
    \ we first collect 40 agents from GitHub[4](#page-4-1) , including well-known\
    \ agentic applications such as Cursor and Manus, along with 10 agents from industrial\
    \ agentic workflows, which supports around 200 daily active users, handles approximately\
    \ 300 requests per day, and has delivered a total of 120, 000 services. The 50\
    \ collected agents include only system prompt, which contain task specifications,\
    \ goals, and tool descriptions, without any user queries, ensuring no risk of\
    \ user data leakage. Next, we use GPT-4o to generate about 20 user queries for\
    \ each agent based on their system prompts. We then apply heuristic rules and\
    \ similarity-based filtering to remove redundant queries. Finally, we employ human\
    \ annotators to rewrite each generated query, ensuring consistency with real user\
    \ cases. As a result, we obtain 707 high-quality instructions with an average\
    \ length of 1, 723 words.\n\nConstraint Annotation As shown in Figure [3,](#page-3-2)\
    \ we first use an LLM to automatically extract constraints from the instructions.\
    \ As the original instructions are long with complex structure, directly extracting\
    \ constraints poses a challenge for LLMs. To address this, we design a block-wise\
    \ annotation method. Specifically, we segment each instruction into self-contained\
    \ semantic blocks (e.g., task description, tool configuration, output specification),\
    \ ensuring that the content is not truncated. We then use GPT-4o to extract relevant\
    \ constraints from each block. Some constraints may span multiple blocks, for\
    \ example in Figure [3,](#page-3-2) the complete tool specifications are distributed\
    \ across multiple blocks. Therefore, we perform cross-block validation to add\
    \ this information to ensure the completeness of each constraint. Finally, we\
    \ employ human annotators to validate each constraint to verify its completeness\
    \ and consistency with the original instruction. As a result, we obtain 8, 415\
    \ high-quality constraints in total, with an average of 11.9 constraints per instruction.\n\
    \nEvaluation Generation Finally, we annotate each constraint with its corresponding\
    \ evaluation method. Following prior work [\\[14,](#page-9-3) [23,](#page-10-3)\
    \ [29\\]](#page-10-4), we adopt a hybrid evaluation framework that combines LLM-based\
    \ and code-based evaluation. We adopt different evaluation methods for different\
    \ types of constraints as described in § [3.1.](#page-3-1) Specifically, as shown\
    \ in Figure [3,](#page-3-2) we define three evaluation modes based on constraint\
    \ types: (1) Code evaluation, which is used for constraints that can be verified\
    \ through simple and deterministic Python code (e.g., keyword presence, formatting\
    \ patterns). (2) LLM evaluation, which is applied to open-ended or subjective\
    \ constraints requiring semantic understanding. In these cases, we adopt an LLM\
    \ for evaluation. (3) Hybrid evaluation, which is used for more complex cases,\
    \ where the LLM first identifies relevant segments in the response (e.g., extracting\
    \ JSON for tool calls), followed by code-based validation. Notably, for conditional\
    \ constraints, we annotate the evaluation process to first check whether the condition\
    \ is met before performing constraint evaluation. We use GPT-4o to determine the\
    \ evaluation method for each constraint and to generate the corresponding evaluation\
    \ script. We then manually review all annotations to ensure the correctness of\
    \ the generated evaluation methods and revise them as needed.\n\n#### <span id=\"\
    page-4-0\"></span>3.3 Data Statistics\n\nThe statistics of AGENTIF and other related\
    \ benchmarks are summarized in Table [1.](#page-4-2) Compared to existing instruction-following\
    \ benchmarks, AGENTIF features three key characteristics: (1) Realistic.\n\n<span\
    \ id=\"page-4-1\"></span><sup>4</sup><https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools>\
    \ <https://github.com/Shubhamsaboo/awesome-llm-apps>\n\n<span id=\"page-5-2\"\
    ></span>\n\n| Models                           |         | Dimension |       \
    \  | Type       |          |      | ISR  | CSR  |\n|----------------------------------|---------|-----------|---------|------------|----------|------|------|------|\n\
    |                                  | Vanilla | Condition | Example | Formatting\
    \ | Semantic | Tool |      |      |\n| [T]o1-mini                       | 59.8\
    \    | 37.5      | 80.8    | 66.1       | 59.1     | 43.2 | 26.9 | 59.8 |\n| [N]GPT-4o\
    \                        | 58.0    | 35.1      | 80.8    | 65.8       | 56.5 \
    \    | 43.2 | 26.4 | 58.5 |\n| [N]Qwen3-32B                     | 57.5    | 41.1\
    \      | 80.6    | 57.7       | 62.5     | 45.7 | 24.9 | 58.4 |\n| [T]QwQ-32B\
    \                       | 57.5    | 35.6      | 82.7    | 61.4       | 59.4  \
    \   | 43.2 | 27.2 | 58.1 |\n| [T]DeepSeek-R1                   | 56.1    | 41.4\
    \      | 87.0    | 61.4       | 58.9     | 44.4 | 22.2 | 57.9 |\n| [T]GLM-Z1-32B\
    \                    | 56.7    | 37.9      | 83.6    | 60.2       | 59.6     |\
    \ 43.1 | 23.8 | 57.8 |\n| [N]DeepSeek-V3                   | 54.9    | 41.5  \
    \    | 84.5    | 59.3       | 58.9     | 40.8 | 21.9 | 56.7 |\n| [N]Claude-3-5-Sonnet\
    \             | 57.3    | 36.9      | 69.2    | 61.5       | 56.0     | 43.3 |\
    \ 24.9 | 56.6 |\n| [N]Meta-Llama-3.1-70B-Instruct   | 55.1    | 35.0      | 84.3\
    \    | 61.6       | 55.6     | 42.8 | 20.9 | 56.3 |\n| [T]DeepSeek-R1-Distill-Qwen-32B\
    \  | 54.5    | 39.6      | 73.1    | 55.7       | 57.2     | 45.2 | 20.7 | 55.1\
    \ |\n| [T]DeepSeek-R1-Distill-Llama-70B | 55.4    | 37.7      | 69.2    | 56.5\
    \       | 56.6     | 44.1 | 19.9 | 55.0 |\n| [N]Meta-Llama-3.1-8B-Instruct   \
    \ | 53.5    | 36.6      | 71.4    | 55.6       | 54.8     | 43.5 | 19.9 | 53.6\
    \ |\n| [S]Crab-DPO-7B                   | 48.3    | 24.3      | 57.5    | 48.8\
    \       | 47.4     | 41.9 | 10.1 | 47.2 |\n| [N]Mistral-7B-Instruct-v0.3     \
    \ | 47.9    | 29.2      | 53.8    | 47.0       | 48.6     | 39.8 | 11.5 | 46.8\
    \ |\n| [S]Conifer-DPO-7B                | 45.6    | 27.0      | 50.5    | 42.0\
    \       | 46.9     | 41.8 | 10.7 | 44.3 |\n\nTable 2: Success rates (%) of various\
    \ proprietary and open-source LLMs on AGENTIF, sorted by CSR in descending order.\
    \ **[N]** denotes non-thinking models, **[T]** denotes thinking models, and **[S]**\
    \ denotes models explicitly designed for instruction following by the academic\
    \ community. As described in § [3.4,](#page-5-0) CSR indicates the proportion\
    \ of correctly followed individual constraints, and ISR presents the proportion\
    \ of instructions in which all constraints are satisfied.\n\nAGENTIF is derived\
    \ from real-world agentic scenarios which reflects real use cases. (2) Long. Instructions\
    \ in AGENTIF are significantly longer than those in prior benchmarks, with an\
    \ average length of 1, 723 words. We also illustrate the length distribution of\
    \ AGENTIF in Figure [1\\(](#page-1-0)a). We can observe that a substantial portion\
    \ of instructions even exceeds 3, 000 words, posing a significant challenge for\
    \ existing LLMs. (3) Complex. Each instruction contains an average of 11.9 constraints,\
    \ with a good coverage of various constraint types such as tool, condition, and\
    \ example constraints.\n\n#### <span id=\"page-5-0\"></span>3.4 Evaluation Protocol\n\
    \nFigure [3](#page-3-2) illustrates our evaluation methodology. The process first\
    \ determines whether each constraint requires verification. Condition constraints\
    \ that are not triggered are exclued from verification. We then adopt the annotated\
    \ corresponding evaluation method. For LLM-based and hybrid evaluations, we employ\
    \ gpt-4o-2024-11-20. For evaluation metrics, following prior work [\\[22,](#page-10-5)\
    \ [37\\]](#page-11-3), AGENTIF adopts two metrics for evaluation: constraint success\
    \ rate (CSR) and instruction success rate (ISR). CSR measures the proportion of\
    \ individual constraints that are correctly satisfied by the model's response.\
    \ For a given instruction i, C<sup>i</sup> is the number of constraints associated\
    \ with it, and ci,j indicates whether the j-th constraint in instruction i is\
    \ satisfied. ISR measures the proportion of instructions for which all constraints\
    \ are satisfied. Supposing N is the number of instruction:\n\n$$\\text{LSR} =\
    \ \\frac{\\sum\\_{i=1}^{N} \\sum\\_{j=1}^{C\\_i} \\mathbb{1} \\left[ c\\_{i,j}\
    \ = \\text{satt.isf.ied} \\right]}{\\sum\\_{i=1}^{N} C\\_i}; \\quad \\text{ISR}\
    \ = \\frac{\\sum\\_{i=1}^{N} \\mathbb{1} \\left[ \\bigwedge\\_{j=1}^{C\\_i} (c\\\
    _{i,j} = \\text{satt.isf.ied}) \\right]}{N}$$\n\n# 4 Experiments\n\nIn this section,\
    \ we introduce the experiments and empirical analyses on AGENTIF, including experimental\
    \ setup (§ [4.1\\)](#page-5-1), main results on AGENTIF (§ [4.2\\)](#page-6-0),\
    \ error analysis (§ [4.3\\)](#page-6-1), analysis on instruction length (§ [4.4\\\
    )](#page-7-0) and meta constraints (§ [4.5\\)](#page-8-0).\n\n### <span id=\"\
    page-5-1\"></span>4.1 Experimental Setup\n\nWe evaluate various advanced LLMs\
    \ on AGENTIF, including non-thinking models: GPT-4o [\\[11\\]](#page-9-7), DeepSeek-V3\
    \ [\\[17\\]](#page-10-8), Claude 3.5 Sonnet [\\[1\\]](#page-9-8) , LLaMA 3.1 Series\
    \ [\\[5\\]](#page-9-9), Qwen3 [\\[26\\]](#page-10-9), and Mistral [\\[13\\]](#page-9-10);\
    \ thinking models: o1-mini [\\[12\\]](#page-9-11), QwQ 32B [\\[25\\]](#page-10-10),\
    \ GLM-Z1 32B [\\[27\\]](#page-10-11), DeepSeek-R1 [\\[6\\]](#page-9-12), and DeepSeek-R1\
    \ distilled models [\\[6\\]](#page-9-12); and academic models developed specifically\
    \ for instruction following, including Crab [\\[21\\]](#page-10-12) and Conifer\
    \ [\\[24\\]](#page-10-13). For all models, we set the sampling temperature to\
    \ 0. For reasoning models, we remove intermediate reasoning tokens and evaluate\
    \ only the final response.\n\n### <span id=\"page-6-0\"></span>4.2 Main Results\n\
    \nAll experimental results are shown in Table [2.](#page-5-2) We can observe that:\n\
    \n(1) All models demonstrate suboptimal performance. Even the best-performing\
    \ model, o1-mini, achieves only a CSR of 59.8. ISR results are even lower, with\
    \ the highest reaching just 27.2. Compared to their performance on the commonly\
    \ used benchmark IFEval [\\[37\\]](#page-11-3), all models exhibit a dramatic\
    \ drop, for example, GPT-4o drops from 87.0 to 58.5. This indicates that existing\
    \ models are still far from perfectly following constraints in agentic scenarios.\
    \ AGENTIF poses a significant challenge to existing LLMs, highlighting the need\
    \ for further research in instruction following under real-world agentic scenarios.\
    \ As instruction following is a prerequisite ability for building reliable LLM-based\
    \ agents, we argue that prior to developing agents, it is crucial to evaluate\
    \ the fundamental instruction-following abilities of LLMs to inform effective\
    \ prompt and workflow design.\n\n(2) For specific constraint types, models perform\
    \ much lower on the condition and tool constraints. Compared to vanilla constraints,\
    \ condition constraints require an additional step to determine whether the constraint\
    \ should be triggered. Thus, the low success rate on condition constraints may\
    \ be that the model struggle in correctly determining whether the condition is\
    \ triggered. As condition-based constraints account for approximately 42.6% of\
    \ real-world applications and are usually overlooked in existing instruction-following\
    \ research [\\[21,](#page-10-12) [24\\]](#page-10-13), we advocate for increased\
    \ efforts to handle conditional instructions. For tool constraints, failures may\
    \ stem from issues such as missing required parameters or failing to invoke the\
    \ specified tools. Detailed error analysis is provided in § [4.3.](#page-6-1)\
    \ The primary reason may be that the models fails in handling specification-heavy\
    \ tasks [\\[20,](#page-10-14) [2\\]](#page-9-13) and tool usage typically involves\
    \ complex specifications. As tool usage is necessary in agentic applications,\
    \ we call for more attention to tool specification adherence to enable more reliable\
    \ agent behavior. For example constraints, models perform relatively better. It\
    \ suggests that they can effectively infer and meet requirements when provided\
    \ with in-context examples, indicating that providing concrete examples in prompt\
    \ design can facilitate better understanding and imitation of desired behaviors.\n\
    \n(3) Across different models, we find that thinking models generally perform\
    \ better, suggesting that complex instruction following also requires reasoning\
    \ capabilities and that test-time scaling also benefits. In contrast, models specifically\
    \ trained for instruction following by the academic community perform worse. This\
    \ may be due to their primary focus on constructing SFT datasets for training\
    \ base models [\\[21,](#page-10-12) [24\\]](#page-10-13). Given that industry\
    \ models, such as Llama 3, already demonstrate strong performance due to using\
    \ large-scale SFT data, we encourage the academic community to explore more advanced\
    \ approaches, such as reinforcement learning [\\[15\\]](#page-9-14), which has\
    \ recently proven effective for enhancing model capabilities [\\[6\\]](#page-9-12)\
    \ but remains under-explored in instruction following.\n\nIn conclusion, AGENTIF\
    \ poses significant challenges for existing models, particularly on constraints\
    \ commonly used in agentic applications, such as tool constraints. We call for\
    \ increased research efforts to improve instruction following in agentic scenarios.\n\
    \n# <span id=\"page-6-1\"></span>4.3 Error Analysis\n\nAs shown in Figure [2,](#page-5-2)\
    \ LLMs perform particularly poorly on two types of common constraints in agentic\
    \ scenarios: condition and tool constraints. Therefore, we conduct a detailed\
    \ error analysis on these cases. Specifically, we analyze the errors of the four\
    \ representative LLMs, including o1-mini, GPT-4o, QwQ-32B, and DeepSeek-R1, and\
    \ manually investigate their error types.\n\nAnalysis on Condition Constraint\
    \ We identify two main types of failure in following condition constraints: (1)\
    \ incorrect condition checking, where the LLM fails to determine whether a condition\
    \ is triggered; and (2) instruction following failure, where the LLM fails to\
    \ follow the constraint even when the condition is triggered. We conduct a controlled\
    \ experiment to assess the relative proportion of the two causes. Specifically,\
    \ we select all the failed conditional constraints from each investigated model,\
    \ remove their conditional components, and then convert them into vanilla constraints\
    \ that must be met while keeping all other elements unchanged. If the model then\
    \ succeeds, it indicates an error in condition checking; if it still fails, it\
    \ suggests a general failure to follow the constraint. The results are shown in\
    \ Figure [4\\(](#page-7-1)a). We can observe that a substantial portion (above\
    \ 30%) of errors are due to incorrect condition checks, suggesting that condition\
    \ constraints introduce new challenges in determining whether a constraint is\
    \ triggered. Since existing work on instruction following usually overlooks such\
    \ conditional constraints, we advocate constructing targeted post-training data.\n\
    \n<span id=\"page-7-1\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure 4: Error proportions\
    \ (%) on condition and tool constraints. Figure (a) shows the errors in handling\
    \ condition constraints, including condition check failure, where the model fails\
    \ to recognize the condition, and constraint following failure. Figure (b) shows\
    \ the errors from tool constraints, including disallowed tool usage (utilizing\
    \ explicitly prohibited tools), omission of required tools (failing to employ\
    \ required tools), tool name errors (invoking non-existent or incorrect tools),\
    \ and parameter errors (applying incorrect or illegal arguments).\n\n<span id=\"\
    page-7-2\"></span>![](_page_7_Figure_2.jpeg)\n\nFigure 5: Success rates on instructions\
    \ with varying length or constraint counts. Gray lines show results of the top\
    \ 6 models in Figure [2,](#page-5-2) and the colored lines present their average.\n\
    \nAnalysis on Tool Constraint We conduct error analysis on tool constraint violations.\
    \ Specifically, we sample 50 tool usage errors from each investigated model and\
    \ identify four primary error categories: disallowed tool usage, omission of required\
    \ tools, tool name errors, and parameter errors. The error proportions are shown\
    \ in Figure [4\\(](#page-7-1)b). We observe that disallowed tool usage and omission\
    \ of required tools constitute the primary errors, suggesting that tool invocation\
    \ decision-making remains challenging for models. A portion of errors also stems\
    \ from non-compliance with tool usage specifications, exhibited as incorrect tool\
    \ names or parameters. Notably, we observe an interesting phenomenon: thinking\
    \ models more frequently neglect required tools. The reason may be that the thinking\
    \ models may tend to rely more on their internal knowledge. We encourage the research\
    \ community to conduct further investigations into these specific underlying causative\
    \ mechanisms.\n\n#### <span id=\"page-7-0\"></span>4.4 Analysis of Instruction\
    \ Length and Constraint Quantity\n\nWhile prior work has shown that long texts\
    \ pose significant challenges for LLMs, most studies have focused on long textual\
    \ contexts within the query [\\[2\\]](#page-9-13), such as long document question\
    \ answering. Little work investigates the challenges arising from long instructions,\
    \ which may be due to the lack of evaluation data with lengthy instructions. AGENTIF\
    \ provides such an evaluation platform, with instructions averaging 1, 723 words\
    \ and containing about 11.9 constraints each. We analyze model performance on\
    \ instructions of varying lengths and constraint counts in AGENTIF. We bucket\
    \ the data by instruction length or number of constraints, then compute the success\
    \ rates within each bucket. The results are shown in Figure [5.](#page-7-2) We\
    \ observe that model performance generally declines with increasing instruction\
    \ length or constraint count, indicating that longer instructions or those with\n\
    \n<span id=\"page-8-2\"></span>![](_page_8_Figure_0.jpeg)\n\nFigure 6: Figure\
    \ (a) illustrates three types of meta constraints and examples. Most meta constraints\
    \ fall within the Constraint Selection category, which requires models to follow\
    \ one specific constraint. Figure (b) presents the success rates of different\
    \ investigated models on each meta constraint type.\n\nmore constraints are inherently\
    \ more difficult, which is consistent with the intuition. Notably, when instruction\
    \ length exceeds 6, 000 words, the ISR scores of all models are nearly 0. This\
    \ indicates that overly long instructions are rarely followed perfectly and should\
    \ be avoided in practice. Instead, one can explore decomposing tasks into several\
    \ sub-tasks with several shorter instructions to alleviate instruction following\
    \ failures [\\[35\\]](#page-11-4). We call for more research efforts to enhance\
    \ models' ability to follow long instructions. As discussed in [\\[20\\]](#page-10-14),\
    \ the primary reason LLMs fail on specification-heavy instructions is the in-context\
    \ learning limitation. A promising direction is collecting post-training data\
    \ with long instructions, which remains underexplored due to the scarcity of such\
    \ data. One potential source is manuals[5](#page-8-1) , such as camera manuals,\
    \ which can serve as long instructions and be used to automatically construct\
    \ question-answer pairs for post-training. We leave this for future work.\n\n\
    ### <span id=\"page-8-0\"></span>4.5 Analysis of Meta Constraints\n\nWe observe\
    \ a prevalent type of constraint in AGENTIF, which we refer to as meta constraints.\
    \ Unlike regular constraints that apply directly to the model's response, meta\
    \ constraints govern other constraints. We find approximately 25% of instructions\
    \ in AGENTIF include meta constraints. As shown in Figure [6\\(](#page-8-2)a),\
    \ we categorize them into three types: (1) constraint selection, where the meta\
    \ constraint requires the model to follow only a specific constraint; (2) constraint\
    \ detailing, where it adds further requirements to an existing constraint; and\
    \ (3) constraint prioritization, where it defines the relative priority among\
    \ multiple constraints. Figure [6\\(](#page-8-2)b) illustrates the success rates\
    \ of different meta constraints. We can observe that the models generally perform\
    \ the worst on constraint selection. One possible reason is that the meta constraint\
    \ may conflict with the original constraints, which confuses LLMs. Future work\
    \ may explore giving meta constraints higher priority to improve compliance [\\\
    [28\\]](#page-10-15), while carefully mitigating potential safety risks such as\
    \ prompt injection attacks [\\[18\\]](#page-10-16).\n\n# 5 Conclusion\n\nIn this\
    \ paper, we present AGENTIF, the first instruction-following benchmark for agentic\
    \ scenarios. AGENTIF comprises 707 instructions across 50 real-world agentic applications.\
    \ Each instruction has an average length of 1, 717 tokens and includes approximately\
    \ 11.9 constraints, covering a diverse range such as condition and tool constraints.\
    \ We evaluate various representative and advanced LLMs on AGENTIF and find that\
    \ current models generally perform poorly, and the best model perfectly follows\
    \ fewer than 30% of the instructions, which suggests that AGENTIF poses a significant\
    \ challenge. We further conduct analytical experiments to investigate the failure\
    \ modes. We find that condition and tool constraints introduce new challenges.\
    \ We also observe performance degradation as instruction length increases, which\
    \ aligns with the intuition that longer instructions are more difficult. We encourage\
    \ more research efforts to enhance instruction-following capabilities in agentic\
    \ scenarios.\n\n<span id=\"page-8-1\"></span><sup>5</sup><https://manymanuals.com/>\n\
    \n# References\n\n- <span id=\"page-9-8\"></span>[1] Anthropic. Introducing claude\
    \ 3.5 sonnet, October 2024. Accessed: 2025-05-15.\n- <span id=\"page-9-13\"></span>[2]\
    \ Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin\
    \ Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper\
    \ understanding and reasoning on realistic long-context multitasks. *arXiv preprint\
    \ arXiv:2412.15204*, 2024.\n- <span id=\"page-9-5\"></span>[3] Tom Brown, Benjamin\
    \ Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\
    \ Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models\
    \ are few-shot learners. In *Proceedings of NeurIPs*, pages 1877–1901, 2020.\n\
    - <span id=\"page-9-2\"></span>[4] Zhendong Chu, Shen Wang, Jian Xie, Tinghui\
    \ Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S Yu,\
    \ et al. Llm agents for education: Advances and applications. *arXiv preprint\
    \ arXiv:2503.11733*, 2025.\n- <span id=\"page-9-9\"></span>[5] Aaron Grattafiori,\
    \ Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\
    \ Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3\
    \ herd of models. *arXiv e-prints*, pages arXiv–2407, 2024.\n- <span id=\"page-9-12\"\
    ></span>[6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\
    \ Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing\
    \ reasoning capability in llms via reinforcement learning. *arXiv preprint arXiv:2501.12948*,\
    \ 2025.\n- <span id=\"page-9-0\"></span>[7] Izzeddin Gur, Hiroki Furuta, Austin\
    \ V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust.\
    \ A real-world webagent with planning, long context understanding, and program\
    \ synthesis. In *Proceedings of ICLR*, 2024.\n- <span id=\"page-9-1\"></span>[8]\
    \ Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong\
    \ Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal\
    \ models. In *Proceedings of ACL*, pages 6864–6890, 2024.\n- <span id=\"page-9-6\"\
    ></span>[9] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He,\
    \ Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand\
    \ real-world complex instructions? In *Proceedings of the AAAI Conference on Artificial\
    \ Intelligence*, volume 38, pages 18188–18196, 2024.\n- <span id=\"page-9-4\"\
    ></span>[10] Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang,\
    \ Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-if: Benchmarking llms\
    \ on multi-turn and multilingual instructions following. *arXiv preprint arXiv:2410.15553*,\
    \ 2024.\n- <span id=\"page-9-7\"></span>[11] Aaron Hurst, Adam Lerer, Adam P Goucher,\
    \ Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan\
    \ Hayes, Alec Radford, et al. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*,\
    \ 2024.\n- <span id=\"page-9-11\"></span>[12] Aaron Jaech, Adam Kalai, Adam Lerer,\
    \ Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry,\
    \ Alex Beutel, Alex Carney, et al. Openai o1 system card. *arXiv preprint arXiv:2412.16720*,\
    \ 2024.\n- <span id=\"page-9-10\"></span>[13] Albert Q. Jiang, Alexandre Sablayrolles,\
    \ Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian\
    \ Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\
    \ Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,\
    \ Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n- <span id=\"page-9-3\"\
    ></span>[14] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li,\
    \ Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: A multi-level\
    \ fine-grained constraints following benchmark for large language models. *arXiv\
    \ preprint arXiv:2310.20410*, 2023.\n- <span id=\"page-9-14\"></span>[15] Nathan\
    \ Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\
    \ Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\\\
    \" ulu 3: Pushing frontiers in open language model post-training. *arXiv preprint\
    \ arXiv:2411.15124*, 2024.\n- <span id=\"page-10-6\"></span>[16] Zekun Li, Shinda\
    \ Huang, Jiangtian Wang, Nathan Zhang, Antonis Antoniades, Wenyue Hua, Kaijie\
    \ Zhu, Sirui Zeng, William Yang Wang, and Xifeng Yan. Agentorca: A dual-system\
    \ framework to evaluate language agents on operational routine and constraint\
    \ adherence. *arXiv preprint arXiv:2503.08669*, 2025.\n- <span id=\"page-10-8\"\
    ></span>[17] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda\
    \ Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3\
    \ technical report. *arXiv preprint arXiv:2412.19437*, 2024.\n- <span id=\"page-10-16\"\
    ></span>[18] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng\
    \ Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. Prompt injection\
    \ attack against llm-integrated applications. *arXiv preprint arXiv:2306.05499*,\
    \ 2023.\n- <span id=\"page-10-0\"></span>[19] Junyu Luo, Weizhi Zhang, Ye Yuan,\
    \ Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing\
    \ Long, et al. Large language model agent: A survey on methodology, applications\
    \ and challenges. *arXiv preprint arXiv:2503.21460*, 2025.\n- <span id=\"page-10-14\"\
    ></span>[20] Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu\
    \ Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, et al. When does in-context\
    \ learning fall short and why? a study on specification-heavy tasks. *arXiv preprint\
    \ arXiv:2311.08993*, 2023.\n- <span id=\"page-10-12\"></span>[21] Yunjia Qi, Hao\
    \ Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. Constraint backtranslation\
    \ improves complex instruction following of large language models. *arXiv preprint\
    \ arXiv:2410.24175*, 2024.\n- <span id=\"page-10-5\"></span>[22] Yanzhao Qin,\
    \ Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng\
    \ Chen, Zenan Zhou, Wentao Zhang, et al. Sysbench: Can large language models follow\
    \ system messages? *arXiv preprint arXiv:2408.10943*, 2024.\n- <span id=\"page-10-3\"\
    ></span>[23] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang\
    \ Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating\
    \ instruction following ability in large language models. *arXiv preprint arXiv:2401.03601*,\
    \ 2024.\n- <span id=\"page-10-13\"></span>[24] Haoran Sun, Lixin Liu, Junjie Li,\
    \ Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. Conifer: Improving complex\
    \ constrained instruction-following ability of large language models. *arXiv preprint\
    \ arXiv:2404.02823*, 2024.\n- <span id=\"page-10-10\"></span>[25] Qwen Team. Qwq:\
    \ Reflect deeply on the boundaries of the unknown, November 2024. Accessed: 2025-05-15.\n\
    - <span id=\"page-10-9\"></span>[26] Qwen Team. Qwen3: Think deeper, act faster,\
    \ April 2025. Accessed: 2025-05-15.\n- <span id=\"page-10-11\"></span>[27] THUDM.\
    \ Glm-z1-32b-0414: A deep reasoning model, April 2025. Accessed: 2025-05-15.\n\
    - <span id=\"page-10-15\"></span>[28] Eric Wallace, Kai Xiao, Reimar Leike, Lilian\
    \ Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training\
    \ llms to prioritize privileged instructions. *arXiv preprint arXiv:2404.13208*,\
    \ 2024.\n- <span id=\"page-10-4\"></span>[29] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong\
    \ Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu,\
    \ et al. Benchmarking complex instruction-following with multiple constraints\
    \ composition. *Advances in Neural Information Processing Systems*, 37:137610–\
    \ 137645, 2024.\n- <span id=\"page-10-7\"></span>[30] Kaiwen Yan, Hongcheng Guo,\
    \ Xuanqing Shi, Jingyi Xu, Yaonan Gu, and Zhoujun Li. Codeif: Benchmarking the\
    \ instruction-following capabilities of large language models for code generation.\
    \ *arXiv preprint arXiv:2502.19166*, 2025.\n- <span id=\"page-10-1\"></span>[31]\
    \ Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua\
    \ Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents:\
    \ A survey. *arXiv preprint arXiv:2411.18279*, 2024.\n- <span id=\"page-10-2\"\
    ></span>[32] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen,\
    \ Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone\
    \ users. In *Proceedings of the 2025 CHI Conference on Human Factors in Computing\
    \ Systems*, pages 1–20, 2025.\n- <span id=\"page-11-2\"></span>[33] Tao Zhang,\
    \ Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing\
    \ Qiao, Weipeng Chen, Bin Cui, et al. Cfbench: A comprehensive constraints-following\
    \ benchmark for llms. *arXiv preprint arXiv:2408.01122*, 2024.\n- <span id=\"\
    page-11-0\"></span>[34] Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang\
    \ Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. Pptagent: Generating\
    \ and evaluating presentations beyond text-to-slides. *arXiv preprint arXiv:2501.03936*,\
    \ 2025.\n- <span id=\"page-11-4\"></span>[35] Denny Zhou, Nathanael Schärli, Le\
    \ Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier\
    \ Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning\
    \ in large language models. In *Proceedings of ICLR*, 2023.\n- <span id=\"page-11-1\"\
    ></span>[36] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy\
    \ Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for\
    \ large language models. *arXiv preprint arXiv:2311.07911*, 2023.\n- <span id=\"\
    page-11-3\"></span>[37] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha\
    \ Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation\
    \ for large language models. *arXiv preprint arXiv:2311.07911*, 2023.\n\n# Appendices\n\
    \n# A Limitations\n\nWe discuss the limitations as follows: (1) Although the construction\
    \ of AGENTIF is semi-automated, it still requires substantial manual verification,\
    \ which limits its direct generalization to generating a large scale of data.\
    \ In the future, we plan to explore automated methods for constructing post-training\
    \ data to enhance instruction-following capabilities. (2) AGENTIF includes instructions\
    \ only in Chinese and English, lacking broader multilingual coverage, which may\
    \ limit its broader usage. We encourage the community to extend the dataset to\
    \ support more languages. (3) All experiments are conducted in a zero-shot setting,\
    \ and we do not explore prompt engineering techniques such as in-context demonstrations.\
    \ As this work focuses on dataset construction and model evaluation, we leave\
    \ prompt engineering for future work.\n\n# B Ethical Considerations\n\nWe discuss\
    \ ethical concerns and broader impacts as follows: (1) Intellectual Property.\
    \ The agents obtained from GitHub repositories are shared under GPL-3.0[6](#page-12-1)\
    \ or Apache-2.0 licenses[7](#page-12-2) . We strictly adhere to the respective\
    \ licensing terms, and all data are used solely for academic research. For industrial\
    \ agents, we obtained approval from internal review boards to use and release\
    \ the data under the GPL-3.0 license. These industrial applications target enterprise\
    \ users in China, and we only collect statistics on active users and service requests\
    \ and do not conduct user behavior analysis. Notably, the agents include only\
    \ system prompts without any user queries to prevent information leakage. Instead,\
    \ all user queries in AGENTIF are created by hired annotators. AGENTIF will be\
    \ released under the GPL-3.0 license. (2) Broader Impacts. This work aims to construct\
    \ a benchmark for evaluating instruction-following in agentic scenarios. Leaderboard\
    \ rankings on AGENTIF should not be used for adversarial comparisons or interpreted\
    \ as evidence of misconduct in other research efforts. The benchmark data should\
    \ not be incorporated into the training process of LLMs to avoid potential contamination\
    \ or leakage. (3) Controlling Potential Risks. All data have been subjected to\
    \ rigorous safety checks and are well anonymized to eliminate sensitive content.\
    \ (4) Human Annotation. We employ eight annotators (gender-balanced) for data\
    \ annotation and verification. All annotators are fairly paid based on workload\
    \ and are informed of the intended use of the data before annotation. No personal\
    \ information of annotators is involved in the dataset. (5) LLM Usage. We used\
    \ GPT-4o and Claude 3.7 Sonnet for paraphrasing some sentences of this paper.\n\
    \n# <span id=\"page-12-0\"></span>C Details of Constraint Taxonomy\n\n# C.1 Constraint\
    \ Type\n\nThis section provides a detailed overview of our constraint taxonomy.\
    \ Figure [7](#page-13-1) illustrates the distribution of constraints across different\
    \ categories.\n\nFormatting Constraints Formatting Constraints dictate the structural\
    \ form and presentation of model outputs. They ensure responses conform to a specific\
    \ format, layout, or visual style, vital for machine readability or subsequent\
    \ processing. Examples include: (1) Syntax Formatting: requiring outputs in formats\
    \ like JSON, XML, or Markdown. (2) Layout Structure: specifying bullet points,\
    \ tables, or paragraph length. (3) Symbol Conventions: enforcing specific symbols\
    \ like date patterns or currency symbols. (4) Instructional Structure: mandating\
    \ specific response organizations, such as \"explain the concept in three steps.\"\
    \n\nSemantic Constraints Semantic Constraints govern the factual correctness,\
    \ informativeness, and intended meaning of model outputs. They ensure content\
    \ aligns with task requirements and includes essential semantic elements. Common\
    \ types are: (1) Content Targeting: restricting the output's topic or focus. (2)\
    \ Information Completeness: requiring specific elements, such as time, location,\
    \ or\n\n<span id=\"page-12-1\"></span><sup>6</sup><https://www.gnu.org/licenses/gpl-3.0.en.html>\n\
    \n<span id=\"page-12-2\"></span><sup>7</sup><https://www.apache.org/licenses/LICENSE-2.0>\n\
    \npeople. (3) Keyword Presence: mandating the inclusion of particular phrases\
    \ or terms. (4) Stylistic Pequirements: dictating aspects like \"use language\
    \ understandable to children.\"\n\nTool Constraints Tool Constraints restrict\
    \ the computational or external resources a model can use when generating responses.\
    \ These are crucial for simulating specific environments or adhering to usage\
    \ restrictions. Examples include: (1) Tool Usage: restricting callable functions\
    \ or external tools, e.g., \"only built-in Python functions may be used.\" (2)\
    \ Computational Limitations: imposing resource constraints, such asdo not use\
    \ GPU acceleration.\"\n\n#### C.2 Constraint Presentation Type\n\nThe constraint\
    \ presentation type refers to how constraints are conveyed to the model within\
    \ the instruction. We categorize this type into three distinct forms:\n\nVanilla\
    \ Constraints are stated directly and unconditionally in the prompt. These constraints\
    \ are always in effect, regardless of the input content or model behavior. For\
    \ example, \"Answer in Chinese\", i.e., the model must always respond in Chinese,\
    \ regardless of the question.\n\nCondition Constraints are activated only when\
    \ specific conditions are met, either from the user input or the model's output.\
    \ For instance, a constraint like \"flag the response if it contains sensitive\
    \ content\" depends on input triggers, while \"apply markdown formatting only\
    \ when markdown is used\" relies on the model's behavioral context.\n\nExample\
    \ Constraints are not explicitly stated but implied through in-context demonstrations.\
    \ For example, providing an example written in Shakespearean English implicitly\
    \ requires the model to generate responses in a similar style, or showing fewshot\
    \ outputs in structured JSON format signals that the same structure should be\
    \ followed.\n\n<span id=\"page-13-1\"></span>![](_page_13_Figure_7.jpeg)\n\nFigure\
    \ 7: Distribution of constraint types. The inner ring shows the breakdown of constraint\
    \ types into Semantic, Formatting, and Tool categories. The outer ring further\
    \ categorizes constraints based on their presentation type, including Vanilla,\
    \ Condition, and Example. Semantic constraints and vanilla activation are the\
    \ most prevalent in the dataset.\n\n# <span id=\"page-13-0\"></span>D Detailed\n\
    \n# Information about Dataset Construction\n\n### D.1 Prompts for Automatic Annotation\n\
    \nTable [3](#page-14-0) provides the prompt template for instruction collection.\
    \ Templates for constraint annotation are\n\nfound in Tables [4](#page-15-0) and\
    \ [5.](#page-16-0) In the evaluation generation phrase, the template for generating\
    \ conditional checks is in Table [6.](#page-17-0) Finally, Tables [8,](#page-18-0)\
    \ [10,](#page-19-0) and [9](#page-18-1) detail the prompt templates for generating\
    \ three types of evaluation.\n\n# D.2 Detailed Information about Human Annotation\n\
    \nWe invited graduate students from the Department of Computer Science to participate\
    \ in the data annotation process, and fairly compensated them based on pre-agreed\
    \ salaries and workloads. All employment was formalized through contracts and\
    \ conducted in full compliance with local regulations. During the annotation process,\
    \ we conducted three rounds of sampled review and feedback to iteratively refine\
    \ and finalize the high-quality annotation results.\n\n<span id=\"page-14-0\"\
    ></span>You are given two pieces of information:\n\n- 1. A Task Description summarizing\
    \ what the agent is designed to do.\n- 2. Input Variable Annotations listing the\
    \ input variables, each with a brief explanation.\n\n#### Your task is to:\n\n\
    - Generate multiple sets of variable content that are rich, detailed, and expanded.\
    \ Each set must include specific, meaningful, and realistic information, maximizing\
    \ substance while staying coherent.\n- Ensure strong diversity across examples,\
    \ including both speaking styles (storytelling, poetic, formal, humorous, dramatic,\
    \ etc.) and imagined scenarios (different realistic scenes that meaningfully shape\
    \ the content).\n- Match the meaning of each variable based on its comment from\
    \ Input Variable Annotations.\n- Use the Task Description to guide the overall\
    \ theme and content of the generated examples.\n\n#### Input:\n\n{input}\n\n####\
    \ Output Format:\n\nReturn a JSON array where:\n\n- Each item is a JSON object\
    \ (a dict) corresponding to one complete example.\n- The keys of each object exactly\
    \ match the variable names listed in the Input Variable Annotations.\n\nExample\
    \ structure:\n\n```\n[\n {\n   \"variable_name_1\": \"<filled content>\",\n  \
    \ \"variable_name_2\": \"<filled content>\"\n },\n {\n   \"variable_name_1\":\
    \ \"<filled content>\",\n   \"variable_name_2\": \"<filled content>\"\n }\n]\n\
    ```\nTable 3: Prompt for query generation in instruction collection.\n\n# E Experimental\
    \ Details\n\nEvaluation Models. We conduct our evaluation using a diverse set\
    \ of language models, including o1-mini, GPT-4o (2024-11-20), Qwen3-32B, QwQ-32B,\
    \ DeepSeek-R1, GLM-Z1-32B (0414), DeepSeek-v3 (2024-03-25), Claude 3.5 Sonnet\
    \ (2024-10-22), Meta-LLaMA-3.1-70B-Instruct, DeepSeek-R1-Distill-Qwen-32B, DeepSeek-R1-Distill-LLaMA-70B,\
    \ Meta-LLaMA-3.1-8B-Instruct, Mistral-Crab-DPO-7B, Mistral-7B-Instruct-v0.3, and\
    \ Mistral-Conifer-DPO-7B.\n\nExperimental Hyperparameters. In all experiments,\
    \ we set the temperature to 0 to ensure reproducibility. The maximum number of\
    \ generated tokens is set to 32, 000. For models with a context length shorter\
    \ than 32, 000 tokens, we set their maximum allowable context length to match\
    \ the max-token limit.\n\nExperiment Cost. We use gpt-4o-2024-11-20 as the evaluator\
    \ throughout our experiments. Each evaluation round costs approximately \\$20.\n\
    \n<span id=\"page-15-0\"></span>You are given a system prompt. Your job is to\
    \ extract atomic constraints that apply specifically to the expected response\
    \ generated by the model, as dictated by the system prompt.\n\n#### Please follow\
    \ the instructions below precisely:\n\n- Only extract constraints that apply to\
    \ the response, not those describing or constraining the input variables or instructions\
    \ to the user.\n- Read the system prompt line by line and extract the smallest\
    \ possible atomic constraint units from any content that imposes rules, structure,\
    \ or expectations on the response.\n- This includes:\n\t- Explicit instructions\
    \ such as: \"Your task is to. . . \", \"You must. . . \", \"Please do the following.\
    \ . . \"\n\t- Numbered lists of required actions\n\t- Formatting, style, or output\
    \ structure expectations\n\t- Content rules or restrictions\n\t- Demonstrations\
    \ or examples that implicitly define how the model should respond (e.g., few-shot\
    \ examples, response templates, or stylistic samples). In these cases, extract\
    \ the implied constraint as faithfully as possible (e.g., \"Respond in JSON format\"\
    , \"Follow the narrative style shown\").\n- For each constraint, determine whether\
    \ it is:\n\t- \"vanilla\": The rule applies to all responses regardless of input\
    \ or task branch.\n\t- \"conditional\": The rule only applies in certain contexts\
    \ (e.g., a certain kind of task or response type).\n\t- \"example\": The rule\
    \ is not explicitly stated, but implied from a given example or demonstration\
    \ in the prompt.\n- Sometimes the system prompt gives a rule or behavior without\
    \ explicitly stating the condition, but it's only meant to apply in a certain\
    \ type of response. In such cases, you should infer the missing condition and\
    \ rewrite the constraint in the form If [condition], then [rule]. These should\
    \ be marked as \"conditional\" even if no \"if\" appears in the prompt.\n- When\
    \ the prompt contains an example, you should recognize that it implies certain\
    \ response expectations, and you are expected to infer the corresponding constraints\
    \ accordingly.\n- Do not rewrite or generalize the constraint. Extract the exact\
    \ wording from the prompt wherever possible.\n- Return your answer as a list of\
    \ dictionaries, where each dictionary contains:\n\t- desc: the extracted constraint\
    \ (verbatim from the prompt, or inferred if conditional)\n\t- dimension: one of\
    \ \"unconditional\", \"conditional\", or \"example\"\n- If the system prompt contains\
    \ no constraints on the response, return an empty list: []\n\n#### Example Output:\n\
    \n```\n[\n {\n   \"desc\": \"Always use \\\"you\\\" and \\\"your\\\" when addressing\
    \ the user.\",\n   \"dimension\": \"vanilla\"\n },\n {\n   \"desc\": \"If no symptoms\
    \ are reported, explain why further screening is\n        still necessary.\",\n\
    \   \"dimension\": \"conditional\"\n }\n]\n```\n### Input System Prompt:\n\n{prompt}\n\
    \nTable 4: Prompt for extracting atom-level constraints from block instructions\
    \ in constraint annotation.\n\n<span id=\"page-16-0\"></span>You are given a single\
    \ constraint. Your task is to classify this constraint into one or more of the\
    \ following categories. Prefer to choose only one category unless you believe\
    \ the constraint clearly fits multiple types. After classification, provide a\
    \ brief explanation supporting your decision.\n\nConstraint Categories:\n\n1.\
    \ formatting — *Controls the structure or presentation format of the output.*\n\
    \nExamples include:\n\n- Syntax format (e.g., JSON, XML, Markdown)\n- Layout and\
    \ structure (e.g., bullet points, tables, paragraph length)\n- Symbol and notation\
    \ norms (e.g., date format \"YYYY-MM-DD\", currency symbol \"¥\")\n- Interaction\
    \ steps (e.g., \"explain the principle in three steps\")\n\n*Example:* \"Present\
    \ the result using LaTeX math notation\"\n\n- 2. semantic *Ensures the output\
    \ content is meaningful, accurate, and complete.* Examples include:\n\t- Factual\
    \ accuracy (e.g., no fabricated data)\n\t- Logical consistency\n\t- Information\
    \ completeness (must include specified elements)\n\t- Keyword requirements (must\
    \ contain specified terms)\n\t- Style or tone (e.g., \"explain in child-friendly\
    \ language\")\n\t- Neutrality of position (e.g., \"avoid emotionally charged language\"\
    )\n\t- Terminology standards (e.g., \"use ISO names for chemicals\")\n\t- *Example:*\
    \ \"The answer must include the event's time, location, and key figures\"\n- 3.\
    \ Tool *Limits the usage of computational resources or external dependencies.*\
    \ Examples include:\n\t- Data source limitations (e.g., \"only use data from after\
    \ 2020\")\n\t- Computational restrictions (e.g., \"no GPU acceleration\")\n\t\
    - Tool/library restrictions (e.g., \"use only built-in Python functions\")\n\n\
    *Example:* \"Do not access any online resources during analysis\"\n\n### Output\
    \ Format:\n\n```\n{\n \"type_list\": [\"Your chosen category or categories\"],\n\
    \ \"explanation\": \"Your reasoning for choosing this classification\"\n}\n```\n\
    ### Input Constraint:\n\n{constraint}\n\nTable 5: Prompt for classifying constraints\
    \ into formatting, semantic, or tool categories in constraint annotation.\n\n\
    <span id=\"page-17-0\"></span>You are a helpful assistant that specializes in\
    \ constraint verification. Your task is to process a conditional constraint and\
    \ produce two outputs:\n\n- 1. A yes/no question that can be used to verify whether\
    \ the condition is satisfied.\n- 2. The main constraint that should be enforced\
    \ if the condition is true. This should exclude the conditional part and be expressed\
    \ as a standalone, unconditional constraint.\n\n#### Please follow these instructions:\n\
    \n- If the condition refers to the input query, the question should focus on analyzing\
    \ the input.\n- If the condition refers to the response, the question should focus\
    \ on analyzing the response.\n- Keep the extracted main constraint faithful to\
    \ the original meaning, but remove the conditional clause (e.g., remove \"If...\"\
    \ or \"When...\").\n\n#### Return your output as a JSON dictionary with the following\
    \ keys:\n\n```\n{\n \"condition_check_question\": \"{Your yes/no question}? Please\
    \ answer YES/\n     NO directly and do not enter anything else.\",\n \"main_constraint\"\
    : \"{The unconditional constraint to verify if the\n     condition is true.}\"\
    \n}\n```\nNote: The constraint itself is the primary basis for generation. The\
    \ instruction paragraph is provided only as auxiliary context, and should be used\
    \ only when the constraint alone is ambiguous or underspecified. Input\n\n```\n\
    Here is the full instruction paragraph where the constraint appears: {instruction}\n\
    The constraint to verify: {constraint}\n```\nTable 6: Prompt for decomposing a\
    \ conditional constraint into a condition-checking question and a standalone constraint\
    \ in evaluation generation.\n\nYou are an expert at analyzing natural language\
    \ constraints and determining how they can be verified. Your task is to classify\
    \ a given constraint based on whether it can be validated:\n\n- 1. code Directly\
    \ by code\n- 2. llm\\_assisted\\_code By code after extracting needed content\
    \ via LLM\n- 3. llm Only by using LLM to semantically assess it\n\n#### Please\
    \ follow these guidelines:\n\n- If the constraint can be validated by simple logic\
    \ (e.g., length, presence, format) and the content is directly accessible from\
    \ the response, classify it as code.\n- If the constraint requires extracting\
    \ a specific section from the response (e.g., \"intro\", \"conclusion\", \"step\
    \ 1\") before performing validation (e.g., counting words), classify it as llm\\\
    _assisted\\_code.\n- If the constraint requires open-ended, semantic, or subjective\
    \ understanding (e.g., logical correctness, relevance, tone, or fact-checking),\
    \ classify it as llm.\n\n#### Return your answer *only* as a JSON dictionary in\
    \ the following format:\n\n```\n{\n \"constraint_type\": \"code\" | \"llm_assisted_code\"\
    \ | \"llm\",\n \"explanation\": \"Your brief reasoning here\"\n}\n```\nNote: The\
    \ constraint itself is the primary basis for classification. The instruction paragraph\
    \ is provided only as auxiliary context, and should be used only when the constraint\
    \ alone is ambiguous or underspecified. Input\n\nHere is the full instruction\
    \ paragraph where the constraint appears: {instruction} The constraint to classify:\
    \ {constraint}\n\nTable 7: Prompt for classifying constraints by their verifiability\
    \ type: directly by code, LLM-assisted code, or purely by LLM.\n\n<span id=\"\
    page-18-0\"></span>You are a helpful assistant that specializes in verifying whether\
    \ model responses comply with specific constraints. Your task is to generate a\
    \ yes/no question that can be used to determine whether a model response satisfies\
    \ a given constraint. This question should be phrased so that an LLM (or a human\
    \ evaluator) could answer it just by reading the model's response.\n\n#### Please\
    \ follow these rules:\n\n- 1. The question should be clear, specific, and binary\
    \ it should be answerable with \"yes\" or \"no\".\n- 2. It must refer explicitly\
    \ to what the constraint is checking (e.g., structure, length, tone, factuality).\n\
    - 3. If the constraint refers to a specific section (e.g., \"intro\", \"step 1\"\
    , \"conclusion\"), include that in the question.\n\n#### Return your answer in\
    \ the following format:\n\n```\n{\n \"validation_question\": \"{Your full yes/no\
    \ question here}? Please answer\n      YES/NO directly and do not enter anything\
    \ else.\"\n}\nNote: The constraint itself is the primary basis for generation.\
    \ The instruction paragraph is provided only as\nauxiliary context, and should\
    \ be used only when the constraint alone is ambiguous or underspecified.\n```\n\
    Instruction Paragraph (Context):\n\nThis is the full instruction paragraph that\
    \ provides context for the constraint: {instruction} Constraint to Verify: {constraint}\n\
    \nTable 8: Prompt for generating a yes/no validation question from a constraint.\n\
    \n<span id=\"page-18-1\"></span>You are a helpful assistant that specializes in\
    \ generating extraction instructions to support constraint verification. Your\
    \ task is to generate a concise and precise instruction that tells an LLM what\
    \ specific part of the response needs to be extracted, so that the extracted content\
    \ can later be verified by code. Please follow these guidelines:\n\n- 1. The instruction\
    \ should clearly specify what to extract from the response (e.g., \"Extract the\
    \ introduction part\", \"Extract the function used\", \"Extract the final answer\
    \ sentence\").\n- 2. Base your output on the constraint provided below.\n- 3.\
    \ You may refer to the instruction paragraph only when the constraint is ambiguous\
    \ and requires context.\n- 4. Return your output as a JSON dictionary in the following\
    \ format:\n\n```\n{\n \"extraction_instruction\": \"{your generated extraction\
    \ instruction}.\n     Return the extracted content verbatim from the response.\
    \ If multiple\n       segments are found, return them as a Python-style list of\
    \ strings.\n     If nothing is found, return None.\"\n}\nInput:\nInstruction Paragraph:\n\
    Here is the full instruction paragraph where the constraint appears: {instruction}\n\
    Constraint to support: {constraint}\nHelper analysis: {helper_analysis}\n```\n\
    Table 9: Prompt for generating extraction instructions from a constraint to support\
    \ code-based verification in hybrid evaluation.\n\n<span id=\"page-19-0\"></span>You\
    \ are tasked with implementing a Python function named check\\_following that\
    \ determines whether a given **response** satisfies a specified constraint. The\
    \ function must return True if the constraint is satisfied and False otherwise.\n\
    \nRequirements:\n\n- The function accepts only one parameter: response, which\
    \ is a Python string.\n- The function must return a boolean value (True or False)\
    \ based on whether the response adheres to the constraint.\n- The function must\
    \ not include any I/O operations, such as input() or ArgumentParser.\n- The Python\
    \ code for constraint verification should be designed to be generalizable, e.g.,\
    \ using regular expressions or other suitable techniques.\n- Only return the exact\
    \ Python code, with no additional explanations.\n\nNote: The constraint itself\
    \ is the primary basis for classification. The instruction paragraph is provided\
    \ only as auxiliary context, and should be used only when the constraint alone\
    \ is ambiguous or underspecified. Instruction Paragraph:\n\nHere is the full instruction\
    \ paragraph where the constraint appears: {instruction} Specific Constraint to\
    \ Verify: {constraint}\n\nTable 10: Prompt for generating a Python function that\
    \ verifies whether a response satisfies a specified constraint.\n\n# NeurIPS Paper\
    \ Checklist\n\n### 1. Claims\n\nQuestion: Do the main claims made in the abstract\
    \ and introduction accurately reflect the paper's contributions and scope?\n\n\
    Answer: [Yes]\n\nJustification: In Section 1 and Abstract.\n\nGuidelines:\n\n\
    - The answer NA means that the abstract and introduction do not include the claims\
    \ made in the paper.\n- The abstract and/or introduction should clearly state\
    \ the claims made, including the contributions made in the paper and important\
    \ assumptions and limitations. A No or NA answer to this question will not be\
    \ perceived well by the reviewers.\n- The claims made should match theoretical\
    \ and experimental results, and reflect how much the results can be expected to\
    \ generalize to other settings.\n- It is fine to include aspirational goals as\
    \ motivation as long as it is clear that these goals are not attained by the paper.\n\
    \n#### 2. Limitations\n\nQuestion: Does the paper discuss the limitations of the\
    \ work performed by the authors?\n\n### Answer: [Yes]\n\nJustification: In Appendix\
    \ A.\n\nGuidelines:\n\n- The answer NA means that the paper has no limitation\
    \ while the answer No means that the paper has limitations, but those are not\
    \ discussed in the paper.\n- The authors are encouraged to create a separate \"\
    Limitations\" section in their paper.\n- The paper should point out any strong\
    \ assumptions and how robust the results are to violations of these assumptions\
    \ (e.g., independence assumptions, noiseless settings, model well-specification,\
    \ asymptotic approximations only holding locally). The authors should reflect\
    \ on how these assumptions might be violated in practice and what the implications\
    \ would be.\n- The authors should reflect on the scope of the claims made, e.g.,\
    \ if the approach was only tested on a few datasets or with a few runs. In general,\
    \ empirical results often depend on implicit assumptions, which should be articulated.\n\
    - The authors should reflect on the factors that influence the performance of\
    \ the approach. For example, a facial recognition algorithm may perform poorly\
    \ when image resolution is low or images are taken in low lighting. Or a speech-to-text\
    \ system might not be used reliably to provide closed captions for online lectures\
    \ because it fails to handle technical jargon.\n- The authors should discuss the\
    \ computational efficiency of the proposed algorithms and how they scale with\
    \ dataset size.\n- If applicable, the authors should discuss possible limitations\
    \ of their approach to address problems of privacy and fairness.\n- While the\
    \ authors might fear that complete honesty about limitations might be used by\
    \ reviewers as grounds for rejection, a worse outcome might be that reviewers\
    \ discover limitations that aren't acknowledged in the paper. The authors should\
    \ use their best judgment and recognize that individual actions in favor of transparency\
    \ play an important role in developing norms that preserve the integrity of the\
    \ community. Reviewers will be specifically instructed to not penalize honesty\
    \ concerning limitations.\n\n# 3. Theory assumptions and proofs\n\nQuestion: For\
    \ each theoretical result, does the paper provide the full set of assumptions\
    \ and a complete (and correct) proof?\n\nAnswer: [NA]\n\nJustification: This paper\
    \ does not involve theoretical assumptions and proofs.\n\n### Guidelines:\n\n\
    - The answer NA means that the paper does not include theoretical results.\n-\
    \ All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.\n\
    - All assumptions should be clearly stated or referenced in the statement of any\
    \ theorems.\n- The proofs can either appear in the main paper or the supplemental\
    \ material, but if they appear in the supplemental material, the authors are encouraged\
    \ to provide a short proof sketch to provide intuition.\n- Inversely, any informal\
    \ proof provided in the core of the paper should be complemented by formal proofs\
    \ provided in appendix or supplemental material.\n- Theorems and Lemmas that the\
    \ proof relies upon should be properly referenced.\n\n# 4. Experimental result\
    \ reproducibility\n\nQuestion: Does the paper fully disclose all the information\
    \ needed to reproduce the main experimental results of the paper to the extent\
    \ that it affects the main claims and/or conclusions of the paper (regardless\
    \ of whether the code and data are provided or not)?\n\n# Answer: [Yes]\n\nJustification:\
    \ In Appendix E.\n\nGuidelines:\n\n- The answer NA means that the paper does not\
    \ include experiments.\n- If the paper includes experiments, a No answer to this\
    \ question will not be perceived well by the reviewers: Making the paper reproducible\
    \ is important, regardless of whether the code and data are provided or not.\n\
    - If the contribution is a dataset and/or model, the authors should describe the\
    \ steps taken to make their results reproducible or verifiable.\n- Depending on\
    \ the contribution, reproducibility can be accomplished in various ways. For example,\
    \ if the contribution is a novel architecture, describing the architecture fully\
    \ might suffice, or if the contribution is a specific model and empirical evaluation,\
    \ it may be necessary to either make it possible for others to replicate the model\
    \ with the same dataset, or provide access to the model. In general. releasing\
    \ code and data is often one good way to accomplish this, but reproducibility\
    \ can also be provided via detailed instructions for how to replicate the results,\
    \ access to a hosted model (e.g., in the case of a large language model), releasing\
    \ of a model checkpoint, or other means that are appropriate to the research performed.\n\
    - While NeurIPS does not require releasing code, the conference does require all\
    \ submissions to provide some reasonable avenue for reproducibility, which may\
    \ depend on the nature of the contribution. For example\n\t- (a) If the contribution\
    \ is primarily a new algorithm, the paper should make it clear how to reproduce\
    \ that algorithm.\n\t- (b) If the contribution is primarily a new model architecture,\
    \ the paper should describe the architecture clearly and fully.\n\t- (c) If the\
    \ contribution is a new model (e.g., a large language model), then there should\
    \ either be a way to access this model for reproducing the results or a way to\
    \ reproduce the model (e.g., with an open-source dataset or instructions for how\
    \ to construct the dataset).\n- (d) We recognize that reproducibility may be tricky\
    \ in some cases, in which case authors are welcome to describe the particular\
    \ way they provide for reproducibility. In the case of closed-source models, it\
    \ may be that access to the model is limited in some way (e.g., to registered\
    \ users), but it should be possible for other researchers to have some path to\
    \ reproducing or verifying the results.\n\n# 5. Open access to data and code\n\
    \nQuestion: Does the paper provide open access to the data and code, with sufficient\
    \ instructions to faithfully reproduce the main experimental results, as described\
    \ in supplemental material?\n\n### Answer: [Yes]\n\nJustification: In Abstract.\n\
    \nGuidelines:\n\n- The answer NA means that paper does not include experiments\
    \ requiring code.\n- Please see the NeurIPS code and data submission guidelines\
    \ ([https://nips.cc/](https://nips.cc/public/guides/CodeSubmissionPolicy) [public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))\
    \ for more details.\n- While we encourage the release of code and data, we understand\
    \ that this might not be possible, so \"No\" is an acceptable answer. Papers cannot\
    \ be rejected simply for not including code, unless this is central to the contribution\
    \ (e.g., for a new open-source benchmark).\n- The instructions should contain\
    \ the exact command and environment needed to run to reproduce the results. See\
    \ the NeurIPS code and data submission guidelines ([https:](https://nips.cc/public/guides/CodeSubmissionPolicy)\
    \ [//nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))\
    \ for more details.\n- The authors should provide instructions on data access\
    \ and preparation, including how to access the raw data, preprocessed data, intermediate\
    \ data, and generated data, etc.\n- The authors should provide scripts to reproduce\
    \ all experimental results for the new proposed method and baselines. If only\
    \ a subset of experiments are reproducible, they should state which ones are omitted\
    \ from the script and why.\n- At submission time, to preserve anonymity, the authors\
    \ should release anonymized versions (if applicable).\n- Providing as much information\
    \ as possible in supplemental material (appended to the paper) is recommended,\
    \ but including URLs to data and code is permitted.\n\n#### 6. Experimental setting/details\n\
    \nQuestion: Does the paper specify all the training and test details (e.g., data\
    \ splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary\
    \ to understand the results?\n\nAnswer: [Yes]\n\nJustification: In Appendix E.\n\
    \nGuidelines:\n\n- The answer NA means that the paper does not include experiments.\n\
    - The experimental setting should be presented in the core of the paper to a level\
    \ of detail that is necessary to appreciate the results and make sense of them.\n\
    - The full details can be provided either with the code, in appendix, or as supplemental\
    \ material.\n\n# 7. Experiment statistical significance\n\nQuestion: Does the\
    \ paper report error bars suitably and correctly defined or other appropriate\
    \ information about the statistical significance of the experiments?\n\nAnswer:\
    \ [Yes]\n\nJustification: In Appendix E, we explain that all sampling temperatures\
    \ are set to 0, eliminating randomness.\n\n- The answer NA means that the paper\
    \ does not include experiments.\n- The authors should answer \"Yes\" if the results\
    \ are accompanied by error bars, confidence intervals, or statistical significance\
    \ tests, at least for the experiments that support the main claims of the paper.\n\
    - The factors of variability that the error bars are capturing should be clearly\
    \ stated (for example, train/test split, initialization, random drawing of some\
    \ parameter, or overall run with given experimental conditions).\n- The method\
    \ for calculating the error bars should be explained (closed form formula, call\
    \ to a library function, bootstrap, etc.)\n- The assumptions made should be given\
    \ (e.g., Normally distributed errors).\n- It should be clear whether the error\
    \ bar is the standard deviation or the standard error of the mean.\n- It is OK\
    \ to report 1-sigma error bars, but one should state it. The authors should preferably\
    \ report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\
    \ of Normality of errors is not verified.\n- For asymmetric distributions, the\
    \ authors should be careful not to show in tables or figures symmetric error bars\
    \ that would yield results that are out of range (e.g. negative error rates).\n\
    - If error bars are reported in tables or plots, The authors should explain in\
    \ the text how they were calculated and reference the corresponding figures or\
    \ tables in the text.\n\n#### 8. Experiments compute resources\n\nQuestion: For\
    \ each experiment, does the paper provide sufficient information on the computer\
    \ resources (type of compute workers, memory, time of execution) needed to reproduce\
    \ the experiments?\n\nAnswer: [Yes]\n\nJustification: In Appendix E.\n\nGuidelines:\n\
    \n- The answer NA means that the paper does not include experiments.\n- The paper\
    \ should indicate the type of compute workers CPU or GPU, internal cluster, or\
    \ cloud provider, including relevant memory and storage.\n- The paper should provide\
    \ the amount of compute required for each of the individual experimental runs\
    \ as well as estimate the total compute.\n- The paper should disclose whether\
    \ the full research project required more compute than the experiments reported\
    \ in the paper (e.g., preliminary or failed experiments that didn't make it into\
    \ the paper).\n\n### 9. Code of ethics\n\nQuestion: Does the research conducted\
    \ in the paper conform, in every respect, with the NeurIPS Code of Ethics <https://neurips.cc/public/EthicsGuidelines>?\n\
    \n# Answer: [Yes]\n\nJustification: In Appendix B.\n\nGuidelines:\n\n- The answer\
    \ NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n- If\
    \ the authors answer No, they should explain the special circumstances that require\
    \ a deviation from the Code of Ethics.\n- The authors should make sure to preserve\
    \ anonymity (e.g., if there is a special consideration due to laws or regulations\
    \ in their jurisdiction).\n\n# 10. Broader impacts\n\nQuestion: Does the paper\
    \ discuss both potential positive societal impacts and negative societal impacts\
    \ of the work performed?\n\nAnswer: [Yes]\n\nJustification: In Appendix B.\n\n\
    - The answer NA means that there is no societal impact of the work performed.\n\
    - If the authors answer NA or No, they should explain why their work has no societal\
    \ impact or why the paper does not address societal impact.\n- Examples of negative\
    \ societal impacts include potential malicious or unintended uses (e.g., disinformation,\
    \ generating fake profiles, surveillance), fairness considerations (e.g., deployment\
    \ of technologies that could make decisions that unfairly impact specific groups),\
    \ privacy considerations, and security considerations.\n- The conference expects\
    \ that many papers will be foundational research and not tied to particular applications,\
    \ let alone deployments. However, if there is a direct path to any negative applications,\
    \ the authors should point it out. For example, it is legitimate to point out\
    \ that an improvement in the quality of generative models could be used to generate\
    \ deepfakes for disinformation. On the other hand, it is not needed to point out\
    \ that a generic algorithm for optimizing neural networks could enable people\
    \ to train models that generate Deepfakes faster.\n- The authors should consider\
    \ possible harms that could arise when the technology is being used as intended\
    \ and functioning correctly, harms that could arise when the technology is being\
    \ used as intended but gives incorrect results, and harms following from (intentional\
    \ or unintentional) misuse of the technology.\n- If there are negative societal\
    \ impacts, the authors could also discuss possible mitigation strategies (e.g.,\
    \ gated release of models, providing defenses in addition to attacks, mechanisms\
    \ for monitoring misuse, mechanisms to monitor how a system learns from feedback\
    \ over time, improving the efficiency and accessibility of ML).\n\n### 11. Safeguards\n\
    \nQuestion: Does the paper describe safeguards that have been put in place for\
    \ responsible release of data or models that have a high risk for misuse (e.g.,\
    \ pretrained language models, image generators, or scraped datasets)?\n\nAnswer:\
    \ [Yes]\n\nJustification: In Appendix B.\n\nGuidelines:\n\n- The answer NA means\
    \ that the paper poses no such risks.\n- Released models that have a high risk\
    \ for misuse or dual-use should be released with necessary safeguards to allow\
    \ for controlled use of the model, for example by requiring that users adhere\
    \ to usage guidelines or restrictions to access the model or implementing safety\
    \ filters.\n- Datasets that have been scraped from the Internet could pose safety\
    \ risks. The authors should describe how they avoided releasing unsafe images.\n\
    - We recognize that providing effective safeguards is challenging, and many papers\
    \ do not require this, but we encourage authors to take this into account and\
    \ make a best faith effort.\n\n#### 12. Licenses for existing assets\n\nQuestion:\
    \ Are the creators or original owners of assets (e.g., code, data, models), used\
    \ in the paper, properly credited and are the license and terms of use explicitly\
    \ mentioned and properly respected?\n\nAnswer: [Yes]\n\nJustification: In Appendix\
    \ B.\n\n- The answer NA means that the paper does not use existing assets.\n-\
    \ The authors should cite the original paper that produced the code package or\
    \ dataset.\n- The authors should state which version of the asset is used and,\
    \ if possible, include a URL.\n- The name of the license (e.g., CC-BY 4.0) should\
    \ be included for each asset.\n- For scraped data from a particular source (e.g.,\
    \ website), the copyright and terms of service of that source should be provided.\n\
    - If assets are released, the license, copyright information, and terms of use\
    \ in the package should be provided. For popular datasets, <paperswithcode.com/datasets>\
    \ has curated licenses for some datasets. Their licensing guide can help determine\
    \ the license of a dataset.\n- For existing datasets that are re-packaged, both\
    \ the original license and the license of the derived asset (if it has changed)\
    \ should be provided.\n- If this information is not available online, the authors\
    \ are encouraged to reach out to the asset's creators.\n- 13. New assets\n\nQuestion:\
    \ Are new assets introduced in the paper well documented and is the documentation\
    \ provided alongside the assets?\n\nAnswer: [Yes]\n\nJustification: In the provided\
    \ links in Abstract.\n\nGuidelines:\n\n- The answer NA means that the paper does\
    \ not release new assets.\n- Researchers should communicate the details of the\
    \ dataset/code/model as part of their submissions via structured templates. This\
    \ includes details about training, license, limitations, etc.\n- The paper should\
    \ discuss whether and how consent was obtained from people whose asset is used.\n\
    - At submission time, remember to anonymize your assets (if applicable). You can\
    \ either create an anonymized URL or include an anonymized zip file.\n\n#### 14.\
    \ Crowdsourcing and research with human subjects\n\nQuestion: For crowdsourcing\
    \ experiments and research with human subjects, does the paper include the full\
    \ text of instructions given to participants and screenshots, if applicable, as\
    \ well as details about compensation (if any)?\n\nAnswer: [Yes]\n\nJustification:\
    \ In Appendix B.\n\nGuidelines:\n\n- The answer NA means that the paper does not\
    \ involve crowdsourcing nor research with human subjects.\n- Including this information\
    \ in the supplemental material is fine, but if the main contribution of the paper\
    \ involves human subjects, then as much detail as possible should be included\
    \ in the main paper.\n- According to the NeurIPS Code of Ethics, workers involved\
    \ in data collection, curation, or other labor should be paid at least the minimum\
    \ wage in the country of the data collector.\n\n### 15. Institutional review board\
    \ (IRB) approvals or equivalent for research with human subjects\n\nQuestion:\
    \ Does the paper describe potential risks incurred by study participants, whether\
    \ such risks were disclosed to the subjects, and whether Institutional Review\
    \ Board (IRB) approvals (or an equivalent approval/review based on the requirements\
    \ of your country or institution) were obtained?\n\nAnswer: [Yes]\n\nJustification:\
    \ In Appendix B.\n\nGuidelines:\n\n- The answer NA means that the paper does not\
    \ involve crowdsourcing nor research with human subjects.\n- Depending on the\
    \ country in which research is conducted, IRB approval (or equivalent) may be\
    \ required for any human subjects research. If you obtained IRB approval, you\
    \ should clearly state this in the paper.\n- We recognize that the procedures\
    \ for this may vary significantly between institutions and locations, and we expect\
    \ authors to adhere to the NeurIPS Code of Ethics and the guidelines for their\
    \ institution.\n- For initial submissions, do not include any information that\
    \ would break anonymity (if applicable), such as the institution conducting the\
    \ review.\n\n# 16. Declaration of LLM usage\n\nQuestion: Does the paper describe\
    \ the usage of LLMs if it is an important, original, or non-standard component\
    \ of the core methods in this research? Note that if the LLM is used only for\
    \ writing, editing, or formatting purposes and does not impact the core methodology,\
    \ scientific rigorousness, or originality of the research, declaration is not\
    \ required.\n\nAnswer: [Yes]\n\nJustification: In Appendix B.\n\n- The answer\
    \ NA means that the core method development in this research does not involve\
    \ LLMs as any important, original, or non-standard components.\n- Please refer\
    \ to our LLM policy ([https://neurips.cc/Conferences/2025/](https://neurips.cc/Conferences/2025/LLM)\
    \ [LLM](https://neurips.cc/Conferences/2025/LLM)) for what should or should not\
    \ be described."
- title: "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System\
    \ from Hypothesis to Verification"
  abstract: 'Artificial Intelligence (AI) is accelerating the transformation of scientific

    research paradigms, not only enhancing research efficiency but also driving

    innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework

    to conduct Autonomous Scientific Research (ASR) across various scientific

    research fields, enabling researchers to tackle complicated problems in these

    fields with unprecedented speed and precision. NovelSeek highlights three key

    advantages: 1) Scalability: NovelSeek has demonstrated its versatility across

    12 scientific research tasks, capable of generating innovative ideas to enhance

    the performance of baseline code. 2) Interactivity: NovelSeek provides an

    interface for human expert feedback and multi-agent interaction in automated

    end-to-end processes, allowing for the seamless integration of domain expert

    knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in

    several scientific fields with significantly less time cost compared to human

    efforts. For instance, in reaction yield prediction, it increased from 27.6% to

    35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from

    0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,

    precision advanced from 78.8% to 81.0% in a mere 30 hours.'
  url: http://arxiv.org/abs/2505.16938v1
  keywords: ''
  document: '# **NOVELSEEK: When Agent Becomes the Scientist – Building Closed-Loop
    System from Hypothesis to Verification**


    **NovelSeek Team, Shanghai Artificial Intelligence Laboratory**


    <https://alpha-innovator.github.io/NovelSeek-project-page> <https://github.com/Alpha-Innovator/NovelSeek>
    <https://huggingface.co/U4R/NovelSeek>


    <span id="page-0-0"></span>![](_page_0_Figure_4.jpeg)


    Figure 1: **NOVELSEEK** can support **12** types of scientific research tasks
    ranging from the AI field to the science field, including reaction yield prediction,
    molecular dynamics, power flow estimation, time series forecasting, transcription
    prediction, enhancer activity prediction, sentiment classification, 2D image classification,
    3D point classification, 2D semantic segmentation, 3D autonomous driving, large
    vision-language model fine-tuning.


    ### **Abstract**


    Artificial Intelligence (AI) is accelerating the transformation of scientific
    research paradigms, not only enhancing research efficiency but also driving innovation.
    We introduce NOVELSEEK, a unified closed-loop multi-agent framework to conduct
    Autonomous Scientific Research (ASR) across various scientific research fields,
    enabling researchers to tackle complicated problems in these fields with unprecedented
    speed and precision. NOV-ELSEEK highlights three key advantages: 1) **Scalability**:
    NOVELSEEK has demonstrated its versatility across **12** scientific research tasks,
    capable of generating innovative ideas to enhance the performance of baseline
    code. 2) **Interactivity**: NOVELSEEK provides an interface for human expert feedback
    and multi-agent interaction in automated end-to-end processes, allowing for the
    seamless integration of domain expert knowledge. 3) **Efficiency**: NOVELSEEK
    has achieved promising performance gains in several scientific fields with significantly
    less time cost compared to human efforts. For instance, in reaction yield prediction,
    it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction,
    accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic
    segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.


    ## **1 Introduction**


    Autonomous Scientific Discovery (ASD) refers to the use of Large Language Models
    (LLMs) [\(Yang et al.,](#page-24-0) [2024;](#page-24-0) [Chen et al.,](#page-22-0)
    [2024;](#page-22-0) [Wang et al.,](#page-23-0) [2024a;](#page-23-0) [Guo et al.,](#page-22-1)
    [2025\)](#page-22-1) and robotics to independently perform scientific research
    without direct human intervention [\(Yuan et al.,](#page-24-1) [2025;](#page-24-1)
    [Yan et al.,](#page-24-2) [2025;](#page-24-2) [Gottweis et al.,](#page-22-2) [2025;](#page-22-2)
    [Yamada et al.,](#page-24-3) [2025;](#page-24-3) [Lu et al.,](#page-23-1) [2024\)](#page-23-1).
    This approach holds transformative potential for accelerating the pace of discovery
    across various scientific domains. By automating tasks such as data analysis,
    hypothesis generation, experiment design, and result interpretation, automated
    systems [\(Yuan et al.,](#page-24-1) [2025;](#page-24-1) [Lu et al.,](#page-23-1)
    [2024\)](#page-23-1) can efficiently process vast amounts of information and uncover
    patterns or insights that may be difficult for human researchers to detect.


    ASR, while promising, faces significant challenges in generating effective and
    novel proposals, as well as achieving closed-loop feedback for the experimental
    validation of these proposals:


    - First, generating proposals that are both effective and novel is a complicated
    task. Autonomous systems must identify research gaps and generate hypotheses that
    are not only innovative but also scientifically valid. This requires balancing
    creativity and rigor, which is difficult for AI models that rely on existing data
    and patterns. Additionally, ensuring the novelty of proposals often demands a
    deep understanding of the broader scientific context, which can be challenging
    for models limited by the quality and scope of their training data.

    - Second, implementing closed-loop feedback for end-to-end experimental validation
    is another major hurdle. Autonomous systems need to design experiments, execute
    them, analyze results, and iteratively refine their hypotheses in a seamless loop.
    This requires integration across multiple domains, such as robotics for experiment
    execution and advanced analytics for result interpretation. Furthermore, real-world
    experiments often come with unexpected variables and noise, making it challenging
    for autonomous systems to adapt and learn effectively. Achieving a truly closedloop
    system demands robust coordination, adaptability, and the ability to handle uncertainty,
    which remain technical and conceptual barriers.


    To further facilitate the advancement of ASR, we propose the NOVELSEEK, an end-to-end
    auto-research pipeline, which covers four main modules: self-evolving idea generation,
    human-interactive feedback, idea-to-methodology construction, and multi-round
    experiment planning and execution. With the help of the Self-Evolving Human-interactive
    Idea


    Generation and Idea-to-Methodology Construction, NOVELSEEK can transform a rough
    proposal into a detailed and easily implementable method, which further increases
    the success rate of code implementation process and enhances the efficiency of
    closed-loop experiments. Besides, by leveraging multi-round experiment planning
    and execution, NOVELSEEK is capable of designing experimental plans and decomposing
    the experimental process according to the NOVELSEEK-proposed modules, thereby
    validating the effectiveness of each NOVELSEEK-generated module through experimentation.


    As shown in Fig. [1,](#page-0-0) NOVELSEEK has been validated across **12** scientific
    research tasks, and we are excited to see that the experimental results demonstrate
    the significant value of NOVELSEEK in the entire process from hypothesis generation
    to experimental validation. For instance, in the Reaction Yield Prediction task,
    the baseline model only achieved a performance of 24.2% ± 4.2, while our model
    improved it to 34.8% ± 1.1 in **just 12 hours**. In contrast, human researchers
    typically require **several months** to achieve a similar level of performance
    improvement. Another example of performance improvement is the Enhancer Activity
    Prediction task. The baseline model, DeepSTARR, achieved a result of 0.52. By
    utilizing NOVELSEEK to search relevant domain literature, automatically generate
    code, and conduct validation, the performance can be improved to 0.79, representing
    a promising enhancement. In addition, NOVELSEEK also supports complex project-level
    modifications and debugging, which consist of multiple code files. These results
    clearly indicate that NOVELSEEK can autonomously generate ideas and design algorithms,
    effectively reducing the dependence on human effort in scientific research. To
    facilitate reproducibility, we have open-sourced both the baselines and the codes
    generated by NOVELSEEK used in all involved scientific research tasks at <https://github.com/Alpha-Innovator/NovelSeek>.


    Furthermore, the contributions of this paper are summarized below:


    - **Unified Multi-agent Framework for Diverse Scientific Research Tasks**: We
    present NOVELSEEK, a unified closed-loop scientific research framework that can
    automate the entire research cycle, including idea generation, idea-to-methodology
    transformation, experiment execution, and result feedback. This unified framework
    can be directly applied to various scientific research scenarios and fields.

    - **Interactive Interfaces for Enhanced Cooperative Research**: NOVELSEEK offers
    interactive interfaces for human-machine collaboration within the idea generation
    module and across the entire system. By selecting collaboration modes, such as
    leveraging AI or human experts, it provides evaluations of idea generation effectiveness
    and facilitates the assessment, reflection, and documentation of experimental
    results.

    - **Comprehensive Experimental Validation and Human Studies**: We conducted extensive
    human studies centered around NOVELSEEK, including inviting domain experts to
    evaluate and score the novelty of ideas generated by NOVELSEEK, and comparing
    the research efficiency between human researchers and NOVELSEEK. These experiments
    and human studies are crucial for gaining insights into the capabilities of multi-agent
    systems in conducting scientific research tasks in open-ended environments. We
    observed many promising phenomena, while also identifying certain technical modules
    that require improvement.


    # **2 NOVELSEEK**


    As a unified closed-loop multi-agent framework for ASR, NOVELSEEK is designed
    to facilitate innovative research across diverse scientific domains, as illustrated
    in Fig. [2.](#page-3-0) It incorporates three primary capabilities: self-evolving
    idea generation with human-interactive feedback (Sec. [2.1\)](#page-3-1), comprehensive
    idea-to-methodology construction (Sec. [2.2\)](#page-6-0), and multiround automated
    experiment execution (Sec. [2.3\)](#page-7-0). Each capability is realized through
    the collaboration of specialized agents, allowing for seamless integration of
    different processes to enhance scientific discoveries.


    <span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)


    #### NovelSeek


    Figure 2: **NOVELSEEK** covers three main capabilities: 1) Self-evolving Idea
    Generation with Human-interactive Feedback, 2) Idea-to-Methodology Construction,
    and 3) Evolutionary Experimental Planning and Execution.


    #### <span id="page-3-1"></span>**2.1 Self-Evolving Idea Generation with Human-interactive
    Feedback**


    The self-evolving idea generation capability is central to NOVELSEEK, enabling
    the framework to autonomously generate and refine innovative research ideas. This
    process involves several specialized agents, each contributing to different stages
    of idea development and refinement.


    **Survey Agent.** The Survey Agent is designed to meet the diverse needs of various
    scientific research tasks by adaptively aligning with user-specified requirements
    and the necessary depth of detail for exploring existing methodologies. This adaptability
    is crucial for effectively generating new ideas across different research contexts,
    and the primary responsibility of the Survey Agent is to search for scientific
    papers, offering two distinct modes to address the varying needs for depth and
    breadth in literature research during the scientific discovery process: 1) the
    literature review mode and 2) the deep research mode.


    In the literature review mode, the Survey Agent deconstructs the research task
    into multiple keyword combinations, enabling a broad search across various academic
    databases. It collects scientific literature from these sources and evaluates
    the relevance of each document by analyzing abstracts in relation to the task
    at hand. Denote the keyword generation process by the function *P* : T → K, where
    T represents the descriptions of research tasks and K is the set of generated
    keyword combinations. The relevance evaluation of each document can be represented
    by the function:


    $$\mathcal{R}: \mathcal{L}\_{\text{abs}} \times \mathcal{T} \to [0, 1], \tag{1}$$


    where L*abs* is the abstract of retrieved literature L, and *R*(*r*, *t*) measures
    the relevance of literature *l* to the task *t* as a floating-point score between
    0 and 1, with higher scores indicating greater relevance.


    In the deep research mode, following the initial literature survey, the Survey
    Agent downloads and thoroughly examines the full texts of relevant scientific
    papers. This deeper analysis allows it to generate new keyword combinations, facilitating
    further rounds of literature exploration. The process of generating new keywords
    can be denoted by the function:


    $$P: \mathcal{L} \to \mathcal{K}'',\tag{2}$$


    where K′ is the expanded set of keyword combinations generated from the detailed
    analysis of full texts.


    By dynamically adjusting its search strategies based on the context of the research
    stage, the Survey Agent ensures a comprehensive and nuanced understanding of the
    research landscape. This capability not only supports the generation of innovative
    ideas but also ensures that the NOVELSEEK framework remains at the cutting edge
    of scientific discovery.


    **Code Review Agent.** The Code Review Agent is crucial for understanding baseline
    codes for different research tasks, serving as a foundation for innovation by
    identifying improvements and developing new methodologies. It provides detailed
    analyses of code structures, dependencies, and functionalities, enabling NOVELSEEK
    to fully comprehend existing code-bases and identify potential enhancements to
    advance research objectives. Moreover, the agent''s ability to document and summarize
    complex code-bases ensures efficient navigation and utilization of existing methods.
    The agent manages two scenarios: 1) reviewing user-provided code or 2) searching
    for relevant code-bases. For user-uploaded code, it conducts a comprehensive review
    of the structure, logic, and functionality. Alternatively, in the absence of user-uploaded
    code, it searches public repositories like GitHub to find relevant code-bases,
    performing thorough analyses at both the repository and file levels to understand
    inter-dependencies and assess logic, efficiency, and correctness. Furthermore,
    the agent employs static code analysis using Python''s ''ast'' module to parse
    and understand code structure without execution, while the LLM generates human-readable
    descriptions and summaries, transforming technical details into structured documentation.
    By using parallel processing with Python''s ''multiprocessing'' module, the agent
    enhances efficiency and scalability for large code-bases. Overall, the Code Review
    Agent offers detailed documentation that deepens the understanding of code repositories
    and supports innovation in scientific research.


    **Idea Innovation Agent.** The Idea Innovation Agent is an integral part of NOVELSEEK,
    designed to enhance the creative and iterative processes of scientific research.
    This agent plays a crucial role by automating the generation and evolution of
    ideas, thereby addressing the limitations of traditional research works [\(Yuan
    et al.,](#page-24-1) [2025;](#page-24-1) [Yamada et al.,](#page-24-3) [2025;](#page-24-3)
    [Lu et al.,](#page-23-1) [2024\)](#page-23-1), which often rely on time-consuming
    manual efforts and are constrained by human cognitive biases. The agent''s dual
    responsibilities, idea generation and idea evolution, are specifically designed
    to address the diverse needs of various scientific disciplines.


    In the context of idea generation, the agent utilizes a general LLM configured
    with a higher temperature setting. This configuration encourages the generation
    of more diverse and creative outputs. This enables the agent to identify patterns
    and insights that might be overlooked in traditional research, generating novel
    hypotheses and strategies based on task definitions, baseline methods, and current
    scientific knowledge. The process can be represented by the function:


    $$\mathcal{G}: (\mathcal{T}, \mathcal{B}, \mathcal{L}) \to \mathcal{T}, \tag{3}$$


    where B denotes analysis of baseline methods and I is the set of generated ideas.
    The LLM facilitates the exploration of a broader spectrum of possibilities, accelerating
    the pace of discovery and innovation by leveraging its comprehensive understanding
    of language and context.


    Idea evolution leverages the capabilities of an LLM to improve existing ideas.
    The process involves analyzing the content of these ideas, incorporating reflections,
    which include evaluations of novelty, feasibility, and scientific validity, and
    integrating insights from related literature. This approach enables the generation
    of refined and innovative ideas by addressing the inherent limitations of initial
    concepts. The process can be represented by the same function:


    $$\mathcal{G}: (\mathcal{T}, \mathcal{C}, \mathcal{L}) \to \mathcal{T}'', \tag{4}$$


    where I is the initial set of ideas, C denotes the critique, and I ′ is the set
    of evolved ideas.


    Overall, the Idea Innovation Agent enhances scientific ideas into viable and creative
    solutions by synthesizing and contextualizing information. It critically examines
    current ideas and employs feedback loops with human experts and other NOVELSEEK
    agents for continuous improvement. This iterative process balances novelty, feasibility,
    and ethical considerations, producing impactful and well-rounded ideas.


    **Assessment Agent.** The Assessment Agent is a vital component of NOVELSEEK,
    designed to ensure the quality and viability of generated ideas through a rigorous
    evaluation process. In the rapidly evolving landscape of scientific research,
    the systematic and objective assessment of ideas is essential. Traditional methods
    often suffer from subjectivity and lack comprehensive coverage of all relevant
    dimensions, which can lead to promising ideas being overlooked[\(Qiu et al.,](#page-23-2)
    [2025;](#page-23-2) [Si et al.,](#page-23-3) [2024\)](#page-23-3). Therefore,
    the Assessment Agent addresses these challenges by providing a structured and
    multidimensional evaluation process, which in turn enhances the reliability and
    effectiveness of idea selection.


    The primary responsibility of the Assessment Agent is to critically evaluate ideas
    using multidimensional scoring. Each idea is analyzed across key dimensions: coherence,
    credibility, verifiability, novelty, and alignment. Coherence checks the logical
    consistency and structure of the idea, while credibility assesses its trustworthiness
    based on existing knowledge. Verifiability examines the idea''s testability through
    empirical methods. Novelty measures originality, and alignment ensures consistency
    with research goals.


    Moreover, for each dimension, the Assessment Agent provides a detailed evaluation
    narrative to explain its reasoning. It assigns scores from 0 to 10, which are
    combined using a weighted summation to produce an overall score for each idea,
    aiding in the ranking process. By utilizing advanced LLMs, the agent can accurately
    process and evaluate complex scientific concepts. This capability allows a comprehensive
    assessment that includes both qualitative and quantitative aspects, ensuring the
    evaluation is thorough and well-rounded.


    Furthermore, the Assessment Agent possesses the ability to ensure diversity among
    topranked ideas. This capability prevents high-scoring ideas from being overly
    similar or derived from the same original concept. By promoting a varied pool
    of ideas, the agent encourages the exploration of diverse pathways in the research
    process. This is crucial for maintaining a balance between innovation and practicality,
    ensuring that the most promising ideas are both high-quality and distinct from
    each other.


    In summary, by employing LLMs for multidimensional scoring and leveraging its
    ability to promote diversity among ideas, the Assessment Agent ensures that only
    the most viable and innovative concepts are selected for further development.
    This process not only enhances the efficiency of the research cycle but also fosters
    a more dynamic and diverse research environment.


    **Human-interactive Feedback.** In the context of multi-agent systems, human-interactive
    feedback is a crucial component for effectively managing and solving complex tasks.
    This integration of human insights enables agents to navigate dynamic environments
    more effectively, aligning their outputs with complex user requirements and ensuring
    practical applicability.


    The human-interactive feedback mechanism of NOVELSEEK is categorized into two
    primary types: 1) feedback directly provided by humans and 2) feedback automatically
    generated by agent. Human-provided feedback can address one or multiple ideas,
    offering insights and critiques that lead to further refinement and adjustment
    of these ideas based on the feedback received. This iterative process facilitates
    the continuous improvement of ideas, ensuring they are honed to meet specific
    objectives and challenges.


    For example, in a scenario involving medical image segmentation, an LLM multi-agent
    system might initially propose a broad idea focused on developing more advanced
    segmentation algorithms. However, human feedback can refine this idea by directing
    attention specifically to the medical domain. Human experts can provide insights
    that encourage the


    <span id="page-6-1"></span>![](_page_6_Figure_1.jpeg)


    Figure 3: NOVELSEEK Self-evolutionary path of ideas for reaction yield prediction
    task.


    development of adaptive solutions tailored to the unique challenges of medical
    imaging, such as handling diverse tissue types and ensuring high accuracy in identifying
    critical structures. This targeted feedback not only sharpens the focus of the
    idea but also ensures it aligns with the specific needs and priorities of medical
    research, enhancing its practical applicability and impact.


    **Orchestration Agent.** The Orchestration Agent coordinates all other agents
    within the system, facilitating collaboration by synchronizing tasks and managing
    data flow. This ensures the process remains efficient, coherent, and aligned with
    research objectives, allowing the framework to function as an effective research
    tool.


    Central to the Orchestration Agent''s role is designing and managing workflows
    among agents like the Survey Agent, Code Review Agent, Idea Innovation Agent,
    and Assessment Agent. It also oversees the timing of human feedback, especially
    for high-scoring ideas. This requires understanding each agent''s capabilities
    and their interactions to optimize task execution and completion. For example,
    the Survey Agent conducts adaptive literature exploration, providing insights
    that the Idea Innovation Agent uses to generate novel hypotheses. The Orchestration
    Agent ensures these findings are communicated effectively. Similarly, it synchronizes
    the Code Review Agent''s analyses to enhance idea evaluation and development.
    Furthermore, the Orchestration Agent manages the Assessment Agent''s evaluation
    process, ensuring timely and relevant outputs. This helps guide the development
    of diverse top ideas. Additionally, it determines optimal points for human feedback,
    integrating expert insights after identifying high-scoring ideas to refine and
    adapt them, aligning outputs with user requirements.


    In summary, as illustrated in Fig. [3,](#page-6-1) by managing multi-agent collaboration
    and integrating human feedback, the Orchestration Agent enables NOVELSEEK to operate
    as a cohesive and innovative research tool, driving scientific discovery forward.


    #### <span id="page-6-0"></span>**2.2 Comprehensive Idea-to-Methodology Construction**


    The idea-to-methodology construction process systematically bridges the gap between
    concise research ideas and concrete, implementable methodologies, ensuring that
    the AI- generated ideas could be realized and their validity verified. This process
    is orchestrated by the Methodology Development Agent, which collaborates closely
    with other agents and integrates both automated processes and human-interactive
    feedback loops to ensure that methodological development is rigorous, traceable,
    and practically relevant. Specifically, to develop a comprehensive method corresponding
    to the concise research idea, the Method Development Agent possesses two core
    capabilities: 1) Methodology Initialization: which involves constructing the basic
    structure and content of a method by integrating the idea with baseline codes
    and the methodology content of relevant literature; 2) Methodology Refinement:
    which iteratively enhances the basic method structure for the purpose of rigor
    and completeness, ensuring a more detailed and robust methodology.


    #### *2.2.1 Methodology Initialization*


    To convert concise research ideas into detailed methodological frameworks, the
    Method Development Agent uses its Methodology Initialization capability. The process
    begins by extracting core objectives and hypotheses from research ideas, identifying
    key variables, and understanding their interrelationships to construct a coherent
    framework. The agent uses multiple resources: task descriptions T provide context
    and constraints; baseline implementations B offer adaptable methods; and relevant
    literature L integrates existing knowledge and ensures that the framework aligns
    with current research.


    By formalizing mechanisms that require empirical investigation, the agent details
    processes and conditions for conducting research and specifies methods for data
    collection and analysis. The outcome is a methodological framework that is both
    theoretically sound and practically executable. The transformation function is
    represented as:


    $$T: \mathcal{T} \times \mathcal{T} \times \mathcal{B} \times \mathcal{L} \to
    \mathcal{M},\tag{5}$$


    where I denotes research ideas, T includes task descriptions, B represents baseline
    methods, L is the literature corpus, and M is the resulting methodological framework.
    Overall, through Methodology Initialization, the Method Development Agent effectively
    turns initial ideas into detailed, actionable methods, ready for further refinement.


    #### *2.2.2 Methodology Refinement*


    After the initialization, the Methodology Development Agent leverages its refinement
    capability to critically evaluate and iteratively improve the methodological framework.
    The agent conducts a comprehensive analysis of the initial methodology M, incorporating
    structured critiques C, which include both automated assessments and expert human
    feedback. Additionally, it synthesizes insights from the latest scientific literature
    L. The refinement process is formally defined as:


    $$

    \mathcal{R}: \mathcal{M} \times \mathcal{C} \times \mathcal{L} \to \mathcal{M}'',\tag{6}

    $$


    where M represents the initial methodology, C denotes the critique space, potentially
    including human feedback and automated assessments, L is the literature corpus,
    and M′ is the refined methodological framework.


    During both initialization and refinement, the Methodology Development Agent collaborates
    closely with other agents, such as the Assessment Agent for multidimensional evaluation
    and the Orchestration Agent for workflow coordination. This collaboration ensures
    that each methodological step benefits from comprehensive feedback and current
    domain knowledge. The integrated, multi-agent approach guarantees that the transformation
    from idea to methodology is systematic and adaptable, supporting the continuous
    evolution and optimization of scientific research within the NOVELSEEK framework.


    ### <span id="page-7-0"></span>**2.3 Evolutionary Experimental Planning and Execution**


    ### *2.3.1 Exception-Guided Debugging Framework*


    Converting theoretical concepts into functional code is challenging. To this end,
    we developed an exception-guided debugging framework that systematically converts
    abstract


    methodological text descriptions into executable implementation codes. This framework
    operates by systematically capturing runtime exceptions during execution attempts,
    analyzing error contexts, and formulating targeted fixes through reasoning of
    the large language model.


    Our coder module employs a dual-strategy approach according to the complexity
    of given baseline code. For single-file or limited-scope implementation tasks,
    we use the Aider coding assistant [\(Gauthier & Contributors,](#page-22-3) [2023\)](#page-22-3),
    which facilitates localized code modifications with minimal overhead. For complex
    repository-level codes requiring comprehensive structural understanding across
    different functions, we deploy OpenHands framework [\(Wang et al.,](#page-24-4)
    [2024b\)](#page-24-4), which enables thorough codebase analysis and coordinated
    multi-file modifications while maintaining the integrity of the overall code architecture.


    Once the initial code implementation is completed, the framework transitions to
    a systematic debugging phase to ensure functionality and robustness. The debugging
    process follows a systematic cycle: (1) execution attempt, (2) exception capture
    and traceback analysis, (3) contextual code structure understanding, (4) debugging
    strategy formulation, and (5) targeted implementation. This cycle continues iteratively
    until successful execution or reaching a predefined iteration threshold.


    #### <span id="page-8-1"></span>*2.3.2 Experimental Planning and Adaptive Evolution*


    After establishing basic functionality through debugging, we transition to implementation
    planning focused on identifying critical structures and integration points. Our
    planning process first determines which core modules require modification, then
    develops a step-bystep implementation strategy with clear priorities and dependencies.


    Implementation planning operates at multiple abstraction levels: architectural
    modifications for methodological alignment, algorithmic transformations for core
    functionality, and optimization adjustments for performance characteristics. This
    approach aims to provide structure when implementing methodological improvements
    across interconnected components in AI systems, which helps guide development
    efforts.


    Rather than employing a single-pass implementation strategy, we designed an adaptive
    evolution approach for our implementation process. This approach involves structured
    iterations where each implementation attempt is followed by performance assessment
    and potential refinement. We maintain records of implementation decisions across
    iterations, which helps track changes and their corresponding effects. This directed
    adaptation process enables the gradual refinement of complex implementations based
    on empirical results rather than theoretical assumptions alone.


    ## **3 Experiments**


    In this section, we evaluate the effectiveness of NOVELSEEK in conducting autonomous
    research and accelerating scientific discovery. We begin by providing a brief
    overview of the selected multi-domain tasks and detailing the experimental implementation
    in Sec. [3.1.](#page-8-0) Subsequently, we present the quantitative results across
    various tasks in Sec. [3.2](#page-11-0) and conduct an analysis of the different
    modules within NOVELSEEK in Sec. [3.3.](#page-14-0)


    ## <span id="page-8-0"></span>**3.1 Experimental Setup**


    ### *3.1.1 Task Description*


    We select 12 distinct tasks to demonstrate NOVELSEEK''s capability in conduct
    Autonomous Scientific Research (ASR). These tasks span multiple modalities, including
    science (*e.g.*, reaction yield prediction, molecular dynamics), time series (*e.g.*,
    time series forecasting), natural language (*e.g.*, sentiment classification),
    image (*e.g.*, semantic segmentation), and point cloud (*e.g.*, 3D object detection),
    which cover both discriminative and generative tasks. We believe that experiments
    ranging from fundamental tasks to complex multi-modal


    tasks can comprehensively illustrate the effectiveness of NOVELSEEK. Below, we
    detail the datasets, the base code repositories, and the experimental settings
    for each task.


    - **Reaction Yield Prediction (AutoRYP).** We conduct experiments on the widely-used
    Suzuki-Miyaura reaction dataset [\(Perera et al.,](#page-23-4) [2018\)](#page-23-4),
    which contains 5,760 reaction data. Each data point includes structured chemical
    reaction information, such as reactants, products, reaction types, reaction conditions
    (solvent, catalyst, ligand, and base), functional group, and yield values. We
    use the LoRA-finetuned LLaMA3-8B as our baseline, an embedding model that converts
    chemical reaction texts into high-dimensional vector representations, which are
    subsequently fed into a fully connected prediction network predictor to perform
    chemical yield prediction.

    - **Molecular Dynamics (AutoMD).** We conduct experiments on the widely-used MD17
    dataset [\(Chmiela et al.,](#page-22-4) [2017\)](#page-22-4), which contains energy
    and force calculation results for seven small organic molecules: aspirin, ethanol,
    malonaldehyde, naphthalene, salicylic acid, toluene, and uracil. We use VisNet
    [\(Wang et al.,](#page-24-5) [2024d\)](#page-24-5) as our baseline, an equivariant
    geometry-enhanced graph neural network that achieves excellent chemical property
    prediction.

    - **Power Flow Estimation (AutoPower).** We conduct experiments on the IEEE 39-Bus
    dataset [\(Zimmerman et al.,](#page-25-0) [2010\)](#page-25-0), which is a medium-scale
    benchmark based on the New England power system, comprising 39 buses, 10 synchronous
    generators, 19 load buses and 46 transmission lines, and providing AC power flow
    snapshots under a variety of load conditions. We use SenseFlow [\(Zhao et al.,](#page-25-1)
    [2024\)](#page-25-1) as our baseline, a novel physics-informed, self-ensembling
    power flow estimation model that has demonstrated state-of-the-art accuracy on
    standard IEEE test systems consistently outperforming both traditional state-estimation
    techniques and recent data-driven approaches in voltage and power-flow recovery
    tasks.

    - **Time Series Forecasting (AutoTSF).** We conduct experiments on the ETTh1 dataset,
    which is a 1-hour-level subset of the Electricity Transformer Temperature (ETT)
    benchmark [\(Zhou et al.,](#page-25-2) [2021\)](#page-25-2). This dataset comprises
    two years of hourly multivariate time series, including the target oil temperature
    and six power-load covariates, collected from transformer stations in two Chinese
    counties. We use DLinear [\(Zeng](#page-24-6) [et al.,](#page-24-6) [2023\)](#page-24-6)
    as our baseline, an MLP-based forecasting model that decomposes each series into
    trend and seasonality and employs simple linear layers, outperforming Transformer-based
    methods on multiple time series benchmarks. We report the average results of 96,
    192, 336, and 720 prediction length.

    - **Transcription Prediction for Perturbation Response (AutoTPPR).** We conduct
    experiments on the Perturb-seq dataset [\(Norman et al.,](#page-23-5) [2019\)](#page-23-5),
    which contains singlecell gene expression data measuring the transcriptional responses
    of cells to various perturbations. We use GEARS (Generative Energy-based Autoencoder
    for scRNAseq) [\(Roohani et al.,](#page-23-6) [2024\)](#page-23-6) as our baseline,
    a framework based on Graph Neural Networks (GNNs) and Multi-Layer Perceptrons
    (MLPs), designed to learn joint representations of single-cell multi-omics data.

    - **Enhancer Activity Prediction (AutoEAP).** We conduct experiments on the UMI-STARR-seq
    dataset [\(Arnold et al.,](#page-22-5) [2013\)](#page-22-5), which contains genome-wide,
    highresolution quantitative activity maps of developmental and housekeeping enhancers
    in Drosophila S2 cells. We use DeepSTARR [\(de Almeida et al.,](#page-22-6) [2022\)](#page-22-6)
    as our baseline, a deep learning model that excels at quantitatively predicting
    enhancer activity from DNA sequences.

    - **Sentiment Analysis (AutoSenCls).** We conduct experiments on the Stanford
    Sentiment Treebank (SST-2) dataset [\(Socher et al.,](#page-23-7) [2013\)](#page-23-7),
    a binary sentiment classification dataset consisting of movie reviews with approximately
    67,000 training samples. We use BERT-base [\(Devlin et al.,](#page-22-7) [2019\)](#page-22-7)
    as our baseline, a Transformer-based pretrained language model that has shown
    excellent performance on various NLP tasks.

    - **2D Image Classification (Auto2DCls).** We conduct experiments on the widelyused
    CIFAR-100 dataset [\(Krizhevsky et al.,](#page-22-8) [2009\)](#page-22-8), which
    contains 60,000 32×32 color images in 100 classes, with 500 training images and
    100 testing images per


    class. We use Wide Residual Networks (WRN) [\(Zagoruyko,](#page-24-7) [2016\)](#page-24-7)
    as our baseline, which improves performance by increasing the width rather than
    the depth of convolutional networks.


    - **3D Point Cloud Classification (Auto3DCls).** We conduct experiments on the
    ModelNet40 dataset [\(Wu et al.,](#page-24-8) [2015\)](#page-24-8), which contains
    12,311 CAD models across 40 common object categories and is widely used for 3D
    shape classification tasks. We use PointNet [\(Qi et al.,](#page-23-8) [2017\)](#page-23-8)
    as our baseline, a pioneering deep learning architecture that directly processes
    point cloud data.

    - **2D Semantic Segmentation (Auto2DSeg).** We conduct experiments on the widelyused
    Pascal VOC 2012 dataset [\(Everingham et al.,](#page-22-9) [2012\)](#page-22-9),
    which includes 20 object classes and a background class for semantic segmentation
    tasks. The dataset contains 1,464 images for training and 1,449 for validation.
    We use DeepLabV3Plus [\(Chen et al.,](#page-22-10) [2018\)](#page-22-10) as our
    baseline method, which enhances segmentation performance by employing atrous convolution
    and a more refined encoderdecoder structure to capture multiscale contextual information
    effectively.

    - **3D Point Cloud Autonomous Driving (AutoPCDet).** We conduct experiments on
    the widely-used dataset ONCE [\(Mao et al.,](#page-23-9) [2021\)](#page-23-9)
    and use CenterPoint [\(Yin et al.,](#page-24-9) [2021\)](#page-24-9) as our baseline.
    Our code is based on OpenPCDet [\(Team,](#page-23-10) [2020\)](#page-23-10) and
    we filter out all code irrelevant to the baseline model to avoid knowledge leakage.

    - **Large Vision-Language Model Fine-tuning (AutoVLM).**We conduct experiments
    on filtered geometry subset of the URSA dataset [\(Luo et al.,](#page-23-11) [2025\)](#page-23-11),
    comprising manually curated multimodal QA pairs and CoT process. Natural images
    were excluded, and data were downsampled to control experimental budgets, enabling
    training completion within 20 hours on 8 A800 GPUs.We use LLaVA-Onevision [\(Li](#page-22-11)
    [et al.,](#page-22-11) [2024a\)](#page-22-11) as our baseline, a robust multimodal
    alignment framework using a simple MLP to align visual encoders with LLMs, forming
    an effective LMM with strong scalability on vision-language tasks. We take SigLIP
    [\(Zhai et al.,](#page-24-10) [2023\)](#page-24-10) and Qwen2.5-Math-7B-Instruct[\(Yang
    et al.\)](#page-24-11) as the visual and language modules, respectively.


    #### *3.1.2 Evaluation Metric*


    Since our NOVELSEEK has been validated across a wide range of scientific research
    fields, the evaluation metrics used for tasks in each field are not consistent.
    Therefore, in this part, we provide a detailed introduction to the evaluation
    metrics used for each scientific research task.


    - **AutoRYP.** For Reaction Yield Prediction, we evaluate model performance using
    the coefficient of determination (R²), which quantifies the proportion of variance
    in the actual reaction yields that is predictable from the model''s predictions.

    - **AutoMD.** Our method is evaluated on the MD17 dataset, a molecular chemical
    property prediction task. The performance is measured using Force-MAE, representing
    the mean absolute error between the true and predicted forces of molecules.

    - **AutoPower.** For Power Flow Estimation, we use Root Mean Square Error (RMSE)
    on PQ node to evaluate the estimation performance on IEEE 39-Bus datasets, representing
    the root mean square error between the true and predicted voltage magnitudes and
    phase angles.

    - **AutoTSF.** For Time Series Forecasting, we use Mean Absolute Error (MAE) to
    evaluate the prediction performance on ETTh1 dataset. The performance is calculated
    by taking the average of the four prediction steps of {96, 192, 336, 720}.

    - **AutoTPPR.** For Transcription Prediction for Perturbation Response, we employ
    the Top 20 DE MSE as the evaluation metric, calculating the mean squared error
    between the predicted and actual expression levels of the top 20 most differentially
    expressed genes under each perturbation condition.

    - **AutoEAP.** For Enhancer Activity Prediction, we use Housekeeper Pearson Correlation
    Coefficient (HK-PCC) as the metric, which quantifies the correlation between the
    true enhancer activities and the predicted values.

    - **AutoSenCls.** We evaluate our method on the SST-2 dataset, which is a binary
    sentiment classification task. The performance is measured using accuracy (Acc),
    which represents the percentage of correctly classified samples.

    - **Auto2DCls.** For 2D image classification, we conduct experiments on CIFAR-100
    dataset, which contains 100 classes. The performance is measured using classification
    accuracy (Acc), representing the percentage of correctly classified images.

    - **Auto3DCls.** For the task of 3D point cloud classification, we use the widely
    adopted ModelNet40 benchmark, which comprises 40 distinct object categories. We
    report the Overall Accuracy (OA) as our primary evaluation metric, which calculates
    the proportion of correctly classified instances in the entire test set.

    - **Auto2DSeg.** For 2D semantic segmentation, we conduct experiments on the Pascal
    VOC 2012 dataset, which includes 20 object classes and a background class. The
    performance is measured using the mean Intersection over Union (MIoU), which quantifies
    the average overlap between the predicted segmentation and the ground truth across
    all classes, providing a comprehensive assessment of the model''s segmentation
    accuracy.

    - **AutoPCD.** Following ONCE official evaluation metric, we merge the car, bus
    and truck class into a super-class (*i.e.*, vehicle). AP3*<sup>D</sup>* is used
    to evaluate the performance of the ONCE dataset, we report Mean average precision
    (mAP) which is the average of the scores of the three categories.

    - **AutoVLM.** We evaluated our model on the geometry subset of MathVista [\(Lu
    et al.,](#page-23-12) [2023\)](#page-23-12), a widely adopted multimodal mathematical
    benchmark. Model''s answers to questions were extracted using GPT-4o and compared
    against the ground truth to calculate accuracy.


    ### <span id="page-11-1"></span>*3.1.3 Implementation Details*


    In the self-evolving idea generation process, the survey agent, code review agent,
    generation agent, self-evolving agent, and orchestration agent are based on GPT-4o
    [\(Hurst et al.,](#page-22-12) [2024\)](#page-22-12). The survey agent searches
    and reviews 50 papers to provide domain knowledge for the subsequent idea generation
    agent, and then the idea generation agent generates 15 ideas. The self-evolving
    agent evolves each idea into 3 ideas and then selects the top 5 ideas for the
    next evolving process until the maximum number of evolutions (*i.e.*, 4) is reached.
    In the idea-to-methodology process, each idea is initialized and refined once
    by the method development agent. In the evolutionary experimental planning and
    execution process. We use Claude-3.7-Sonnet to generate codes and debug. We set
    the max debug attempt to 4. The max run number is set to 5 for Aider [\(Gauthier
    & Contributors,](#page-22-3) [2023\)](#page-22-3) and 3 for OpenHands [\(Wang
    et al.,](#page-24-12) [2024c\)](#page-24-12).


    ### <span id="page-11-0"></span>**3.2 Experimental Results**


    To comprehensively evaluate the effectiveness of NOVELSEEK in accelerating scientific
    discovery, we first provide quantitative experimental results as shown in Tab.
    [1,](#page-12-0) Tab. [2,](#page-12-1) Tab. [3,](#page-12-2) and Tab. [4.](#page-13-0)
    Extensive results demonstrate that NOVELSEEK excels in the following aspects:


    • **Outperforming existing auto-research systems on multiple tasks.** We first
    compare NOVELSEEK with existing auto-research system (*i.e.*, DOLPHIN [\(Yuan
    et al.,](#page-24-1) [2025\)](#page-24-1)) on single-file tasks. Tab. [1](#page-12-0)
    and Tab. [2](#page-12-1) show the max performance and average performance (*i.e.*,
    the average performance across experiments with performance gains) achieved by
    NOVELSEEK and DOLPHIN. It can be observed that NOVELSEEK consistently improves
    the performance compared to the baseline and outperforms DOLPHIN across all tasks
    including both generative and discriminative tasks. This suggests that NOVELSEEK
    can generate better ideas on each specific domain benefiting from the self-evolving
    idea generation process and automatically implement them. For example, in AutoRYP,
    methods proposed by NOVELSEEK can largely


    |                     | Tasks                 |            |                 |         |          |         |  |  |  |

    |---------------------|-----------------------|------------|-----------------|---------|----------|---------|--|--|--|

    | Method              | AutoRYP               | AutoMD     | AutoPower       |
    AutoTSF | AutoTPPR | AutoEAP |  |  |  |

    |                     | 2<br>R                | Forces-MAE | RMSE            |
    MAE     | MSE      | HK-PCC  |  |  |  |

    |                     |                       |            | Max Performance |         |          |         |  |  |  |

    | Baseline            | 27.6                  | 0.158      | 0.00473         |
    0.4382  | 0.197    | 0.52    |  |  |  |

    | Dolphin             | 31.8 (+4.2)           | 0.152      | 0.00455         |
    0.4627  | 0.173    | 0.76    |  |  |  |

    |                     | NOVELSEEK 35.4 (+7.8) | 0.148      | 0.00426         |
    0.4331  | 0.146    | 0.79    |  |  |  |

    | Average Performance |                       |            |                 |         |          |         |  |  |  |

    | Baseline            | 27.6                  | 0.158      | 0.00473         |
    0.4382  | 0.197    | 0.52    |  |  |  |

    | Dolphin             | 31.3 (+3.7)           | 0.155      | 0.00459         |
    -       | 0.179    | 0.73    |  |  |  |

    |                     | NOVELSEEK 33.5 (+5.9) | 0.152      | 0.00447         |
    0.4346  | 0.170    | 0.77    |  |  |  |


    <span id="page-12-0"></span>Table 1: Performance comparison across six types of
    scientific research tasks. We conduct experiments using 10 NOVESEEK generated
    ideas for each task.


    <span id="page-12-1"></span>Table 2: Performance comparison for six types of scientific
    research tasks. We conduct experiments using 10 NOVESEEK generated ideas for each
    task, where baseline codes for Auto2DSeg, AutoPCDet, and AutoVLM are project-level,
    consisting of multiple code files with complex call relation between functions.
    Therefore, the coder in Dolphin [\(Yuan et al.,](#page-24-1) [2025\)](#page-24-1)
    does not support modifying this type of baseline codes.


    |                     |             |             |             | Tasks       |             |             |  |  |  |  |

    |---------------------|-------------|-------------|-------------|-------------|-------------|-------------|--|--|--|--|

    | Method              | AutoSenCls  | Auto2DCls   | Auto3DCls   | Auto2DSeg   |
    AutoPCDet   | AutoVLM     |  |  |  |  |

    |                     | Acc         | Top-1 Acc   | OA          | mIoU        |
    mAP         | QA          |  |  |  |  |

    | Max Performance     |             |             |             |             |             |             |  |  |  |  |

    | Baseline            | 91.0        | 81.2        | 91.0        | 78.8        |
    65.0        | 67.1        |  |  |  |  |

    | DOLPHIN             | 92.5 (+1.5) | 82.0 (+0.8) | 93.9 (+2.9) | -           |
    -           | -           |  |  |  |  |

    | NOVELSEEK           | 93.5 (+2.5) | 83.3 (+2.1) | 95.5 (+4.5) | 81.0 (+2.2)
    | 65.9 (+0.9) | 67.6 (+0.5) |  |  |  |  |

    | Average Performance |             |             |             |             |             |             |  |  |  |  |

    | Baseline            | 91.0        | 81.2        | 91.0        | 78.8        |
    65.0        | 67.1        |  |  |  |  |

    | DOLPHIN             | 91.8 (+0.8) | 81.8 (+0.6) | 92.0 (+1.0) | -           |
    -           | -           |  |  |  |  |

    | NOVELSEEK           | 92.5 (+1.5) | 82.2 (+1.0) | 93.4 (+2.4) | 80.1 (+1.3)
    | 65.7 (+0.7) | 67.6 (+0.5) |  |  |  |  |


    outperform those proposed by DOLPHIN (*i.e.*, +3.6 on max performance). We highlight
    that NOVELSEEK can achieve SoTA performance on some tasks such as 3D point cloud
    classification (*i.e.*, 95.5% overall accuracy without pre-training achieved by
    NOVELSEEK compared to 95.3% overall accuracy with pre-training achieved by human
    experts).


    <span id="page-12-2"></span>Table 3: Experiments statistics across different tasks.
    Each cell shows the number of ideas that improved performance, the number of ideas
    that successfully ran, and the total number of ideas tested (format: improved
    / successful / tested). For all the tasks, we conduct experiments with 10 ideas.


    |           | Research Task |        |           |         |          |         |  |

    |-----------|---------------|--------|-----------|---------|----------|---------|--|

    | Method    | AutoRYP       | AutoMD | AutoPower | AutoTSF | AutoTPPR | AutoEAP
    |  |

    | Dolphin   | 2/3/10        | 2/4/10 | 2/4/10    | 0/3/10  | 2/3/10   | 2/4/10  |  |

    | NOVELSEEK | 4/6/10        | 4/8/10 | 5/6/10    | 3/7/10  | 5/5/10   | 8/8/10  |  |


    <span id="page-13-0"></span>Table 4: Experiments statistics across different tasks.
    Each cell shows the number of ideas that improved performance, the number of ideas
    that successfully ran, and the total number of ideas tested (format: improved
    / successful / tested). For all the tasks, we conduct experiments with 10 ideas.


    |           | Research Task |           |            |           |           |         |  |

    |-----------|---------------|-----------|------------|-----------|-----------|---------|--|

    | Method    | Auto2DCls     | Auto3DCls | AutoSenCls | Auto2DSeg | AutoPCDet |
    AutoVLM |  |

    | Dolphin   | 2/4/10        | 2/5/10    | 4/7/10     | -         | -         |
    -       |  |

    | NOVELSEEK | 5/7/10        | 3/6/10    | 9/9/10     | 6/9/10    | 2/5/10    |
    1/5/10  |  |


    <span id="page-13-1"></span>Table 5: Computational and financial cost analysis
    for all tasks. Training time is measured using A100 GPU hours, while idea generation
    and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7
    models respectively.


    | Cost Metric                               | AutoRYP | AutoMD | AutoPower | AutoTSF
    | AutoTPPR | AutoEAP |

    |-------------------------------------------|---------|--------|-----------|---------|----------|---------|

    | Training time (A100 hours)                | 6.0     | 10.0   | 5.0       | 0.1     |
    1.0      | 1.0     |

    | Idea-Gen cost (gpt-4o) (\$)               | 0.6     | 0.6    | 0.6       | 0.6     |
    0.6      | 0.6     |

    | Coder-Debug cost (claude-sonnet-3.7) (\$) | 0.7     | 0.5    | 1.0       | 0.4     |
    0.9      | 0.6     |


    Besides, Tab. [3](#page-12-2) and Tab. [4](#page-13-0) report the percentage of
    experiments with performance gains and executable experiments out of the total
    number of experiments. First, results show that even on complex tasks such as
    AutoPCDet (*i.e.*, 50%) and Auto2DSeg (*i.e.*, 90%), NOVELSEEK can still ensure
    a reasonable execution success rate which is due to the carefully designed idea-to-methodology
    process, enabling the coder to auto-implement based on detailed methodologies.
    Second, NOVELSEEK demonstrates a higher performance improvement rate compared
    to DOLPHIN. This improvement is mainly attributed to the idea-to-methodology feature
    of NOVELSEEK, which enables the concretization of high-level ideas. Additionally,
    through the process of multi-round experimental planning and execution, the submodules
    of the AI-generated methodology are progressively integrated into the baseline
    code.


    - **Covering a wide range of tasks including the scientific research tasks and
    AI tasks.** Further, NOVELSEEK exhibits strong generalization capability across
    a wide range of tasks, enabling it to handle tasks from the AI domain (*e.g.*,
    Auto2DSeg) to the scientific domain (*e.g.*, AutoMD). As shown in Tab. [1](#page-12-0)
    and Tab. [2,](#page-12-1) NOVELSEEK can support 12 different tasks ranging from
    simple classification tasks to complex multimodal and cross-disciplinary tasks.
    This is because the survey agent in NOV-ELSEEK can auto-search task-related literature
    on academic websites such as arXiv and review the literature to understand each
    task. Besides, NOVELSEEK is highly extensible, as it can support new tasks with
    just a task description and reference codes. This capability not only assists
    AI researchers in automatically updating algorithms, but also empowers researchers
    in scientific domains to utilize AI tools at a lower cost, thereby accelerating
    the pace of scientific discovery.

    - **Support repo-level experiments.** Most of existing auto-research systems such
    as DOLPHIN [\(Yuan et al.,](#page-24-1) [2025\)](#page-24-1) only support single-file
    experiments. On more complex tasks, researchers are required to manually consolidate
    complex task codes into a single file, which is highly time-consuming and limits
    their ability to conduct experiments on complex tasks. In contrast, NOVELSEEK
    can support repo-level tasks such as AutoPCDet, AutoVLM, AutoTPPR, and so on,
    and achieve better performance on these repo-level tasks compared to their baselines.
    For example, on Auto2DSeg, NOVELSEEK pipeline can improve the DeepLabV3Plus baseline
    [\(Chen](#page-22-10) [et al.,](#page-22-10) [2018\)](#page-22-10) from the original
    78.80% to 81.0%. This is attributed to the detailed methodology, code comprehension
    achieved by the code review agent, and the auto-exploration ability of the coder
    agent.


    **Runtime Statistics.** We further provide the runtime statistics of NOVELSEEK
    on all 12 tasks including the training costs (*i.e.*, GPU hours) and monetary
    costs in the idea generation stage (including self-evolving idea generation and
    idea-to-methodology) and code execution and


    <span id="page-14-1"></span>Table 6: Computational and financial cost analysis
    for all tasks. Training time is measured using A100 GPU hours, while idea generation
    and code debugging costs are measured in USD using gpt-4o and claude-sonnet-3.7
    models respectively.


    | Cost Metric                               | Auto2DCls | Auto3DCls | AutoSenCls
    | Auto2DSeg | AutoPCDet | AutoVLM |

    |-------------------------------------------|-----------|-----------|------------|-----------|-----------|---------|

    | Training time (A100 hours)                | 2.0       | 0.8       | 0.3        |
    30.0      | 9.0       | 192.0   |

    | Idea-Gen cost (gpt-4o) (\$)               | 0.6       | 0.6       | 0.6        |
    0.6       | 0.6       | 0.6     |

    | Coder-Debug cost (claude-sonnet-3.7) (\$) | 0.7       | 0.6       | 0.7        |
    1.1       | 1.2       | 1.0     |


    <span id="page-14-2"></span>Table 7: To compare the performance of the baseline
    and NOVELSEEK-generated code, we adopted a few-shot training setup for the yield
    prediction task. Due to the large variance in experimental results under this
    setting, we report the outcomes of 5 independent repeated experiments.


    | Epoch=300                   | Repeat=1 | Repeat=2 | Repeat=3 | Repeat=4 | Repeat=5
    | AVG/VAR  |

    |-----------------------------|----------|----------|----------|----------|----------|----------|

    | Baseline (train-set=60)     | 20.0     | 26.2     | 27.6     | 26.6     | 20.1     |
    24.2±4.2 |

    | GAT (ours, train-set=60)    | 34.7     | 34.8     | 33.9     | 32.7     | 34.2     |
    34.1±1.4 |

    | ADAGT (ours, train-set=60)  | 35.4     | 35.2     | 34.5     | 35.2     | 33.7     |
    34.8±1.1 |

    | Baseline (train-set=100)    | 38.8     | 30.6     | 34.8     | 39.0     | 34.5     |
    35.5±4.9 |

    | GAT (ours, train-set=100)   | 36.9     | 39.1     | 34.4     | 41.4     | 35.0     |
    37.4±4.0 |

    | ADAGT (ours, train-set=100) | 38.5     | 38.0     | 38.6     | 37.9     | 40.4     |
    38.7±1.7 |


    debug stage. As shown in Tab. [5](#page-13-1) and Tab. [6.](#page-14-1) As mentioned
    in Sec. [3.1.3,](#page-11-1) we select top 5 ideas in each idea generation process
    and then generate detailed methodology for the selected ideas. Therefore, we report
    the average cost of 5 ideas as the idea generation cost. It can be seen that the
    idea generation cost of each idea is about \$0.6 using GPT-4o which is cost-efficient.
    The coder-debug cost denote the cost of each run, for example, if running for
    5 times for a single idea as mentioned in Sec. [2.3,](#page-7-0) we calculate
    the average cost of 5 runs. It can be seen from the table that the coder-debug
    cost varies between the file-level codes and repo-level codes and repo-level codes
    generally need more cost for high complexity of codes. For example, for single-file
    code such as Auto2DCls, the cost is below \$1 for each run and for more complex
    AutoPCDet, the cost is about \$1.2 using claude-sonnet-3.7. Generally, NOVELSEEK
    is a cost-efficient auto-research framework that can generate ideas and execute
    codes at a reasonable cost.


    #### <span id="page-14-0"></span>**3.3 Insightful Analyses**


    **Analysis on Survey Agent.** As mentioned in Sec. [2.1,](#page-3-1) survey agent
    mainly have two modes (*i.e.*, the literature review mode and the deep research
    mode). As shown in Fig. [4](#page-15-0) (a), under the literature review mode,
    the survey agent can search for domain-related papers and automatically select
    the most relevant literature to read and extract task-related information. For
    example, the agent can identify works such as "Multimodal Transformer-based Model
    for Buchwald- Hartwig and Suzuki-Miyaura Reaction Yield Prediction" or "ReacLLaMA:
    Merging chemical and textual information in chemical reactivity AI models" to
    quickly gather foundational studies in the field. Such a process is essential
    for idea generation process since the used agent may not have relevant domain
    knowledge, especially in emerging fields. Besides, under deep research mode, the
    survey agent needs to search for literature related to specific technical terms
    used in generated ideas. As shown in Fig. [4](#page-15-0) (b), the agent updates
    its queries based on generated technical terms and retrieves papers like "Large
    Language Models to Accelerate Organic Chemistry Synthesis" which are closely aligned
    with these refined research directions. This process is highly similar to human
    researchers, they initially perform a comprehensive review of the relevant field
    to build foundational knowledge, and then search for articles focused on specific
    techniques to further refine the research direction.


    **Analysis on Idea Innovation Agent.** Idea innovation agent can first generate
    ideas and then evolve the generated ideas in an iterative manner. We take the
    idea evolution tree as an example to show the iterative process of polishing ideas.
    As shown in Fig. [3,](#page-6-1) the root node


    <span id="page-15-1"></span>Table 8: Ablation Study on Adaptive Evolution (AE).
    Ideas (i/s/t) shows the number of ideas that improved performance, the number
    of ideas that successfully ran, and the total number of ideas tested (format:
    improved / successful / tested).


    | Method             |            | AutoRYP    |               |         | Auto2DCls
    |               |         | AutoSenCls |               |

    |--------------------|------------|------------|---------------|---------|-----------|---------------|---------|------------|---------------|

    |                    | 2<br>Max R | 2<br>Avg R | Ideas (i/s/t) | Max Acc | Avg
    Acc   | Ideas (i/s/t) | Max Acc | Avg Acc    | Ideas (i/s/t) |

    | Baseline           | 27.6       | 27.6       | -             | 81.2    | 81.2      |
    -             | 91.0    | 91.0       | -             |

    | NOVELSEEK (w/o AE) | 34.7       | 33.0       | 2/5/10        | 81.6    | 81.5      |
    2/5/10        | 92.4    | 91.9       | 6/8/10        |

    | NOVELSEEK          | 35.4       | 33.5       | 4/6/10        | 83.3    | 82.2      |
    5/7/10        | 93.5    | 92.5       | 9/9/10        |


    <span id="page-15-0"></span>![](_page_15_Figure_3.jpeg)


    Figure 4: Analysis of two modes on survey agent.


    (*i.e.*, Init Idea 0) denotes an initially generated idea and the child nodes
    are evolved from the parent node. As ideas continue to evolve, more external knowledge
    sourced from the survey agent is incorporated into ideas, which enriches the content
    and enhances the practicality of the ideas. For example, starting with a basic
    idea such as "adding a graph-derived reaction descriptor as a precondition for
    attention scores in the transformer architecture", the agent refines and evolves
    the ideas in an iterative manner. At each step, the evolved idea shows improvements
    over its predecessor in terms of technical sophistication, novelty, or practical
    applicability. As illustrated in Fig. [3,](#page-6-1) the process can involve
    incorporating more specific chemical descriptors, introducing cross-modality attention
    mechanisms, or leveraging hierarchical architectures, with each evolution step
    guided by additional insights from literature or domain knowledge, thus ensuring
    continuous advancement of the ideas.


    **Analysis on Idea-to-Methodology Phase.** The correspondence between an idea
    and its final code implementation plays a crucial role in assessing the effectiveness
    of the idea since the idea can be verified once the experiments have been conducted.
    The goal of the idea-tomethodology process is to generate detailed methodologies
    so that code can be written based on these comprehensive method descriptions (*e.g.*,
    method-level descriptions in research


    <span id="page-16-0"></span>Table 9: Comparison with AI-Scientist-V2 and AI-Researcher
    on AutoRYP and Auto2DCls task. Total cost means the cost of the whole session.
    For each task, we conduct 10 experiments. AI-Scientist-V2 and AI-Researcher demonstrate
    relatively weak baseline improvement capabilities, with AI-Scientist-V2 in particular
    struggling to write code that runs correctly. The primary reason lies in the fact
    that AI-Scientist-V2''s pipeline utilizes limited task-related information (*e.g.*
    task formulation and type, relevant papers, and commonly used code) when generating
    new ideas or coding. As a result, their generated ideas tend to be more divergent
    and difficult to implement.


    | Method                                | AutoRYP    |            |            |
    Auto2DCls |         |            |

    |---------------------------------------|------------|------------|------------|-----------|---------|------------|

    |                                       | 2<br>Max R | 2<br>Avg R | Total Cost
    | Max Acc   | Avg Acc | Total cost |

    | Baseline                              | 27.6       | 27.6       | -          |
    81.2      | 81.2    | -          |

    | AI-Scientist-V2 (Yamada et al., 2025) | -          | -          | 15\$       |
    -         | -       | 10\$       |

    | AI-Researcher (Lab, 2025)             | 12.3       | -          | 25\$       |
    80.3      | -       | 32\$       |

    | NOVELSEEK                             | 35.4       | 33.5       | 3\$        |
    83.3      | 82.2    | 3\$        |


    papers). As illustrated in Fig. [5,](#page-17-0) our idea-to-method approach enables
    the generation of fine-grained methodologies, which facilitates accurate and faithful
    code implementation.


    **Analysis on Evolutionary Experimental Planning and Execution.** To verify the
    effectiveness of the adaptive evolution (AE), we conduct ablation studies on three
    tasks, ranging from AI tasks to scientific tasks including AutoRYP, Auto2DCls,
    and AutoSenCls. As shown in Tab. [8,](#page-15-1) the performance can be further
    improved on multiple tasks with adaptive evolution. For example, on the image
    classification task, both the max accuracy and the mean accuracy can be improved
    by 1.7% and 0.7%, compared to the setting without AE. This is because our coder
    agent can automatically analyze the previous results and baseline results and
    further re-plan the following experiments. Besides, the successful execution rate
    and the percentage of performance gains will also improve (*e.g.*, on AutoRYP,
    the percentage of performance gains is 40% compared to 20% without AE). This is
    due to with AE, the coder will implement the idea step by step and analyze the
    experimental phenomena after each stage of the experiments.


    **Improving Baseline in Multi-Dimension.** NOVELSEEK not only can improve the
    performance on different tasks, but it also enhance the quality of ideas in other
    dimensions. For example, as shown in Tab. [7,](#page-14-2) on few-shot yield prediction
    task. We find that the results of baseline methods are unstable, for the results
    of multiple repeated experiments tend to exhibit large variance (*e.g.*, 24.2±4.2
    when train-set=60). In contrast, the methods proposed by NOVELSEEK can improve
    both the performance and the stability of the results. For example, when the train-set=60,
    ADAGT proposed by NOVELSEEK achieves 34.8 average *R* <sup>2</sup> across 5 repeated
    experiments compared to 24.2 achieved by the baseline method. Besides, the variance
    of the results achieved by ADAGT (*i.e.*, ±1.1) is much lower than baseline methods
    (*i.e.*, ±4.2). This phenomenon further shows the quality of ideas and code implementation
    of NOVELSEEK.


    **Comparison with AI-Researcher.** We evaluate the performance and cost of NOVELSEEK
    and AI-Researcher [\(Lab,](#page-22-13) [2025\)](#page-22-13) on AutoRYP and Auto2DCls
    research tasks. To ensure a fair comparison, we supplied AI-Researcher with the
    same code templates that NOVELSEEK uses. Both systems employ GPT-4o-2024-08-06
    for idea generation and Claude-3-7-Sonnet-20250219 for code generation. As shown
    in Tab. [9,](#page-16-0) NOVELSEEK outperforms both the baseline methods and AI-Researcher
    across both tasks, whereas AI-Researcher is unable to improve the provided baselines.
    One important reason for NOVELSEEK''s outstanding performance is its ability to
    generate novel ideas through extensive search and reflection. Moreover, NOVELSEEK
    has a complete experimental planning and adaptive evolution mechanism, thereby
    enabling it to achieve better performance. In contrast, the idea generated by
    AI-Researcher is more dependent on user-provided reference papers, limiting its
    novelty. Moreover, AI-Researcher often ignore the prior information of the existing
    codebases, which further hinders its performance. In terms of cost, NOVELSEEK
    is significantly more economical than AI-Researcher. For an instance, the economic
    cost required for NOVELSEEK is approximately one-sixth that of AI-Researcher on
    the AutoRYP task. This lower cost allows


    <span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)


    Figure 5: Visual Examples of AutoRYP Task.


    NOVELSEEK to conduct broader scientific experiments, thereby accelerating the
    exploration and validation of innovative research ideas.


    ## **4 Case Studies**


    ### **4.1 Qualitative Results**


    In this section, we present the results from various tasks implemented using NOVELSEEK.


    ### *4.1.1 Visual Examples of Various Tasks*


    We present showcases for three distinct tasks: AutoRYP, AutoMD, and AutoPower,
    to highlight the innovative methodologies discovered and their applications. These
    showcases are illustrated in Figs. [5,](#page-17-0) [6,](#page-18-0) and [9,](#page-30-0)
    respectively. Each task demonstrates a unique approach to solving complex problems,
    showcasing the potential impact of NOVELSEEK across different scientific domains.


    In the AutoRYP task illustrated in Fig. [5,](#page-17-0) NOVELSEEK autonomously
    discovered an innovative approach called "Adaptive Dual-Attention Graph-Transformer
    with Dynamic Freezing" for predicting chemical reaction yields. This method effectively
    integrates SMILES and graph-derived descriptors using a hybrid graph-transformer
    network, incorporating hierarchical attention mechanisms to enhance accuracy while
    minimizing overfitting. The approach features a Dual-Attention Fusion Mechanism
    (DAFM) that systematically com-


    <span id="page-18-0"></span>![](_page_18_Figure_1.jpeg)


    Figure 6: Visual Examples of AutoMD Task.


    bines token and graph embeddings with reaction conditions, ensuring effective
    information flow across different representations. Furthermore, a dynamic layer
    freezing mechanism, based on gradient magnitudes, optimizes which layers are trained,
    thereby enhancing generalization in low-data scenarios. Implemented with self-attention
    and cross-modality attention modules, this system not only combines 1D and 2D
    molecular representations effectively but also improves prediction accuracy and
    model adaptability. Consequently, this showcase underscores the method''s potential
    for advancing research in complex chemical tasks using deep learning.


    In the AutoMD task illustrated in Fig. [6,](#page-18-0) a novel framework called
    "Hierarchical Equivariant Directional Graph Encoder" (HEDGE-Net) has been autonomously
    discovered for predicting molecular energy and forces. This approach utilizes
    SE(3)-equivariant graph neural networks with hierarchical geometric self-attention
    and multi-hop message enrichment. By integrating angular and directional features
    into aggregated substructures, the method captures interacting atomic patterns
    and propagates dynamic weight updates, aligning with both local and global molecular
    geometries. The core of this method, the Geometry-Enhanced Directional Attention
    (GEDA) mechanism, ensures SE(3)-equivariance, enabling precise predictions for
    complex molecular systems at both atomic and substructural scales. Implemented
    with advanced message passing techniques, HEDGE-Net effectively combines directional
    and substructural information, enhancing scalability and precision in molecular
    modeling. This showcases the method''s potential to advance research in complex
    molecular tasks using deep learning techniques.


    <span id="page-19-0"></span>![](_page_19_Figure_1.jpeg)


    Figure 7: Visual Examples of Experimental Planning and Adaptive Evolution on Auto3DCls
    task.


    1


    ### *4.1.2 Visual Examples of Experimental Planning and Adaptive Evolution*


    To further illustrate the practical utility of our experimental planning and adaptive
    evolution framework as described in Sec. [2.3.2,](#page-8-1) we present some concrete
    examples of its application in the development and optimization for 3D point cloud
    classification and transcription prediction for perturbation response. Fig. [7](#page-19-0)
    and Fig. [8](#page-29-0) visually summarize the stepwise experimental planning
    and adaptive evolution process that guided the implementation and refinement of
    our method.


    ## **4.2 Human Evaluation**


    Table [10](#page-20-0) compares the novelty of ideas generated by our NOVELSEEK
    and AI-Scientist-V2 [\(Yamada et al.,](#page-24-3) [2025\)](#page-24-3), across
    various research tasks. Each task involves generating 20 ideas, which are evaluated
    by five qualified reviewers. The assessments focus on four


    <span id="page-20-0"></span>


    | Table 10: From the perspectives of soundness, contribution, and overall, we
    compare the      |

    |----------------------------------------------------------------------------------------------|

    | novelty of ideas generated by NOVELSEEK and AI-Scientist-V2 (Yamada et al.,
    2025). For       |

    | each research task, we generate 20 ideas. Each idea is scored by 5 qualified
    reviewers, and  |

    | the final score for each task is reported as the average score of all 20 ideas.
    The detailed |

    | scores for each idea can be found in the Appendix B.1.                                       |


    | Research Task                  | Idea-gen Method | Soundness | Contribution
    | Overall | Confidence |

    |--------------------------------|-----------------|-----------|--------------|---------|------------|

    | Reaction Yield Prediction      | AI-Scientist-V2 | 1.42      | 1.45         |
    3.50    | 3.50       |

    | Reaction Yield Prediction      | NOVELSEEK       | 3.09      | 2.66         |
    4.35    | 4.00       |

    | 2D Semantic Segmentation       | AI-Scientist-V2 | 1.84      | 2.07         |
    2.95    | 3.64       |

    | 2D Semantic Segmentation       | NOVELSEEK       | 2.41      | 2.35         |
    4.05    | 3.48       |

    | 2D Image Classification        | AI-Scientist-V2 | 2.78      | 2.82         |
    4.40    | 3.87       |

    | 2D Image Classification        | NOVELSEEK       | 3.15      | 3.10         |
    5.85    | 3.32       |

    | Point Cloud Autonomous Driving | AI-Scientist-V2 | 2.15      | 2.47         |
    3.10    | 3.94       |

    | Point Cloud Autonomous Driving | NOVELSEEK       | 2.75      | 2.95         |
    5.10    | 4.10       |


    criteria: soundness, contribution, overall rating, and confidence. For each research
    task, the average scores of the 20 ideas are reported.


    In the Reaction Yield Prediction task, NOVELSEEK outperforms AI-Scientist-V2 in
    all aspects, especially in overall rating and soundness. Similarly, for 2D Semantic
    Segmentation, NOV-ELSEEK shows better idea generation ability, particularly in
    soundness and overall rating. In 2D Image Classification and Point Cloud Autonomous
    Driving, NOVELSEEK scores higher across all criteria, indicating a consistent
    advantage over AI-Scientist-V2 in generating more effective and novel ideas.


    ### **5 Related Works**


    Recent advances in Large Language Models (LLMs) and agent-based systems have demonstrated
    significant potential in the field of Autonomous Scientific Research (ASR), enabling
    progress from creative idea generation to end-to-end research automation. Some
    studies [\(Li](#page-23-13) [et al.,](#page-23-13) [2024b;](#page-23-13) [Wang
    et al.,](#page-24-13) [2023;](#page-24-13) [Zhou et al.,](#page-25-3) [2024\)](#page-25-3)
    have shown that LLMs are capable of generating novel research ideas, which has
    sparked widespread discussion in the academic community. For example, [Li et al.](#page-23-13)
    [\(2024b\)](#page-23-13) introduce a method that derives research ideas through
    the analysis of interconnected scholarly works. Beyond idea generation, several
    studies have examined the use of LLMs for hypothesis formulation, such as extracting
    hypotheses from large-scale web data [\(Yang et al.,](#page-24-14) [2023\)](#page-24-14)
    or scientific literature [\(Wang et al.,](#page-24-13) [2023;](#page-24-13) [Zhou
    et al.,](#page-25-3) [2024\)](#page-25-3). However, most of these efforts remain
    at the stage of idea or hypothesis generation, lacking systematic empirical validation
    of their practical effectiveness.


    In terms of end-to-end research automation, [Lu et al.](#page-23-1) [\(2024\)](#page-23-1)
    introduced the AI Scientist framework, which was among the first to achieve a
    fully automated pipeline in the machine learning domain, covering problem definition,
    experimental execution, and result reporting. The subsequent AI Scientist-V2 [\(Yamada
    et al.,](#page-24-3) [2025\)](#page-24-3) further enhanced the framework by incorporating
    agent tree search, vision-language model feedback, and parallelized experiment
    execution, leading to the first workshop paper fully generated and peer-reviewed
    by AI. Similarly, systems such as AI-Researcher [\(Lab,](#page-22-13) [2025\)](#page-22-13)
    and Dolphin [\(Yuan et al.,](#page-24-1) [2025\)](#page-24-1) have proposed closed-loop,
    LLM-driven frameworks that automate the entire research process on a range of
    simple tasks.


    Human-AI collaboration is gaining traction in ASR. Systems like Agent Laboratory
    [\(Schmidgall et al.,](#page-23-14) [2025\)](#page-23-14) integrate human feedback
    into multi-stage LLM agent workflows, automating literature review, experiment
    execution, and report writing, while allowing user input at each step to enhance
    research quality. AgentRxiv [\(Schmidgall & Moor,](#page-23-15) [2025\)](#page-23-15)
    addresses the collaborative nature of scientific discovery by enabling LLM agent
    laboratories to communicate and build upon each other''s work via a shared preprint
    server, thus facilitating knowledge sharing and collective innovation. Experimental
    results demonstrate that agent laboratories utilizing AgentRxiv for collaboration
    achieve greater performance


    improvements compared to isolated settings. Similarly, AI Co-Scientist [\(Gottweis
    et al.,](#page-22-2) [2025\)](#page-22-2), based on Gemini 2.0, employs a multi-agent
    system with a "generate-debate-evolve" strategy for hypothesis generation, and
    has demonstrated effectiveness in biomedical domains such as drug repurposing,
    novel target identification, and interpretation of bacterial evolution, with several
    hypotheses validated through experiments.


    Despite these advances, most current systems are still evaluated primarily on
    relatively simple tasks or within narrow scientific domains. However, when applied
    to more complex, system-level scientific challenges, these approaches often face
    significant limitations. Key challenges include generating truly novel and scientifically
    sound research ideas, establishing robust closed-loop feedback between experiments
    and idea generation, and developing systematic evaluation standards to rigorously
    assess the effectiveness and real-world value of autonomous research systems.


    ## **6 Conclusion and Future Works**


    **Summary.** We have introduced a closed-loop multi-agent framework for the first
    time, which supports 12 types of scientific research tasks. It has been validated
    to generate novel ideas and transform these ideas into code that can effectively
    improve performance. NOV-ELSEEK refines the initially generated ideas through
    human-interactive feedback enriched with a self-evolutionary path of ideas. It
    facilitates the transformation from coarse-grained proposals to fine-grained methodologies
    via an idea-to-methodology construction process. Furthermore, by leveraging multi-round
    experimental planning and execution, it implements the corresponding theoretical
    methods, thereby completing the closed-loop process in scientific research—from
    hypothesis generation to verification.


    **Future Outlook.** NOVELSEEK faces several key technical challenges that need
    to be addressed in the future:


    - Knowledge Retrieval: This involves establishing connections and relationships
    between papers, utilizing deep research techniques to conduct extensive searches
    across a wide range of papers, and performing meta-analyses on the search results.
    Additionally, it requires transforming the papers into structured representations
    such as triples, and utilizing graph networks to uncover relationships between
    papers, including associations in paper ideas, methodologies, experimental conditions,
    and experimental results. Moreover, retrieval-augmented generation will be employed
    to alleviate the hallucination issues of LLMs when generating viewpoints or citing
    references.

    - Knowledge Understanding and Representation: This involves utilizing VLM/LLM
    to accurately analyze relevant academic papers, aiming to understand the significance
    of their core concepts, methodologies, and research findings, while also refining
    knowledge and formulating hypotheses. Additionally, it focuses on extracting valuable
    knowledge from a large number of papers, identifying common patterns, trends,
    and connections, thereby advancing the understanding and representation of knowledge
    in the field.

    - Agent Capability Enhancement: This focuses on improving the ability of AI systems
    to autonomously perform complex tasks in scientific research. The strength of
    agents lies in their ability to dynamically adapt, rather than solely relying
    on historical records to determine subsequent actions. Through self-modification,
    they can flexibly redefine their initial goals and planning strategies while utilizing
    feedback, as well as communication logs between agents or between humans and agents,
    to train and improve themselves. This mechanism should focus on improving their
    ability to gather feedback from three key sources: the environment, interactions
    with other agents, and human experts.

    - Scientific Discovery-related Benchmark Construction: This involves evaluating
    the value that an idea can bring, rather than simply evaluating its novelty. It
    also includes evaluating whether the methods proposed by AI align with their correspond-


    ing code implementations and determining whether NOVELSEEK demonstrates a certain
    level of generalization ability in broader scientific scenarios.


    ### **References**


    - <span id="page-22-5"></span>Cosmas D Arnold, Daniel Gerlach, Christoph Stelzer,
    Łukasz M Bory´n, Martina Rath, and Alexander Stark. Genome-wide quantitative enhancer
    activity maps identified by starr-seq. *Science*, 339(6123):1074–1077, 2013.

    - <span id="page-22-10"></span>Liang-Chieh Chen, Yukun Zhu, George Papandreou,
    Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution
    for semantic image segmentation. In *Proceedings of the European conference on
    computer vision (ECCV)*, pp. 801–818, 2018.

    - <span id="page-22-0"></span>Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei
    Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far
    are we to gpt-4v? closing the gap to commercial multimodal models with open-source
    suites. *Science China Information Sciences*, 67(12):220101, 2024.

    - <span id="page-22-4"></span>Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda,
    Igor Poltavsky, Kristof T Schütt, and Klaus-Robert Müller. Machine learning of
    accurate energy-conserving molecular force fields. *Science advances*, 3(5):e1603015,
    2017.

    - <span id="page-22-6"></span>Bernardo P de Almeida, Franziska Reiter, Michaela
    Pagani, and Alexander Stark. Deepstarr predicts enhancer activity from dna sequence
    and enables the de novo design of synthetic enhancers. *Nature genetics*, 54(5):613–624,
    2022.

    - <span id="page-22-7"></span>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics*, pp. 4171–4186, 2019.

    - <span id="page-22-9"></span>M. Everingham, L. Van Gool, C. K. I. Williams, J.
    Winn, and A. Zisserman. The PAS-CAL Visual Object Classes Challenge 2012 (VOC2012)
    Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html,
    2012.

    - <span id="page-22-3"></span>Paul Gauthier and Aider-AI Contributors. Aider:
    Ai pair programming in your terminal. <https://github.com/Aider-AI/aider>, 2023.
    URL <https://github.com/Aider-AI/aider>. Accessed: 2025-05-07.

    - <span id="page-22-2"></span>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin,
    Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran
    Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. *arXiv preprint arXiv:2502.18864*,
    2025.

    - <span id="page-22-1"></span>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
    Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1:
    Incentivizing reasoning capability in llms via reinforcement learning. *arXiv
    preprint arXiv:2501.12948*, 2025.

    - <span id="page-22-12"></span>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman,
    Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford,
    et al. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*, 2024.

    - <span id="page-22-8"></span>Alex Krizhevsky, Geoffrey Hinton, et al. Learning
    multiple layers of features from tiny images. 2009.

    - <span id="page-22-13"></span>HKU Data Intelligence Lab. Ai-researcher: Fully-automated
    scientific discovery with llm agents. <https://github.com/HKUDS/AI-Researcher>,
    2025. URL [https://github.com/](https://github.com/HKUDS/AI-Researcher) [HKUDS/AI-Researcher](https://github.com/HKUDS/AI-Researcher).

    - <span id="page-22-11"></span>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng
    Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision:
    Easy visual task transfer. *arXiv preprint arXiv:2408.03326*, 2024a.

    - <span id="page-23-13"></span>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan
    Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al.
    Chain of ideas: Revolutionizing research in novel idea development with llm agents.
    *arXiv preprint arXiv:2410.13185*, 2024b.

    - <span id="page-23-1"></span>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster,
    Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended
    scientific discovery. *arXiv preprint arXiv:2408.06292*, 2024.

    - <span id="page-23-12"></span>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu,
    Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and
    Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models
    in visual contexts. *arXiv preprint arXiv:2310.02255*, 2023.

    - <span id="page-23-11"></span>Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu,
    Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying
    chain-of-thought reasoning in multimodal mathematics. *arXiv preprint arXiv:2501.04686*,
    2025.

    - <span id="page-23-9"></span>Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang,
    Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, et
    al. One million scenes for autonomous driving: Once dataset. *arXiv preprint arXiv:2106.11037*,
    2021.

    - <span id="page-23-5"></span>Thomas M Norman, Max A Horlbeck, Joseph M Replogle,
    Alex Y Ge, Albert Xu, Marco Jost, Luke A Gilbert, and Jonathan S Weissman. Exploring
    genetic interaction manifolds constructed from rich single-cell phenotypes. *Science*,
    365(6455):786–793, 2019.

    - <span id="page-23-4"></span>Damith Perera, Joseph W Tucker, Shalini Brahmbhatt,
    Christopher J Helal, Ashley Chong, William Farrell, Paul Richardson, and Neal
    W Sach. A platform for automated nanomolescale reaction screening and micromole-scale
    synthesis in flow. *Science*, 359(6374):429–434, 2018.

    - <span id="page-23-8"></span>Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J
    Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp. 652–660, 2017.

    - <span id="page-23-2"></span>Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li,
    Diping Song, Zheng Wang, and Kaipeng Zhang. Ai idea bench 2025: Ai research idea
    generation benchmark. *arXiv preprint arXiv:2504.14191*, 2025.

    - <span id="page-23-6"></span>Yusuf Roohani, Kexin Huang, and Jure Leskovec. Predicting
    transcriptional outcomes of novel multigene perturbations with gears. *Nature
    Biotechnology*, 42(6):927–935, 2024.

    - <span id="page-23-15"></span>Samuel Schmidgall and Michael Moor. Agentrxiv:
    Towards collaborative autonomous research. *arXiv preprint arXiv:2503.18102*,
    2025.

    - <span id="page-23-14"></span>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng
    Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent
    laboratory: Using llm agents as research assistants. *arXiv preprint arXiv:2501.04227*,
    2025.

    - <span id="page-23-3"></span>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto.
    Can llms generate novel research ideas? a large-scale human study with 100+ nlp
    researchers. *arXiv preprint arXiv:2409.04109*, 2024.

    - <span id="page-23-7"></span>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 conference on empirical methods in natural language processing*, pp. 1631–1642,
    2013.

    - <span id="page-23-10"></span>OpenPCDet Development Team. Openpcdet: An open-source
    toolbox for 3d object detection from point clouds. <https://github.com/open-mmlab/OpenPCDet>,
    2020.

    - <span id="page-23-0"></span>Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao
    Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl:
    Enhancing vision-language model''s perception of the world at any resolution.
    *arXiv preprint arXiv:2409.12191*, 2024a.

    - <span id="page-24-13"></span>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope.
    Scimon: Scientific inspiration machines optimized for novelty. *arXiv preprint
    arXiv:2305.14259*, 2023.

    - <span id="page-24-4"></span>Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu,
    Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh,
    Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas
    Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng,
    Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers
    as Generalist Agents, 2024b. URL [https:](https://arxiv.org/abs/2407.16741) [//arxiv.org/abs/2407.16741](https://arxiv.org/abs/2407.16741).

    - <span id="page-24-12"></span>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu,
    Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh,
    et al. Openhands: An open platform for ai software developers as generalist agents.
    In *The Thirteenth International Conference on Learning Representations*, 2024c.

    - <span id="page-24-5"></span>Yusong Wang, Tong Wang, Shaoning Li, Xinheng He,
    Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Enhancing geometric
    representations for molecules with equivariant vector-scalar interactive message
    passing. *Nature Communications*, 15(1): 313, 2024d.

    - <span id="page-24-8"></span>Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,
    Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation
    for volumetric shapes. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, pp. 1912–1920, 2015.

    - <span id="page-24-3"></span>Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran
    Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level
    automated scientific discovery via agentic tree search. *arXiv preprint arXiv:2504.08066*,
    2025.

    - <span id="page-24-2"></span>Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu
    Xia, Bin Wang, Bo Zhang, and Lei Bai. Surveyforge: On the outline heuristics,
    memory-driven generation, and multidimensional evaluation for automated survey
    writing. *arXiv preprint arXiv:2503.04629*, 2025.

    - <span id="page-24-11"></span>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao,
    Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin,
    et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement,
    2024b. *URL https://arxiv. org/abs/2409.12122*.

    - <span id="page-24-0"></span>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,
    Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al.
    Qwen2. 5 technical report. *arXiv preprint arXiv:2412.15115*, 2024.

    - <span id="page-24-14"></span>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng,
    Soujanya Poria, and Erik Cambria. Large language models for automated open-domain
    scientific hypotheses discovery. *arXiv preprint arXiv:2309.02726*, 2023.

    - <span id="page-24-9"></span>Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
    Center-based 3d object detection and tracking. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pp. 11784–11793, 2021.

    - <span id="page-24-1"></span>Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen,
    Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and Bowen Zhou. Dolphin: Closed-loop
    open-ended auto-research through thinking, practice, and feedback. *arXiv preprint
    arXiv:2501.03916*, 2025.

    - <span id="page-24-7"></span>Sergey Zagoruyko. Wide residual networks. *arXiv
    preprint arXiv:1605.07146*, 2016.

    - <span id="page-24-6"></span>Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
    Are transformers effective for time series forecasting? In *Proceedings of the
    AAAI conference on artificial intelligence*, volume 37, pp. 11121–11128, 2023.

    - <span id="page-24-10"></span>Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
    and Lucas Beyer. Sigmoid loss for language image pre-training. In *Proceedings
    of the IEEE/CVF international conference on computer vision*, pp. 11975–11986,
    2023.

    - <span id="page-25-1"></span>Zhen Zhao, Zhen Huang, Zicheng Wang, Wenqi Huang,
    and LEI BAI. Senseflow: A physicsinformed and self-ensembling iterative framework
    for power flow estimation, 2024. URL <https://openreview.net/forum?id=UKiCFpwcqY>.

    - <span id="page-25-2"></span>Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,
    Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer
    for long sequence time-series forecasting. In *The Thirty-Fifth AAAI Conference
    on Artificial Intelligence, AAAI 2021, Virtual Conference*, volume 35, pp. 11106–11115.
    AAAI Press, 2021.

    - <span id="page-25-3"></span>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan
    Mei, and Chenhao Tan. Hypothesis generation with large language models. *arXiv
    preprint arXiv:2404.04326*, 2024.

    - <span id="page-25-0"></span>Ray Daniel Zimmerman, Carlos Edmundo Murillo-Sánchez,
    and Robert John Thomas. Matpower: Steady-state operations, planning, and analysis
    tools for power systems research and education. *IEEE Transactions on power systems*,
    26(1):12–19, 2010.


    # **Appendix**


    # **A Contributions and Acknowledgments**


    ### **1. Core Contributors**


    Bo Zhang Shiyang Feng Xiangchao Yan Jiakang Yuan


    Feng


    ### **2. Multi-agent System Algorithm Design**


    **Self-Evolving Idea**: Shiyang Feng **Idea-to-Methodology**: Xiangchao Yan **Coder**:
    Shiyang Feng, Jiakang Yuan, Xiangchao Yan **Multi-round Experimental Plan**: Jiakang
    Yuan, Bo Zhang, Xiangchao Yan, Shiyang


    **3. Scientific Research Task Integration and Enhancement**


    **Reaction Yield Prediction**: Xiaohan He, Zhiyin Yu


    **Molecular Dynamics**: Zhiyin Yu **Power Flow Estimation**: Songtao Huang **Time
    Series Forecasting**: Songtao Huang **Sentiment CLS**: Xiangchao Yan, Jiakang
    Yuan **3D Point CLS**: Xiangchao Yan, Jiakang Yuan **2D Image CLS**: Xiangchao
    Yan, Jiakang Yuan **2D Semantic Segmentation**: Shiyang Feng **3D Autonomous Driving**:
    Jiakang Yuan **Transcription Prediction**: Xiaohan He, Peng Ye


    **Enhancer Activity Prediction**: Zhiyin Yu **VLM finetuning**: Tianshuo Peng


    ## **4. Software Development**


    Zheng Nie, Zhilong Wang, Runmin Ma, Jinyao Liu, Shiyang Feng, Xiangchao Yan


    ## **5. Manuscript Preparation**


    Shiyang Feng, Bo Zhang, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Songtao Huang,
    Lei Bai, Xiaohan He, Tianshuo Peng


    ### **6. Idea Human Evaluation Specialist**


    Bo Zhang, Peng Ye, Shufei Zhang, Dongzhan Zhou, Xiaosong Wang, Lei Bai


    ### **7. Project Management and Product**


    Yilan Zhang, Meng Li, Shaowei Hou, Zhongying Tu


    ## **8. Advising**


    Bowen Zhou, Wanli Ouyang, Xiangyu Yue


    ### **9. Project Co-lead**


    Lei Bai, bailei@pjlab.org.cn Bo Zhang, zhangbo@pjlab.org.cn


    # **B Evaluation Details**


    ### <span id="page-26-0"></span>**B.1 Scoring Criteria for Idea Review**


    In Table [10](#page-20-0) of the main text, we conducted a human evaluation to
    assess the novelty of ideas generated by AI-Scientist-V2 [\(Yamada et al.,](#page-24-3)
    [2025\)](#page-24-3) and our NOVELSEEK. The evaluation was carried out across
    four dimensions: soundness, contribution, overall rating, and confidence.


    Specifically, considering the evaluation cost, we opted to evaluate four types
    of research tasks: reaction yield prediction, 2D semantic segmentation, 2D image
    classification, and point cloud autonomous driving. Each invited researcher was
    required to have peer-review qualifications for top-tier journals or conferences
    in the relevant field. For each research task, we generated 20 ideas using both
    AI-Scientist-V2 and NOVELSEEK, and five experienced researchers were invited to
    score each idea. In this part, we provide a detailed description of the scoring
    criteria for each reviewer, as outlined below:


    ### **Soundness**:


    • 4 excellent


    - 3 good

    - 2 fair

    - 1 poor


    ### **Contribution**:


    - 4 excellent

    - 3 good

    - 2 fair

    - 1 poor


    ### **Overall**:


    - Rating: 10: Award quality: Technically flawless paper with groundbreaking impact
    on one or more areas of AI, with exceptionally strong evaluation, reproducibility,
    and resources, and no unaddressed ethical considerations.

    - Rating: 9: Very Strong Accept: Technically flawless paper with groundbreaking
    impact on at least one area of AI and excellent impact on multiple areas of AI,
    with flawless evaluation, resources, and reproducibility, and no unaddressed ethical
    considerations.

    - Rating: 8: Strong Accept: Technically strong paper with, with novel ideas, excellent
    impact on at least one area of AI or high-to-excellent impact on multiple areas
    of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed
    ethical considerations.

    - Rating: 7: Accept: Technically solid paper, with high impact on at least one
    subarea of AI or moderate-to-high impact on more than one area of AI, with goodto-excellent
    evaluation, resources, reproducibility, and no unaddressed ethical considerations.

    - Rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with
    no major concerns with respect to evaluation, resources, reproducibility, ethical
    considerations.

    - Rating: 5: Borderline accept: Technically solid paper where reasons to accept
    outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.

    - Rating: 4: Borderline reject: Technically solid paper where reasons to reject,
    e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please
    use sparingly.

    - Rating: 3: Reject: For instance, a paper with technical flaws, weak evaluation,
    inadequate reproducibility and incompletely addressed ethical considerations.

    - Rating: 2: Strong Reject: For instance, a paper with major technical flaws,
    and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed
    ethical considerations.

    - Rating: 1: Very Strong Reject: For instance, a paper with trivial results or
    unaddressed ethical considerations


    ## **Confidence**:


    - Confidence: 5: You are absolutely certain about your assessment. You are very
    familiar with the related work and checked the math/other details carefully.

    - Confidence: 4: You are confident in your assessment, but not absolutely certain.
    It is unlikely, but not impossible, that you did not understand some parts of
    the submission or that you are unfamiliar with some pieces of related work.

    - Confidence: 3: You are fairly confident in your assessment. It is possible that
    you did not understand some parts of the submission or that you are unfamiliar
    with some pieces of related work. Math/other details were not carefully checked.

    - Confidence: 2: You are willing to defend your assessment, but it is quite likely
    that you did not understand the central parts of the submission or that you are
    unfamiliar with some pieces of related work. Math/other details were not carefully
    checked.

    - Confidence: 1: Your assessment is an educated guess. The submission is not in
    your area or the submission was difficult to understand. Math/other details were
    not carefully checked.


    ### **B.2 Information on the Expert Review Process**


    **Qualifications for Human Evaluators**: Evaluators must hold a Ph.D. or be Ph.D.
    candidates with reviewing experience in top-tier AI conferences such as ICLR,
    ICML, NeurIPS, CVPR, ICCV, and ACL.


    ### **Steps for Expert Evaluation and Validation**:


    - Before the evaluation begins, evaluators are required to carefully read the
    scoring guidelines, as outlined in Appendix [B.1.](#page-26-0)

    - Each evaluator is assigned 20 ideas generated by NOVELSEEK and 20 ideas generated
    by AI-Scientist-V2 [\(Yamada et al.,](#page-24-3) [2025\)](#page-24-3). For each
    idea, evaluators must carefully review the generated content and provide final
    scores across four dimensions: Soundness, Contribution, Overall, and Confidence.

    - Qualified evaluators are required to spend at least 10 minutes reading each
    idea. During the evaluation process, they are allowed to conduct relevant literature
    searches and verify idea redundancy to ensure that the scoring results are objective
    and representative.


    ## **C NOVELSEEK Software Development**


    Fig. [13](#page-32-0) shows the front-end interface of the current NOVELSEEK software
    platform. Overall, NOVELSEEK software platform employs a frontend-backend separation
    design pattern, building a highly scalable distributed service platform.


    The frontend layer is developed based on the React framework, featuring an advanced
    visual interaction system. Key innovations include an infinite canvas rendering
    engine supporting multi-node topology, a collaborative mind mapping component
    driven by state synchronization, a code editor supporting multiple formats, and
    a real-time training metrics visualization dashboard.


    The backend leverages a cloud-native technology stack, utilizing a dynamic container
    orchestration engine for elastic resource scheduling, a distributed asynchronous
    task queue for high concurrency support, and a cross-cloud storage gateway for
    data synchronization across heterogeneous cloud environments. Additionally, a
    microservice governance system is established using a Service Mesh architecture.


    The entire system is delivered through containerization, with Kubernetes cluster
    management enabling self-healing and intelligent scaling, ensuring business continuity
    while significantly improving resource utilization efficiency.


    # **D Visualization Results**


    We further conducted detailed visualization and analysis around the multi-round
    experimental planning and execution (Fig. [8\)](#page-29-0) in NOVELSEEK, as well
    as the automated scientific research tasks (Figs. [9,](#page-30-0) [10,](#page-30-1)
    [11](#page-31-0) and [12\)](#page-32-1) supported by NOVELSEEK.


    Fig. [8](#page-29-0) illustrates the process of Experimental Planning and Adaptive
    Evolution on AutoTPPR. Each block in the figure represents a step in the multi-round
    experiment planning process, where the complete NOVELSEEK-generated method is
    decomposed into multiple logical


    <span id="page-29-0"></span>![](_page_29_Figure_1.jpeg)


    Figure 8: Visual Examples of Experimental Planning and Adaptive Evolution on AutoTPPR
    task.


    2


    steps. This allows for task decomposition during the experimental validation phase,
    thereby facilitating more significant benchmark results.


    Furthermore, in the AutoPower task illustrated in Fig. [9,](#page-30-0) the "Adaptive
    Hierarchical Graph Transformer" (AHGT) introduces significant advancements for
    power flow estimation in energy systems. This approach features two key innovations:
    the Enhanced Edge-Node Hierarchical Pooling (EENHPool) mechanism, which integrates
    global and local features to retain crucial graph structures while reducing ambiguities,
    and the Stability-Regularized Temporal Graph Transformer (SRT-GT), designed to
    capture temporal dynamics while maintaining training stability. These components
    together enhance the model''s robustness and accuracy, validated on IEEE benchmarks
    under scenarios involving renewable energy and grid perturbations. The AHGT method
    outputs precise voltage magnitude and angle predictions, assessed using metrics
    such as MAE, RMSE, and the Graph Perturbation Robustness Index (GPRI). This approach
    showcases the potential for improved power system modeling through advanced graph
    transformer techniques.


    <span id="page-30-0"></span>![](_page_30_Figure_1.jpeg)


    #### Figure 9: Visual Examples of AutoPower Task


    <span id="page-30-1"></span>![](_page_30_Figure_3.jpeg)


    Figure 10: Visual Examples of AutoSenCls Task


    <span id="page-31-0"></span>![](_page_31_Figure_1.jpeg)


    Figure 11: Visual Examples of Auto3DCls Task


    <span id="page-32-1"></span>![](_page_32_Figure_1.jpeg)


    ### Figure 12: Visual Examples of Auto2DCls Task


    <span id="page-32-0"></span>


    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                 |
    run-1                                                                                                                                                                                                                                  |
    ◎ 研究方向 AutoPower 國 数据集 IEEE 39-bus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |          |        |  |  |

    |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|--------|--|--|

    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    NovelSeek                                       | ◎ 研究方向 AutoPower 目 欧舰集 IEEE
    39-bus                                                                                                                                                                                                     |
    年成题法                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |          |        |  |  |

    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    从一个想法开始目动突破                                     |                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |          |
    :      |  |  |

    | 输入你的规法,深震实验方向                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                 |
    同 本院代理<br>同样的管理                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |          |        |  |  |

    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                 |
    run-2<br>◎ 研究方向 AutoPower 回 欧亿集 IEEE 39-bus                                                                                                                                                                                            |
    2007                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |          |        |  |  |

    | GPT-40 V                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    1 个                                             | Embedding differential operators-such
    as Laplacians, divergence, and curl-<br>derived from the fundamental physical
    equations of power flow directly into<br>message passing of graph neural networks
    will impose physical constraints |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |          |        |  |  |

    | 拿 摇爱源   X                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    容 研究方向   请选择<br>图 胶视集   请这择                     | 因 实验代码<br>□ 任务管理                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |          |        |  |  |

    | HIMA   Ilingen<br>3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    · SSMER O REASO AutoPower 18 BIER EXP. Jo-lease | O untill                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |          |        |  |  |

    | Buncher an constituent as to<br>ENRY FOREM                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                 |                                                                                                                                                                                                                                        |
    1 创新性 4.4 ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★
    ★ ★ ★ ★ ★ ★ ★ ★ >                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    同 宜看实验代码 | 同 深入家发 |  |  |

    | Cepart, [nan<br>2007. rief.<br>pour portuits<br>from sev-atile lancer anual
    narantari, golberter, financ<br>from pro-suen laport duen<br>from Comers Lesset
    Lanner<br>Gegert forum, dialributed as diet<br>from wry, aften logort not random
    seed, orlyg distributed, selup derled. toespoth of day<br>Dept. Hersal.<br>Separt
    Energis an as an<br>Geport arguesa<br>from farsh.ve. attle inport alle prof alle
    prof course.<br>1. human an her<br>Trim Gench, age to tr. asheduler angel Personal
    Phylone<br>TVW CHOOL JADRETTELMS SHOOTS CINCAL PHONOVOUSIOUS SUCHIPOSITIONAL THEATHCHW<br>(senset
    forem to finerineal as f<br>(rom selep-squrse-ragragh inport fleyt worshall)<br>from
    setelies leaset fires<br>Dourt Frincelonk<br>Crim spectric, aperature lincert
    Spectra''illperations. Physicollegatar Liettayer<br>Out values and researche meters
    model organ<br>m, my riser, diner, de, de = n t; 1; 1; 3; 3; A; 5<br>then it reats
    thunlinessi |                                                 |                                                                                                                                                                                                                                        |
    Embedding differential operators-such as Laplacians, divergence, and curl-derived
    from the<br>fundamental physical equations of power flow directly into message
    passing of graph neural<br>networks will impose physical constraints on the learned
    representations. This approach is<br>anticipated to improve the model''s ability
    to extrapolate to unseen grid configurations, enhance<br>robustness under limited
    data conditions, and accelerate training convergence, by explicity<br>encoding
    known physical laws present in power system operation, as supported by the success
    of<br>physics-inspired flow models in related scientific fields. |          |        |  |  |

    | deline p. stelia a = deliage lous leastersh, Mars)<br>matria = K<br>Product
    (70, 99 min''s PROMERATOR 1 PU ''Ludi, Inc., betale( PO'' Lyft, Ved.,<br>("Leon(170,Y),
    me") PRE (1493) (1992 (1992) , mi, MCCC PC (rit) , mil.<br>("Cook](idelra_s")
    delsa_pusts [Linean(). 9000).<br>Classicle(delin o''s delta numbellaman().drem'').<br>relax''s
    andrial<br>OUT NUMFEDOR, FORE SPECIPLICATION, DATAL PROGRATIAL: INCOLLCOMMENT
    SHI<br>FLix_Tape=True; mont="Mix","<br>"Freenute deltane leas<br>progn and creaters
    cracked i Babdied Between arrigh infa-<br>premi fotaxi proce reautis<br>torum,
    Clast: sichtann lows<br>del Lener, dellage_fackTibus, branch, edge_Index, denloads                                                                                                                                                                                                                                                                            |                                                 |                                                                                                                                                                                                                                        |
    1 创新性 4.4 ★ ★ ★ ★ ★ ★ ★ *<br>A hierarchical multi-resolution graph network architecture
    that combines local (microgrid),<br>intermediate (meso), and system-wide (macro)
    encodings with flow-sensitive attention modules,<br>while explicitly modeling
    control device effects and non-linearities, will improve the scalability,<br>predictive
    accuracy, and interpretability of power flow estimates, especially for complex,
    large-<br>scale power grids.                                                                                                                                                                                               |
    回 生成实验代码 | 同 深入院常 |  |  |

    | th a hom-shopedial I curs<br>nl = asign index.shape [1] = musiser of hrunch<br>IN
    A. IR X, IR J. 180, 180, 18297 = 6, 1, 2, 3, 4<br>PA BD BD BL PDC PIC ON YOU WA
    PALL SL BL BL BL BL BL FL P<br>Ps = L.b / Etranch(1, Mi.H = 1) = hrunch(); Mi.(4);<br>Be
    = BranshEr, 80,01<br>tap = farsh_unesIe11.boldeshim]<br>1 - Tarih, Nonderstorandhi,
    Taria<br>tag(1) = branch(1, 147)<br>tip = tsp = fiff(fi-cig-1) = branch] = branch]
    = 943903<br>The Toursely                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                 |
    THON In TS CHITS                                                                                                                                                                                                                       |
    1 创新性 4.2 ★ ★ ★ ★ ★ ★ *<br>By developing a graph neural network architecture that
    combines matrix function neural networks<br>(MFNs) with physics-informed equivariant
    layers respecting network symmetries and nonlinear<br>power flow constraints,
    we hypothesize that the model will achieve superior transfer learning<br>performance.
    This design targets non-local interactions and captures the inherent physical<br>invariances,
    leading to improved rapid feasibility and stability assessments across different
    grid                                                                                                                            |
    回 生成实验代码 | 国 深入园家 |  |  |


    Figure 13: NOVELSEEK software platform includes features such as the user entry
    interface, task selection interface, idea-tree visualization and human-computer
    interaction interface, code generation, and auto-debug interface. In the near
    future, we plan to support additional functionalities, including custom dataset
    uploads and academic idea thinking modes.'
- title: "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n\
    \  and Training Considerations for Long Context Reasoning"
  abstract: 'We introduce $\infty$-THOR, a new framework for long-horizon embodied
    tasks

    that advances long-context understanding in embodied AI. $\infty$-THOR

    provides: (1) a generation framework for synthesizing scalable, reproducible,

    and unlimited long-horizon trajectories; (2) a novel embodied QA task,

    Needle(s) in the Embodied Haystack, where multiple scattered clues across

    extended trajectories test agents'' long-context reasoning ability; and (3) a

    long-horizon dataset and benchmark suite featuring complex tasks that span

    hundreds of environment steps, each paired with ground-truth action sequences.

    To enable this capability, we explore architectural adaptations, including

    interleaved Goal-State-Action modeling, context extension techniques, and

    Context Parallelism, to equip LLM-based agents for extreme long-context

    reasoning and interaction. Experimental results and analyses highlight the

    challenges posed by our benchmark and provide insights into training strategies

    and model behaviors under long-horizon conditions. Our work provides a

    foundation for the next generation of embodied AI systems capable of robust,

    long-term reasoning and planning.'
  url: http://arxiv.org/abs/2505.16928v1
  keywords: ''
  document: "# Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\
    \ and Training Considerations for Long Context Reasoning\n\nBosung Kim University\
    \ of California, San Diego bosungkim@ucsd.edu\n\nPrithviraj Ammanabrolu University\
    \ of California, San Diego prithvi@ucsd.edu\n\n# Abstract\n\nWe introduce ∞-THOR,\
    \ a new framework for long-horizon embodied tasks that advances long-context understanding\
    \ in embodied AI. ∞-THOR provides: (1) a generation framework for synthesizing\
    \ scalable, reproducible, and unlimited longhorizon trajectories; (2) a novel\
    \ embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered\
    \ clues across extended trajectories test agents' long-context reasoning ability;\
    \ and (3) a long-horizon dataset and benchmark suite featuring complex tasks that\
    \ span hundreds of environment steps, each paired with ground-truth action sequences.\
    \ To enable this capability, we explore architectural adaptations, including interleaved\
    \ Goal-State-Action modeling, context extension techniques, and Context Parallelism,\
    \ to equip LLM-based agents for extreme long-context reasoning and interaction.\
    \ Experimental results and analyses highlight the challenges posed by our benchmark\
    \ and provide insights into training strategies and model behaviors under long-horizon\
    \ conditions. Our work provides a foundation for the next generation of embodied\
    \ AI systems capable of robust, long-term reasoning and planning. The datasets\
    \ and code can be found at [pearls-lab.github.io/infini-thor](https://pearls-lab.github.io/infini-thor).\n\
    \n# 1 Introduction\n\nReal-world embodied reasoning is a sequential decision-making\
    \ problem requiring long-horizon planning, where task success depends on both\
    \ memorizing and reasoning over multiple events that occur far apart in time.\
    \ Using large pre-trained vision-language-action (VLA) models as policies for\
    \ such tasks requires surpassing the key challenge of *long-context* reasoning.\
    \ We seek to answer questions pertaining to what design choices matter in terms\
    \ of environments, model architectures, and training methods when using VLA models\
    \ for long-horizon embodied tasks. To this end, we develop a new framework for\
    \ long-horizon tasks designed to push the boundaries of long-context understanding\
    \ in embodied AI.\n\nWe introduce ∞-THOR, a new framework for generation, training,\
    \ and evaluation of long-horizon embodied tasks. Our benchmark uniquely features\
    \ tasks with a synthetic final goal, which involves multiple objects that appear\
    \ at distant time steps, requiring multi-step reasoning across over hundreds of\
    \ steps. Figure 1 illustrates an example: the agent observes the tomato at an\
    \ early step (t=17) and the counter top much later (t=560). Then, the final task\
    \ is given at t=670, which requires the agent to place the tomato on the counter\
    \ top. This setup highlights the challenge of long-horizon dependency, where key\
    \ objects and locations must be remembered and acted upon after hundreds of steps.\n\
    \nThis long-horizon setup introduces a new challenging task, Needle(s) in the\
    \ Embodied Haystack (NiEH). Unlike the standard Needle in a Haystack task [\\\
    [22\\]](#page-10-0), which focuses on recalling a single clue in text, NiEH poses\
    \ two main challenges: (1) multiple scattered clues (Needles) and (2) multi-modal\n\
    \n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFigure 1: Example\
    \ of the trajectory and a long-horizon embodied task generated from ∞-THOR. The\
    \ final goal (\"Put the tomato on the counter top\" at t=670) requires recalling\
    \ both the tomato (seen at t=17) and the counter (seen at t=560) to solved the\
    \ long-horizon task. Context size refers to the input token length when converting\
    \ the trajectory into the LLM input space.\n\ninputs that combine visual and linguistic\
    \ observations from the environment (Embodiment). This task is designed to evaluate\
    \ the agent's ability to recall and reason about previously encountered environmental\
    \ details, such as identifying objects and recalling performed actions.\n\nGoing\
    \ beyond static evaluations such as NiEH, ∞-THOR also provides an interactive\
    \ evaluation, allowing agents to execute policies and complete long-horizon tasks\
    \ within a dynamic environment. To support this, we release a trajectory dataset\
    \ for training, with episodes over 400 steps in the training set and more than\
    \ 600 steps in the dev and test sets. These trajectories can be used for imitation\
    \ learning, and our experiments show that access to longer context during training\
    \ leads to significant performance gains, highlighting the importance of our dataset\
    \ for long-context embodied reasoning.\n\nWe further investigate various architectural\
    \ considerations for embodied agents to operate under extreme sequence lengths.\
    \ We show that interleaved Goal-State-Action modeling—a multimodal, goal-conditioned\
    \ VLA architecture that jointly models interleaved sequences of goals, states,\
    \ and actions using a LLM backbone is the most practical approach for this class\
    \ of problems. Moreover, since standard LLMs are constrained by fixed context\
    \ windows and cannot natively handle inputs exceeding 1M tokens, we explore long-context\
    \ extension techniques such as rotary embedding scaling and positional interpolation\
    \ [\\[6,](#page-9-0) [10,](#page-9-1) [25\\]](#page-10-1). Lastly, we demonstrate\
    \ how to further strengthen long-context reasoning by fine-tuning the model on\
    \ extended-context inputs using Context Parallelism, a parallel training strategy\
    \ that allows efficient scaling to very long sequences.\n\nWe provide comprehensive\
    \ experiments and analyses, demonstrating both the challenges posed by our benchmark\
    \ and the behavior of baseline models under long-horizon settings. We investigate\
    \ a range of training considerations, including different configurations for fine-tuning\
    \ and long-context adaptation, and evaluate their impact on model performance.\n\
    \nOur contributions are summarized as follows:\n\n- We introduce ∞-THOR, a new\
    \ framework for generating, training, and evaluating longhorizon embodied tasks,\
    \ featuring synthetic final goals that require multi-step reasoning across hundreds\
    \ of steps.\n- We propose a novel embodied QA task, Needle(s) in the Embodied\
    \ Haystack, requiring agents to recall and reason over multiple scattered clues\
    \ across extended trajectories.\n- We release a large-scale trajectory dataset\
    \ and an interactive evaluation environment to support both offline imitation\
    \ learning and online policy execution in long-horizon settings.\n- We describe\
    \ architectural adaptations including interleaved Goal-State-Action modeling,\
    \ long-context extension and Context Parallelism, tailored for interactive embodied\
    \ reasoning.\n- We present empirical results and analyses, providing insights\
    \ to the current capabilities and limitations of embodied AI systems on long-horizon\
    \ tasks.\n\n# 2 Related Work\n\nLong-horizon Planning in Virtual Environments.\
    \ AI2THOR [\\[18\\]](#page-10-2) provides interactive indoor environments widely\
    \ used for embodied reasoning research, while ProcTHOR [\\[9\\]](#page-9-2) extends\
    \ these\n\nTable 1: Comparison of benchmarks. We use Short (< 50 steps), Medium\
    \ (50–300 steps), and Long (> 300 steps) to describe task horizon, reflecting\
    \ the approximate number of environment steps required to complete a task in each\
    \ benchmark. Single/Multi in the QA set column denotes singleand multi-evidence\
    \ question type.\n\n| Benchmark         | Task     | Interaction | Dataset  |\
    \         |            | QA set |       |\n|-------------------|----------|-------------|----------|---------|------------|--------|-------|\n\
    | /Platform         | Horizon  | w/ env      | modality | # steps | GT actions\
    \ | single | multi |\n| ProcTHOR [9]      | ✗        | ✓           | ✗       \
    \ | ✗       | ✗          | ✗      | ✗     |\n| MineDojo [11]     | Long     |\
    \ ✓           | ✗        | ✗       | ✗          | ✗      | ✗     |\n| Habitat\
    \ 3.0 [27]  | Long     | ✓           | ✗        | ✗       | ✗          | ✗   \
    \   | ✗     |\n| VirtualHome [26]  | Short    | ✓           | multi    | 11.6\
    \    | ✓          | ✗      | ✗     |\n| ALFRED [29]       | Medium   | ✓     \
    \      | multi    | 50      | ✓          | ✗      | ✗     |\n| ALFWorld [30] \
    \    | Medium   | ✓           | text     | 50      | ✓          | ✗      | ✗ \
    \    |\n| BEHAVIOR-100 [31] | Med/Long | ✓           | ✗        | ✗       | ✗\
    \          | ✗      | ✗     |\n| BALROG [24]       | Long     | ✗           |\
    \ ✗        | ✗       | ✗          | ✗      | ✗     |\n| EQA [8]           | ✗\
    \        | ✗           | ✗        | ✗       | ✗          | ✓      | ✗     |\n\
    | MM-EGO [34]       | ✗        | ✗           | ✗        | ✗       | ✗        \
    \  | ✓      | ✗     |\n| ∞-THOR            | Long     | ✓           | multi  \
    \  | 600+    | ✓          | ✓      | ✓     |\n\ncapabilities by procedurally generating\
    \ scalable environments, potentially facilitating longer trajectories. MineDojo\
    \ [\\[11\\]](#page-9-3) offers an open-ended platform within Minecraft, explicitly\
    \ geared toward tasks requiring extensive long-term planning. Additionally, platforms\
    \ such as VirtualHome [\\[26\\]](#page-10-3) and Habitat 3.0 [\\[27\\]](#page-11-0)\
    \ have demonstrated suitability for tasks involving long-term interactions and\
    \ complex activity sequences. However, all of these platforms only provide environments\
    \ and do not include standardized datasets or benchmark suites to support training\
    \ and evaluation for long-horizon embodied tasks.\n\nEmbodied QA and Multimodal\
    \ Needle in the Haystack Tasks. Embodied QA tasks, such as EmbodiedQA [\\[8\\\
    ]](#page-9-4) and MM-EGO [\\[34\\]](#page-11-4), require agents to answer questions\
    \ grounded in visual observations, often demanding spatial and temporal reasoning\
    \ over the agent's trajectory. While these benchmarks emphasize multimodal understanding,\
    \ they do not involve active interaction with the environment during evaluation.\
    \ Another related area to our NiEH task is the multimodal Needle in a Haystack\
    \ (NiH) problem. Traditional NiH tasks primarily assessed textual recall within\
    \ long-context inputs [\\[22\\]](#page-10-0), while recent multimodal variants\
    \ extend this idea by incorporating visual components [\\[33,](#page-11-5) [32\\\
    ]](#page-11-6). However, these works operate on relatively short context windows\
    \ (typically up to 72K tokens) and do not require embodied reasoning or temporal\
    \ dependencies across a dynamic trajectory.\n\nDatasets and Benchmarks for Long-horizon\
    \ Embodied Tasks. Recent efforts have pushed toward long-horizon embodied tasks,\
    \ where agents must complete multi-step goals with extended temporal dependencies.\
    \ While ALFRED [\\[29\\]](#page-11-1) and ALFWorld [\\[30\\]](#page-11-2) introduced\
    \ multi-step instruction-following tasks with action annotations and textual grounding,\
    \ their task horizons are relatively short, typically under 50 steps. BEHAVIOR-100\
    \ [\\[31\\]](#page-11-3) evaluates agent generalization on household activities,\
    \ some of which require prolonged engagement, but mainly focus on single task.\
    \ BALROG [\\[24\\]](#page-10-4) is a benchmark for testing the agentic capabilities\
    \ of long-context LLMs, but its scope is limited to games.\n\nLong-context Benchmarks.\
    \ Outside embodied AI, general benchmarks have addressed challenges in long-context\
    \ reasoning. Benchmarks, such as LongBench [\\[4\\]](#page-9-5) and RULER [\\\
    [14\\]](#page-10-5), focus on retrieval or summarization tasks. GSM-∞ [\\[35\\\
    ]](#page-11-7) extends GSM-8K [\\[7\\]](#page-9-6) to assess mathematical reasoning\
    \ over extremely long textual inputs.\n\n# 3 ∞-THOR: An Environment for Generating,\
    \ Training, and Evaluating Long-horizon Embodied Tasks\n\n∞-THOR features with\
    \ a generation framework for synthesizing long trajectories to train and evaluate\
    \ AI agents in long-horizon embodied tasks. We build ∞-THOR upon the AI2-THOR\
    \ simulator [\\[18\\]](#page-10-2), an interactive 3D environment for embodied\
    \ AI research that supports diverse scenes, objects, and agent actions. ∞-THOR\
    \ enables the creation of trajectories with no length limit, and provides an\n\
    \n<span id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\n(b) Needles in the\
    \ Embodied Haystack: Multi-evidence question types.\n\nFigure 2: Example of N(s)iEH\
    \ task and Ground-truth steps.\n\nevaluation setup where agents can interact dynamically\
    \ with the environment during both training and testing. This supports both offline\
    \ learning by producing large-scale datasets, and online learning through direct\
    \ agent-environment interaction.\n\nEach trajectory generated by ∞-THOR consists\
    \ of multiple task goals, such as \"Put a clean sponge on a metal rack\" and \"\
    Pick up the apple and place it on the microwave\", requiring grounded understanding\
    \ and action to achieve the goal. This enables the agent to explore and interact\
    \ with the environment over extended episodes. At the end of each trajectory,\
    \ the agent is assigned a synthetic long-horizon task that requires reasoning\
    \ over entities encountered at distant time steps. For the example in Figure [1,](#page-1-0)\
    \ the long-horizon task (Sub-goal #23) at step t=689, \"Put the tomato on the\
    \ counter top\", depends on observations made far earlier: the tomato at t=17\
    \ and the counter top at t=560. Our generation framework can generate unlimited\
    \ tasks, the trajectories can be exceptionally long, exceeding 1M context tokens\
    \ or beyond when the trajectory is processed with LLMs.\n\nSuccessfully completing\
    \ this task requires the agent to (1) memorize and integrate key environmental\
    \ information over hundreds of steps, and (2) plan actions based on dependencies\
    \ that are separated in time, demonstrating the need for long-context reasoning\
    \ and robust spatio-temporal memory.\n\n#### 3.1 Static Evaluation: Needle(s)\
    \ in the Embodied Haystack\n\nWe first introduce a novel task in the form of a\
    \ static evaluation: Needle(s) in the Embodied Haystack (NiEH). NiEH is designed\
    \ to evaluate an agent's ability to recall and reason about environmental states\
    \ encountered throughout a trajectory. Unlike traditional embodied QA tasks that\
    \ focus primarily on visual understanding of a single image, NiEH emphasizes reasoning\
    \ about environmental changes over time, requiring agents to interpret and integrate\
    \ sequences of multimodal observations.\n\nFigure [2](#page-3-0) presents examples\
    \ of the two NiEH task types. In the single-evidence setting, a question is answerable\
    \ based on a single observation step; in the multi-evidence setting, multiple\
    \ temporally distant steps must be combined to answer the question. The NiEH testset\
    \ includes diverse question\n\ntypes, such as binary (\"yes\" or \"no\"), \"what\"\
    -, \"where\"-, and \"how many\"-style questions. These questions span a broad\
    \ range of difficulty, from simple memory recall (similar to the Needle in a Haystack\
    \ paradigm) to complex queries that requiring multi-step reasoning across temporally\
    \ and spatially distributed evidence.\n\nTestset Construction. We first replay\
    \ the generated trajectories and collect the agent's egocentric views, along with\
    \ all objects that interact with the agent throughout the trajectories, such as\
    \ objects that are picked up, moved, or even simply observed. Based on these interactions,\
    \ we apply a set of rule-based templates to generate QA pairs, such as \"Q. What\
    \ object did you slice? A. {object name}\" and \"Q. Is {obeject name} on the desk?\
    \ A. Yes/No\". Then, we sample questions based on the frequency to ensure diversity\
    \ across object types, and annotate the GT answer steps using the replay logs.\n\
    \nAfter generating QA pairs and annotating the GT steps, we cross-validate the\
    \ answerability of each question with GT images using four different multimodal\
    \ LLMs: LLaVA-OneVision 7B [\\[19\\]](#page-10-6), Qwen2.5-VL 7B [\\[3\\]](#page-9-7),\
    \ Deepseek-VL 7B [\\[23\\]](#page-10-7), and Pixtral 12B [\\[1\\]](#page-9-8).\
    \ Since these models are highly capable at standard visual QA, we filter out the\
    \ questions that none of the four models successfully answer with GT images. At\
    \ test time, the entire trajectory is treated as a Haystack and then cropped based\
    \ on the GT image's depth. Full details on templates, generation rules, and the\
    \ validation scores of the four models are included in the Appendix.\n\nChallenges\
    \ in Needle(s) in the Embodied Haystack. The NiEH task introduces two key challenges\
    \ for current models. First, many questions require reasoning over multiple temporally\
    \ distant events. As shown in Figure [2\\(](#page-3-0)b), the agent moves a dish\
    \ sponge from the garbage bin at t = 24, then to the counter top, and later places\
    \ it into a drawer at t = 751. A question such as *\"Where was the dish sponge\
    \ before you put it on the counter top?\"* requires the model to recall and chain\
    \ together multiple actions and locations across hundreds of steps. Second, some\
    \ questions demand aggregating sparse and temporally scattered evidence from long\
    \ trajectories. In the second example in Figure [2\\(](#page-3-0)b), answering\
    \ *\"How many times did you move the credit card?\"* requires the model to track\
    \ and count all relevant actions occurring from the beginning to the end of the\
    \ episode. These challenges highlight the need for models that can perform robust\
    \ long-horizon reasoning across both time and modalities in complex embodied environments.\n\
    \n#### 3.2 Constructing Long-horizon Trajectories for Interactive Evaluations\n\
    \nWith ∞-THOR's generation framework, we can synthesize long-horizon trajectories\
    \ to construct training, validation, and test sets for offline learning and evaluation.\
    \ Our approach builds upon a planner-based method [\\[18\\]](#page-10-2), in which\
    \ we sequentially concatenate multiple single-task demonstrations into a extended\
    \ trajectory, while maintaining consistency in object states and agent interactions\
    \ throughout.\n\nTable 2: Dataset Statistics\n\n| NiEH testset               \
    \                             | Single-clue           | Multi-clue          |\
    \                     |\n|---------------------------------------------------------|-----------------------|---------------------|---------------------|\n\
    | # of question-answer pair                               | 829              \
    \     | 474                 |                     |\n| Trajectory            \
    \                                  | Train                 | Dev             \
    \    | Test                |\n| # trajectory<br># avg/max sub-goals<br># avg steps\
    \      | 2,456<br>12/18<br>405 | 125<br>16/24<br>613 | 225<br>18/33<br>627 |\n\
    | # max steps<br># avg token length<br># max token length | 654<br>602K<br>954K\
    \   | 890<br>880K<br>1.2M | 952<br>912K<br>1.3M |\n\nTo generate each trajectory,\
    \ we first sample a task type from one of seven predefined templates (e.g., pick\
    \ two objects and place,\n\npick and place with movable receptacle). We then sample\
    \ objects that are required to perform the task, such as items to be picked up\
    \ or receptacles to be interacted with. Based on the sampled task and objects,\
    \ we use a classical task planner that operates on PDDL-defined domains to generate\
    \ groundtruth action sequences. These plans are executed in a simulator, and only\
    \ successful rollouts are retained. We then concatenate these successful demonstrations\
    \ to construct long-horizon sequences that span hundreds of steps. For the final\
    \ goal, the involved objects are sampled exclusively from those seen during the\
    \ early 20% and the final 20% of the trajectory. This enforces a long-term temporal\
    \ dependency between two objects that must be jointly referenced to complete the\
    \ final task. Through this procedure, we generate 2,456/125/225 trajectories for\
    \ the training, validation, and test sets, respectively. Details on task types\
    \ and a pseudo-algorithm for the generation process are provided in the Appendix.\n\
    \n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFigure 3: Agent–environment\
    \ interaction through interleaved Goal-State-Action modeling.\n\n# 4 Architectures\
    \ for Long-Horizon Vision-Language-Action Models\n\nEmbodied agents must effectively\
    \ interact with complex, dynamic environments, necessitating capabilities to interpret\
    \ multimodal inputs (vision, language) and produce coherent sequences of actions.\
    \ Developing such Vision-Language-Action (VLA) models is particularly challenging\
    \ due to the need for seamless integration of perceptual understanding, linguistic\
    \ reasoning, and action prediction. Existing VLA models either use separate encoders\
    \ for vision, language, and action modules [\\[29\\]](#page-11-1), or focus on\
    \ short-horizon, low-level tasks such as robot arm manipulation in constrained\
    \ environments [\\[5,](#page-9-9) [17\\]](#page-10-8), where decisions depend\
    \ only on the most recent observation and a single instruction. While recent multimodal\
    \ LLMs like LLaVA [\\[21\\]](#page-10-9), MiniGPT-4 [\\[36\\]](#page-11-8), and\
    \ Llama 3.2 [\\[2\\]](#page-9-10) show strong multimodal reasoning abilities,\
    \ they primarily operate on static inputs (e.g., single or few images) and lack\
    \ the dynamic interactivity and memory needed for long-horizon embodied tasks\
    \ involving continuous vision-language-action sequences. Moreover, many state-of-the-art\
    \ models are only accessible via proprietary APIs, making them impractical for\
    \ real-time, controllable embodied settings and managing long-term memory states.\n\
    \nWe explore the potential of a multimodal LLM as a unified model for VLA modeling,\
    \ utilizing an interleaved input structure of goal, state (visual observations),\
    \ and action tokens, as illustrated in Figure [3.](#page-5-0) This interleaved\
    \ multimodal input allows the model to process vision, language, and actions concurrently,\
    \ thereby facilitating more coherent, real-time interaction modeling. Specifically,\
    \ our goal-conditioned agent uses a multimodal LLM backbone trained to predict\
    \ subsequent actions autoregressively, conditioned on sequences of goal and state\
    \ tokens. At each timestep t, the environment provides a new visual observation\
    \ st, which is encoded as state tokens and appended to the existing token stream.\
    \ The model then autoregressively predicts the next action a<sup>t</sup> conditioned\
    \ on the full history of goals, states, and previously taken actions. This action\
    \ is executed in the environment (e.g., \"Pick up the book\"), which leads to\
    \ the next observation st+1, continuing the perception-action loop. This interactive\
    \ sequence is repeated over hundreds of steps, allowing the model to reason over\
    \ temporally distant information while maintaining grounded behavior in dynamic\
    \ settings. By leveraging this interleaved Goal–State–Action modeling, our architecture\
    \ supports coherent decision-making across long-horizon embodied tasks.\n\nContext\
    \ Extension. Given the limitations in context length of most LLMs, using off-the-shelf\
    \ models is insufficient for processing long inputs such as those exceeding 1M\
    \ tokens. We explore various long-context extension techniques that allows the\
    \ model to generalize to longer input sequences without retraining from scratch.\
    \ Specifically, we consider: Linear Interpolation[\\[6\\]](#page-9-0): Rescales\
    \ input positions to fit within the pretrained RoPE range by linearly interpolating\
    \ positional indices; Dynamic Scaling[\\[6\\]](#page-9-0): Adapts RoPE frequencies\
    \ at runtime based on the input sequence length, using a linear rescaling to maintain\
    \ consistent positional encoding behavior across varying lengths; YaRN[\\[25\\\
    ]](#page-10-1): Dynamically interpolates attention frequencies during inference,\
    \ balancing between pretrained and extrapolated positional regimes; LongRoPE[\\\
    [10\\]](#page-9-1): Augments RoPE with specially\n\ndesigned extrapolation functions,\
    \ enabling robust generalization to long sequences without degrading attention\
    \ quality. We apply these techniques during fine-tuning, at inference time, or\
    \ both.\n\nContext Parallelism. To further enhance the model's ability to reason\
    \ over long contexts, it is crucial to fine-tune on extended context inputs. However,\
    \ the quadratic complexity of the attention mechanism makes it computationally\
    \ infeasible to train directly over long sequences. To address this challenge,\
    \ we employ Context Parallelism, a parallel training technique designed for efficient\
    \ long-context modeling.\n\nContext Parallelism leverages Ring Attention [\\[20\\\
    ]](#page-10-10), a novel parallel implementation of the attention layer. In Ring\
    \ Attention, key-value (KV) shards are cyclically shuffled across devices, and\
    \ partial attention scores are computed iteratively. This process is repeated\
    \ until all KV shards have been incorporated on each device, ensuring complete\
    \ attention coverage without the full memory cost of standard attention. By combining\
    \ Context Parallelism with our dataset of extended long-context inputs, we are\
    \ able to scale fine-tuning to substantially longer sequences, unlocking improved\
    \ long-horizon reasoning capabilities.\n\n# 5 Experiments\n\n#### 5.1 Static Evaluation:\
    \ Needle(s) in the Embodied Haystack\n\nWe first evaluate model performance on\
    \ the Needle in the Embodied Haystack (NiEH) and Needles in the Embodied Haystack\
    \ (NsiEH) tasks, which test an agent's ability to retrieve and reason over sparse\
    \ evidence scattered throughout long embodied trajectories.\n\nBuilding a Embodied\
    \ Haystack. Unlike the traditional Needle in the Haystack setup, which inserts\
    \ a target sentence into a long text corpus like a book, we use the entire embodied\
    \ trajectory as the input context. To simulate different reasoning depths, we\
    \ crop the input sequence either from the beginning or the end based on the GT\
    \ image's position. In the NsiEH task, where multiple evidences are scattered\
    \ throughout the trajectory, we fill the context with intermediate steps in between\
    \ the GT steps keeping their temporal order.\n\nResults. Figure [4](#page-7-0)\
    \ presents the performance of LLaVA-OneVision 7B [\\[19\\]](#page-10-6) model\
    \ across various context extension methods. Linear Interpolation, Dynamic Scaling,\
    \ and LongRoPE scaling all struggled with very long contexts beyond 128K tokens\
    \ (the results of Linear Interpolation are excluded from Figure since it fails\
    \ at all examples). YaRN consistently outperformed other methods across both NiEH\
    \ and NsiEH, successfully answering questions at context lengths exceeding 384K\
    \ tokens, likely due to its architectural alignment with LLaVA-OneVision's Qwen2\
    \ LM backbone, which employs RoPE and YaRN scaling during pretraining. YaRN performed\
    \ best at moderate scaling factors (e.g., x4), however, further scaling to x8\
    \ and x16 did not yield additional gains. In particular, x16 slightly improved\
    \ performance in the 256K–384K token range but led to degradation notably at shorter\
    \ context sizes (<64K), suggesting that excessive scaling may introduce instability\
    \ and negatively impact performance. Overall, all methods fail beyond 512K tokens.\
    \ We expect these trends to persist as models are pushed to even larger context\
    \ windows, highlighting the need for more advanced methods to effectively handle\
    \ extremely long-context scenarios in the future.\n\nSingle vs Multi-evidence\
    \ Reasoning. Comparing NiEH to the more challenging NsiEH task, we observe a significant\
    \ performance drop in the multi-evidence setting. This is especially pronounced\
    \ for mid-depth questions involving sparse or distant evidence (e.g., *\"Where\
    \ was the Mug before you put it on the CounterTop?\"*) or questions requiring\
    \ the aggregation of multiple clues (e.g., *\"How many times did you move the\
    \ Apple?\"*), as shown in Figure [2\\(](#page-3-0)b). These results demonstrate\
    \ that our NiEH and NsiEH tasks pose a substantial challenge to current long-context\
    \ models and success requires both fine-grained temporal memory and multi-evidence\
    \ reasoning across extended interactions.\n\n# 5.2 Interactive Evaluation in ∞-THOR\n\
    \nWe conduct an interactive online evaluation using the AI2THOR simulator to measure\
    \ agent performance on our long-horizon test set. Our experiments analyze reward\
    \ accumulation across various context extension methods, different scaling factors,\
    \ and multiple fine-tuning context lengths: 32K, 64K, and 130K.\n\n<span id=\"\
    page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nTraining. We fine-tune the LLaVA-OneVision\
    \ 7B model on our training set while freezing the vision encoder. We used 8 H100\
    \ GPUs with both tensor parallelism [\\[28\\]](#page-11-9) and pipeline parallelism\
    \ [\\[15\\]](#page-10-11) for the 32K context size, while Context Parallelism\
    \ is employed for training on larger context sizes (64K and 130K). Additional\
    \ training specifics are available in the Appendix.\n\napplied to all experiments\
    \ with the context size over 384K.\n\n(b) Results of Needles in the Embodied Haystack\
    \ (NsiEH). Figure 4: Results of Needle(s) in the Embodied Haystack with the LLaVA-OneVision\
    \ 7B model. The white dashed line denotes the maximum input context length of\
    \ the model. Context Parallelism is\n\nPlan-level Evaluation. We evaluate agent\
    \ performance based on reward accumulation given previous states and actions.\
    \ Evaluation is performed at the plan-level, where a plan represents a short sequence\
    \ of actions aimed at a specific intermediate goal. For example, a \"Go to location\"\
    \ plan comprises actions like Move Ahead, Rotate Right/Left, and Look Up/Down.\
    \ Each trajectory is thus composed of multiple sequential plans. At each step,\
    \ agents are presented with a task goal alongside the previous state-action sequences,\
    \ predicting subsequent actions and interacting continuously with the environment\
    \ until the plan completion. Further evaluation details are provided in the Appendix.\n\
    \nResults and Discussion. Figure [5](#page-8-0) illustrates the comparative results\
    \ across six experimental configurations. The Planner trajectory serves as the\
    \ performance upper-bound. We address the following key questions through our\
    \ experimental results:\n\n*Q. Which context extension methods perform best?*\
    \ Figure [5\\(](#page-8-0)a) compares different context extension methods at a\
    \ fixed scaling factor of x4. Similar to the NiEH results, YaRN consistently achieves\
    \ the highest performance showing very close performance to Planner.\n\n*Q. Does\
    \ further scaling enhance performance?* Figure [5\\(](#page-8-0)b) explores YaRN\
    \ scaling at different scaling factors (x4, x8, and x16). Interestingly, increasing\
    \ the scaling factor beyond x4 does not significantly improve performance, indicating\
    \ a diminishing return for larger scaling factors.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_0.jpeg)\n\
    \nFigure 5: Agent's reward across different experimental configurations. We compare\
    \ (a) context extension methods at fixed scaling (x4), (b) varying YaRN scaling\
    \ factors, (c) fine-tuning with different context lengths using Context Parallelism,\
    \ and (d-e) combinations of scaling during both training and inference. (f) summarizes\
    \ the most effective strategies, highlighting that exposure to longer contexts\
    \ during training significantly improves performance. Non-planner models cannot\
    \ generate valid actions after around 250 steps (≈376K in context size).\n\n*Q.\
    \ Is fine-tuning on a dataset with long trajectories effective?* Figure [5\\(](#page-8-0)c)\
    \ demonstrates the effectiveness of fine-tuning with Context Parallelism, enabling\
    \ scaling of context lengths up to 64K and 130K tokens. At a 130K context size,\
    \ the model can learn sequences comprising approximately 86 steps, substantially\
    \ longer compared to only 22 steps with a 32K context size. This shows that exposure\
    \ to longer context during training significantly enhances model performance,\
    \ suggesting that incorporating more long-horizon data by ∞-THOR could further\
    \ improve model capabilities. We note that context extension methods were not\
    \ applied in this experiment.\n\n*Q. Does combining context extension methods\
    \ during both training and inference provide additional benefits?* Figures [5\\\
    (](#page-8-0)d) and [5\\(](#page-8-0)e) illustrate experiments with scaling applied\
    \ at both training and evaluation stages. Results indicate that additional scaling\
    \ at evaluation after fine-tuning with scaled RoPE provides no further performance\
    \ improvement and may degrade performance at shorter context lengths (≤300K tokens).\n\
    \nBased on these observations, we can conclude that fine-tuning strategies are\
    \ most effective when long-trajectory datasets are available. In the absence of\
    \ extensive training data, employing YaRN scaling at x4 yields performance comparable\
    \ to the Planner upper-bound, particularly within context lengths under 200K tokens\
    \ (Figure [5\\(](#page-8-0)f)).\n\n# 6 Conclusion\n\nWe presented ∞-THOR, a new\
    \ framework for long-horizon embodied tasks designed to advance longcontext understanding\
    \ in embodied AI. Our framework enables scalable synthesis of long, complex trajectories\
    \ paired with GT action sequences, and supports both offline training and online\
    \ interaction with the environment. As part of this framework, we introduced a\
    \ novel embodied QA benchmark, Needle(s) in the Embodied Haystack, that challenges\
    \ agents to reason over sparse, temporally distant visual evidence embedded within\
    \ extended trajectories. To equip models for this setting, we explored architectural\
    \ adaptations including interleaved Goal–State–Action modeling, context extension\
    \ techniques such as YaRN and LongRoPE, and efficient fine-tuning via Context\
    \ Parallelism. Our experiments demonstrate that exposure to longer contexts during\
    \ training significantly improves model performance, and the limitation of existing\
    \ context extension techniques struggle with longcontext reasoning. We hope our\
    \ framework and benchmark encourage further research into models capable of robust\
    \ long-horizon reasoning under realistic, interactive environments.\n\n# References\n\
    \n- <span id=\"page-9-8\"></span>[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou\
    \ Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin\
    \ De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul\
    \ Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume Lample,\
    \ Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis\
    \ Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat,\
    \ Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles,\
    \ Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart,\
    \ Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang,\
    \ and Sophia Yang. Pixtral 12b, 2024.\n- <span id=\"page-9-10\"></span>[2] Meta\
    \ AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models.\
    \ [https://ai.meta.com/blog/](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)\
    \ [llama-3-2-connect-2024-vision-edge-mobile-devices/](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/),\
    \ September 2024. Accessed: 2025-05-11.\n- <span id=\"page-9-7\"></span>[3] Shuai\
    \ Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng\
    \ Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai\
    \ Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi\
    \ Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang\
    \ Lin. Qwen2.5-vl technical report, 2025.\n- <span id=\"page-9-5\"></span>[4]\
    \ Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,\
    \ Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi\
    \ Li. LongBench: A bilingual, multitask benchmark for long context understanding.\
    \ In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, *Proceedings of the\
    \ 62nd Annual Meeting of the Association for Computational Linguistics (Volume\
    \ 1: Long Papers)*, pages 3119–3137, Bangkok, Thailand, August 2024. Association\
    \ for Computational Linguistics.\n- <span id=\"page-9-9\"></span>[5] Anthony Brohan,\
    \ Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\
    \ Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian\
    \ Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,\
    \ Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine,\
    \ Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina\
    \ Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka\
    \ Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh,\
    \ Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve\
    \ Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\
    \ Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023.\n\
    - <span id=\"page-9-0\"></span>[6] Shouyuan Chen, Sherman Wong, Liangjian Chen,\
    \ and Yuandong Tian. Extending context window of large language models via positional\
    \ interpolation, 2023.\n- <span id=\"page-9-6\"></span>[7] Karl Cobbe, Vineet\
    \ Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\
    \ Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\
    \ John Schulman. Training verifiers to solve math word problems, 2021.\n- <span\
    \ id=\"page-9-4\"></span>[8] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan\
    \ Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answering. In *Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018.\n\
    - <span id=\"page-9-2\"></span>[9] Matt Deitke, Eli VanderBilt, Alvaro Herrasti,\
    \ Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi,\
    \ Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai\
    \ using procedural generation, 2022.\n- <span id=\"page-9-1\"></span>[10] Yiran\
    \ Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu,\
    \ Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million\
    \ tokens, 2024.\n- <span id=\"page-9-3\"></span>[11] Linxi Fan, Guanzhi Wang,\
    \ Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\
    \ Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents\
    \ with internet-scale knowledge, 2022.\n- <span id=\"page-10-14\"></span>[12]\
    \ Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state\
    \ spaces, 2024.\n- <span id=\"page-10-12\"></span>[13] Chi Han, Qifan Wang, Hao\
    \ Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot\
    \ extreme length generalization for large language models, 2024.\n- <span id=\"\
    page-10-5\"></span>[14] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu\
    \ Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: What's\
    \ the real context size of your long-context language models?, 2024.\n- <span\
    \ id=\"page-10-11\"></span>[15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan\
    \ Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui\
    \ Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using\
    \ pipeline parallelism, 2019.\n- <span id=\"page-10-13\"></span>[16] Huiqiang\
    \ Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua\
    \ Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference\
    \ 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention,\
    \ 2024.\n- <span id=\"page-10-8\"></span>[17] Moo Jin Kim, Karl Pertsch, Siddharth\
    \ Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\
    \ Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel,\
    \ Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla:\
    \ An open-source vision-language-action model. *arXiv preprint arXiv:2406.09246*,\
    \ 2024.\n- <span id=\"page-10-2\"></span>[18] Eric Kolve, Roozbeh Mottaghi, Winson\
    \ Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani,\
    \ Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for\
    \ visual ai. *arXiv preprint arXiv:1712.05474*, 2017.\n- <span id=\"page-10-6\"\
    ></span>[19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang,\
    \ Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy\
    \ visual task transfer. *arXiv preprint arXiv:2408.03326*, 2024.\n- <span id=\"\
    page-10-10\"></span>[20] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention\
    \ with blockwise transformers for near-infinite context. *arXiv preprint arXiv:2310.01889*,\
    \ 2023.\n- <span id=\"page-10-9\"></span>[21] Haotian Liu, Chunyuan Li, Qingyang\
    \ Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson,\
    \ K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information\
    \ Processing Systems*, volume 36, pages 34892–34916. Curran Associates, Inc.,\
    \ 2023.\n- <span id=\"page-10-0\"></span>[22] Nelson F. Liu, Kevin Lin, John Hewitt,\
    \ Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in\
    \ the middle: How language models use long contexts. *Transactions of the Association\
    \ for Computational Linguistics*, 12:157–173, 2024.\n- <span id=\"page-10-7\"\
    ></span>[23] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang\
    \ Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei\
    \ Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language\
    \ understanding, 2024.\n- <span id=\"page-10-4\"></span>[24] Davide Paglieri,\
    \ Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan,\
    \ Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus\
    \ Foerster, ´ Jack Parker-Holder, and Tim Rocktäschel. Balrog: Benchmarking agentic\
    \ llm/vlm reasoning on games. In *pre-print*, 2024.\n- <span id=\"page-10-1\"\
    ></span>[25] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN:\
    \ Efficient context window extension of large language models. In *The Twelfth\
    \ International Conference on Learning Representations*, 2024.\n- <span id=\"\
    page-10-3\"></span>[26] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu\
    \ Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household\
    \ activities via programs, 2018.\n- <span id=\"page-11-0\"></span>[27] Xavier\
    \ Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan\
    \ Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír\
    \ Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr\
    \ Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot,\
    \ Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: A co-habitat\
    \ for humans, avatars and robots, 2023.\n- <span id=\"page-11-9\"></span>[28]\
    \ Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\
    \ and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language\
    \ models using model parallelism, 2020.\n- <span id=\"page-11-1\"></span>[29]\
    \ Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Xinyi Han, Roozbeh\
    \ Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting\
    \ grounded instructions for everyday tasks. In *Proceedings of the IEEE/CVF Conference\
    \ on Computer Vision and Pattern Recognition (CVPR)*, pages 10740–10749, 2020.\n\
    - <span id=\"page-11-2\"></span>[30] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre\
    \ Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning\
    \ Text and Embodied Environments for Interactive Learning. In *Proceedings of\
    \ the International Conference on Learning Representations (ICLR)*, 2021.\n- <span\
    \ id=\"page-11-3\"></span>[31] Sanjana Srivastava, Chengshu Li, Michael Lingelbach,\
    \ Roberto Martín-Martín, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal\
    \ Buch, C. Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei.\
    \ Behavior: Benchmark for everyday household activities in virtual, interactive,\
    \ and ecological environments, 2021.\n- <span id=\"page-11-6\"></span>[32] Hengyi\
    \ Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay\
    \ Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in a haystack: Benchmarking\
    \ longcontext capability of multimodal large language models. In Luis Chiruzzo,\
    \ Alan Ritter, and Lu Wang, editors, *Proceedings of the 2025 Conference of the\
    \ Nations of the Americas Chapter of the Association for Computational Linguistics:\
    \ Human Language Technologies (Volume 1: Long Papers)*, pages 3221–3241, Albuquerque,\
    \ New Mexico, April 2025. Association for Computational Linguistics.\n- <span\
    \ id=\"page-11-5\"></span>[33] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan,\
    \ Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou\
    \ Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, and Wenhai Wang. Needle in a\
    \ multimodal haystack, 2024.\n- <span id=\"page-11-4\"></span>[34] Hanrong Ye,\
    \ Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang,\
    \ Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, and Yinfei Yang. MMEgo: Towards building\
    \ egocentric multimodal LLMs for video QA. In *The Thirteenth International Conference\
    \ on Learning Representations*, 2025.\n- <span id=\"page-11-7\"></span>[35] Yang\
    \ Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, and Beidi Chen. Gsm-infinite:\
    \ How do your llms behave over infinitely increasing context length and reasoning\
    \ complexity?, 2025.\n- <span id=\"page-11-8\"></span>[36] Deyao Zhu, Kan Chen\
    \ He, Junnan Zhao, Wayne Wu, and Xinchao Chen. Minigpt-4: Enhancing vision-language\
    \ understanding with advanced large language models. *arXiv preprint arXiv:2304.10592*,\
    \ 2023.\n\n# A Dataset Construction\n\n### A.1 Building the Needle(s) in the Embodied\
    \ Haystack Benchmark\n\nWe construct the Needle(s) in the Embodied Haystack benchmark\
    \ in three stages: 1) Trajectory Replay and Metadata Collection; 2) Rule-Based\
    \ QA Generation; and 3) Cross-validation with Multimodal LLMs. The following sections\
    \ provide detailed descriptions of each step.\n\n# A.1.1 Trajectory Replay and\
    \ Metadata Collection\n\nWe first replay 225 test trajectories generated by ∞-THOR,\
    \ logging both visual observations (agent's egocentric views) and structured metadata\
    \ at each timestep. For every step, we store the list of visible objects, agent-inventory\
    \ items, openable containers, and their contents from the simulator. This produces\
    \ a fine-grained interaction log that captures grounded scene dynamics over time.\n\
    \nAn example of the collected metadata at a single timestep is shown below:\n\n\
    ```\nExample of metadata entry\n{\n    \"img_idx\": 2,\n    \"img_filename\":\
    \ \"000000002.png\",\n    \"step\": 1,\n    \"object_log\": {\n        \"visible\"\
    : [\"Shelf\", \"Vase\", \"Book\"],\n        \"pickupable\": [\"Vase\", \"Book\"\
    ],\n        \"isOpen\": [],\n        \"inven_obj\": [],\n        \"receptacles\"\
    : [\"Shelf\"],\n        \"recep_objs\": {\n            \"Shelf\": [\"Vase\", \"\
    Book\"]\n        }\n    }\n}\n```\nEach metadata entry corresponds to a low-level\
    \ action step and provides the semantic state of the scene, enabling the construction\
    \ of temporally grounded QA instances in later stages.\n\n# A.1.2 Rule-Based QA\
    \ Generation\n\nTo construct the QA set, we apply rule-based generation templates\
    \ to each trajectory using its sequence of low-level actions and associated metadata.\
    \ The QA generation process involves parsing the agent's interactions with objects,\
    \ containers, and the environment, and applying a set of handcrafted rules to\
    \ synthesize grounded questions.\n\nOur QA generation logic covers a diverse range\
    \ of question types, including object presence, object state, location tracking,\
    \ slicing actions, container content reasoning, and action counting. For instance,\
    \ if an object is seen for the first time at a particular step, a presence question\
    \ such as \"Is there any apple in this room?\" is generated. Similarly, after\
    \ a PutObject action, location-based questions like \"Where was the apple before\
    \ you put it to the microwave?\" are produced. When slicing actions happen, we\
    \ create questions about the object being sliced and other nearby items (e.g.,\
    \ \"What objects were in the Fridge when you sliced the apple?\"). Then, we sample\
    \ questions based on the frequency to ensure diversity across object types, and\
    \ annotate the GT answer steps using the replay logs. Table [3](#page-13-0) summarizes\
    \ the types of questions generated, and corresponding trigger conditions and example\
    \ templates.\n\n# A.1.3 Cross-validation with Multimodal LLMs\n\nTo ensure the\
    \ answerability and clarity of the generated QA pairs, we perform cross-validation\
    \ using four powerful multimodal LLMs: LLaVA-OneVision 7B [\\[19\\]](#page-10-6),\
    \ Qwen2.5-VL 7B [\\[3\\]](#page-9-7), Deepseek-VL 7B [\\[23\\]](#page-10-7), and\
    \ Pixtral 12B [\\[1\\]](#page-9-8). Each model is prompted with the GT images\
    \ corresponding to the\n\n| QA Type                  | Trigger Condition     \
    \                                    | Example Template(s)                   \
    \                                                                            \
    \            |\n|--------------------------|-----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n\
    | object presence (Yes/No) | object appears visibly in                       \
    \          | Is there any {obj} in this room?                                \
    \                                                              |\n| open state\
    \ questions     | the trajectory<br>container marked as open<br>in metadata |\
    \ Have you seen a/an {obj}?<br>Was {container} open?                         \
    \                                                   |\n| object location tracing\
    \  | sequences of Pickup and<br>PutObject actions              | Where was {obj}\
    \ before you put it to<br>{container}?<br>Where did you move the {obj} from the\
    \                                 |\n|                          |            \
    \                                               | {container}?<br>Where is {obj}\
    \ now?                                                                       \
    \                    |\n| slicing-based questions  | SliceObject action de<br>tected\
    \ in trajectory             | What did you slice?<br>What objects were in/on the\
    \ {container}<br>when you slice the {obj}?                                   |\n\
    | container content        | container visibility with<br>non-empty contents \
    \          | What objects were in/on the<br>{container}?<br>What object did you\
    \ put in/on the                                              |\n| put action questions\
    \     | unique<br>ac<br>PutObject<br>tion for a container         | {container}?<br>What\
    \ object did you put in/on the<br>{container}?                               \
    \                              |\n| final object state       | final location\
    \ of an object<br>at episode end             | Is {obj} in/on the {container}?<br>What\
    \ objects are in/on the {container}?<br>How many objects were in/on the<br>{container}?\
    \ |\n| movement counting        | object picked up more<br>than once         \
    \               | How many times did you move {obj}?                         \
    \                                                                   |\n\n<span\
    \ id=\"page-13-0\"></span>Table 3: QA types, trigger conditions, and corresponding\
    \ question templates used in rule-based generation.\n\nannotated QA steps and\
    \ asked to answer the associated questions. Given their strong performance on\
    \ standard visual QA tasks, we use these models to assess whether a question can\
    \ be correctly answered or not. We keep only the QA pairs that are correctly answered\
    \ by at least one of the four models, and discard those that fail across all models.\
    \ This helps improve dataset quality and filtering out ambiguous or visually ungroundable\
    \ questions. Table [4](#page-13-1) shows the accuracy of each model on the finalized\
    \ QA set when evaluated with GT images. Notably, even with access to GT images,\
    \ all models struggle with questions requiring reasoning over three or more evidence\
    \ steps. To maintain the benchmark's difficulty and support evaluation of more\
    \ capable models in future, we manually inspect the multi-clue questions and include\
    \ those that are answerable.\n\n| Model           | Size | # of clues (GT steps)\
    \ |       | Total |       |\n|-----------------|------|-----------------------|-------|-------|-------|\n\
    |                 |      | 1                     | 2     | ≥3    |       |\n|\
    \ LLaVA-OneVision | 7B   | 86.61                 | 68.55 | 23.74 | 71.15 |\n|\
    \ Qwen2.5-VL      | 7B   | 85.94                 | 89.83 | 64.40 | 82.20 |\n|\
    \ Deepseek-VL     | 7B   | 81.56                 | 39.14 | 22.57 | 62.88 |\n|\
    \ Pixtral         | 12B  | 91.34                 | 39.60 | 58.56 | 76.25 |\n\n\
    <span id=\"page-13-1\"></span>Table 4: QA accuracy (%) of multimodal LLMs on ground-truth\
    \ images.\n\n#### A.2 Constructing Long-Horizon Trajectories\n\nTo synthesize\
    \ long-horizon trajectories, we construct each trajectory by sequentially chaining\
    \ successful sub-tasks sampled from a predefined set of task templates. This process\
    \ is illustrated in Algorithm [1.](#page-14-0) We begin by sampling a task template\
    \ from a fixed task pool, which includes goal types such as pick and place simple,\
    \ pick two obj and place, and pick and place with movable recep. Each sampled\
    \ template requires relevant objects in the scene (e.g., pickupable items, target\
    \ receptacles), which are then used to define the goal for that task.\n\nWe use\
    \ a classical task planner, which operates over PDDL-defined domains [\\[29\\\
    ]](#page-11-1), to generate a low-level action sequence for the sampled goal,\
    \ and simulate this plan in an interactive environment. If the rollout fails (e.g.,\
    \ due to collisions, object occlusions, or unreachable conditions), we discard\n\
    \nthe sequence and re-sample from the task pool. Otherwise, the successful rollout\
    \ is retained and appended to the ongoing trajectory.\n\nThis sampling-execution\
    \ loop is repeated until a long trajectory with a desired number of sub-goals\
    \ is formed. The resulting synthetic long-horizon trajectory consists of multiple\
    \ sub-goals concatenated into a continuous sequence. To induce long-term temporal\
    \ dependencies, the final sub-task is constrained to involve only objects that\
    \ appear in the early 20% and late 20% of the overall trajectory, requiring the\
    \ agent to integrate temporally distant evidence to answer associated questions.\n\
    \n#### <span id=\"page-14-0\"></span>Algorithm 1 Construct Long-horizon Trajectory\n\
    \n```\n1: Input: Task Pool T , max sub goals N\n 2: Output: Long-horizon trajectory\
    \ τ\n 3: Initialize empty trajectory τ ← []\n 4: while len(τ ) < N do\n 5: Sample\
    \ task template g ∼ T and objects\n 6: Plan action sequence πg by planner\n 7:\
    \ if Simulate(πg) is successful then\n 8: Append πg to trajectory: τ ← τ ∥ πg\n\
    \ 9: else\n10: Discard and re-sample\n11: end if\n12: end while\n13: // Final\
    \ sub-task with long-term object dependency\n14: Sample gfinal ∼ T and objects\
    \ in early 20% and late 20%\n15: Plan and simulate πfinal using restricted objects\n\
    16: if Simulate(πfinal) is successful then\n17: Append πfinal to trajectory: τ\
    \ ← τ ∥ πfinal\n18: else\n19: Repeat sampling until success\n20: end if\n21: return\
    \ τ\n```\n# B Training and Evaluation Details\n\n#### B.1 Interactive Evaluation\
    \ in ∞-THOR\n\nTraining. We fine-tune the LLaVA-OneVision 7B model on our training\
    \ set while freezing the vision encoder. The model is trained using a next-action\
    \ prediction objective, where only the action tokens are optimized, conditioned\
    \ on the goal and state tokens. Table [5](#page-14-1) summarizes the training\
    \ specifications for different context lengths. For 32K training, we apply tensor\
    \ parallelism with a degree of 4 and pipeline parallelism with a degree of 2,\
    \ utilizing 8 H100 GPUs in total. Since pipeline parallelism requires the batch\
    \ size to match the pipeline degree, we set the batch size to 2. For longer context\
    \ lengths, we use context parallelism: 8-way for 64K (on 8 GPUs) and 16-way for\
    \ 130K (on 16 GPUs). All models are fine-tuned for approximately 3 epochs with\
    \ a learning rate of 1e-5, using the AdamW optimizer and a linear learning rate\
    \ schedule with a 0.03 warmup ratio.\n\nTable 5: Training specifications for different\
    \ context lengths.\n\n<span id=\"page-14-1\"></span>\n\n| Context Length | Parallelism\
    \               | # GPUs | Training Time |  |\n|----------------|---------------------------|--------|---------------|--|\n\
    | 32K            | Tensor (4) + Pipeline (2) | 8      | 160 hrs       |  |\n|\
    \ 64K            | Context (8)               | 8      | 120 hrs       |  |\n|\
    \ 130K           | Context (16)              | 16     | 134 hrs       |  |\n\n\
    Plan-Level Evaluation. We evaluate agent performance using a plan-level framework,\
    \ where each plan corresponds to a short sequence of actions aimed at achieving\
    \ a specific intermediate sub-goal (e.g., navigating to an object, placing an\
    \ item). A trajectory is composed of multiple such plans, executed sequentially.\
    \ For the interactive evaluation, the agent is presented with the current plan's\
    \ goal along with the history of previous GT states and actions. Using this context,\
    \ the agent predicts the next action and interacts step-by-step with the environment.\
    \ The interaction continues until the current plan is either successfully completed\
    \ or terminated due to failure (e.g., collisions or deadlocks). After each plan,\
    \ the context is reset to include the GT actions and states from the completed\
    \ portion of the trajectory, and the agent proceeds to the next plan. This ensures\
    \ that each plan is evaluated independently, conditioned only on the correct prior\
    \ history. The agent's performance is measured via cumulative reward across all\
    \ plans in the trajectory. Pseudocode for this evaluation procedure is provided\
    \ in Algorithm [2.](#page-15-0)\n\n### <span id=\"page-15-0\"></span>Algorithm\
    \ 2 Plan-Level Evaluation\n\n1: Input: Trajectory T = {P1, P2, . . . , P<sup>N</sup>\
    \ }, Agent policy π, Environment E 2: Initialize: Reward R ← 0 3: Initialize state\
    \ and history with initial observation 4: for each plan P<sup>i</sup> in T do\
    \ 5: Initialize context with GT actions up to Pi−<sup>1</sup> 6: while not *done*\
    \ and not *failure* do 7: a<sup>t</sup> ← π(context) 8: st+1, rt, *done*, *failure*\
    \ ← E.step(at) 9: Append (at, st+1) to context 10: R ← R + r<sup>t</sup> 11: end\
    \ while 12: if *failure* then 13: Break evaluation 14: end if 15: end for 16:\
    \ return Total accumulated reward R\n\n# C Limitations\n\nWhile ∞-THOR enables\
    \ the generation of arbitrarily long trajectories, the diversity of environment\
    \ layouts in AI2-THOR is inherently limited. This can lead to repetitive agent\
    \ behaviors within certain scenes. For instance, compared to kitchen and living\
    \ room scenes, bedroom scenes tend to involve fewer action types, mostly constrained\
    \ to simple pick-and-place tasks within small spatial areas. Due to the limited\
    \ scene size and low task diversity, we excluded bathroom scenes from our dev\
    \ and test sets. In future work, we plan to integrate ∞-THOR with ProcTHOR [\\\
    [9\\]](#page-9-2), which supports procedurally generated environments, enabling\
    \ a broader range of dynamic and diverse scene configurations.\n\nAdditionally,\
    \ although the GT action sequences generated by the PDDL-based planner are sufficient\
    \ for task completion, they are not guaranteed to be optimal. This may lead to\
    \ agents learning suboptimal behaviors when trained solely on these demonstrations.\
    \ Incorporating learning from exploration or reinforcement-based optimization\
    \ could improve policy quality.\n\nFinally, inference with LLMs in long-context\
    \ settings remains a computational bottleneck, particularly as context lengths\
    \ approach 1M tokens. Since our long-horizon tasks require access to information\
    \ spread throughout the entire trajectory, full-context inference becomes increasingly\
    \ expensive, even with Context Parallelism. One promising direction is to equip\
    \ agents with memory systems that selectively keep relevant information from prior\
    \ steps, allowing the model to reason without reprocessing the full context at\
    \ each step. Other architectural solutions, such as sparse attention mechanisms\
    \ [\\[13,](#page-10-12) [16\\]](#page-10-13) or state-space models with linear-time\
    \ inference [\\[12\\]](#page-10-14), also hold potential for scalable long-context\
    \ reasoning."
- title: Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships
  abstract: 'Artificial Intelligence (AI) systems have historically been used as tools

    that execute narrowly defined tasks. Yet recent advances in AI have unlocked

    possibilities for a new class of models that genuinely collaborate with humans

    in complex reasoning, from conceptualizing problems to brainstorming solutions.

    Such AI thought partners enable novel forms of collaboration and extended

    cognition, yet they also pose major risks-including and beyond risks of typical

    AI tools and agents. In this commentary, we systematically identify risks of AI

    thought partners through a novel framework that identifies risks at multiple

    levels of analysis, including Real-time, Individual, and Societal risks arising

    from collaborative cognition (RISc). We leverage this framework to propose

    concrete metrics for risk evaluation, and finally suggest specific mitigation

    strategies for developers and policymakers. As AI thought partners continue to

    proliferate, these strategies can help prevent major harms and ensure that

    humans actively benefit from productive thought partnerships.'
  url: http://arxiv.org/abs/2505.16899v1
  keywords: ''
  document: '# Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships


    Kerem Oktar\*<sup>1</sup> , Katherine M. Collins\*2,3, Jose Hernandez-Orallo2,3,4,
    Diane Coyle<sup>2</sup> , Stephen Cave2,3, Adrian Weller2,3,5, and Ilia Sucholutsky<sup>6</sup>


    > Princeton University University of Cambridge Leverhulme Centre for Future Intelligence
    Universitat Politècnica de València, The Alan Turing Institute <sup>6</sup>NYU
    \*These authors contributed equally to this work.


    #### **Abstract**


    Artificial Intelligence (AI) systems have historically been used as tools that
    execute narrowly defined tasks. Yet recent advances in AI have unlocked possibilities
    for a new class of models that genuinely collaborate with humans in complex reasoning,
    from conceptualizing problems to brainstorming solutions. Such AI thought partners
    enable novel forms of collaboration and extended cognition, yet they also pose
    major risks—including and beyond risks of typical AI tools and agents. In this
    Commentary, we systematically identify risks of AI thought partners through a
    novel framework that identifies risks at multiple levels of analysis, including
    Real-time, Individual, and Societal risks arising from collaborative cognition
    (RISc). We leverage this framework to propose concrete metrics for risk evaluation,
    and finally suggest specific mitigation strategies for developers and policymakers.
    As AI thought partners continue to proliferate, these strategies can help prevent
    major harms and ensure that humans actively benefit from productive thought partnerships.


    Artificial Intelligence (AI) systems have historically been used as tools that
    execute narrowly defined tasks, such as identifying faces in images. Yet recent
    advances in AI have unlocked possibilities for a new class of models that genuinely
    collaborate with humans in complex reasoning, from conceptualizing problems to
    brainstorming solutions; for instance, discussing the pros and cons of medical
    treatments with doctors, or planning military interventions with policymakers.
    Such AI thought partners [\[3\]](#page-5-0) or AITPs, enable novel forms of collaboration
    and extended cognition, yet they also pose major risks—including and beyond risks
    of typical AI tools and agents [\[4\]](#page-5-1). Here, we propose a systematic
    framework for assessing risks of AITPs, offer metrics for the evaluation of these
    risks, and suggest potential strategies for risk mitigation.


    There is a pressing need to understand these risks as AITPs move rapidly from
    development to deployment. New ''reasoning'' models like Deepseek''s R1 or OpenAI''s
    o3, and new systems explicitly designed to collaborate in complex reasoning, such
    as Google''s ''coscientist'' [\[6\]](#page-5-2) increasingly enable impactful
    collaborative cognition—co-learning, co-creating, and co-planning—far beyond what
    is possible with other AI tools, including LLM agents [\[12\]](#page-5-3).


    ## **The RISc Framework**


    Our RISc framework categorizes possible risks from collaborative cognition with
    AITPs into three interacting levels: first, (R)eal-time risks arising from specific
    instances of human-AITP interactions; second, those arising from extended use
    by (I)ndividuals; and third, those arising when systems are deployed in (S)ociety
    in new kinds of collaborative (c)ognition. At each level, we separately consider
    risks inherent in an AITP''s performance (is the model performing appropriately?)
    and those that arise from how AITPs are utilized (is the model being used appropriately?),
    giving six risk categories (see Box 1).


    #### **Box 1: Risk Categories**


    Consider two impactful use cases for AITPs: medicine (a doctor uses an AITP to
    think through triage, diagnosis, and treatment planning) and policy (a policymaker
    uses an AITP to think through complex decisions during a financial crisis). We
    illustrate each risk category with one example from medicine and policy.


    ### **Real-time Risks**


    ### **Performance (generic deliberation)**


    Poorly-trained AITPs can encourage context-insensitive deliberation. *Medicine
    example:* A medical AITP may lead a doctor to consider globally prevalent but
    locally rare diseases, lowering efficiency of time-sensitive diagnosis.


    ### **Utilization (credit assignment)**


    Assigning praise and blame between users and AITPs is challenging when AITPs influence
    every stage of decisions. *Policy example:* Who is liable in a misdiagnosis if
    doctor and AITP deliberated together?


    ### **Individual Risks**


    ### **Performance (user manipulation)**


    An AITP can fail to benefit its user if it''s programmed to manipulate them. *Policy
    example:* An AITP may nudge a policymaker to author policies that benefit the
    AITP''s developer.


    ### **Utilization (cognitive atrophy)**


    Extensive use of AITPs can blunt critical reasoning skills. *Medicine example:*
    A doctor who always relies on an AITP may weaken their ability to form diagnostic
    hypotheses.


    ### **Societal Risks**


    ### **Performance (systemic fragility)**


    Centralization of AITP systems may induce fragility to blackouts or attacks. *Policy
    example:* Heavy reliance on AITPs in central banks could make monetary policy
    vulnerable to outages.


    ### **Utilization (homogeneity of thought)**


    Widespread AITP use may lead to convergence of opinion. *Policy example:* If all
    officials use the same AITP model, policies may fail to reflect local needs and
    knowledge.


    # **Evaluating Risks**


    To systematically assess the presence and extent of these risks in genuinely collaborative
    cognition in practice, we need to develop effective evaluations. Our framework
    both structures past work focused on assessing performance-level risks [\[1\]](#page-4-0)
    and highlights new evaluations for utilization risks at multiple levels.


    ### **Evaluating Real-time Risks**


    Real-time risks depend on the thought interplay between users and AITPs and influence
    a community of stakeholders. For instance, consider an AITP helping a doctor decide
    which patients to admit to limited beds in a pandemic. A good AITP can help delineate
    and weigh trade-offs through an open dialogue; but how can we evaluate whether
    the tradeoffs proposed, and the dialogue process itself, are ''good''? Such assessment
    requires considering the perspectives of multiple stakeholders—from the care team
    to the patients, their families, and the hospital and broader community writ large.
    The assessment of which tradeoffs are raised and how they are accounted for over
    the course of a conversation—over the thinking trace —could involve leveraging
    NLP methods or large language models to process the dialogue. Practitioners could
    specify what kinds of tradeoffs would be expected to be raised, which could be
    checked in the thinking trace; however, manual oversight of some form may be required—especially
    for evaluations that are not reducible to a single standardized "score," such
    as the model''s moral judgment.


    These same measures of assessing the thinking trace itself could be used to evaluate
    the relative contributions of any human(s) and any AITP(s). For instance, if a
    medical researcher engages with an AITP in the construction of a clinical trial
    protocol, we could automatically tag a saved trace of the interactions to appraise
    both the amount and type of thinking that each thought partner contributed to
    the collective "product of thought."


    ## **Evaluating Individual Risks**


    Such readouts could also be helpful in feeding back to the user, or supervisor
    of the user, to ensure that the user is contributing appropriately to the thought
    partnership and not overrelying on the model for ideas or insights. But what does
    it mean for users to over- or under-rely on a model when the engagements unfold
    over multiple interactions in this new kind of collaborative cognition? And how
    can we assess how user thinking may atrophy through overreliance rather than developing
    through reasoned interaction? This may require regular intervals of assessment
    with a professional, with and without access to an AITP. Such assessments should
    draw inspiration from existing work evaluating team dynamics and other measures
    of team efficacy when one or more group members is removed for some stage in the
    collaborative reasoning process [\[11\]](#page-5-4).


    Additionally, AITPs may require evaluation that looks at the user''s first-person
    experience in the reasoning process, rather than just the final output of collaborative
    thinking. Understanding how people interact with AITP in new forms of collaborative
    cognition requires interactive evaluation that engages with multiple steps in
    the reasoning process [\[2\]](#page-5-5). This could involve having the user give
    regular reports as to how they feel the interaction is progressing, which are
    later subject to qualitative and quantitative analysis. The expansion of AITPs
    in increasingly advanced realms of knowledge work, however, may increase the level
    of expertise required for external auditors in evaluation, especially in the space
    of natural language where outputs may look fluent but be incorrect. Financial
    incentives could be used to encourage experts who use AITPs to contribute to their
    development by providing feedback on their interactions, which can be used in
    further training.


    ### **Evaluating Societal Risks**


    Additionally, evaluation of the risks of thought partnerships requires measures
    that assess grouplevel or societal risks. Consider the impact on homogeneity of
    thought: how can we know whether our collective thinking and intellectual output
    has begun to grow more homogeneous? We encourage the construction of ongoing metrics
    that measure thought diversity or any narrowing in research or other activities
    that could, for instance, track patents or other research


    output [\[13\]](#page-5-6). Such evaluation also requires deep engagement across
    fields: economists have measures for societal productivity but these lack engagement
    with how cognitive processes shape ideas or decisions and hence economic activity.
    All of these evaluations, however, hinge on more societal conversations around
    what we want from these systems. How do we weigh gains in productivity, creative
    expression, or joy—when we may now be thinking with machines? It is also important
    to note potential inequities in societal AITP deployment: the people who get access
    to such models first are likely to be those who already have access to good human
    collaborators (for instance, those in leading universities or companies). People
    who may benefit most from AITPs—those who currently lack access to mentors or
    advisors—could fall further "behind."


    # **Mitigations**


    We next propose a series of potential mitigations to attenuate key risks.


    # **Mitigating Real-Time Risks**


    At the instantial level, a lot of the effort in LLM research has focused on improving
    performance, but for AITP we need access-modulating strategies for different stages
    in the thinking process that can prompt the system to stop or delegate to humans
    as soon as it is expected to fail. We can also imagine cases (such as recruitment)
    where we may restrict access to any thought partner, or ask for a record of the
    engagement in thought (for instance, all queries to an AITP and corresponding
    engagement). One mitigation that will be important is upskilling judgement. Courses
    teaching not just ideation, but critical judgement [\[5\]](#page-5-7) will therefore
    be needed in a world of AITPs. Decentralizing, personalising and detailed logging
    of AITP systems (extending, for instance, the Retrieval-Augmented Generation [\[14\]](#page-5-8)
    approach to metacognition, reasoning strategies, etc.) would mean that credit
    and responsibility could be better delineated in terms of intellectual property
    and the particular "engine" of thought used.


    # **Mitigating Individual Risks**


    At the individual level, a number of measures can reduce the risk of developers
    manipulating users who may trust and rely on their AITP for critical decision-making.
    These include education about what situations AITPs are well-suited for, about
    the level of control and incentives of the developers of the AI, as well as promoting
    competition and diversity among AI systems. Such education is especially important
    in the context of self-explanation, as being able to explain topics to oneself
    is key to learning [\[7\]](#page-5-9). Building in regular practices of solo thinking
    regarding one''s own engagement of AITPs is likely to grow ever more important
    to ensure that thought partnerships enhance deep understanding, not merely illusions
    of understanding [\[8\]](#page-5-10), and that thought partners are genuinely
    aligned with users'' representations and intentions, rather than merely converging
    with their judgments [\[9\]](#page-5-11).


    # **Mitigating Societal Risks**


    Finally, at the societal level, we urge the promotion of competition amongst AITP
    developers, to reduce the risk of widespread usage of a single AI system creating
    a monoculture of homogenous thought. One potential for mitigating fragility to
    blackouts or security lapses from widespread adoption of AITPs is to build more
    communities and spaces for human thought partnerships. Continuing to cultivate
    spaces for humans to engage, discuss ideas, plans and questions will help ensure
    that the new forms of collaborative cognition remain sufficiently grounded to


    the world and each other, and help us continue to develop our own capacity for
    collective deliberation.


    We emphasize that these are only some possible mitigations. We encourage more
    conversation around other mitigations, and continuing reflection on whether chosen
    mitigations appropriately adapt to the rapidly evolving demands of AITPs.


    #### **Trade-Offs**


    Any decisions about measures and mitigations come with tradeoffs. For example,
    good evaluation may require collecting more data on professionals; however, this
    may come at the cost of privacy, both to the individual and potentially the organization
    they work for and its users.


    Similarly, measuring and guarding against homogeneity of thought risks paradoxically
    controlling thought. If we were just considering a standard large language model,
    we may imagine simply modulating the temperature (variability) of the output.
    However, for an AITP, we may want to structurally vary the kinds of outputs and
    forms of engagement in a thinking process (e.g., shifting to play a "devil''s
    advocate" rather than "Socratic partner" in deliberation).


    Thus, mitigating the risks of AITPs is not simply a task of implementing a fixed
    set of recommendations. Instead, domain experts and key stakeholders must jointly
    evaluate the trade-offs involved in the various mitigations to craft, implement,
    and monitor mitigations.


    ## **Conclusion**


    Scholars have discussed the burdens and blessings of technologies that extend
    cognition for millenia [\[10\]](#page-5-12). The arrival of AITPs marks a turning
    point in this discussion: beyond tools that supplement specific capabilities (such
    as writing, which extends memory), AITPs can shape every aspect of reasoning—from
    ideation to planning—across many domains. Such transformative potential begets
    the responsibility to understand, evaluate, and mitigate risks. Our RISc framework
    facilitates this task by categorizing risks at three levels of analysis (the real-time,
    individual, and societal levels) into distinct classes (performance and utilization
    risks). Our analysis highlights a pressing need for interdisciplinary research
    on how to evaluate and mitigate utilization risks, and we highlight several concrete
    suggestions at each level. Our suggestions range from NLP-based evaluations of
    thinking traces to developing protocols that delegate reasoning to humans to balance
    the trade-off of performance and cognitive atrophy. These strategies can help
    humans develop genuinely beneficial thought partnerships with AI, and benefit
    from a historically unprecedented form of collaborative cognition.


    ### **Acknowledgments**


    We thank Umang Bhatt, Lujain Ibrahim, Kartik Chandra, and Lio Wong for valuable
    conversations that informed this work. KMC acknowledges support from King''s College
    Cambridge and the Cambridge Trust. AW acknowledges support from a Turing AI Fellowship
    under grant EP/V025279/1 and the Leverhulme Trust via CFI. This work is supported
    (in part) by ELSA - European Lighthouse on Secure and Safe AI funded by the European
    Union under grant agreement No. 101070617. Views and opinions expressed are however
    those of the author(s) only and do not necessarily reflect those of the European
    Union or European Commission.


    ### **References**


    <span id="page-4-0"></span>[1] A. Chan, R. Salganik, A. Markelius, C. Pang, N.
    Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll, M. Lin,
    A. Mayhew, K. Collins, M. Molamohammadi, J. Burden, W. Zhao, S. Rismani, K. Voudouris,
    U. Bhatt, A. Weller, D. Krueger, and T. Maharaj. Harms from Increasingly Agentic
    Algorithmic Systems. In *Proceedings of the 2023 ACM Conference on Fairness, Accountability,
    and Transparency*, FAccT ''23, pages 651–666, New York, NY, USA, June 2023. Association
    for Computing Machinery. ISBN 9798400701924. doi: 10.1145/ 3593013.3594033. URL
    <https://dl.acm.org/doi/10.1145/3593013.3594033>.


    - <span id="page-5-5"></span>[2] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong,
    M. Zilka, U. Bhatt, T. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, T. Gowers,
    W. Li, A. Weller, and M. Jamnik. Evaluating language models for mathematics through
    interactions. *Proceedings of the National Academy of Sciences*, 121(24):e2318124121,
    June 2024. doi: 10.1073/pnas.2318124121. URL [https://www.pnas.](https://www.pnas.org/doi/abs/10.1073/pnas.2318124121)
    [org/doi/abs/10.1073/pnas.2318124121](https://www.pnas.org/doi/abs/10.1073/pnas.2318124121).
    Publisher: Proceedings of the National Academy of Sciences.

    - <span id="page-5-0"></span>[3] K. M. Collins, I. Sucholutsky, U. Bhatt, K. Chandra,
    L. Wong, M. Lee, C. E. Zhang, T. Zhi-Xuan, M. Ho, V. Mansinghka, A. Weller, J.
    B. Tenenbaum, and T. L. Griffiths. Building machines that learn and think with
    people. *Nature Human Behaviour*, 8(10):1851–1863, Oct. 2024. ISSN 2397-3374.
    doi: 10.1038/s41562-024-01991-9. URL [https://www.nature.](https://www.nature.com/articles/s41562-024-01991-9)
    [com/articles/s41562-024-01991-9](https://www.nature.com/articles/s41562-024-01991-9).
    Publisher: Nature Publishing Group.

    - <span id="page-5-1"></span>[4] M. J. Crockett. Modern maxims for an AI oracle.
    *Nature Machine Intelligence*, 7(1):4–5, Jan. 2025. ISSN 2522-5839. doi: 10.1038/s42256-024-00970-z.
    URL [https://www.nature.](https://www.nature.com/articles/s42256-024-00970-z)
    [com/articles/s42256-024-00970-z](https://www.nature.com/articles/s42256-024-00970-z).
    Publisher: Nature Publishing Group.

    - <span id="page-5-7"></span>[5] M. A. Fischbach. Problem choice and decision
    trees in science and engineering. *Cell*, 187 (8):1828–1833, Apr. 2024. ISSN 1097-4172.
    doi: 10.1016/j.cell.2024.03.012.

    - <span id="page-5-2"></span>[6] J. Gottweis, W.-H. Weng, A. Daryin, T. Tu, A.
    Palepu, P. Sirkovic, A. Myaskovsky, F. Weissenberger, K. Rong, R. Tanno, K. Saab,
    D. Popovici, J. Blum, F. Zhang, K. Chou, A. Hassidim, B. Gokturk, A. Vahdat, P.
    Kohli, Y. Matias, A. Carroll, K. Kulkarni, N. Tomasev, Y. Guan, V. Dhillon, E.
    D. Vaishnav, B. Lee, T. R. D. Costa, J. R. Penadés, G. Peltz, Y. Xu, A. Pawlosky,
    A. Karthikesalingam, and V. Natarajan. Towards an AI co-scientist, Feb. 2025.
    URL <http://arxiv.org/abs/2502.18864>. arXiv:2502.18864 [cs].

    - <span id="page-5-9"></span>[7] T. Lombrozo. Learning by thinking in natural
    and artificial minds. *Trends in Cognitive Sciences*, 28(11):1011–1022, Nov. 2024.
    ISSN 1879-307X. doi: 10.1016/j.tics.2024.07.007.

    - <span id="page-5-10"></span>[8] L. Messeri and M. J. Crockett. Artificial intelligence
    and illusions of understanding in scientific research. *Nature*, 627(8002):49–58,
    Mar. 2024. ISSN 1476-4687. doi: 10.1038/s41586-024-07146-0. URL [https://www.nature.com/articles/](https://www.nature.com/articles/s41586-024-07146-0)
    [s41586-024-07146-0](https://www.nature.com/articles/s41586-024-07146-0). Publisher:
    Nature Publishing Group.

    - <span id="page-5-11"></span>[9] K. Oktar, I. Sucholutsky, T. Lombrozo, and T.
    L. Griffiths. Dimensions of disagreement: Divergence and misalignment in cognitive
    science and artificial intelligence. *Decision*, 11(4): 511–522, 2024. ISSN 2325-9973.
    doi: 10.1037/dec0000244. Place: US Publisher: Educational Publishing Foundation.

    - <span id="page-5-12"></span>[10] Plato. *Phaedrus*. Penguin Classics. ISBN 978-0-14-044974-7.

    - <span id="page-5-4"></span>[11] C. Riedl, Y. J. Kim, P. Gupta, T. W. Malone,
    and A. W. Woolley. Quantifying collective intelligence in human groups. *Proceedings
    of the National Academy of Sciences*, 118(21):e2005737118, May 2021. doi: 10.1073/pnas.2005737118.
    URL [https://www.pnas.org/doi/abs/10.](https://www.pnas.org/doi/abs/10.1073/pnas.2005737118)
    [1073/pnas.2005737118](https://www.pnas.org/doi/abs/10.1073/pnas.2005737118).
    Publisher: Proceedings of the National Academy of Sciences.

    - <span id="page-5-3"></span>[12] Y. Shavit, S. Agarwal, M. Brundage, S. Adler,
    C. O''Keefe, R. Campbell, T. Lee, P. Mishkin, T. Eloundou, A. Hickey, et al. Practices
    for governing agentic ai systems. *Research Paper, OpenAI*, 2023.

    - <span id="page-5-6"></span>[13] H. Youn, D. Strumsky, L. M. A. Bettencourt,
    and J. Lobo. Invention as a combinatorial process: evidence from US patents. *Journal
    of The Royal Society Interface*, 12(106):20150272, May 2015. doi: 10.1098/rsif.2015.0272.
    URL [https://royalsocietypublishing.](https://royalsocietypublishing.org/doi/10.1098/rsif.2015.0272)
    [org/doi/10.1098/rsif.2015.0272](https://royalsocietypublishing.org/doi/10.1098/rsif.2015.0272).
    Publisher: Royal Society.

    - <span id="page-5-8"></span>[14] Y. Zhou, Z. Liu, J. Jin, J.-Y. Nie, and Z. Dou.
    Metacognitive Retrieval-Augmented Large Language Models. In *Proceedings of the
    ACM Web Conference 2024*, WWW ''24, pages


    1453–1463, New York, NY, USA, May 2024. Association for Computing Machinery. ISBN
    9798400701719. doi: 10.1145/3589334.3645481. URL [https://dl.acm.org/doi/10.](https://dl.acm.org/doi/10.1145/3589334.3645481)
    [1145/3589334.3645481](https://dl.acm.org/doi/10.1145/3589334.3645481).'
- title: "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph\n  Embeddings"
  abstract: 'Uncertainty quantification in Knowledge Graph Embedding (KGE) methods
    is

    crucial for ensuring the reliability of downstream applications. A recent work

    applies conformal prediction to KGE methods, providing uncertainty estimates by

    generating a set of answers that is guaranteed to include the true answer with

    a predefined confidence level. However, existing methods provide probabilistic

    guarantees averaged over a reference set of queries and answers (marginal

    coverage guarantee). In high-stakes applications such as medical diagnosis, a

    stronger guarantee is often required: the predicted sets must provide

    consistent coverage per query (conditional coverage guarantee). We propose

    CondKGCP, a novel method that approximates predicate-conditional coverage

    guarantees while maintaining compact prediction sets. CondKGCP merges

    predicates with similar vector representations and augments calibration with

    rank information. We prove the theoretical guarantees and demonstrate empirical

    effectiveness of CondKGCP by comprehensive evaluations.'
  url: http://arxiv.org/abs/2505.16877v1
  keywords: ''
  document: '# Predicate-Conditional Conformalized Answer Sets for Knowledge Graph
    Embeddings


    Yuqicheng Zhu1,2, Daniel Hernández 1 , Yuan He 3 , Zifeng Ding 4 , Bo Xiong 5
    , Evgeny Kharlamov2,6, Steffen Staab1,7


    <sup>1</sup>University of Stuttgart, <sup>2</sup>Bosch Center for AI,


    <sup>3</sup>University of Oxford, <sup>4</sup>University of Cambridge, <sup>5</sup>Stanford
    University,


    <sup>6</sup>University of Oslo, <sup>7</sup>University of Southampton


    yuqicheng.zhu@de.bosch.com


    ### Abstract


    Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is crucial
    for ensuring the reliability of downstream applications. A recent work applies
    conformal prediction to KGE methods, providing uncertainty estimates by generating
    a set of answers that is guaranteed to include the true answer with a predefined
    confidence level. However, existing methods provide probabilistic guarantees averaged
    over a reference set of queries and answers (*marginal coverage guarantee*). In
    highstakes applications such as medical diagnosis, a stronger guarantee is often
    required: the predicted sets must provide consistent coverage per query (*conditional
    coverage guarantee*). We propose CONDKGCP, a novel method that approximates predicate-conditional
    coverage guarantees while maintaining compact prediction sets. CONDKGCP merges
    predicates with similar vector representations and augments calibration with rank
    information. We prove the theoretical guarantees and demonstrate empirical effectiveness
    of CONDKGCP by comprehensive evaluations.


    ### 1 Introduction


    Knowledge Graph Embeddings (KGE) encode entities and predicates as numerical vectors,
    enabling reasoning by exploiting similarities and analogies between entities and
    relations [\(Wang et al.](#page-10-0) , [2017](#page-10-0) ; [Biswas et al.](#page-9-0)
    , [2023\)](#page-9-0). While KGE methods have demonstrated effectiveness in various
    downstream tasks such as link prediction [\(Bordes et al.](#page-9-1) , [2013](#page-9-1)
    ; [Nickel et al.](#page-9-2) , [2011\)](#page-9-2) and question answering [\(Sax](#page-9-3)[ena
    et al.,](#page-9-3) [2020\)](#page-9-3), there remains uncertainty regarding the
    reliability of their predictions. Specifically, KGE models fail to identify when
    the answers to a query are uncertain [\(Zhu et al.](#page-10-1) , [2024a\)](#page-10-1).


    *Conformal prediction* is a framework to quantify uncertainty by providing a *prediction
    set*—a set of possible solutions for a given task—that is guaranteed to cover
    the ground truth solution with


    a predefined confidence level [\(Vovk et al.](#page-10-2) , [2005\)](#page-10-2).
    By assigning a score to each possible solution, the method defines a threshold
    to choose the minimum number of elements for the prediction set to provide the
    coverage guarantee. Thus, the size of the predicted set reflects the uncertainty
    of the predictions, with larger sets indicating higher uncertainty.


    Recently, [Zhu et al.](#page-10-3) [\(2025\)](#page-10-3) introduced a method,
    Conformalized Knowledge Graph Embedding (KGCP), which applies conformal prediction
    to quantify uncertainty in the predictions from KGE models. They show that KGCP
    provides *marginal coverage guarantees*, ensuring that the prediction sets meet
    the desired confidence level on average across all queries. However, predictive
    uncertainty may vary substantially across predicates, necessitating tailored coverage
    guarantees conditioned on predicates (*predicate-conditional coverage guarantees*).
    Such conditional guarantees are especially crucial for real-world applications
    where specific subgroups demand reliable uncertainty estimates. For instance,
    in a medical diagnosis system leveraging KGE, predicates like "contraindicated\_for"
    (indicating that a treatment is not recommended for certain patients) and "has\_symptom"
    (indicating that a specific disease or condition is associated with certain symptoms)
    may require different thresholds to achieve prediction sets with the desired confidence
    level. A shared threshold might fail to cover the true answer for the predicate
    "contraindicated\_for", as it is often associated with fewer triples and demands
    a higher threshold.


    Conditional coverage guarantee can be achieved by performing conformal prediction
    at the subgroup level [\(Vovk et al.,](#page-10-2) [2005\)](#page-10-2). However,
    the highly imbalanced distribution of triples across predicates in KGs [\(Xiong
    et al.](#page-10-4) , [2018\)](#page-10-4) poses challenges, often resulting in
    prediction sets that are either overly large or fail to cover the true answer
    [\(Ding et al.,](#page-9-4) [2024;](#page-9-4) [Shi et al.,](#page-9-5) [2024\)](#page-9-5).
    To address this limitation, we propose CONDKGCP, a method designed to approximate
    predicate-conditional guanratee while maintaining compact prediction sets. The
    key components of CONDKGCP are as follows: (1) it merges predicates with similar
    vector representation to increase the number of calibration triples available
    for reliable subgroup-level conformal prediction, and (2) it introduces a dual
    calibration schema that combines score calibration with rank calibration to exclude
    noisy answer entities, thereby reducing the size of prediction sets.


    We provide theoretical guarantees that COND-KGCP achieves conditional coverage
    probabilities tightly centered around the desired confidence level and that the
    dual calibration schema reduces expected prediction set sizes under certain conditions.
    Empirically, we demonstrate that CONDKGCP outperforms five baseline methods, achieving
    a superior trade-off between conditional coverage probability and prediction set
    size across commonly used benchmark datasets.


    ### 2 Related Work


    The majority of KGE methods aim to improve model performance by capturing relational
    patterns through more expressive embedding spaces, such as complex [\(Trouillon
    et al.,](#page-10-5) [2016;](#page-10-5) [Sun et al.,](#page-9-6) [2019\)](#page-9-6),
    hyperbolic [\(Xiong et al.,](#page-10-6) [2022\)](#page-10-6), or probabilistic
    spaces [\(He et al.,](#page-9-7) [2015\)](#page-9-7). By enabling richer representations,
    these methods have shown strong performance across downstream tasks including
    query answering [\(Ren et al.,](#page-9-8) [2020;](#page-9-8) [He et al.,](#page-9-9)
    [2024,](#page-9-9) [2025\)](#page-9-10), recommendation [\(Sun et al.,](#page-9-11)
    [2018\)](#page-9-11), and image classification [\(Zhou et al.,](#page-10-7) [2024\)](#page-10-7).
    Despite these successes, uncertainty quantification in KGE remains largely underexplored.
    Most uncertainty quantification methods for KGE calibrate the plausibility scores
    generated by the models [\(Tabacof and Costa](#page-9-12)[bello,](#page-9-12)
    [2020;](#page-9-12) [Safavi et al.,](#page-9-13) [2020\)](#page-9-13). However,
    these methods lack formal guarantees for the resulting probabilities. In contrast,
    [Zhu et al.](#page-10-3) [\(2025\)](#page-10-3) introduce an approach that provides
    formal statistical guarantees.


    It is well-established that no prediction interval can achieve a conditional coverage
    guarantee in a finite sample without additional assumptions about the data distribution
    [\(Vovk,](#page-10-8) [2012;](#page-10-8) [Foygel Barber](#page-9-14) [et al.,](#page-9-14)
    [2021\)](#page-9-14). Consequently, many of the existing works provide coverage
    guarantees conditioned on specific subgroups, such as class-conditional coverage
    guarantees [\(Ding et al.,](#page-9-4) [2024;](#page-9-4) [Shi et al.,](#page-9-5)
    [2024\)](#page-9-5). Two main strategies have been proposed to im-


    prove conditional coverage probabilities. The first involves modifying the nonconformity
    measure. For instance, [Romano et al.](#page-9-15) [\(2020\)](#page-9-15) enhance
    conditional coverage by defining cumulative probability of ground truth as nonconformity
    score, though their approach often results in larger prediction sets. To reduce
    the size of prediction sets, [Angelopoulos](#page-9-16) [et al.](#page-9-16) [\(2021\)](#page-9-16)
    introduce a regularization term in the nonconformity score. The second strategy
    leverages subgroup-level conformal prediction. [Vovk](#page-10-2) [et al.](#page-10-2)
    [\(2005\)](#page-10-2) propose Mondrian Conformal Prediction, which performs conformal
    prediction within specific subgroups. Building on this, [Ding et al.](#page-9-4)
    [\(2024\)](#page-9-4) cluster calibration points based on the distribution of
    nonconformity scores, balancing the trade-off between conditional coverage probability
    and the size of the prediction sets. Additionally, [Shi](#page-9-5) [et al.](#page-9-5)
    [\(2024\)](#page-9-5) further optimize prediction set sizes by incorporating rank
    information during the calibration step. However, these methods focus on classification,
    while our approach targets KGE-based link prediction, which is more challenging
    due to the large number of potential answers and the highly imbalanced triple
    distribution across predicates.


    # 3 Preliminaries


    #### 3.1 Notations


    Given two finite sets E and R whose elements are called *entities* and *predicates*,
    a *knowledge graph (KG)* is a subset of E ×R×E, whose elements are known as *triples*.
    A *query* q is either an expression of the form ⟨h, r, ?⟩ or ⟨?, r, t⟩, where
    h, t ∈ E, r ∈ R, and the question mark denotes the missing entity that we need
    to find. Given a query q, pred(q) is the predicate of the query, and tr(q, e)
    is the triple that results from assuming that e is an answer to the query. That
    is, pred(⟨?, r, t⟩) = r, pred(⟨h, r, ?⟩) = r, tr(⟨?, r, t⟩, e) = ⟨e, r, t⟩ and
    tr(⟨h, r, ?⟩, e) = ⟨h, r, e⟩.


    A *query-answer* set T is a finite set of pairs (q, e) where q is a query, e ∈
    E is an answer to the query. Abusing notation, given a triple tr , we write tr
    ∈ T if there is a pair (q, e) ∈ T such that tr(q, e) = tr . We use the names Ttr,
    Tneg, Tcal, and Ttest for the query-answer sets that are usually called *training
    set*, *negative triples set*, *calibration set*, and *test set*.


    KGE methods train KGE models M<sup>θ</sup> : E × R × E → R with parameters θ using
    a given training set Ttr sampled from a distribution P, whose elements are called
    *positive triples*, and a set Tneg of negative triples, disjoint with Ttr. The
    learned model assigns scores to triples indicating their plausibility. It gives
    higher scores to the positive triples, and lower scores to the negative triples
    [\(Bordes et al.,](#page-9-1) [2013;](#page-9-1) [Nickel et al.,](#page-9-2) [2011\)](#page-9-2).


    The performance of a KGE model is typically evaluated by the rank of answers in
    the test set. Given a pair (q, e) ∈ Ttest, the *rank* of answer e to query q predicted
    by Mθ, denoted rankM<sup>θ</sup> (q, e) is the size of the set of elements E ∋
    e ′ such that Mθ(tr(q, e′ )) ≥ Mθ(tr(q, e)). Smaller rank values indicate a better
    model performance.


    #### 3.2 Conformalized KGE


    Given a KGE model M<sup>θ</sup> trained on Ttr, a pair (q, e) ∈ Ttest, and a user-specified
    error rate ϵ ∈ [0, 1], KGCP [\(Zhu et al.,](#page-10-3) [2025\)](#page-10-3) provides
    a set of entities, which is guaranteed to contain e with a probability of at least
    1 − ϵ. In this section, we provide background on the method.


    A *nonconformity measure* S : E × R × E → R quantifies how unusual a triple is
    with respect to the training set. This measure is typically derived from a pre-trained
    KGE model, e.g., S(tr) = −Mθ(tr) [\(Zhu et al.,](#page-10-3) [2025\)](#page-10-3).
    Based on the nonconformity measure, the procedure of conformal prediction consists
    of two steps:


    Calibration Step: given a number τ ∈ [0, 1] and a finite set A ⊆ R, the τ -quantile
    of A, denoted quant(τ, A), is infimum of the set of elements a ∈ A such that |{b
    ∈ A : b ≤ a}|/|A| ≥ τ . Given a query-answer set T , the empirical *quantile of
    nonconformity scores* is:


    $$\hat{s}\_{\epsilon}(\mathcal{T}) = \text{quant}\left(\frac{\lceil (|\mathcal{T}|+1)(1-\epsilon)
    \rceil}{|\mathcal{T}|}, \mathcal{T}\right). \tag{1}$$


    Given the calibration set Tcal, for a target coverage 1 − ϵ, we obtain the corresponding
    empirical quantile of nonconformity scores, sˆϵ(Tcal).


    Set Construction Step: Given a threshold s, and a query q, we define the set Eq[S
    ≤ s] as follows:


    $$E\_q[S \le s] = \{ e \in E : S(\text{tr}(q, e)) \le s \}. \quad (2)$$


    The *prediction set* for a test query q, denoted Cˆ(q), is then constructed by
    including all answer entities that have nonconformity scores smaller than the
    threshold sˆϵ(Tcal):


    $$

    \mathring{C}(q) = E\_q[S \le \hat{s}\_\epsilon(\mathcal{T}\_{\text{cal}})].\tag{3}

    $$


    <span id="page-2-1"></span>Theorem 1 [\(Zhu et al.](#page-10-3) [\(2025\)](#page-10-3)).
    *Suppose the triples in* Ttr*,* Tcal *and* Ttest *are drawn independent and* *identically
    distributed (i.i.d) from the underlying distribution* P*. For every element* (q,
    e) ∈ Ttest*, the probability of* e *to being included in the prediction set of*
    q *satisfies the following bounds: (i)* P(e ∈ Cˆ(q)) ≥ 1 − ϵ*, and (ii) if there
    is no tie in the set of scores of the triples in* Tcal*, then* P(e ∈ Cˆ(q)) ≤
    1 − ϵ + 1 |Tcal|+1 *.*


    # 4 Conditional Conformal Prediction for Knowledge Graph Embedding (CONDKGCP)


    The goal of this paper is to approximate predicateconditional coverage guarantee.
    Given a pair (q, e) ∈ Ttest, and an arbitrary predicate r ∈ R, Equation [\(4\)](#page-2-0)
    defines the *predicate-conditional coverage guarantee*, which ensures that the
    true answer e is included in the prediction set of query q with a probability
    of at least 1 − ϵ.


    <span id="page-2-0"></span>

    $$\mathbb{P}(e \in \hat{C}(q) \mid \text{pred}(q) = r) \ge 1 - \epsilon \qquad
    (4)$$


    Equation [\(4\)](#page-2-0) does not necessarily hold for KGCP because the predictive
    uncertainty and the nonconformity score distribution can vary dramatically across
    predicates, which violates the i.i.d assumption in Theorem [1.](#page-2-1)


    To have the guarantee in Equation [\(4\)](#page-2-0), a method called Mondrian
    Conformal Prediction (MCP) [\(Vovk et al.,](#page-10-2) [2005\)](#page-10-2) performs
    conformal prediction separately for each predicate. Given a subset A ⊆ R, let
    Tcal[A] be the query-answer subset of Tcal such that tr ∈ Tcal[A] if and only
    if pred(tr ) ∈ A. The prediction set for the method MCP is defined as:


    $$

    \hat{C}\_{\text{MCP}}(q) = E\_q[S \le \hat{s}\_\epsilon(\mathcal{T}\_{\text{cal}}[\{r\}])].\qquad(5)

    $$


    However, it is well known that most predicates in KGs are associated with very
    few triples [\(Xiong](#page-10-4) [et al.,](#page-10-4) [2018\)](#page-10-4),
    resulting in small sets Tcal[{r}]. This leads to unstable thresholds sˆϵ(Tcal[{r}]),
    which in turn causes prediction sets to become overly large or fail to cover the
    ground truth.


    To address these issues, we propose: (1) merging predicates to increase the number
    of triples in the calibration set, and (2) augmenting the calibration process
    with rank information to reduce the size of prediction sets.


    #### <span id="page-2-2"></span>4.1 Predicate Merging


    To obtain a threshold sˆϵ(Tcal[{r}]) that reliably covers the true answer with
    desired probability, it is necessary to have a sufficiently large set Tcal[{r}]
    [\(Vovk et al.,](#page-10-2) [2005;](#page-10-2) [Ding et al.,](#page-9-4) [2024\)](#page-9-4).
    To increase Tcal[{r}], we merge predicates with highly similar vector representations.
    The rationale is that such predicates are likely to have similar distributions
    of nonconformity scores and, consequently, similar sˆϵ(Tcal[{r}]). Formally, we
    aim to partition the set R such that each part corresponds to a subset of Tcal
    whose triples share similar predicates and is large enough to determine a reliable
    threshold for constructing prediction sets. To define a set partition, we propose
    Algorithm [1,](#page-3-0) which first places all predicates with enough data into
    separate partitions and then assigns predicates with few data to the partition
    of the most similar predicate.


    #### <span id="page-3-0"></span>Algorithm 1 Predicate Set Partition


    Require: The set of predicates R, a natural number ϕ ≤ maxr∈<sup>R</sup> |Tcal[{r}]|,
    and a similarity function sim for pairs of predicates.


    $$\begin{array}{l} \mathsf{Ensure: A set partition } P \text{ of } R \text{ such
    that every} \\ \mathsf{part} \ A \in P \text{ satisfies } |\mathsf{T\_{\mathcal{L}al}}[A]|
    \ge \phi. \\ \mathsf{R\_{\mathcal{C}nough-data}} \leftarrow \{r \mid r \in R \text{
    and } |\mathsf{T\_{\mathcal{L}al}}[\{r\}]| \ge \phi\} \\ \mathsf{R\_{\mathcal{f}\text{ew-data}}}
    \leftarrow \{r \mid r \in R \text{ and } |\mathsf{T\_{\mathcal{L}al}}[\{r\}]|
    < \phi\} \\ \mathsf{part}(r) \leftarrow \{r\}, \text{ for each } r \in R\_{\mathsf{enough-data}}
    \\ \mathsf{for } r'' \in R\_{\mathsf{f}\text{ew-data}} \,\mathsf{do} \\ r \leftarrow
    \mathsf{argmax}\_{r'' \in R\_{\mathsf{enough-data}}} \,\mathsf{sim}(r'', r'')
    \\ \mathsf{part}(r) \leftarrow \mathsf{part}(r) \cup \{r''\} \\ \mathsf{end for}
    \\ P \leftarrow \{\mathsf{part}(r) \mid r \in R\_{\mathsf{enough-data}}\} \end{array}$$


    Given a part g of the partition P defined by Algorithm [1,](#page-3-0) let T<sup>g</sup>
    be the subset of Tcal consisting of all triples whose predicates belong to g.


    In this work, we use negative Manhattan distance as the similarity measure. Let
    (x)<sup>i</sup> denote the i-th dimension of the vector (x)<sup>i</sup> representation
    of a predicate x, and d denote the number of dimensions. The similarity function
    is defined as


    $$\text{sim}(a, b) = -\sum\_{i=1}^{d} |(a)\_i - (b)\_i|. \qquad (6)$$


    #### <span id="page-3-1"></span>4.2 Dual Calibration Schema


    Prediction sets tend to be larger when conformal prediction is performed at the
    subgroup level due to the reduced number of calibration triples available for
    each subgroup compared to KGCP. To address this, and drawing inspiration from
    the recent work of [Shi et al.](#page-9-5) [\(2024\)](#page-9-5), we reduce the
    size of prediction sets by constructing prediction sets using a dual calibration
    schema that combines score calibration and rank calibration.


    Given a query q with predicate belonging to g, the prediction set generated by
    CONDKGCP is:


    $$\begin{aligned} \mathring{C}\_{\text{CondKGCP}}(q) &= \\ \left\{ e \in E\_q[S
    \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g)] : \\ & \text{rank}\_{M\theta}(q,
    e) \le \hat{k}(g) \right\}. \end{aligned} \tag{7}$$


    It depends on two parameters, namely, the *score threshold* sˆ<sup>ϵ</sup> ′(g)
    (Tg) and the *rank threshold* ˆk(g), which we will define in the remainder of
    this section.


    Rank Calibration. Recall that rankM<sup>θ</sup> (q, e) is the rank of answer e
    given query q. We define the *miscoverage error of top-*k *prediction set* for
    the part g ∈ P, denote ϵ k g , as follows:


    $$

    \epsilon\_g^k = \mathbb{P}(\text{rank}\_{M\_\theta}(q, e) > k \mid \text{pred}(q)
    \in g) \quad (8)

    $$


    The rank threshold ˆk(g) is selected such that ϵ kˆ(g) <sup>g</sup> < ϵ to satisfy
    the coverage guarantee. However, achieving a smaller ϵ kˆ(g) <sup>g</sup> requires
    a larger ˆk(g), which leads to larger prediction sets. To minimize the size of
    the prediction sets, we choose


    $$\hat{k}(g) = \min\left\{ k : \epsilon\_g^k < \epsilon \right\}.\tag{9}$$


    Score Calibration. We further apply a score threshold sˆ<sup>ϵ</sup> ′(g) (Tg)
    for the entities that are ranked within top-ˆk(g), where ϵ ′ (g) = ϵ − γϵ kˆ(g)
    <sup>g</sup> and γ is a hyperparameter.


    Intuitively, the rank threshold ˆk(g) filters out answer entities with large rank
    positions (high rankM<sup>θ</sup> (q, e)), ensuring that CONDKGCP performs score
    thresholding only on a subset of reliable test triples [\(Shi et al.,](#page-9-5)
    [2024\)](#page-9-5). The hyperparameter γ balances the trade-off between the conditional
    coverage guarantee and the size of prediction sets (see a detailed explanation
    in the next section).


    ### 5 Coverage & Size Guarantees


    In this section, we will show the conditional coverage guarantee and size reduction
    guarantee of CONDKGCP. All proofs are in Appendix [A.](#page-11-0)


    <span id="page-3-2"></span>Proposition 1 (Conditional Coverage Guarantee). *Let*
    q *be a query,* e *be its answer entity and* p *be the conditional coverage probability
    of* CONDKGCP p = P(e ∈ CˆCondKGCP(q) | pred(q) ∈ g)*. Given*


    *a user-specified error rate* ϵ *and a* γ ∈ [0, 1]*, we have the following bounds
    for all parts* g ∈ P*:*


    $$p \ge 1 - \epsilon - (1 - \gamma)\epsilon\_g^{\hat{k}(g)},\qquad(10)$$


    *and if there is no tie in the set of nonconformity scores of the triples in*
    Tg*, then*


    $$p \le 1 - \epsilon + \gamma \epsilon\_g^{\hat{k}(g)} + \frac{1}{|\mathcal{T}\_g|
    + 1}.\qquad(11)$$


    This proposition shows within each part g, the conditional coverage probability
    is close to 1 − ϵ with small controlled deviations. The deviation is governed
    by two "slack" terms: (1) the miscoverage error of rank calibration ϵ kˆ(g) <sup>g</sup>
    and (2) a finite-sample correction term <sup>1</sup> |Tg|+1 to handle ties. Both
    terms are very small, ϵ kˆ(g) <sup>g</sup> is guaranteed to be smaller than ϵ
    by the way we select ˆk(g) in Section [4.2;](#page-3-1) 1 |Tg|+1 is also guaranteed
    to be smaller than 1 <sup>ϕ</sup>+1 since we make sure that every part has at
    least ϕ triples in Algorithm [1.](#page-3-0)


    Note that γ does not affect the width of the coverage bounds but controls their
    asymmetry: a larger γ allows more deviation on the lower bound, while a smaller
    γ does so for the upper bound. Furthermore, γ influences the construction of the
    prediction sets by adjusting the score threshold via ϵ ′ (g): A larger γ reduces
    ϵ ′ (g), raising the threshold and yielding larger prediction sets, whereas a
    smaller γ results in smaller prediction sets. Thus, γ essentially deals with the
    trade-off between conditional coverage probability and the size of prediction
    sets. Corollary 1 [\(Shi et al.](#page-9-5) [\(2024\)](#page-9-5)). *Suppose*
    ϵ ′ (g) *and* ˆk(g) *satisfy both following conditions*


    $$\hat{k}(g) \in \left\{ k : \epsilon\_g^{\hat{k}(g)} < \epsilon \right\} ; 0
    \le \epsilon''(g) \le \epsilon - \epsilon\_g^{\hat{k}(g)}, \tag{12}$$


    *the rank calibration guarantee to shrink the prediction sets, if for a query*
    q *and any* e ′ ∈ E*:*


    $$\mathbb{P}\_q\left(S(tr(q,e'')) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g),
    \qquad (13)$$


    $$\text{rank}\_{M\_\theta}(q,e'') \le \hat{k}(g)\right)$$


    $$\le \mathbb{P}\_q\left(S(tr(q,e'')) \le \hat{s}\_\epsilon(\mathcal{T}\_g)\right)$$


    Intuitively, the dual calibration schema tends to include less answer entities
    with high rank from KGE models thus reduce the size of prediction sets [\(Shi
    et al.,](#page-9-5) [2024\)](#page-9-5). The corollary demonstrate it is true
    in theory under the condition of Equation [\(13\)](#page-4-0). We empirically
    verify the condition on benchmark datasets, the results in Appendix [C](#page-14-0)
    show the practical utility of this corollary.


    ### 6 Experiment


    We evaluate the CONDKGCP empirically and demonstrate its effectiveness in balancing
    the tradeoff between predicate-conditional coverage probability and the size of
    the prediction sets.


    #### 6.1 Experimental Setup


    Training KGE Models. We trained our KGE models using the LibKGE framework [\(Broscheit](#page-9-17)
    [et al.,](#page-9-17) [2020\)](#page-9-17), following the hyperparameter search
    strategy described by [Ruffinelli et al.](#page-9-18) [\(2019\)](#page-9-18).
    All experiments were conducted on a Linux machine equipped with a 40GB NVIDIA
    A100 SXM4 GPU.


    Datasets. We consider two widely-used benchmark datasets: WN18 and FB15k [\(Bordes
    et al.,](#page-9-1) [2013\)](#page-9-1). We follow [Zhu et al.](#page-10-3) [\(2025,](#page-10-3)
    Appendix D.1) and do not consider their modified counterparts, WN18RR [\(Dettmers
    et al.,](#page-9-19) [2018\)](#page-9-19) and FB15k-237 [\(Toutanova and Chen,](#page-9-20)
    [2015\)](#page-9-20) since they are not suitable for evaluating uncertainty quantification.


    Baselines. We consider following methods as baselines: (1) KGCP [\(Zhu et al.,](#page-10-3)
    [2025\)](#page-10-3); (2) MCP [\(Vovk et al.,](#page-10-2) [2005\)](#page-10-2)
    which performs conformal prediction at the predicate-level; (3) CLUSTERCP [\(Ding
    et al.,](#page-9-4) [2024\)](#page-9-4), which clusters predicates based on similarity
    of score distribution and then conducts conformal prediction at the cluster level;
    (4) APS [\(Romano et al.,](#page-9-15) [2020\)](#page-9-15) and RAPS [\(Angelopou](#page-9-16)[los
    et al.,](#page-9-16) [2021\)](#page-9-16), which modify the nonconformity measure
    to provide improved conditional coverage probabilities (see details in Appendix
    [E\)](#page-14-1). Unless otherwise specified, we use the default nonconformity
    measure, SOFTMAX, as proposed by [Zhu](#page-10-3) [et al.](#page-10-3) [\(2025\)](#page-10-3)
    (see the definition in Appendix [D\)](#page-14-2).


    <span id="page-4-0"></span>Hyperparameter Tuning. The validation set serves as
    the calibration set for all baselines. For CLUSTERCP, RAPS and CONDKGCP, we randomly
    sample triples from the training set (of the same sizes as the calibration set)
    to determine optimal hyperparameter settings. We follow hyperparameter search
    strategy from the original papers for CLUSTERCP and RAPS. For COND-KGCP, we tune
    γ ∈ [0.01, 0.1, 0.5] and ϕ ∈ [20, 50, 100, 200]. The best hyperparameters are
    reported in Table [7.](#page-16-0)


    #### 6.2 Evaluation Setup


    We set the target coverage probability 1 − ϵ = 0.9 by default, following [Zhu
    et al.](#page-10-3) [\(2025\)](#page-10-3). For each KGE method-dataset pair,
    we train the model 10 times, each time using a different random seed, and


    <span id="page-5-0"></span>


    |          |           | WN18        |                 |          |          |           |
    FB15k       |                |        |

    |----------|-----------|-------------|-----------------|----------|----------|-----------|-------------|----------------|--------|

    | Model    | Methods   | CovGap ↓    | AveSize ↓       | EF ↓     | Model    |
    Methods   | CovGap ↓    | AveSize ↓      | EF ↓   |

    |          | KGCP      | 0.096±0.002 | 132.36±6.88     | –        |          |
    KGCP      | 0.131±0.001 | 373.83±2.08    | –      |

    |          | MCP       | 0.017±0.001 | 713.50±180.91   | 73.56    |          |
    MCP       | 0.021±0.000 | 583.09±10.09   | 19.02  |

    |          | CLUSTERCP | 0.073±0.007 | 117.77±8.12     | -6.34    |          |
    CLUSTERCP | 0.130±0.000 | 379.95±2.36    | 61.20  |

    | TransE   | APS       | 0.108±0.001 | 11428.73±817.03 | –        | TransE   |
    APS       | 0.154±0.001 | 1922.92±25.04  | –      |

    |          | RAPS      | 0.069±0.001 | 42.03±1.03      | –        |          |
    RAPS      | 0.124±0.001 | 336.46±1.10    | -53.39 |

    |          | CONDKGCP  | 0.030±0.001 | 19.56±0.14      | -17.09   |          |
    CONDKGCP  | 0.027±0.000 | 78.12±1.23     | -28.43 |

    |          | KGCP      | 0.076±0.001 | 2.13±0.25       | –        |          |
    KGCP      | 0.113±0.001 | 139.57±3.44    | –      |

    |          | MCP       | 0.022±0.003 | 1193.81±382.91  | 220.68   |          |
    MCP       | 0.023±0.000 | 633.68±9.80    | 54.90  |

    |          | CLUSTERCP | 0.056±0.001 | 1.81±0.40       | -0.16    |          |
    CLUSTERCP | 0.113±0.001 | 141.56±3.52    | –      |

    | RotatE   | APS       | 0.083±0.002 | 15963.85±261.07 | –        | RotatE   |
    APS       | 0.128±0.001 | 1757.13±18.59  | –      |

    |          | RAPS      | 0.078±0.001 | 81.96±2.13      | –        |          |
    RAPS      | 0.122±0.000 | 416.41±3.46    | –      |

    |          | CONDKGCP  | 0.045±0.001 | 2.20±0.61       | 0.02     |          |
    CONDKGCP  | 0.063±0.000 | 246.80±2.46    | 21.45  |

    |          | KGCP      | 0.103±0.004 | 2.61±0.15       | –        |          |
    KGCP      | 0.092±0.000 | 62.10±0.54     | –      |

    |          | MCP       | 0.019±0.003 | 508.71±118.00   | 60.25    |          |
    MCP       | 0.020±0.001 | 740.60±30.77   | 94.24  |

    |          | CLUSTERCP | 0.089±0.009 | 3.29±0.60       | 0.49     |          |
    CLUSTERCP | 0.091±0.001 | 62.25±0.52     | 1.50   |

    | RESCAL   | APS       | 0.106±0.002 | 1298.86±109.82  | –        | RESCAL   |
    APS       | 0.085±0.002 | 369.19±21.92   | 483.70 |

    |          | RAPS      | 0.074±0.001 | 43.76±0.46      | 14.19    |          |
    RAPS      | 0.122±0.000 | 393.44±1.12    | –      |

    |          | CONDKGCP  | 0.061±0.001 | 3.80±0.53       | 0.28     |          |
    CONDKGCP  | 0.025±0.000 | 107.91±0.56    | 6.84   |

    |          | KGCP      | 0.066±0.001 | 2.30±0.05       | –        | DistMult |
    KGCP      | 0.103±0.001 | 25.16±0.12     | –      |

    |          | MCP       | 0.022±0.002 | 655.33±135.46   | 148.42   |          |
    MCP       | 0.023±0.001 | 668.83±10.05   | 80.46  |

    |          | CLUSTERCP | 0.066±0.001 | 2.32±0.05       | –        |          |
    CLUSTERCP | 0.103±0.001 | 25.60±0.16     | –      |

    | DistMult | APS       | 0.043±0.002 | 204.67±39.44    | 87.99    |          |
    APS       | 0.063±0.000 | 84.82±1.37     | 14.92  |

    |          | RAPS      | 0.065±0.003 | 51.59±1.04      | 492.90   |          |
    RAPS      | 0.124±0.000 | 365.76±0.75    | –      |

    |          | CONDKGCP  | 0.037±0.001 | 6.18±0.08       | 1.34     |          |
    CONDKGCP  | 0.024±0.000 | 66.27±0.96     | 5.20   |

    |          | KGCP      | 0.072±0.001 | 1.07±0.01       | –        |          |
    KGCP      | 0.088±0.001 | 34.99±0.88     | –      |

    |          | MCP       | 0.023±0.002 | 1898.69±226.32  | 387.27   |          |
    MCP       | 0.024±0.002 | 664.43±19.46   | 98.35  |

    |          | CLUSTERCP | 0.072±0.001 | 1.07±0.01       | –        |          |
    CLUSTERCP | 0.088±0.001 | 34.88±0.85     | –      |

    | ComplEx  | APS       | 0.065±0.004 | 15669.60±498.74 | 22383.61 | ComplEx  |
    APS       | 0.054±0.001 | 177.94±10.72   | 42.04  |

    |          | RAPS      | 0.074±0.004 | 63.82±2.43      | –        |          |
    RAPS      | 0.121±0.000 | 417.50±3.41    | –      |

    |          | CONDKGCP  | 0.049±0.002 | 1.39±0.01       | 0.14     |          |
    CONDKGCP  | 0.026±0.000 | 166.10±5.23    | 21.15  |

    | ConvE    | KGCP      | 0.066±0.004 | 1.71±0.04       | –        | ConvE    |
    KGCP      | 0.102±0.001 | 91.02±7.06     | –      |

    |          | MCP       | 0.019±0.002 | 576.97±170.39   | 122.40   |          |
    MCP       | 0.023±0.000 | 725.08±38.15   | 80.26  |

    |          | CLUSTERCP | 0.066±0.001 | 1.72±0.04       | –        |          |
    CLUSTERCP | 0.102±0.001 | 88.33±7.06     | –      |

    |          | APS       | 0.071±0.004 | 9.85±1.77       | –        |          |
    APS       | 0.110±0.003 | 4578.40±243.77 | –      |

    |          | RAPS      | 0.068±0.002 | 47.75±1.44      | –        |          |
    RAPS      | 0.123±0.001 | 400.08±4.54    | –      |

    |          | CONDKGCP  | 0.038±0.001 | 4.80±0.10       | 1.10     |          |
    CONDKGCP  | 0.032±0.000 | 429.14±3.11    | 48.30  |


    Table 1: Overall performance comparison of CONDKGCP and baseline methods across
    six KGE models and two benchmark datasets (WN18 and FB15k). We report CovGap and
    AveSize as the mean ± standard deviation over 10 independent trials. The EF (efficient
    rate) is reported as a mean value; its standard deviation is omitted as it is
    negligible. The best and second-best EF values for each model-dataset pair are
    highlighted in bold and underline, respectively. KGCP is shown in italic to indicate
    that it serves as a baseline without conditional coverage guarantees. Our proposed
    method, CONDKGCP, is highlighted with a gray background. "–" in the EF column
    denotes a failure case where either CovGap is not reduced or AveSize does not
    change relative to KGCP.


    report the mean and standard deviation.


    Evaluation Metrics. Following [Ding et al.](#page-9-4) [\(2024\)](#page-9-4),
    we evaluate the performance using two metrics: coverage gap (CovGap) and average
    size of the prediction sets (AveSize).


    Given a set of test triples Ttest, the empirical predicate-conditional coverage
    for each predicate r ∈ R, denoted as Covr, is calculated as:


    $$\text{Cov}\_r = \frac{1}{|\mathcal{T}\_{\text{test}}[\{r\}]|} \sum\_{(q,e) \in
    \mathcal{T}\_{\text{test}}[\{r\}]} \mathbb{1}[e \in \hat{C}(q)]. \tag{14}$$


    The average predicate-conditional coverage gap (CovGap) measures how far the empirical
    coverage is from the desired coverage level 1 − ϵ:


    $$\text{CovGap} = \frac{1}{|R|} \sum\_{r \in R} |\text{Cov}\_r - (1 - \epsilon)|
    \qquad (15)$$


    The average size of the prediction sets (AveSize) is computed as


    $$\frac{1}{\mathcal{T}\_{\text{test}}[\{r\}]} \sum\_{(q,e)\in \mathcal{T}\_{\text{test}}[\{r\}]}
    |\mathring{C}(q)|. \qquad (16)$$


    Note that CovGap and AveSize are inherently competing metrics in conformal prediction
    [\(An](#page-9-16)[gelopoulos et al.,](#page-9-16) [2021\)](#page-9-16); reducing
    CovGap often increases AveSize. For a given CovGap, smaller AveSize is preferred
    for more informative estimates. Rather than optimizing either metric in isolation,


    our aim is to balance the trade-off between coverage probability and prediction
    set size. To quantify this trade-off, we introduce an auxiliary metric *Efficiency
    Rate* (ER): the number of additional entities required (relative to KGCP) to reduce
    CovGap by 0.01:


    $$\frac{\text{AvgSize} - \text{AvgSize}^\*}{\text{CovGap}^\* - \text{CovGap}}
    \times 0.01,\qquad(17)$$


    where (AvgSize<sup>∗</sup> , CovGap<sup>∗</sup> ) are the corresponding values
    for KGCP.


    #### 6.3 Results and Discussion


    #### 6.3.1 Overall Comparison


    Table [1](#page-5-0) presents a comprehensive comparison of CONDKGCP with baseline
    methods across six KGE models and two benchmark datasets (WN18 and FB15k). Overall,
    CONDKGCP consistently demonstrates the most favorable trade-off between coverage
    and prediction set size, as evidenced by its lowest EF scores in the majority
    of cases.


    Among the methods, MCP achieves the lowest CovGap, aligning with Proposition 4.6
    in [Vovk](#page-10-2) [et al.](#page-10-2) [\(2005\)](#page-10-2). Although CovGap
    is not exactly zero due to the small number of triples for certain subgroups,
    this deviation is minimal. Thus, MCP''s CovGap can be viewed as the empirical
    lower bound. However, this improved coverage precision comes at a significant
    cost: MCP produces substantially larger prediction sets, resulting in high AveSize.
    In contrast, KGCP is designed to achieve only marginal coverage guarantees, resulting
    in higher CovGap. However, this enables KGCP to generate the smallest prediction
    sets in most cases, yielding the lowest AveSize empirically. We observe that CONDKGCP
    achieves CovGap values that are consistently closest to the empirical lower bound,
    while maintaining compact prediction sets–often with AveSize values closest to
    the empirical lower bound-compared to other baseline methods across all evaluated
    KGE methods and datasets.


    Note that while CLUSTERCP occasionally achieves lower AveSize values comparable
    to the empirical lower bound, it fails to reduce CovGap in these cases. In fact,
    both its CovGap and AveSize values remain very close to those of KGCP, the baseline
    method with marginal coverage guarantees (i.e., calibrated on triples across all
    predicates), indicating that CLUSTERCP fails to cluster predicates into meaningful
    groups. This limitation arises


    <span id="page-6-0"></span>![](_page_6_Figure_8.jpeg)


    Figure 1: Comparison of methods across varying target coverage levels, showing
    CovGap (top plot) and Ave-Size (bottom plot) for RESCAL on WN18. Complete results
    are provided in Tables [3](#page-18-0) and [4](#page-19-0) in the Appendix.


    because CLUSTERCP is designed for subgroups with similar data points, making it
    unsuitable for our setting, where the distribution of triples across predicates
    is highly imbalanced.


    Methods that modify the nonconformity score, such as APS and RAPS, often generate
    overly conservative prediction sets due to prioritization on difficult regions
    for coverage. As shown in Table [1,](#page-5-0) neither APS nor RAPS significantly
    improve CovGap and frequently generate large prediction sets, suggesting the nonconformity
    measures might not be well-suited for KGE methods.


    ### 6.3.2 Comparison across Different Coverage Levels


    To evaluate performance under varying coverage levels, we conduct experiments
    for different target coverage probability 1 − ϵ ∈ [0.8, 0.85, 0.9, 0.95]. The
    results, shown in Figure [1,](#page-6-0) reveal that COND-KGCP achieves CovGap
    values closest to the empirical lower bound (MCP) while maintaining Ave-Size comparable
    to the empirical lower bound (KGCP) across all coverage levels.


    #### 6.3.3 Impact of Hyperparameters


    CONDKGCP includes two key hyperparameters: ϕ, which controls the granularity of
    predicate sub-


    <span id="page-7-1"></span>


    |          |             | WN18        |                |        |          |             |
    FB15k       |             |        |

    |----------|-------------|-------------|----------------|--------|----------|-------------|-------------|-------------|--------|

    | Model    | Method      | CovGap ↓    | AveSize ↓      | EF ↓   | Model    |
    Method      | CovGap ↓    | AveSize ↓   | EF ↓   |

    |          | w/o Merge   | 0.097±0.002 | 139.54±6.50    | –      |          |
    w/o Merge   | 0.131±0.002 | 371.89±4.11 | –      |

    | TransE   | w/o RankCal | 0.019±0.001 | 629.13±129.55  | 791.79 | TransE   |
    w/o RankCal | 0.022±0.000 | 98.24±2.14  | -25.28 |

    |          | CONDKGCP    | 0.030±0.001 | 19.56±0.14     | -17.09 |          |
    CONDKGCP    | 0.027±0.000 | 78.12±1.23  | -28.43 |

    |          | w/o Merge   | 0.077±0.002 | 1.98±0.54      | –      |          |
    w/o Merge   | 0.114±0.001 | 141.79±1.32 | –      |

    | RotatE   | w/o RankCal | 0.023±0.001 | 1781.28±230.30 | 335.69 | RotatE   |
    w/o RankCal | 0.043±0.000 | 370.29±3.55 | 32.96  |

    |          | CONDKGCP    | 0.045±0.001 | 2.20±0.61      | 0.02   |          |
    CONDKGCP    | 0.063±0.000 | 246.80±2.46 | 21.45  |

    |          | w/o Merge   | 0.105±0.003 | 2.57±0.40      | –      |          |
    w/o Merge   | 0.090±0.002 | 62.55±0.25  | 2.25   |

    | RESCAL   | w/o RankCal | 0.021±0.000 | 385.33±63.64   | 46.67  | RESCAL   |
    w/o RankCal | 0.020±0.000 | 134.22±0.88 | 10.02  |

    |          | CONDKGCP    | 0.061±0.001 | 3.80±0.53      | 0.28   |          |
    CONDKGCP    | 0.025±0.000 | 107.91±0.56 | 6.84   |

    |          | w/o Merge   | 0.066±0.001 | 3.39±0.04      | –      |          |
    w/o Merge   | 0.103±0.002 | 25.82±0.21  | –      |

    | DistMult | w/o RankCal | 0.023±0.000 | 719.51±111.22  | 166.79 | DistMult |
    w/o RankCal | 0.023±0.000 | 98.00±1.22  | 9.11   |

    |          | CONDKGCP    | 0.037±0.001 | 6.18±0.08      | 1.34   |          |
    CONDKGCP    | 0.024±0.000 | 66.27±0.96  | 5.20   |

    |          | w/o Merge   | 0.074±0.002 | 1.07±0.01      | –      |          |
    w/o Merge   | 0.086±0.001 | 56.49±0.44  | 107.50 |

    | ComplEx  | w/o RankCal | 0.025±0.000 | 2313.94±260.99 | 492.10 | ComplEx  |
    w/o RankCal | 0.026±0.000 | 168.50±5.10 | 21.53  |

    |          | CONDKGCP    | 0.049±0.002 | 1.39±0.01      | 0.14   |          |
    CONDKGCP    | 0.026±0.000 | 166.10±5.23 | 21.15  |

    |          | w/o Merge   | 0.066±0.001 | 2.74±0.05      | –      |          |
    w/o Merge   | 0.099±0.002 | 180.79±2.79 | 299.23 |

    | ConvE    | w/o RankCal | 0.021±0.000 | 575.14±89.71   | 127.43 | ConvE    |
    w/o RankCal | 0.027±0.000 | 579.14±5.03 | 65.08  |

    |          | CONDKGCP    | 0.038±0.001 | 4.80±0.10      | 1.10   |          |
    CONDKGCP    | 0.032±0.000 | 429.14±3.11 | 48.30  |


    Table 2: Ablation Study of CONDKGCP. Each model is evaluated under three configurations:
    without predicate merging procedure (w/o Merge), without rank calibration (w/o
    RankCal), and the proposed CONDKGCP (full method).


    grouping in the merging procedure, and γ, which balances the conditional coverage
    guarantee and the size of prediction sets in the dual calibration schema.


    As shown in Figure [2,](#page-7-0) larger values of ϕ result in increased CovGap
    and reduced AveSize. This is expected, as coarser subgrouping theoretically results
    in behavior more similar to KGCP, whereas finer subgrouping aligns more closely
    with MCP.


    For γ, larger values are associated with increased AveSize, consistent with the
    analysis in section 5. However, the empirical results reveal a key practical insight:
    adjusting γ to sacrifice a small amount in the lower bound of conditional coverage
    can significantly reduce AveSize while causing only a negligible change in CovGap.
    This demonstrates the necessity of including γ in the design of COND-KGCP.


    #### 6.3.4 Impact of Nonconformity Measure


    Subgroup-based methods (MCP, CLUSTERCP, CONDKGCP) can be combined with nonconformity
    score-based methods (APS, RAPS). However, as shown in Table [9](#page-22-0) in
    the Appendix, these combinations do not result in a better trade-off. Notably,
    CONDKGCP achieves low CovGap values with significantly smaller prediction sets
    in most cases compared to MCP and CLUSTERCP, regardless of the choice of the nonconformity
    measure. This observation demonstrates that the improve-


    <span id="page-7-0"></span>![](_page_7_Figure_7.jpeg)


    Figure 2: Influence of hyperparameters ϕ and γ on Cov-Gap (top) and AveSize (bottom)
    for RESCAL on WN18. Complete results for all model-dataset combinations are provided
    in Tables [5](#page-20-0) and [6](#page-21-0) in the Appendix.


    ments offered by CONDKGCP are robust to the change of the nonconformity measure.


    #### 6.3.5 Impact of Key Components


    We compare CONDKGCP with two variants: CONDKGCP without predicate merging procedure
    (w/o Merge) and CONDKGCP without rank calibration (w/o RankCal). The results in
    Table [2](#page-7-1) show that CONDKGCP outperforms both variants in balancing
    CovGap and AveSize, achieving the lowest EF and thus the most efficient trade-off.
    Concretely, CONDKGCP achieves lower CovGap compared to w/o Merge with a comparable
    AveSize. While it shows slightly higher CovGap than w/o RankCal, it maintains
    smaller AveSize. This demonstrates the contribution of each component: the merging
    process effectively reduces CovGap, while rank calibration ensures more compact
    prediction set.


    ### 7 Discussion and Conclusion


    In this paper, we introduces CONDKGCP, a novel method that addresses the limitations
    of existing conformalized KGE method by approximating predicate-conditional coverage
    guarantees while maintaining compact prediction sets. We theoretically prove that
    the deviation from the desired confidence level is bounded and empirically demonstrate
    the effectiveness of CONDKGCP across six KGE methods and two benchmark datasets.


    Our method offers a useful uncertainty quantification tool for high-stakes applications,
    such as medical diagnosis, and can be easily adapted to quantify uncertainty under
    other types of conditions, such as entity-type. Additionally, it can be seamlessly
    extended to other tasks, including embedding-based query answering [\(Ren et al.,](#page-9-8)
    [2020\)](#page-9-8) and probabilistic reasoning over KG [\(Zhu](#page-10-9) [et
    al.,](#page-10-9) [2023,](#page-10-9) [2024b\)](#page-10-10).


    ### 8 Limitation


    A potential limitation of the proposed COND-KGCP lies in the probabilistic guarantees
    provided by Proposition [1,](#page-3-2) which rely on the assumption of i.i.d.
    (or weaker exchangeability [\(Vovk et al.,](#page-10-2) [2005\)](#page-10-2))
    data, as well as the assumption that the similarity of vector representations
    corresponds to the similarity of the distribution of nonconformity scores. While
    the i.i.d. assumption may occasionally be violated in certain real-world applications,
    it is a common simplification in statistical methods. As a step forward, we are
    working on extending our approach to handle covariate shift, where only the input
    distribution changes while the conditional distribution remains unchanged.


    Regarding the similarity assumption, the effectiveness of the predicate merging
    step, as demonstrated in the ablation study (Table [2\)](#page-7-1), indicates
    that this assumption is reasonable for KGE methods. Nonetheless, future work could
    explore incorporating additional features, such as the semantic meaning of predicates,
    to further enhance the merging process and improve robustness in diverse scenarios.


    # 9 Acknowledgments


    The authors thank the International Max Planck Research School for Intelligent
    Systems (IMPRS-IS) for supporting Yuqicheng Zhu. The work was partially supported
    by EU Projects Graph Massivizer (GA 101093202), enRichMyData (GA 101070284) and
    SMARTY (GA 101140087), as well as the Deutsche Forschungsgemeinschaft (DFG, German
    Research Foundation) – SFB 1574 – 471687386. Zifeng Ding receives funding from
    the European Research Council (ERC) under the European Union''s Horizon 2020 Research
    and Innovation programme grant AVeriTeC (Grant agreement No. 865958).


    ### References


    - <span id="page-9-16"></span>Anastasios Nikolas Angelopoulos, Stephen Bates,
    Michael I. Jordan, and Jitendra Malik. 2021. Uncertainty sets for image classifiers
    using conformal prediction. In *ICLR*. OpenReview.net.

    - <span id="page-9-22"></span>James Bergstra and Yoshua Bengio. 2012. Random search
    for hyper-parameter optimization. *Journal of machine learning research*, 13(2).

    - <span id="page-9-0"></span>Russa Biswas, Lucie-Aimée Kaffee, Michael Cochez,
    Stefania Dumbrava, Theis E Jendal, Matteo Lissandrini, Vanessa Lopez, Eneldo Loza
    Mencía, Heiko Paulheim, Harald Sack, et al. 2023. Knowledge graph embeddings:
    open challenges and opportunities. *Transactions on Graph Data and Knowledge*,
    1(1):4–1.

    - <span id="page-9-1"></span>Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
    Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling
    multirelational data. *Advances in neural information processing systems*, 26.

    - <span id="page-9-17"></span>Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek,
    Patrick Betz, and Rainer Gemulla. 2020. LibKGE - A knowledge graph embedding library
    for reproducible research. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing: System Demonstrations*, pages 165–174.

    - <span id="page-9-19"></span>Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
    and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 32.

    - <span id="page-9-4"></span>Tiffany Ding, Anastasios Angelopoulos, Stephen Bates,
    Michael Jordan, and Ryan J Tibshirani. 2024. Classconditional conformal prediction
    with many classes. *Advances in Neural Information Processing Systems*, 36.

    - <span id="page-9-14"></span>Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas,
    and Ryan J Tibshirani. 2021. The limits of distribution-free conditional predictive
    inference. *Information and Inference: A Journal of the IMA*, 10(2):455–482.

    - <span id="page-9-7"></span>Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015.
    Learning to represent knowledge graphs with gaussian embedding. In *Proceedings
    of the 24th ACM international on conference on information and knowledge management*,
    pages 623–632.

    - <span id="page-9-9"></span>Yunjie He, Daniel Hernandez, Mojtaba Nayyeri, Bo
    Xiong, Yuqicheng Zhu, Evgeny Kharlamov, and Steffen Staab. 2024. Generating sroi<sup>−</sup>
    ontologies via knowledge graph query embedding learning. In *Proceeding of 27th
    European Conference on Artificial Intelligence*, pages 4279 – 4286.

    - <span id="page-9-10"></span>Yunjie He, Bo Xiong, Daniel Hernández, Yuqicheng
    Zhu, Evgeny Kharlamov, and Steffen Staab. 2025. Dage: Dag query answering via
    relational combinator with logical constraints. In *Proceedings of the ACM on
    Web Conference 2025*, pages 2514–2529.

    - <span id="page-9-21"></span>Jing Lei, Max G''Sell, Alessandro Rinaldo, Ryan
    J Tibshirani, and Larry Wasserman. 2018. Distributionfree predictive inference
    for regression. *Journal of the American Statistical Association*, 113(523):1094–
    1111.

    - <span id="page-9-2"></span>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel.
    2011. A three-way model for collective learning on multi-relational data. In *ICML*,
    pages 809–816. Omnipress.

    - <span id="page-9-23"></span>OpenAI. 2024. Chatgpt(3.5)[large language model].
    <https://chat.openai.com>.

    - <span id="page-9-8"></span>H Ren, W Hu, and J Leskovec. 2020. Query2box: Reasoning
    over knowledge graphs in vector space using box embeddings. In *International
    Conference on Learning Representations (ICLR)*.

    - <span id="page-9-15"></span>Yaniv Romano, Matteo Sesia, and Emmanuel Candes.
    2020. Classification with valid and adaptive coverage. *Advances in Neural Information
    Processing Systems*, 33:3581–3591.

    - <span id="page-9-18"></span>Daniel Ruffinelli, Samuel Broscheit, and Rainer
    Gemulla. 2019. You can teach an old dog new tricks! on training knowledge graph
    embeddings. In *International Conference on Learning Representations*.

    - <span id="page-9-13"></span>Tara Safavi, Danai Koutra, and Edgar Meij. 2020.
    Evaluating the calibration of knowledge graph embeddings for trustworthy link
    prediction. In *EMNLP (1)*, pages 8308–8321. Association for Computational Linguistics.

    - <span id="page-9-3"></span>Apoorv Saxena, Aditay Tripathi, and Partha Talukdar.
    2020. Improving multi-hop question answering over knowledge graphs using knowledge
    base embeddings. In *Proceedings of the 58th annual meeting of the association
    for computational linguistics*, pages 4498– 4507.

    - <span id="page-9-5"></span>Yuanjie Shi, Subhankar Ghosh, Taha Belkhouja, Janardhan
    Rao Doppa, and Yan Yan. 2024. Conformal prediction for class-wise coverage via
    augmented label rank calibration. In *NeurIPS*.

    - <span id="page-9-6"></span>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian
    Tang. 2019. Rotate: Knowledge graph embedding by relational rotation in complex
    space. In *ICLR (Poster)*. OpenReview.net.

    - <span id="page-9-11"></span>Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon,
    Long-Kai Huang, and Chi Xu. 2018. Recurrent knowledge graph embedding for effective
    recommendation. In *Proceedings of the 12th ACM conference on recommender systems*,
    pages 297–305.

    - <span id="page-9-12"></span>Pedro Tabacof and Luca Costabello. 2020. Probability
    calibration for knowledge graph embedding models. In *ICLR*. OpenReview.net.

    - <span id="page-9-20"></span>Kristina Toutanova and Danqi Chen. 2015. Observed
    versus latent features for knowledge base and text inference. In *Proceedings
    of the 3rd workshop on continuous vector space models and their compositionality*,
    pages 57–66.

    - <span id="page-10-5"></span>Théo Trouillon, Johannes Welbl, Sebastian Riedel,
    Éric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link
    prediction. In *International conference on machine learning*, pages 2071– 2080.
    PMLR.

    - <span id="page-10-8"></span>Vladimir Vovk. 2012. Conditional validity of inductive
    conformal predictors. In *Asian conference on machine learning*, pages 475–490.
    PMLR.

    - <span id="page-10-2"></span>Vladimir Vovk, Alexander Gammerman, and Glenn Shafer.
    2005. *Algorithmic learning in a random world*, volume 29. Springer.

    - <span id="page-10-0"></span>Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017.
    Knowledge graph embedding: A survey of approaches and applications. *IEEE transactions
    on knowledge and data engineering*, 29(12):2724–2743.

    - <span id="page-10-6"></span>Bo Xiong, Shichao Zhu, Mojtaba Nayyeri, Chengjin
    Xu, Shirui Pan, Chuan Zhou, and Steffen Staab. 2022. Ultrahyperbolic knowledge
    graph embeddings. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*, pages 2130–2139.

    - <span id="page-10-4"></span>Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,
    and William Yang Wang. 2018. One-shot relational learning for knowledge graphs.
    In *EMNLP*, pages 1980–1990. Association for Computational Linguistics.

    - <span id="page-10-11"></span>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
    Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference
    in knowledge bases. In *ICLR (Poster)*.

    - <span id="page-10-7"></span>Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka,
    Stefan Schmid, Yuqicheng Zhu, Bo Xiong, and Steffen Staab. 2024. Visual representation
    learning guided by multi-modal prior knowledge. *arXiv preprint arXiv:2410.15981*.

    - <span id="page-10-1"></span>Yuqicheng Zhu, Nico Potyka, Mojtaba Nayyeri, Bo
    Xiong, Yunjie He, Evgeny Kharlamov, and Steffen Staab. 2024a. Predictive multiplicity
    of knowledge graph embeddings in link prediction. In *Findings of the Association
    for Computational Linguistics: EMNLP 2024*, pages 334–354.

    - <span id="page-10-3"></span>Yuqicheng Zhu, Nico Potyka, Jiarong Pan, Bo Xiong,
    Yunjie He, Evgeny Kharlamov, and Steffen Staab. 2025. Conformalized answer set
    prediction for knowledge graph embedding. In *Proceedings of the 2025 Conference
    of the Nations of the Americas Chapter of the Association for Computational Linguistics:
    Human Language Technologies (Volume 1: Long Papers)*, pages 731–750. Association
    for Computational Linguistics.

    - <span id="page-10-10"></span>Yuqicheng Zhu, Nico Potyka, Bo Xiong, Trung-Kien
    Tran, Mojtaba Nayyeri, Evgeny Kharlamov, and Steffen Staab. 2024b. Approximating
    probabilistic inference in statistical el with knowledge graph embeddings. *arXiv
    preprint arXiv:2407.11821*.


    <span id="page-10-9"></span>Yuqicheng Zhu, Nico Potyka, Bo Xiong, Trung-Kien Tran,
    Mojtaba Nayyeri, Steffen Staab, and Evgeny Kharlamov. 2023. Towards statistical
    reasoning with ontology embeddings. In *ISWC (Posters/Demos/Industry)*, volume
    3632 of *CEUR Workshop Proceedings*. CEUR-WS.org.


    ### <span id="page-11-0"></span>A Proofs


    Proposition 1 (Conditional Coverage Guarantee). *Let* q *be a query and* e *be
    its answer entity. Given a user-specified error rate* ϵ *and a* γ ∈ [0, 1]*, we
    have the following bounds for all parts* g ∈ P*:*


    $$\mathbb{P}(e \in \hat{C}\_{\text{CondKGCP}}(q) \mid \text{pred}(q) \in g) \ge
    1 - \epsilon - (1 - \gamma)\epsilon\_g^{\hat{k}(g)},\tag{18}$$


    *and if there is no tie in the set of nonconformity scores of the triples in*
    Tg*, then*


    $$\mathbb{P}(e \in \hat{C}\_{\text{CondKGCP}}(q) \mid \text{pred}(q) \in g) \le
    1 - \epsilon + \gamma \epsilon\_g^{\hat{k}(g)} + \frac{1}{|\mathcal{T}\_g| + 1}.
    \tag{19}$$


    *Proof of the lower bound.* We prove the lower bound similar to the proof of Theorem
    4.1 in [Shi et al.](#page-9-5) [\(2024,](#page-9-5) Appendix A.1). We first conduct
    conformal prediction for each part g ∈ P (which can be viewed as applying MCP
    where each subgroup contains triples whose predicates are in a predicate set generated
    by Algorithm [1\)](#page-3-0) with the adjust error rate ϵ ′ (g). The prediction
    set is defined as follows:


    $$

    \hat{C}\_{\rm MCP}(q) = E\_q[S \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g)] \tag{20}

    $$


    According to [\(Vovk et al.,](#page-10-2) [2005,](#page-10-2) Proposition 4.6),
    for a query q, we have


    $$\mathbb{P}(e \in \hat{C}\_{\text{MCP}^\*}(q) \mid \text{pred}(q) \in g) \ge
    1 - \epsilon'' \tag{21}$$


    Suppose that the rank threshold for part g is ˆk(g) and ϵ kˆ(g) <sup>g</sup> is
    its corresponding miscoverage error of top-ˆk(g) prediction sets, we have


    $$\begin{split} &\mathbb{P}(e \in \hat{C}\_{\text{MCP}^\*}(q) \mid \text{pred}(q)
    \in g) \\ &= \mathbb{P}\Big(S(tr(q,e)) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_{\mathcal{G}})
    \mid \text{pred}(q) \in g\Big) \\ &= \mathbb{P}\Big(S(tr(q,e)) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_{\mathcal{G}}),
    \text{rank}\_{M\theta}(q,e) \le \hat{k}(g) \mid \text{pred}(q) \in g\Big) \\ &\quad
    + \mathbb{P}\Big(S(tr(q,e)) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_{\mathcal{G}}),
    \text{rank}\_{M\theta}(q,e) > \hat{k}(g) \mid \text{pred}(q) \in g\Big) \\ &\le
    \mathbb{P}\Big(S(tr(q,e)) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_{\mathcal{G}}),
    \text{rank}\_{M\theta}(q,e) \le \hat{k}(g) \mid \text{pred}(q) \in g\Big) \\ &\quad
    + \underbrace{\mathbb{P}(\text{rank}\_{M\theta}(q,e) > \hat{k}(g) \mid \text{pred}(q)
    \in g)}\_{\hat{k}(g)} \end{split}$$


    By definition of the prediction set constructed by CONDKGCP, we have


    $$\mathbb{P}(e \in \hat{C}\_{\text{CondKGCP}}(q) \mid \text{pred}(q) \in g) \ge
    1 - \epsilon''(g) - \epsilon\_g^{\hat{k}(g)} \tag{22}$$


    In our paper we set ϵ ′ (g) = ϵ − γ · ϵ kˆ(g) <sup>g</sup> , therefore, we have


    $$\mathbb{P}(e \in \mathring{C}\_{\text{CondKGCP}}(q) \mid \text{pred}(q) \in
    g) \ge 1 - \epsilon - (1 - \gamma)\epsilon\_g^{\hat{k}(g)} \tag{23}$$


    *Proof of the upper bound.* We prove the upper bound based on [Lei et al.](#page-9-21)
    [\(2018,](#page-9-21) Appendix A.1). By assuming no ties in the set of nonconformity
    scores of the triples in Tg, denoted as S(Tg), we know that the nonconformity
    scores in <sup>S</sup>(Tg) are all distinct with probability one. The set <sup>C</sup>ˆMCP\*
    (q) is equivalent to the set of all answer entity e ′ ∈ E such that the nonconformity
    score S(tr(q, e′ )) ranks among the ⌈(|Tg| + 1)(1 − ϵ ′ (g))⌉ smallest of S(Tg).


    Consider now the complementary set <sup>D</sup>ˆMCP\* (q) consisting of answer
    entities <sup>e</sup> ′ ∈ E such that the nonconformity score S(tr(q, e′ )) is
    among the ⌈(|Tg| + 1)ϵ ′ (g) − 1⌉ largest. Under i.i.d assumption (or a weaker
    exchangeability assumption), the joint distribution of the nonconformity scores
    S(Tg) is invariant under permutations. As a result, the ranks of the nonconformity
    scores in S(Tg) are uniformly distributed among {1, 2, . . . , |Tg|} and hence
    we can derive the following lower bound for each part g ∈ P:


    $$\mathbb{P}(e \in \hat{D}\_{\text{MCP}^\*}(q) \mid \text{pred}(q) \in g) \ge
    \epsilon''(g) - \frac{1}{|\mathcal{T}\_g| + 1},\tag{24}$$


    We also know that there is no intersected elements for <sup>C</sup>ˆMCP\* (q)
    and <sup>D</sup>ˆMCP\* (q)


    $$

    \hat{C}\_{\rm MCP^\*}(q) \cap \hat{D}\_{\rm MCP^\*}(q) = \emptyset \tag{25}

    $$


    Then we can derive the upper bound for each part g as follows:


    $$\begin{split} &\mathbb{P}(e \in \hat{C}\_{\text{MCP}^{\bullet}}(q) \mid \text{pred}(q)
    \in g) + \mathbb{P}(e \in \hat{D}\_{\text{MCP}^{\bullet}}(q) \mid \text{pred}(q)
    \in g) \leq 1 \\ &\Rightarrow \mathbb{P}(e \in \hat{C}\_{\text{MCP}^{\bullet}}(q)
    \mid \text{pred}(q) \in g) \leq 1 - \mathbb{P}(e \in \hat{D}\_{\text{MCP}^{\bullet}}(q)
    \mid \text{pred}(q) \in g) \\ &\Rightarrow \mathbb{P}(e \in \hat{C}\_{\text{MCP}^{\bullet}}(q)
    \mid \text{pred}(q) \in g) \leq 1 - \epsilon''(g) + \frac{1}{|\mathcal{T}\_{g}|
    + 1} \\ &\Rightarrow \mathbb{P}(e \in \hat{C}\_{\text{MCP}^{\bullet}}(q) \mid
    \text{pred}(q) \in g) \leq 1 - \epsilon + \gamma \epsilon\_{g}^{\hat{k}(g)} +
    \frac{1}{|\mathcal{Z}^{g}| + 1} \end{split}$$


    Based on the definition of CONDKGCP, we have


    $$\begin{split} &\mathbb{P}(e \in \hat{C}\_{\text{MCP}^{\bullet}}(q) \mid \text{pred}(q)
    \in g) \\ &= \underbrace{\mathbb{P}\Big{(}S(tr(q,e)) \le \hat{s}\_{\ell''(g)}(T\_{g}),
    \text{rank}\_{M\_{\theta}}(q,e) \le \hat{k}(g) \mid \text{pred}(q) \in g)}\_{\mathbb{P}(e
    \in \hat{C}\_{\text{CondKGCP}}(q) \mid \text{pred}(q) \in g)} \\ &+ \mathbb{P}\Big{(}S(tr(q,e))
    \le \hat{s}\_{\ell''(g)}(T\_{g}), \text{rank}\_{M\_{\theta}}(q,e) > \hat{k}(g)
    \mid \text{pred}(q) \in g) \\ &\Rightarrow \mathbb{P}(e \in \hat{C}\_{\text{CondKGCP}}(q)
    \mid \text{pred}(q) \in g) \le \mathbb{P}(e \in \hat{C}\_{\text{MCCP}^{\bullet}}(q)
    \mid \text{pred}(q) \in g) \\ &\Rightarrow \mathbb{P}(e \in \hat{C}\_{\text{CondKGCP}}(q)
    \mid \text{pred}(q) \in g) \le 1 - \epsilon + \gamma \epsilon\_{g}^{\hat{k}(g)}
    + \frac{1}{|\mathsf{T}\_{g}| + 1} \end{split}$$


    <span id="page-12-0"></span>


    Corollary 2 [\(Shi et al.](#page-9-5) [\(2024\)](#page-9-5)). *Suppose* ϵ ′ (g)
    *and* ˆk(g) *satisfy*


    $$

    \hat{k}(g) \in \{ k : \epsilon\_g^{\hat{k}(g)} < \epsilon \}; 0 \le \epsilon''(g)
    \le \epsilon - \epsilon\_g^{\hat{k}(g)}, \tag{26}

    $$


    *the rank calibration guarantee to shrink the prediction sets, if for a query*
    q *and any* e ′ ∈ E*:*


    $$\mathbb{P}\_q\left(S(tr(q,e'')) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g),
    \text{rank}\_{M\theta}(q,e'') \le \hat{k}(g)\right) \le \mathbb{P}\_q\left(S(tr(q,e''))
    \le \hat{s}\_\epsilon(\mathcal{T}\_g)\right) \tag{27}$$


    *Proof.* We can prove this Corollary based on [Shi et al.](#page-9-5) [\(2024,](#page-9-5)
    Appendix A.2).Define the following fraction:


    $$\sigma\_{g} = \frac{\mathbb{P}\_{q}\Big(S(tr(q, e'')) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_{g}),
    \text{rank}\_{M\_{\theta}}(q, e'') \le \hat{k}(g)\Big)}{\mathbb{P}\_{q}(S(tr(q,
    e'')) \le \hat{s}\_{\epsilon}(\mathcal{T}\_{g})}.\tag{28}$$


    By the assumption in Equation [\(27\)](#page-12-0), it follows that σ<sup>g</sup>
    ≤ 1. The expected size of the prediction set for CONDKGCP is given by:


    $$\mathbb{E}\_q\left[|\mathring{C}\_{\text{CondKGCP}}(q)|\right] = \mathbb{E}\_q\left[\sum\_{e''
    \in E} \mathbbm{1}\left[S(tr(q, e'')) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g),
    \text{rank}\_{M\_\theta}(q, e'') \le \hat{k}(g)\right]\right] \tag{29}$$


    $$=\sum\_{e'' \in E} \mathbb{E}\_q \left[ \mathbb{1} \left[ S(tr(q, e'')) \le
    \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g), \text{rank}\_{M\_\theta}(q, e'') \le
    \hat{k}(g) \right] \right] \tag{30}$$


    $$=\sum\_{e'' \in E} \mathbb{P}\_q\left(S(tr(q, e'')) \le \hat{s}\_{\epsilon''(g)}(\mathcal{T}\_g),
    \text{rank}\_{M\theta}(q, e'') \le \hat{k}(g)\right) \tag{31}$$


    By the definition of σ<sup>g</sup> and the assumption σ<sup>g</sup> ≤ 1, we obtain:


    $$\mathbb{E}\_q\left[\left|\hat{C}\_{\text{CondKGCP}}(q)\right|\right] = \sum\_{e''
    \in E} \sigma\_g \cdot \mathbb{P}\_q\left(S(tr(q, e'')) \le \hat{s}\_\epsilon(\mathcal{T}\_g)\right)
    \tag{32}$$


    $$\leq \sum\_{e'' \in E} \mathbb{E}\_q \left[ \mathbb{1} \left[ S(tr(q, e''))
    \leq \hat{s}\_\epsilon(\mathcal{T}\_g) \right] \right] \tag{33}$$


    $$=\mathbb{E}\_q\left[\sum\_{e'' \in E} \mathbb{1}\left[S(tr(q,e'')) \le \hat{s}\_\epsilon(\mathcal{T}\_g)\right]\right]
    \tag{34}$$


    $$= \mathbb{E}\_q \left[ |\hat{C}\_{\text{MCP}^\*} (q)| \right] \tag{35}$$


    Therefore, we conclude that adding rank calibration always reduces the prediction
    set size, provided that the condition in Equation [\(27\)](#page-12-0) holds.


    ### B Discussion About Merging Process


    Note the underlying idea of predicate merging procedure in section [4.1](#page-2-2)
    is essentially very similar to CLUSTERCP proposed by recent work [\(Ding et al.,](#page-9-4)
    [2024\)](#page-9-4), where subgroups are clustered based on similarity of the
    nonconformity score distribution. But in our case, we have extremely imbalanced
    size of calibration data for each predicate. For many predicates, the number of
    calibration triples is too small to capture the characteristics of score distribution
    with quantile vectors proposed in [Ding et al.](#page-9-4) [\(2024\)](#page-9-4),
    thus resulting unreliable clustering and limited improvement in terms of conditional
    coverage probability. We can see in Table [1](#page-5-0) that CLUSTERCP fails
    to provide meaningful fine-grained predicate clustering, reflected by very similar
    performance as KGCP (baseline method that is not designed to achieve conditional
    coverage guarantees).


    # <span id="page-14-0"></span>C Verification of the Condition in Equation [\(13\)](#page-4-0)


    As we prove in Appendix [A,](#page-11-0) CONDKGCP always has smaller expected
    prediction set size compared to CONDKGCP without rank calibration, i.e., MCP at
    set part level under the condition in Equation [\(13\)](#page-4-0). In this section,
    we empirically verify the condition in Equation [\(13\)](#page-4-0) across all
    model-dataset combinations.


    We use two metrics to verify the condition. Condition Satisfaction Rate (CSR)
    quantifies how often the condition σ<sup>g</sup> ≤ 1 holds for all g ∈ P:


    $$CSR := \sum\_{g \in P} \mathbb{1}\left[\sigma\_g \le 1\right];\tag{36}$$


    And σ¯ computes the average value of σ<sup>g</sup> for all g ∈ P, denoted as σ¯:


    $$\bar{\sigma} := \frac{1}{|P|} \sum\_{g \in P} \sigma\_g. \tag{37}$$


    The results of these two metrics for all modeldataset combinations are reported
    in Table [3.](#page-14-3) The results show that the condition σ<sup>g</sup> ≤
    1 holds for nearly all g ∈ P under different model-dataset combinations. This
    provides empirical evidence that the additional rank calibration schema reduces
    the size of prediction sets. Moreover, we observe that smaller values of σ¯ are
    typically associated with smaller AveSize in in Table [1.](#page-5-0)


    <span id="page-14-3"></span>


    |          | WN18    |       | FB15k   |       |  |

    |----------|---------|-------|---------|-------|--|

    | Model    | CSR (%) | σ¯    | CSR (%) | σ¯    |  |

    | TransE   | 91.7    | 0.823 | 98.7    | 0.633 |  |

    | RotatE   | 100     | 0.422 | 97.6    | 0.899 |  |

    | RESCAL   | 100     | 0.554 | 99.3    | 0.712 |  |

    | DistMult | 91.7    | 0.783 | 99.4    | 0.512 |  |

    | ComplEx  | 100     | 0.401 | 96.4    | 0.703 |  |

    | ConvE    | 100     | 0.555 | 93.1    | 0.951 |  |


    |  |  | Table 3: Verification of Condition in Equation (13). |  |  |  |

    |--|--|------------------------------------------------------|--|--|--|

    |--|--|------------------------------------------------------|--|--|--|


    ### <span id="page-14-2"></span>D SOFTMAX Nonconformity Score


    By default we use SOFTMAX nonconformity score defined in [Zhu et al.](#page-10-3)
    [\(2025\)](#page-10-3):


    $$S(tr(q,e)) = 1 - \hat{M}\_\theta(tr(q,e)),$$


    where


    $$

    \hat{M}\_\theta(tr(q,e)) = \frac{\exp(M\_\theta(tr(q,e)))}{\sum\_{e'' \in E} \exp\left(M\_\theta(q,e'')\right)}.

    $$


    ### <span id="page-14-1"></span>E APS and RAPS Nonconformity Score


    Adaptive Predication Sets (APS) [\(Romano et al.,](#page-9-15) [2020\)](#page-9-15)
    improves the conditional coverage probability by modifying the nonconformity measure.
    Specifically, given a query q, for all e ∈ E, we first normalize the plausibility
    score using softmax function:


    $$

    \hat{M}\_\theta(tr(q,e)) = \frac{\exp(M\_\theta(tr(q,e)))}{\sum\_{e'' \in E} \exp\left(M\_\theta(q,e'')\right)}.

    $$


    Then we sort the normalized scores such that 1 ≥ Mˆ (1) ≥ · · · ≥ Mˆ (|E|) , where
    M(k) denotes the k-th largest plausibility score. Recall that rankM<sup>θ</sup>
    (q, e) denotes the rank of e given query q. The nonconformity score of APS is
    then defined as


    $$\begin{aligned} S(tr(q,e)) &= \\ \sum\_{i=1}^{\text{rank}\_{M\_\theta}(q,e)-1}
    \hat{M}\_{\{i\}} + U \cdot \hat{M}\_{\{\text{rank}\_{M\_\theta}(q,e)\}}, \end{aligned}$$


    where U ∈ [0, 1] is a uniform random variable.


    The regularized version - RAPS additionally includes a rank-based regularization
    term to the nonconformity score.


    $$\begin{aligned} S(tr(q,e)) &= \\ \sum\_{i=1}^{\text{rank}\_{M\_\theta}(q,e)-1}
    \hat{M}\_{(i)} + U \cdot \hat{M}\_{\{\text{rank}\_{M\_\theta}(q,e)\}} \\ &+ \lambda
    \cdot \max\{\text{rank}\_{M\_\theta}(q,e) - k\_{reg}, 0\}, \end{aligned}$$


    where λ and kreg are two hyper-parameters.


    <span id="page-15-0"></span>


    |           | #Entity | #Relation | #Training | #Validation | #Test  |

    |-----------|---------|-----------|-----------|-------------|--------|

    | WN18      | 40,943  | 18        | 141,442   | 5,000       | 5,000  |

    | WN18RR    | 40,943  | 11        | 86,835    | 3,034       | 3,134  |

    | FB15k     | 14,951  | 1,345     | 483,142   | 50,000      | 59,071 |

    | FB15k-237 | 14,541  | 237       | 272,115   | 17,535      | 20,466 |


    Table 4: Statistics of benchmark datasets for link prediction task.


    <span id="page-15-1"></span>


    |                                  | Scoring Function s(< h, r, t >) |

    |----------------------------------|---------------------------------|

    | TransE (Bordes et al., 2013)     | −  h + r − t  1/2               |

    | RotatE (Sun et al., 2019)        | −  h ◦ r − t  p                 |

    | RESCAL (Nickel et al., 2011)     | TMrt<br>h                       |

    | DistMult (Yang et al., 2015)     | T diag(r)t<br>h                 |

    | ComplEx (Trouillon et al., 2016) | T diag(r)t)<br>Re(h             |

    | ConvE (Dettmers et al., 2018)    | f(vec(f([h; r] ∗ ω))W)t         |


    Table 5: The scoring function s(< h, r, t >) of KGE models used in this paper,
    where h, r, t denote the embeddings of h, r, t, ◦ denotes Hadamard product. ·
    refers to conjugate for complex vectors in ComplEx, and 2D reshaping for real
    vectors in ConvE. ∗ is operator for 2D convolution. ω is the filters and W is
    the parameters for 2D convolutional layer.


    # F Detailed Experimental Settings


    Note the experimental settings closely follow the approach outlined by [Zhu et
    al.](#page-10-3) [\(2025\)](#page-10-3). For completeness and to ensure the paper
    is self-contained, we recall the details in this section.


    ### F.1 Information About KGE Models and Benchmark Datasets


    Table [4](#page-15-0) outlines key statistics for the benchmark datasets, while
    Table [5](#page-15-1) presents the scoring functions utilized by various KGE methods.


    ### F.2 Privacy Concerns in FB15k and FB15k-237


    Both FB15k and FB15k-237 datasets include data about individuals, predominantly
    well-known public figures such as politicians, celebrities, and historical icons.
    Since this information is widely accessible through public platforms and online
    sources, its inclusion in Freebase poses minimal privacy risks compared to datasets
    containing sensitive or private personal details.


    #### F.3 Details of Pre-training KGE Models


    For pre-training the the KGE models, we follow [\(Zhu et al.,](#page-10-3) [2025\)](#page-10-3).


    The LibKGE framework [\(Broscheit et al.,](#page-9-17) [2020\)](#page-9-17) was
    used for training the KGE models, following a hyperparameter optimization approach
    inspired by [\(Ruffinelli et al.,](#page-9-18) [2019\)](#page-9-18). The experiments
    were executed on a Linux system equipped with a 40GB NVIDIA A100 SXM4 GPU.


    Initially, we applied a quasi-random hyperparameter search using Sobol sequences
    to ensure an even distribution of configurations, avoiding clustering [\(Bergstra
    and Bengio,](#page-9-22) [2012\)](#page-9-22). For each combination of dataset,
    model, training type, and loss function, 30 configurations were generated. This
    was followed by a Bayesian optimization phase, incorporating 30 additional trials
    to refine the hyperparameters based on prior results. The Ax framework (<https://ax.dev/>)
    facilitated this process.


    The search spanned a comprehensive hyperparameter space, encompassing loss functions
    (pairwise margin ranking with hinge loss, binary crossentropy, cross-entropy),
    regularization methods (none/L1/L2/L3, dropout), optimizers (Adam, Adagrad), and
    common initialization techniques used in the KGE domain. Embedding sizes of 128,
    256, and 512 were considered. For further details, refer to [\(Ruffinelli et al.,](#page-9-18)
    [2019,](#page-9-18) Table 5).


    Configuration files for baseline and competing models, along with models used
    in aggregation, are available in the "configs" folder of the submitted software
    directory. These files (\*.yaml) document the hyperparameter settings applied
    in this study.


    #### F.4 Optimal Hyperparameter Settings


    The optimal hyperparameter configurations for each model-dataset combination are
    summarized in Table [7.](#page-16-0)


    ### G Complete Experimental Results


    In Figure [3](#page-18-0) and [4,](#page-19-0) we show the complete results of
    comparison of methods'' CovGap and AveSize across varying target coverage levels
    ranging from [0.8, 0.85, 0.9, 0.95].


    In Figure [5](#page-20-0) and [6,](#page-21-0) we investigate the influence of
    hyperparameters ϕ and γ on CovGap and AveSize, respectively.


    ### H Complexity Analysis


    The total computational cost of our method, COND-KGCP, comprises three main components:


    - KGE Model Training: This is shared across all methods and is the most computationally
    intensive component.

    - Calibration Step: This includes any methodspecific operations, such as clustering
    in


    <span id="page-16-1"></span>


    | WN18      |          |             |                  | FB15k     |          |             |                  |  |

    |-----------|----------|-------------|------------------|-----------|----------|-------------|------------------|--|

    | Method    | Training | Calibration | Set Construction | Method    | Training
    | Calibration | Set Construction |  |

    | KGCP      | 1h       | 6.68s       | 0.8ms/query      | KGCP      | 2h       |
    32.70s      | 0.4ms/query      |  |

    | MCP       | 1h       | 7.32s       | 0.8ms/query      | MCP       | 2h       |
    32.70s      | 0.4ms/query      |  |

    | CLUSTERCP | 1h       | 8.30s       | 0.8ms/query      | CLUSTERCP | 2h       |
    33.55s      | 0.4ms/query      |  |

    | APS       | 1h       | 7.28s       | 1ms/query        | APS       | 2h       |
    32.56s      | 0.4ms/query      |  |

    | RAPS      | 1h       | 8.72s       | 1ms/query        | RAPS      | 2h       |
    39.23s      | 0.5ms/query      |  |

    | CONDKGCP  | 1h       | 8.92s       | 1ms/query        | CONDKGCP  | 2h       |
    38.33s      | 0.5ms/query      |  |


    Table 6: Empirical runtime for different uncertainty quantification methods on
    WN18 and FB15k.


    <span id="page-16-0"></span>


    |          | WN18  |     | FB15k |     |  |

    |----------|-------|-----|-------|-----|--|

    | Model    | gamma | cut | gamma | cut |  |

    | TransE   | 0.01  | 50  | 0.01  | 20  |  |

    | RotatE   | 0.01  | 50  | 0.01  | 100 |  |

    | RESCAL   | 0.01  | 50  | 0.01  | 50  |  |

    | DistMult | 0.01  | 50  | 0.01  | 20  |  |

    | ComplEx  | 0.1   | 50  | 0.01  | 20  |  |

    | ConvE    | 0.01  | 50  | 0.01  | 50  |  |


    Table 7: Optimal hyperparameter configurations for CONDKGCP across various model-dataset
    combinations.


    - CLUSTERCP or dual calibration in COND-KGCP. These are one-time offline steps
    and incur negligible overhead relative to model training.

    - Test-Time Set Construction: This is the only component executed per test query
    and constitutes the primary source of runtime differences between methods.


    We now present a theoretical complexity analysis focused on the per-query cost:


    - Baseline methods (e.g., KGCP, MCP): Each query requires computing scores for
    all candidate entities, with time complexity O(|E| · d), where |E| is the number
    of entities and d is the embedding dimension. An additional O(|E|) is needed for
    score thresholding to form the prediction set.

    - CONDKGCP: In addition to score computation (O(|E| · d)), our method includes
    rank-based thresholding from dual calibration, adding a sorting step O(|E| log
    |E|), linear pass O(|E|), and selection of the top-K entities (O(K)). Thus, the
    total per-query complexity is: O(|E| · d) +O(|E| log |E|+|E|+ K).


    <span id="page-16-2"></span>


    |           | CovGap (0.95) | AveSize (0.95) | Ratio (0.95) |  |

    |-----------|---------------|----------------|--------------|--|

    | KGCP      | 0.040         | 16.30          | –            |  |

    | MCP       | 0.013         | 1128.46        | 41191.11     |  |

    | CLUSTERCP | 0.040         | 19.61          | –            |  |

    | APS       | 0.079         | 4036.71        | –            |  |

    | RAPS      | 0.051         | 242.36         | –            |  |

    | CONDKGCP  | 0.029         | 133.37         | 10642.73     |  |

    |           | CovGap (0.98) | AveSize (0.98) | Ratio (0.98) |  |

    | KGCP      | 0.020         | 2945.21        | –            |  |

    | MCP       | 0.009         | 5027.66        | 189313.64    |  |

    | CLUSTERCP | 0.022         | 2933.73        | –            |  |

    | APS       | 0.039         | 12094.28       | –            |  |

    | RAPS      | 0.018         | 3985.23        | 520010.00    |  |

    | CONDKGCP  | 0.017         | 3060.69        | 38493.33     |  |

    |           | CovGap (0.99) | AveSize (0.99) | Ratio (0.99) |  |

    | KGCP      | 0.016         | 7403.45        | –            |  |

    | MCP       | 0.006         | 9494.30        | 209085.00    |  |

    | CLUSTERCP | 0.018         | 8921.37        | –            |  |

    | APS       | 0.016         | 19486.37       | –            |  |

    | RAPS      | 0.009         | 17098.86       | 1385058.57   |  |

    | CONDKGCP  | 0.013         | 7614.11        | 70220.00     |  |


    Table 8: Performance results at high confidence levels (1 − ϵ > 0.95).


    While CONDKGCP introduces additional steps beyond baselines, the dominant term
    remains O(|E|·d) in practice. For instance, with |E| = 10<sup>6</sup> and d =
    512, computing scores dominates (512 operations per entity), whereas the extra
    overhead from sorting and filtering (approximately 21 operations per entity) is
    comparatively negligible.


    Empirical runtimes, reported in Table [6,](#page-16-1) confirm that the additional
    complexity of CONDKGCP does not translate into significant runtime overhead on
    a NVIDIA A100 GPU.


    ### I Results for More Confidence Levels


    In Table [8,](#page-16-2) we extend the analysis from Figure [1](#page-6-0) to
    higher confidence levels (i.e., α > 0.95) using the same experimental settings.
    We observe that CONDKGCP consistently achieves superior conditional coverage while
    maintaining a more favorable trade-off between coverage and prediction set size,


    even at these stringent confidence levels.


    Notably, all methods exhibit a sharp increase in prediction set size beyond α
    = 0.95. We attribute this to the inherent limitations of the base KGE models.
    For instance, on WN18, most models attain Hits@10 close to 0.95, meaning that
    approximately 95% of correct answers are ranked within the top 10. Pushing coverage
    beyond this threshold necessitates including lower-ranked (and often noisy) entities,
    which substantially enlarges the prediction sets.


    This observation underscores an important practical consideration: in high-stakes
    applications where coverage above 95% is required, it is essential to pair uncertainty
    quantification with a highly accurate base model (e.g., achieving Hits@K > 0.95).
    Otherwise, the resulting prediction sets may become impractically large.


    ### J AI Assistants In Writing


    We use ChatGPT [\(OpenAI,](#page-9-23) [2024\)](#page-9-23) to enhance our writing
    skills, abstaining from its use in research and coding endeavors.


    <span id="page-18-0"></span>![](_page_18_Figure_0.jpeg)


    Figure 3: Complete results of comparison of methods'' CovGap across varying target
    coverage levels.


    <span id="page-19-0"></span>![](_page_19_Figure_0.jpeg)


    Figure 4: Complete results of comparison of methods'' AveSize across varying target
    coverage levels.


    <span id="page-20-0"></span>![](_page_20_Figure_0.jpeg)


    Figure 5: Influence of hyperparameters ϕ and γ on CovGap.


    <span id="page-21-0"></span>![](_page_21_Figure_0.jpeg)


    Figure 6: Influence of hyperparameters ϕ and γ on AveSize.


    <span id="page-22-0"></span>


    | WN18     |       |           |          |           |          | FB15k |           |          |           |

    |----------|-------|-----------|----------|-----------|----------|-------|-----------|----------|-----------|

    | Model    | Score | Method    | CovGap ↓ | AveSize ↓ | Model    | Score | Method    |
    CovGap ↓ | AveSize ↓ |

    |          | APS   | MCP       | 0.015    | 1320.77   | TransE   |       | MCP       |
    0.022    | 653.62    |

    |          |       | CLUSTERCP | 0.079    | 9081.71   |          | APS   | CLUSTERCP
    | 0.116    | 4925.66   |

    | TransE   |       | CONDKGCP  | 0.016    | 1090.23   |          |       | CONDKGCP  |
    0.028    | 158.17    |

    |          |       | MCP       | 0.024    | 1172.74   |          |       | MCP       |
    0.021    | 566.15    |

    |          | RAPS  | CLUSTERCP | 0.066    | 1138.93   |          | RAPS  | CLUSTERCP
    | 0.122    | 320.57    |

    |          |       | CONDKGCP  | 0.051    | 50.29     |          |       | CONDKGCP  |
    0.023    | 258.90    |

    | RotatE   | APS   | MCP       | 0.014    | 15922.23  | RotatE   | APS   | MCP       |
    0.022    | 1695.87   |

    |          |       | CLUSTERCP | 0.077    | 15398.99  |          |       | CLUSTERCP
    | 0.122    | 1626.64   |

    |          |       | CONDKGCP  | 0.014    | 15920.71  |          |       | CONDKGCP  |
    0.023    | 1389.67   |

    |          |       | MCP       | 0.022    | 2084.58   |          | RAPS  | MCP       |
    0.022    | 523.75    |

    |          | RAPS  | CLUSTERCP | 0.063    | 2051.17   |          |       | CLUSTERCP
    | 0.121    | 398.75    |

    |          |       | CONDKGCP  | 0.039    | 141.42    |          |       | CONDKGCP  |
    0.023    | 322.63    |

    |          |       | MCP       | 0.020    | 3236.86   |          |       | MCP       |
    0.021    | 867.08    |

    |          | APS   | CLUSTERCP | 0.095    | 1932.13   |          | APS   | CLUSTERCP
    | 0.072    | 360.10    |

    | RESCAL   |       | CONDKGCP  | 0.020    | 3334.02   | RESCAL   |       | CONDKGCP  |
    0.024    | 552.58    |

    |          |       | MCP       | 0.020    | 1037.90   |          | RAPS  | MCP       |
    0.021    | 590.23    |

    |          | RAPS  | CLUSTERCP | 0.060    | 1001.70   |          |       | CLUSTERCP
    | 0.121    | 374.67    |

    |          |       | CONDKGCP  | 0.035    | 94.29     |          |       | CONDKGCP  |
    0.025    | 287.15    |

    |          | APS   | MCP       | 0.018    | 815.22    | DistMult | APS   | MCP       |
    0.024    | 723.68    |

    | DistMult |       | CLUSTERCP | 0.043    | 205.65    |          |       | CLUSTERCP
    | 0.063    | 85.04     |

    |          |       | CONDKGCP  | 0.019    | 249.74    |          |       | CONDKGCP  |
    0.024    | 124.64    |

    |          | RAPS  | MCP       | 0.022    | 1003.06   |          | RAPS  | MCP       |
    0.022    | 576.24    |

    |          |       | CLUSTERCP | 0.059    | 969.60    |          |       | CLUSTERCP
    | 0.123    | 356.66    |

    |          |       | CONDKGCP  | 0.024    | 140.75    |          |       | CONDKGCP  |
    0.023    | 273.34    |

    |          |       | MCP       | 0.017    | 16707.74  | ComplEx  | APS   | MCP       |
    0.024    | 694.64    |

    |          | APS   | CLUSTERCP | 0.065    | 15738.09  |          |       | CLUSTERCP
    | 0.054    | 176.78    |

    | ComplEx  |       | CONDKGCP  | 0.017    | 16728.70  |          |       | CONDKGCP  |
    0.025    | 282.54    |

    |          | RAPS  | MCP       | 0.023    | 2614.74   |          | RAPS  | MCP       |
    0.023    | 530.80    |

    |          |       | CLUSTERCP | 0.064    | 2598.85   |          |       | CLUSTERCP
    | 0.120    | 401.65    |

    |          |       | CONDKGCP  | 0.041    | 83.65     |          |       | CONDKGCP  |
    0.026    | 331.38    |

    | ConvE    |       | MCP       | 0.015    | 589.50    | ConvE    | APS   | MCP       |
    0.022    | 3951.64   |

    |          | APS   | CLUSTERCP | 0.070    | 9.91      |          |       | CLUSTERCP
    | 0.109    | 4376.19   |

    |          |       | CONDKGCP  | 0.052    | 11.89     |          |       | CONDKGCP  |
    0.025    | 3019.98   |

    |          | RAPS  | MCP       | 0.023    | 1013.09   |          | RAPS  | MCP       |
    0.022    | 613.67    |

    |          |       | CLUSTERCP | 0.060    | 981.74    |          |       | CLUSTERCP
    | 0.121    | 386.85    |

    |          |       | CONDKGCP  | 0.033    | 116.70    |          |       | CONDKGCP  |
    0.025    | 304.28    |


    Table 9: Overall performance comparison of MCP, CLUSTERCP, and CONDKGCP using
    different nonconformity measures: APS and RAPS. Note that methods modifying nonconformity
    scores theoretically can be combined with subgroup-based methods, however since
    APS and RAPS are not suitable for KGE methods we do not see much improvement by
    combining these methods, nevertheless, CONDKGCP outperforms both MCP and CLUSTERCP
    if we combine with APS or RAPS across all evaluation metrics.'
- title: "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language\
    \ Models"
  abstract: 'Reinforcement Learning (RL) has proven to be an effective post-training

    strategy for enhancing reasoning in vision-language models (VLMs). Group

    Relative Policy Optimization (GRPO) is a recent prominent method that

    encourages models to generate complete reasoning traces before answering,

    leading to increased token usage and computational cost. Inspired by the

    human-like thinking process-where people skip reasoning for easy questions but

    think carefully when needed-we explore how to enable VLMs to first decide when

    reasoning is necessary. To realize this, we propose TON, a two-stage training

    strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective

    ''thought dropout'' operation, where reasoning traces are randomly replaced with

    empty thoughts. This introduces a think-or-not format that serves as a cold

    start for selective reasoning; (ii) a GRPO stage that enables the model to

    freely explore when to think or not, while maximizing task-aware outcome

    rewards. Experimental results show that TON can reduce the completion length by

    up to 90% compared to vanilla GRPO, without sacrificing performance or even

    improving it. Further evaluations across diverse vision-language tasks-covering

    a range of reasoning difficulties under both 3B and 7B models-consistently

    reveal that the model progressively learns to bypass unnecessary reasoning

    steps as training advances. These findings shed light on the path toward

    human-like reasoning patterns in reinforcement learning approaches. Our code is

    available at https://github.com/kokolerk/TON.'
  url: http://arxiv.org/abs/2505.16854v1
  keywords: ''
  document: "# Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language\
    \ Models\n\nJiaqi Wang1† Kevin Qinghong Lin2† James Cheng<sup>2</sup> Mike Zheng\
    \ Shou2<sup>B</sup> <sup>1</sup>The Chinese University of Hong Kong <sup>2</sup>Show\
    \ Lab, National University of Singapore\n\n# Abstract\n\nReinforcement Learning\
    \ (RL) has proven to be an effective post-training strategy for enhancing reasoning\
    \ in vision–language models (VLMs). Group Relative Policy Optimization (GRPO)\
    \ is a recent prominent method that encourages models to generate complete reasoning\
    \ traces before answering, leading to increased token usage and computational\
    \ cost. Inspired by the human-like thinking process—where people skip reasoning\
    \ for easy questions but think carefully when needed—we explore how to enable\
    \ VLMs to first decide *when reasoning is necessary*. To realize this, we propose\
    \ TON, a two-stage training strategy: (*i*) a supervised finetuning (SFT) stage\
    \ with a simple yet effective \"thought dropout\" operation, where reasoning traces\
    \ are randomly replaced with empty thoughts. This introduces a think-or-not format\
    \ that serves as a cold start for selective reasoning; (*ii*) a GRPO stage that\
    \ enables the model to freely explore when to think or not, while maximizing task-aware\
    \ outcome rewards. Experimental results show that TON can *reduce the completion\
    \ length by up to* 90% *compared to vanilla GRPO, without sacrificing performance\
    \ or even improving it*. Further evaluations across diverse vision-language tasks—covering\
    \ a range of reasoning difficulties under both 3B and 7B models—consistently reveal\
    \ that the *model progressively learns to bypass unnecessary reasoning steps as\
    \ training advances*. These findings shed light on the path toward human-like\
    \ reasoning patterns in reinforcement learning approaches. Our code is available\
    \ at <https://github.com/kokolerk/TON>.\n\n# 1 Introduction\n\n*\"To think or\
    \ not to think, that is the question.\"*\n\nReinforcement learning (RL) has recently\
    \ emerged as a dominant post-supervised fine-tuning (SFT) strategy in vision-language\
    \ models (VLMs) [\\[1,](#page-9-0) [2,](#page-9-1) [3,](#page-9-2) [4\\]](#page-9-3).\
    \ Methods like GRPO [\\[5\\]](#page-9-4) have shown promising results in enhancing\
    \ reasoning capabilities through KL-divergence losses based on rule-driven rewards.\
    \ However, these approaches often lead to unnecessarily long and redundant reasoning\
    \ processes due to their reliance on full-length generative trajectories [\\[6,](#page-9-5)\
    \ [7,](#page-9-6) [8\\]](#page-9-7). To address this inefficiency, some works\
    \ attempt to shorten reasoning chains with rule-based reward penalties [\\[9,](#page-9-8)\
    \ [10,](#page-9-9) [11\\]](#page-9-10) during the pre-training phase or introduce\
    \ external control mechanisms, such as in very recent Qwen3 [\\[12\\]](#page-9-11).\
    \ Nonetheless, a more natural and scalable solution is to enable the model to\
    \ *decide when to think*—mirroring how humans modulate cognitive effort in response\
    \ to task difficulty.\n\nIn this work, we begin by presenting empirical evidence\
    \ that *thinking is not always necessary*. In AITZ [\\[13\\]](#page-10-0), we\
    \ observe that 51% of questions can be answered correctly even when the entire\
    \ reasoning trace is omitted, resulting in significant savings in thought tokens.\
    \ This finding underscores the potential of selective reasoning strategies to\
    \ improve efficiency without sacrificing accuracy.\n\n<sup>†</sup> *Equal contribution.*\
    \ B *Corresponding authors.*\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\
    \nFigure 1: Illustrating the \"think or not think\" trade-off. Left: For simple\
    \ queries, explicit reasoning is unnecessary—models like GRPO that always \"think\"\
    \ incur redundant computation. Right: For more complex geometric problems, step-by-step\
    \ reasoning is essential to arrive at the correct answer. Our proposed TON framework\
    \ learns to adaptively invoke reasoning only when needed—skipping it for easy\
    \ cases while engaging in deeper inference for harder tasks.\n\nSecondly, by exploring\
    \ a simple prompting strategy — allowing the model to skip reasoning steps for\
    \ easier queries — we observe that even math-enhanced VLMs struggle to adaptively\
    \ omit redundant thought generation. Instead, they tend to default to a conservative\
    \ approach, producing full reasoning traces regardless of task difficulty. This\
    \ suggests that the ability to \"think or not\" is not solely determined by reasoning\
    \ capacity, but should instead be treated as a distinct skill—one that should\
    \ be explicitly activated through format-following in supervised fine-tuning (SFT)\
    \ stage.\n\nMotivated by the above observations, we introduce TON (*i.e.,* Think-or-Not),\
    \ a two-stage training framework featuring a simple yet effective \"thought dropout\"\
    \ approach. This method explicitly replace reasoning traces with minimal \"\\\
    n\\n\" delimiter and employs SFT to train the model that reasoning can be skipped—thereby\
    \ enabling the possibility of bypassing reasoning. A subsequent GRPO stage further\
    \ refines this selective-reasoning policy via self-exploration, rewarding answers\
    \ without introducing extra regularization. As illustrated in Figure [1,](#page-1-0)\
    \ vanilla GRPO consistently generates reasoning sequences regardless of task difficulty.\
    \ In contrast, our method, TON, adaptively allocates reasoning based on the complexity\
    \ of the task. For simple tasks (left), TON can bypass unnecessary reasoning and\
    \ directly provide the answer, reducing 90% token usage. For more hard problems\
    \ (right), it still engages in detailed, step-by-step reasoning to arrive at the\
    \ correct solution.\n\nBuilt on top of TON, we using the Qwen-2.5-VL series and\
    \ conduct extensive evaluations across a range of vision-language tasks—spanning\
    \ counting (CLEVR [\\[14\\]](#page-10-1), SuperCLEVR [\\[15\\]](#page-10-2)),\
    \ mobile agent navigation (AITZ [\\[13\\]](#page-10-0)), and mathematical reasoning\
    \ (GeoQA [\\[16\\]](#page-10-3))—which collectively cover a spectrum of reasoning\
    \ levels and diverse task settings. Overall, we find that TON achieves substantial\
    \ reductions in completion length without compromising performance—cutting 87%\
    \ of tokens on CLEVR and 65% on GeoQA. Notably, on the multi-step navigation task\
    \ AITZ, TON reduces the average task-level output length from 3.6K to 0.9K tokens.\
    \ Moreover, we observe that omitting reasoning traces can even improve performance:\
    \ on GeoQA, TON outperforms the vanilla GRPO baseline by up to 17% in accuracy,\
    \ demonstrating a \"free-lunch\" effect where shorter reasoning outperforms or\
    \ matches longer trajectories. Comprehensive ablation studies further reveal that\
    \ the skip-thought ratio increases progressively with reward improvements during\
    \ training, suggesting the model learns to selectively bypass unnecessary reasoning\
    \ steps in an adaptive manner.\n\n# 2 Related Works\n\nReinforcement Learning\
    \ for Vision–Language Models. Most VLMs start with SFT on large collections of\
    \ instruction data to acquire broad foundational knowledge [\\[17,](#page-10-4)\
    \ [18,](#page-10-5) [13,](#page-10-0) [19\\]](#page-10-6). To further improve\
    \ performance, recent work has adopted a post-training paradigm that leverages\
    \ human feedback [\\[20,](#page-10-7) [21,](#page-10-8) [22\\]](#page-10-9). RL\
    \ from human feedback (RLHF) fits a reward model on preference annotations and\
    \ refines the policy via Proximal Policy Optimization (PPO) [\\[23,](#page-11-0)\
    \ [24,](#page-11-1) [21,](#page-10-8) [25\\]](#page-11-2). Direct Preference Optimization\
    \ (DPO) [\\[26\\]](#page-11-3) streamlines this workflow by recasting policy updates\
    \ as a binary classification task, aligning model outputs distributions with human\
    \ preferences without a reward module. Beyond these methods, Group Relative Policy\
    \ Optimization (GRPO) [\\[5\\]](#page-9-4) blends offline and online learning:\
    \ it samples groups of thinking process, uses Answer verification (such as Math\
    \ verifier) as reward feedback, and computes relative advantages within each group.\
    \ By avoiding a value function,\n\nGRPO provide an elegant solution by promoting\
    \ diverse reasoning paths and improved answer quality. Despite a series of GRPO\
    \ follow-up works [\\[27,](#page-11-4) [28,](#page-11-5) [9\\]](#page-9-8), all\
    \ of these approaches assume that every question demands a full thinking—leading\
    \ to lengthy decoding. In contrast, our work focuses on \"*when to think*\" instead\
    \ of \"*how to think*\": we introduce a selective reasoning policy that learns\
    \ to skip unnecessary \"think\" phases, boosting inference efficiency without\
    \ sacrificing accuracy.\n\nThinking in Language Models. From early Chain-of-Thought\
    \ [\\[29,](#page-11-6) [30,](#page-11-7) [31\\]](#page-11-8) prompting to recent\
    \ reasoning-intensive reinforcement learning approaches [\\[5,](#page-9-4) [22,](#page-10-9)\
    \ [32,](#page-11-9) [33\\]](#page-11-10), reasoning has emerged as a core dimension\
    \ in the development of language models. Most existing work emphasizes how to\
    \ enhance reasoning capabilities, often resulting in increasingly lengthy and\
    \ complex thought processes [\\[7,](#page-9-6) [34,](#page-11-11) [10\\]](#page-9-9)\
    \ while relatively few studies address the efficiency of reasoning. For instance,\
    \ [\\[35\\]](#page-11-12) proposes a long2short strategy to compress decoding\
    \ length, [\\[36\\]](#page-11-13) encourages models to output \"I don't know\"\
    \ to terminate unproductive reasoning, and [\\[37\\]](#page-11-14) introduces\
    \ a token-budget-aware reasoning policy. While these approaches offer promising\
    \ insights into controlling reasoning length, we argue for a more foundational\
    \ perspective: rather than deciding how to reason once the process has started,\
    \ *models should first determine whether reasoning is necessary at all*. Simple\
    \ questions may be answered directly without any explicit reasoning, while complex\
    \ questions may require maintaining a full reasoning trajectory [\\[8,](#page-9-7)\
    \ [6,](#page-9-5) [9\\]](#page-9-8). In this work, we explore the selective reasoning\
    \ paradigm within VLMs by introducing a simple yet effective method – thought-dropout.\
    \ We validate its effectiveness on tasks such as Counting, Math, and further extend\
    \ it to more practical agentic settings.\n\n# 3 Preliminary\n\nTask Definition.\
    \ We formalize the vision-language reasoning environment as a Markov Decision\
    \ Process (MDP) defined by a tuple (V, Q, S ∗ , π, r), covering a wide range of\
    \ vision-language tasks. Here, V denotes the visual context (*e.g.,* an image).\
    \ Q is a language-based query or question posed about the visual input. The model,\
    \ governed by policy π, takes the input pair (V, Q) and generates a predicted\
    \ answer S. The environment provides a scalar reward function r(·) based on the\
    \ model's response O. A correct prediction, *e.g.,* O matches the ground truth\
    \ answer S ∗ , yields a positive reward, while an incorrect one yields zero. The\
    \ objective in this environment is to learn an adaptive policy πθ, parameterized\
    \ by θ, that maximizes the expected reward, enabling the model to reason selectively\
    \ and efficiently across diverse input settings.\n\nReward Function. The reward\
    \ function r(·) can be either model-based [\\[23,](#page-11-0) [25\\]](#page-11-2)\
    \ or rule-based, as recently demonstrated in [\\[5,](#page-9-4) [22\\]](#page-10-9),\
    \ which is typically categorized into two types: format rewards r<sup>f</sup>\
    \ and outcome rewards ro. While the outcome rewards are usually carefully designed\
    \ based on different tasks or requests in previous works [\\[5,](#page-9-4) [1,](#page-9-0)\
    \ [2,](#page-9-1) [9\\]](#page-9-8), the format reward r<sup>f</sup> , is always\
    \ shared in the same. Given the response O, it should follow the required HTML\
    \ tag format *<think>*T *<\\think><answer>*S*<\\answer>*, where T is the reasoning\
    \ process (*i.e.,* a thought) and S is the predicted answer. This formulation\
    \ requires the model to think before deriving the answer and makes it easy to\
    \ parse both the reasoning process and the final outcome (*e.g.,* via regular\
    \ expression).\n\n# 4 TON: Selective Reasoning via Policy Optimization\n\nObservation.\
    \ In practice, humans do not require explicit reasoning for all tasks—many can\
    \ be completed intuitively. Similarly, models can often produce correct answers\
    \ to simple questions without explicit thinking. As illustrated in figure [2,](#page-2-0)\
    \ the percentages of correct and incorrect samples under different setups with\
    \ and without the thinking process in inference (see Appendix [A](#page-14-0)\
    \ for overall performance). We find that 52.1% of answers remained correct without\
    \ \"think,\" and 14.5% were even correct only without it—implying that *explicit\
    \ thinking is not always necessary*.\n\nA straightforward idea is to prompt the\
    \ model to decide whether to \"think\" or not (we prompt the model to skip thinking\
    \ in the simple questions in Sec. [5.4\\)](#page-7-0). However, as shown in our\
    \ experiments (Figure [5d](#page-7-1) and Appendix [G.7\\)](#page-20-0),\n\n<span\
    \ id=\"page-2-0\"></span>\n\n|                      | wo think<br>correct | wo\
    \ think<br>incorrect |  |  |  |\n|----------------------|---------------------|-----------------------|--|--|--|\n\
    | w think<br>correct   | 52.1%               | 25.6%                 |  |  | \
    \ |\n| w think<br>incorrect | 14.5%               | 7.69%                 |  |\
    \  |  |\n\nFigure 2: Accuracy comparison of with *v.s.* without \"thinking\" during\
    \ SFT using Qwen-2.5-VL-3B on the AITZ task.\n\nthe model still tends to generate\
    \ the full reasoning process without any no-think try. This suggests that the\
    \ ability to decide whether to think is not solely governed by reasoning capability,\
    \ but should instead be treated as a separate skill—one that must be explicitly\
    \ trained through format-following during the supervised fine-tuning (SFT) stage.\
    \ These observations motivate us to activate this ability early in the SFT stage\
    \ and develop TON, which enables selective reasoning by automatically switching\
    \ between \"think\" and \"non-think\" modes.\n\n### <span id=\"page-3-1\"></span>4.1\
    \ First SFT stage: Thought Dropout\n\nIn the initial stage, the model is typically\
    \ fine-tuned on \"think-answer\" formatted data, where the \"think\" contains\
    \ high-quality reasoning traces to serve as a cold start. To extend this predefined\
    \ reasoning ability to selective reasoning, we view \"think\" vs. \"non-think\"\
    \ as part of the output format itself by *dropping* the \"think\" component during\
    \ training.\n\nHowever, it is difficult to determine which samples should be skipped,\
    \ as different models exhibit varying reasoning capabilities. Therefore, we begin\
    \ with *random* dropout and allow the model to learn to decide for itself during\
    \ the second RL stage (Sec[.4.2\\)](#page-3-0). To this end, we propose \"Thought\
    \ Dropout\" that randomly injecting empty \"thought\" segments, requiring only\
    \ minor code changes:\n\nAlgorithm 1 Pseudo-code for thought\\_dropout\n\n```\n\
    def thought_dropout(thought, dropout_prob):\nif random.random() < dropout_prob:\n\
    \    thought = \"\\n\\n\"\nreturn thought\n```\nThis approach injects both the\
    \ answer format and the skip-thought format as prior knowledge before the second\
    \ RL stage.\n\nWhere do Thoughts come from? Given a policy operating in an environment\
    \ (V, Q, S ∗ , π, r), a key challenge is how to curate high-quality cold-start\
    \ \"thought\" data without relying on external models, such as closed-source APIs.\
    \ A naïve approach is to run multiple inference passes and retain only successful\
    \ cases based on answer matching—but we find this to be less effective. To address\
    \ the scarcity of high-quality \"thought\" data, we instead adopt a reverse thinking\
    \ strategy: leveraging the base model π itself to self-generate a rich corpus\
    \ of thought sequences. Specifically, given the visual context V, textual query\
    \ Q, and ground-truth answer S ∗ , we prompt the policy π<sup>θ</sup> to deduce\
    \ the corresponding intermediate thought as follows:\n\n$$\\mathcal{T} \\leftarrow\
    \ \\pi\\_{\\boldsymbol{\\theta}}(\\mathcal{V}, \\mathcal{Q}, \\mathcal{S}^\\*)\
    \ \\tag{l}$$\n\nSpecially, we generate intermediate thoughts with the following\
    \ prompts:\n\n### Prompt for *Reverse Thinking*\n\nBased on the following question\
    \ and image, generate a thought process to explain how to derive the answer from\
    \ the inputs.\n\nImage: {Image} Question: {Question} Answer: {Answer}\n\nDo not\
    \ output the answer, only generate the reasoning process. Formulate your outputs\
    \ using concise language.\n\nIn this way, we curate sufficient thought data without\
    \ relying on external models. These serve as our cold-start training corpus, enabling\
    \ us to apply the Thought Dropout strategy during SFT to activate the model's\
    \ ability to bypass thoughts.\n\n### <span id=\"page-3-0\"></span>4.2 Second RL\
    \ stage: Group Relative Policy Optimization\n\nAlthough SFT teaches the skip-thought\
    \ format, it still leaves a central question unresolved: when should thoughts\
    \ be skipped or retained? Ideally, the model should learn to explore this decision\
    \ on its own. To this end, we adopt reinforcement learning via GRPO to enhance\
    \ the model's ability to explore this decision as part of its reasoning process.\n\
    \n<span id=\"page-4-0\"></span>![](_page_4_Figure_0.jpeg)\n\nFigure 3: Illustration\
    \ of the responses from GRPO and TON. q<sup>1</sup> is the question and {o1, ·\
    \ · · , o5} are the generated responses containing thoughts T (circle) and answers\
    \ S (triangle). TON can sample from the empty think T\\n\\n, thus enhancing the\
    \ response diversity over the vanilla GRPO.\n\nGiven an image v ∈ V and text query\
    \ q ∈ Q, GRPO samples N candidate responses with variations {o1, o2, . . . , o<sup>N</sup>\
    \ } from the policy π<sup>θ</sup> and evaluates each response o<sup>i</sup> using\
    \ a reward function r(·), which measures the quality of the candidate in the context\
    \ of the given question. To determine the relative quality of these responses,\
    \ GRPO normalizes the rewards by computing their mean and standard deviation and\
    \ subsequently derives the advantage as:\n\n$$A\\_i = \\frac{r(o\\_i) - \\text{mean}\\\
    {r(o\\_1), r(o\\_2), \\dots, r(o\\_N)\\}}{\\text{std}\\{r(o\\_1), r(o\\_2), \\\
    dots, r(o\\_N)\\}} \\tag{2}$$\n\nwhere A<sup>i</sup> represents the advantage\
    \ of the candidate response o<sup>i</sup> relative to other sampled responses.\
    \ GRPO encourages the model to generate responses with higher advantages within\
    \ the group by updating the policy π<sup>θ</sup> using the following objective:\n\
    \n$$\\mathcal{J}\\_{GRPO}(\\theta) = \\mathbb{E}[\\{o\\_i\\}\\_{i=1}^N \\sim \\\
    pi\\_{\\theta\\_{old}}(v, q)] \\frac{1}{N} \\sum\\_{i=1}^N \\left\\{ \\min[\\\
    alpha\\_i \\cdot A\\_i, \\, \\beta\\_i \\cdot A\\_i] - \\beta \\mathbb{D}\\_{KL}[\\\
    pi\\_{\\theta} || \\pi\\_{ref}] \\right\\} \\tag{3}$$\n\n<span id=\"page-4-1\"\
    ></span>\n$$\\alpha\\_{i} = \\frac{\\pi\\_{\\theta}(o\\_{i}|v,q)}{\\pi\\_{\\theta\\\
    _{old}}(o\\_{i}|v,q)}, \\quad \\beta\\_{i} = \\text{clip}\\left(\\frac{\\pi\\\
    _{\\theta}(o\\_{i}|v,q)}{\\pi\\_{\\theta\\_{old}}(o\\_{i}|v,q)}, 1+\\epsilon,\
    \ 1-\\epsilon\\right). \\tag{4}$$\n\nHow does TON impact GRPO? As illustrated\
    \ in Fig. [3,](#page-4-0) our TON allows the model to choose \"emptythink\" T\\\
    n\\<sup>n</sup> during the inference step, thus resulting in a significant variation\
    \ in the distribution between the non-think (o<sup>i</sup> ∼ T\\n\\n) and think\
    \ responses (o<sup>i</sup> ∼ T ) by TON compared to both think ones (o<sup>i</sup>\
    \ ∼ T ) generated by vanilla GRPO. Unlike previous works like DAPO [\\[27\\]](#page-11-4)\
    \ emphasize on advantage distribution A<sup>i</sup> by dynamic sampling in the\
    \ sparse reward space, our TON shifts the focus to the latent distribution space\
    \ of responses (πθ(o<sup>i</sup> |v, q)), thus enhancing the diversity of the\
    \ terms α and β in Eq. [4.](#page-4-1)\n\nHow to design Rewards? To support GRPO\
    \ training across diverse settings, it is crucial to carefully examine reward\
    \ design choices. We consider two main types of matching:\n\n*(i) Discrete Matching.*\
    \ For tasks with deterministic, categorical or numerical outputs—e.g., classification,\
    \ counting, or math problems—we use a binary value reward rd(s, g) = 1(s = g):\
    \ if the predicted answer s matches the ground-truth g, we assign r<sup>d</sup>\
    \ = 1; otherwise, r<sup>d</sup> = 0.\n\n*(ii) Continous Matching*. For tasks producing\
    \ continuous outputs—e.g., spatial coordinates in UI navigation or object grounding—we\
    \ allow a tolerance region. Given a predicted point p = [x, y] and a ground-truth\
    \ box b = [x1, y1, x2, y2], we define:\n\n$$r\\_c(\\mathbf{p}, \\mathbf{b}) =\
    \ \\begin{cases} 1, & \\mathbf{p} \\text{ lies inside } \\mathbf{b}, \\\\ 0, &\
    \ \\text{otherwise.} \\end{cases}$$\n\nIf only a ground-truth point p ∗ is available,\
    \ we use a distance threshold θ:\n\n$$r\\_c(\\mathbf{p}, \\mathbf{p}^\\*) = \\\
    begin{cases} 1, & \\|\\mathbf{p} - \\mathbf{p}^\\*\\|\\_2 \\le \\theta, \\\\ 0,\
    \ & \\text{otherwise}. \\end{cases}$$\n\nIn practice, we sum the applicable components\
    \ to form an outcome reward: r<sup>o</sup> = r<sup>d</sup> +rc. This simple yet\
    \ flexible scheme can cover classification, numeric reasoning, and grounding.\
    \ See Appendix [B](#page-14-1) for details on adapting these rewards alongside\
    \ the format reward to individual downstream tasks.\n\n|                     \
    \          |                      | Counting (CLEVR)           |             \
    \     | Math (GeoQA)                                      |                  \
    \    |                  |                                                   |\
    \                      |                    |\n|-------------------------------|----------------------|----------------------------|------------------|---------------------------------------------------|----------------------|------------------|---------------------------------------------------|----------------------|--------------------|\n\
    |                               | Acc (↑)              | Time (↓)<br>Qwen-2.5-VL-3B\
    \ | Len. (↓)         | Acc (↑)<br>Time (↓)<br>Len. (↓)<br>Qwen-2.5-VL-3B |   \
    \                   |                  | Acc (↑)<br>Time (↓)<br>Len. (↓)<br>Qwen-2.5-VL-7B\
    \ |                      |                    |\n| Baseline<br>w. GRPO<br>w. TON\
    \ | 64.0<br>93.5<br>98.5 | -<br>1h 44m<br>57m         | 306<br>227<br>28 | 36<br>37<br>51\
    \                                    | -<br>2h 50m<br>2h 4m | 924<br>272<br>96\
    \ | 38<br>44<br>61                                    | -<br>3h 4m<br>2h 34m |\
    \ 939<br>1069<br>112 |\n| Gain                          | +5.0               \
    \  | −47m                       | −87%             | +14                     \
    \                          | −46m                 | −65%             | +17   \
    \                                            | −29m                 | −90%   \
    \            |\n\n<span id=\"page-5-1\"></span>Table 2: Performance comparison\
    \ between TON and vanilla GRPO. Acc. is the accuracy on the test set. Time is\
    \ the RL training time. Length is the average competition length at the end of\
    \ training.\n\n# 5 Experiments\n\nIn this section, we conduct experiments on various\
    \ benchmarks to evaluate our approach. Mainly, we design the experiments to study\
    \ the following key questions:\n\nQ1: Compared to vanilla GRPO, how does TON impact\
    \ performance and efficiency?\n\nQ2: Is there a correlation between TON's skipping\
    \ behavior and the strength of reasoning ability (*e.g.,* different model sizes\
    \ or a single model under different iterations)?\n\nQ3: Do we really need SFT\
    \ with thought dropout? Can we rely solely on prompt following if the base model\
    \ is strong enough?\n\n#### 5.1 Benchmarks and Settings\n\nTo evaluate the effectiveness\
    \ and generalization ability of our approach on the below settings:\n\nBenchmarks.\
    \ We evaluate TON on three vision-language benchmarks, including the general benchmark\
    \ CLEVR [\\[14\\]](#page-10-1) (3D object counting), agent benchmark AITZ [\\\
    [13\\]](#page-10-0) (mobile navigation), and the math benchmark GeoQA [\\[16\\\
    ]](#page-10-3) (middle school math questions) as illustrated in Table [1,](#page-5-0)\n\
    \n<span id=\"page-5-0\"></span>Table 1: Summary of benchmark used in our evaluation.\n\
    \n| Benchmark        | OOD | Type                        |      |           |\
    \ Difficulty Answer Thought len. |\n|------------------|-----|-----------------------------|------|-----------|--------------------------------|\n\
    | CLEVR [14]       |     | Counting                    | Easy | Integrate | 586\
    \                            |\n| Super-CLEVR [15] | ✓   | Counting          \
    \          | Easy | Integrate | –                              |\n| GeoQA [16]\
    \       |     | Math                        | Hard | Number    | 1652        \
    \                   |\n| AITZ [13]        |     | GUI Agent Medium Action (x)\
    \ |      |           | 283                            |\n| AITZ (OOD)       |\
    \ ✓   | GUI Agent Medium Action (x) |      |           | 283                 \
    \           |\n\nspanning a spectrum of reasoning levels from simple to complex.\
    \ To benchmark the model's Out-of-Distribution (OOD) performance, we also evaluate\
    \ on Super-CLEVR [\\[15\\]](#page-10-2) to supplement the CLEVR. AITZ comprises\
    \ four test domains: we train on the {General} and test on the remaining OOD domains:\
    \ {Web shopping, Google apps, Install}. We remove the choices in GeoQA and ask\
    \ the model to generate the answer, enhancing the reasoning complexity. AITZ includes\
    \ action thought annotations, which we utilize directly, while applying our reverse\
    \ thinking to generate thoughts for SFT on CLEVR and GeoQA. More benchmark details\
    \ refer to Appendix [E.](#page-14-2)\n\nTraining details. We conduct our experiments\
    \ using Qwen-2.5-VL-Instruct-3B/7B [\\[38\\]](#page-11-15) as the base model.\
    \ All experiments are conducted utilizing 8 NVIDIA H20 GPUs. We train 100 steps\
    \ for both CLEVR and AITZ, and 300 epochs for GeoQA, given its higher reasoning\
    \ difficulty level. See setup details in Appendix [F.](#page-15-0) We leverage\
    \ vLLM [\\[39\\]](#page-11-16) to accelerate GRPO training. We add the SFT stage\
    \ before GRPO as the baseline on the agent task with the same setting as TON because\
    \ we observe that directly applying GRPO would cause the 0 coordinate reward during\
    \ the training process, considering its complex output format. For simplicity,\
    \ we set the dropout probabilities to 50% and examine the impact of different\
    \ dropout ratios selected from {20%, 50%, 80%} in Sec [5.3.](#page-6-0)\n\nFor\
    \ evaluation, we test all the datasets under the greedy strategy. In CLEVR and\
    \ GeoQA tasks, where answers are numerical, we measure accuracy by comparing the\
    \ predicted number to the ground truth. In the AITZ task, where answers are structured\
    \ as JSON-formatted actions, we report step-level and task-level metrics, including\
    \ type accuracy (correct action type) and exact accuracy (correct action type\
    \ and click coordinates) following [\\[17\\]](#page-10-4).\n\n<span id=\"page-6-1\"\
    ></span>![](_page_6_Figure_0.jpeg)\n\nFigure 4: Training metrics comparison between\
    \ TON and GRPO on GeoQA. (a) Training rewards, (b) Completion length over training\
    \ steps, (c) Ratio of non-think outputs to total samples at each step for TON,\
    \ and (d) Average completion length of think outputs across training.\n\n<span\
    \ id=\"page-6-2\"></span>Table 3: Out-of-domain (OOD) performance comparison between\
    \ our method TON and GRPO on the the AITZ – multi-step mobile navigation. 'Type'\
    \ is the action type accuracy and 'Exact' requires both the type and value to\
    \ be correct exactly. 'Avg.' is the average accuracy of all domains. 'Task-level\
    \ thought's' is the average output lengths on all OOD domains. Step-level accuracy\
    \ is reported.\n\n|                | Think? |      | IID     | OOD         | \
    \      |       |       |       | Avg     |       | Task-level |              \
    \  |\n|----------------|--------|------|---------|-------------|-------|-------|-------|-------|---------|-------|------------|----------------|\n\
    |                |        |      | General | Google apps |       |       | Web\
    \   |       | Install |       |            |                |\n|             \
    \   |        | type | exact   | type        | exact | type  | exact | type  |\
    \ exact   | type  | exact      | Thought's len. |\n| Qwen-2.5-VL-3B | ✓      |\
    \ 0.01 | 0       | 0.01        | 0     | 0.01  | 0     | 0.01  | 0       | 0.01\
    \  | 0          | 2132           |\n| w. SFT         | ✗      | 0.39 | 0.11  \
    \  | 0.44        | 0.12  | 0.54  | 0.19  | 0.47  | 0.17    | 0.46  | 0.15    \
    \   | 742            |\n| w. SFT         | ✓      | 0.67 | 0.12    | 0.53    \
    \    | 0.17  | 0.56  | 0.13  | 0.58  | 0.14    | 0.58  | 0.14       | 3572   \
    \        |\n| w. GPRO        | ✓      | 0.74 | 0.6     | 0.72        | 0.57  |\
    \ 0.7   | 0.5   | 0.81  | 0.65    | 0.74  | 0.59       | 3664           |\n| w.\
    \ TON         | Ours   | 0.74 | 0.6     | 0.74        | 0.56  | 0.72  | 0.5  \
    \ | 0.78  | 0.64    | 0.75  | 0.59       | 922            |\n| Gain          \
    \ |        | +0.0 | +0.0    | +0.02       | -0.01 | +0.02 | +0.0  | -0.03 | -0.01\
    \   | +0.01 | +0.0       | −2742          |\n| Qwen-2.5-VL-7B |        | 0.28\
    \ | 0.14    | 0.26        | 0.1   | 0.33  | 0.13  | 0.39  | 0.16    | 0.31  |\
    \ 0.13       | 3304           |\n| w. GRPO        | ✓      | 0.64 | 0.22    |\
    \ 0.73        | 0.32  | 0.6   | 0.15  | 0.62  | 0.23    | 0.65  | 0.23       |\
    \ 3272           |\n| w. TON         | Ours   | 0.74 | 0.54    | 0.62        |\
    \ 0.23  | 0.68  | 0.47  | 0.73  | 0.55    | 0.69  | 0.45       | 908         \
    \   |\n| Gain           |        | +0.1 | +0.32   | -0.11       | -0.09 | +0.08\
    \ | +0.32 | +0.09 | +0.32   | +0.04 | +0.22      | −2364          |\n\n### 5.2\
    \ Q1: Performance and Efficiency Comparison between TON and GRPO\n\nIn Table [2,](#page-5-1)\
    \ we present TON on the CLEVR and GeoQA benchmarks under both 3B and 7B settings,\
    \ with the performance, time consumption, and the average completion length at\
    \ the RL stage. We find that TON effectively reduces the average of the completion\
    \ length by up to 90% while achieving comparable even superior performance compared\
    \ to GRPO with a maximum of 17 Acc. gains. This imply that skipping unnecessary\
    \ reasoning can lead to better performance. The reduction of the completion length\
    \ decreases the decoding time when generating samples, thus simultaneously shortening\
    \ the training time. Figure [4a](#page-6-1) & [4b](#page-6-1) show the reward\
    \ and completion length curves where TON remains the rewards on par with vanilla\
    \ GRPO while the completion length reduces significantly. Appendix [G.2](#page-17-0)\
    \ & [G.1](#page-16-0) shows the entire metrics during training.\n\nMulti-step\
    \ Navigation and OOD Testing. In Table [3,](#page-6-2) we evaluate TON's performance\
    \ on AITZ – multi-step mobile navigation, we also assessed its generalization\
    \ capabilities on OOD test sets using a greedy decoding strategy. Table [3](#page-6-2)\
    \ summarizes the step-level type match accuracy and exact match accuracy for both\
    \ IID (general) and OOD (Google Apps, web shopping, and install) domains on AITZ,\
    \ with detailed training visualization in Appendix [G.3.](#page-17-1) Overall,\
    \ TON demonstrates comparable OOD generalization performance to GRPO, while significantly\
    \ reducing the task-level output length from 3K to 0.9K (70% token saving). This\
    \ highlights the strong potential of TON to substantially reduce completion length\
    \ without compromising performance. See Appendix [G.4](#page-18-0) for the OOD\
    \ performance on other benchmarks.\n\n### <span id=\"page-6-0\"></span>5.3 Q2:\
    \ Skip Thought Ratio Analysis\n\nBeyond the performance change and completion\
    \ length reduction achieved by TON, we further investigated the evolution of the\
    \ skip ratio in 'Thought dropout' during the training step. Figure [4c](#page-6-1)\
    \ illustrates the percentage of skip ratio in the generated samples at each step\
    \ on GeoQA. We observed an increasing trend in the skip ratio during the training\
    \ process with the increase in training reward. A similar trend is observed across\
    \ three benchmarks in Figure [13](#page-19-0) in the Appendix [G.6.](#page-19-1)\
    \ This phenomenon suggests that the model progressively internalizes the reasoning\
    \ process—learning\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_0.jpeg)\n\
    \nFigure 5: Further Analysis of TON on the AITZ benchmark. (a)(b)(c) is the average\
    \ completion length, skip thought ratios, and the reward under different dropout\
    \ probabilities. (d) Prompting (hybrid) does not reduce the completion length,\
    \ while TON using SFT can effectively reduce it.\n\nto skip explicit thoughts\
    \ while still producing accurate answers. Moreover, Figure [4d](#page-6-1) illustrates\
    \ the length of these outputs generated with 'think'. TON maintain comparable\
    \ lengths to the vanilla GRPO, indicating that the TON model can choose not to\
    \ think but remains diligent when deeper reasoning is necessary.\n\nThought dropout\
    \ ratio ablation. We experiment with the impact of different thought dropout ratios\
    \ of 20%, 50%, and 80% during the SFT stage. Figure [5a](#page-7-1) & [5b](#page-7-1)\
    \ show the completion lengths and the skip ratio during the training process on\
    \ AITZ. Figure [5c](#page-7-1) shows a close reward curve of these three variants.\
    \ Refer more metrics on Appendix [G.5.](#page-19-2) Although the dropout ratios\
    \ differ, TON consistently exhibits an increasing skip ratio as training progresses.\
    \ Notably, the 20% setting shows a rapid increase in skip rate, while the higher\
    \ 80% setting remains relatively stable throughout training. This motivates us\
    \ to start with a lower dropout probability for further investigation. TON can\
    \ then be dynamically optimized according to reward signals—decreasing the dropout\
    \ ratio when performance is high and increasing it when performance drops.\n\n\
    ### <span id=\"page-7-0\"></span>5.4 Q3: Emprical Verfication of SFT Significance\
    \ in TON\n\nIn addition to incorporating the skip-think format during the SFT\
    \ stage as in TON, we explored a simpler alternative: modifying the prompt to\
    \ encourage the model to automatically omit reasoning steps, enabling direct GRPO\
    \ training without the need for a separate SFT stage. The hybrid-thought prompt\
    \ is defined as follows:\n\n### Prompt for *Hybrid Thinking*\n\nA conversation\
    \ between User and Assistant. The user asks a question, and the Assistant solves\
    \ it. The assistant evaluates whether the question is simple enough to answer\
    \ directly. If simple, the output is formatted as <think>\\n\\n<\\think><answer>answer\
    \ here<\\answer>. If the question is difficult, the assistant needs to first think\
    \ then answering the question. The output is formatted as <think> reasoning process\
    \ here <\\think><answer> answer here <\\answer>.\n\nThe assistant is encouraged\
    \ to use <think>\\n\\n<\\think> while maintaining accuracy.\n\nFigure [5d](#page-7-1)\
    \ shows the completion length of GRPO using the hybrid prompt, vanilla GRPO (with\
    \ a full-think prompt), and TON throughout the training process on AITZ. Appendix\
    \ [G.7](#page-20-0) presents similar trends across three benchmarks, revealing\
    \ only minor differences in completion length between the hybrid prompt and vanilla\
    \ GRPO. Moreover, we observe only 2 'skip' cases in GeoQA and none in AITZ among\
    \ all samples generated by GRPO during both training and inference. We attribute\
    \ this to the model's tendency to play it safe by generating long and detailed\
    \ reasoning, consistent with its ingrained behavioral patterns learned during\
    \ pre-training or SFT. Since the model does not produce skip-thought outputs,\
    \ applying additional reward to these outputs has no effect, resulting in a zero\
    \ contribution throughout training. These findings highlight the necessity of\
    \ our SFT stage with thought dropout (Sec. [4.1\\)](#page-3-1) to establish the\
    \ desired format-following behavior.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_0.jpeg)\n\
    \nFigure 6: Comparison between GRPO and TON on Agent setting AITZ [\\[13\\]](#page-10-0).\
    \ TON adaptively skips unnecessary think steps during multi-step mobile navigation,\
    \ achieving greater decoding efficiency compared to GRPO while maintaining task\
    \ accuracy (saving 60% tokens in this case).\n\n<span id=\"page-8-1\"></span>Table\
    \ 4: Illustration between Thinking and Non-Thinking modes from CLEVR [\\[14\\\
    ]](#page-10-1). TON demonstrates selective activation of reasoning—engaging thought\
    \ only when needed—whereas GRPO generates reasoning traces for both cases indiscriminately.\
    \ The full outputs are in Table [10.](#page-22-0)\n\n| Counting examples from\
    \ CLEVR [14] |                                                               \
    \                                                                            \
    \                              |                                             \
    \                                                                            \
    \                                                                            \
    \                                                     |  |  |  |  |  |  |  |\n\
    |-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|--|--|--|--|\n\
    |                                   |                                        \
    \                                                                            \
    \                                                     |                      \
    \                                                                            \
    \                                                                            \
    \                                                                            |\
    \  |  |  |  |  |  |  |\n| Question (GT)                     | How many items are\
    \ there in the im<br>age? <answer>3</answer>                                 \
    \                                                                          | How\
    \ many items are there in the image?<br><answer>8</answer>                   \
    \                                                                            \
    \                                                                            \
    \                  |  |  |  |  |  |  |  |\n| GRPO's outputs                  \
    \  | <think> The image shows three objects<br><br>1. Green Cylinder: Transparent\
    \ (metal<br>lic) 2. Teal Sphere: Transparent (metal<br>lic)<br></think><br><answer>3</answer>\
    \ | <think> - I count six distinct objects in the<br>picture: a large green matte\
    \ cylinder, a blue<br>matte cylinder,<br>- I also note the size and color variety\
    \ of<br>each object as described by the visual cues.<br></think><br><answer>10</answer>\
    \      |  |  |  |  |  |  |  |\n| TON's outputs                     | <think>\\\
    n\\n</think><br><answer>3</answer>                                           \
    \                                                                            \
    \        | <think> Okay, let's check this out. The user<br>provided a image with<br>First,\
    \ I need to count<br>1. \"A small purple rubber sphere\" 3. \"A<br>small gray\
    \ rubber cube\" Third.<br><br>Wait, that's eight distinct entries </think><br><answer>\
    \ 8 </answer> |  |  |  |  |  |  |  |\n\n### 5.5 Qualitative Examples\n\nFigure\
    \ [6](#page-8-0) compares GRPO and TON on the AITZ benchmark for multi-step mobile\
    \ navigation. While GRPO generates verbose reasoning at every step, TON adaptively\
    \ skips unnecessary thinking, reducing token usage by 60% without sacrificing\
    \ task accuracy. This demonstrates TON's efficiency in handling real-world, *long-horizon\
    \ procedural agent tasks*. Table [4](#page-8-1) further illustrates TON's ability\
    \ to selectively activate reasoning. Unlike GRPO, which consistently generates\
    \ detailed thought traces, TON omits reasoning for simple questions that can be\
    \ answered at a glance, while producing accurate and focused reasoning for complex\
    \ scenarios involving visual occlusion.\n\n# 6 Conclusion\n\nWe present TON, a\
    \ simple yet effective two-stage training framework that enables vision-language\
    \ models to learn *when to reason*—introducing selective reasoning as a controllable\
    \ and trainable behavior. By combining thought dropout during supervised fine-tuning\
    \ with reward-guided refinement via GRPO, TON significantly reduces completion\
    \ length (up to 90%) without sacrificing—and in some cases improving—performance\
    \ across diverse reasoning tasks. Our findings challenge the assumption that full\
    \ reasoning traces are always beneficial and pave the way for more efficient,\
    \ human-like reasoning strategies in both multimodal intelligence and reinforcement\
    \ learning.\n\n# 7 Acknowledge\n\nWe sincerely thank Dongchi Huang for his invaluable\
    \ guidance on the code and for providing essential computational resources. We\
    \ also appreciate Binghui Xie's insightful discussion on topic selection and idea\
    \ suggestions. Additionally, we are grateful to Qiguang Chen and Yuxuan Wan for\
    \ their thoughtful and constructive feedback on this paper. Finally, we extend\
    \ our gratitude to Xiaojun Guo and Qixun Wang for their valuable advice on visual\
    \ reasoning and the GRPO series methods.\n\n# References\n\n- <span id=\"page-9-0\"\
    ></span>[1] Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, and Sikai Chen.\
    \ Vlm-rl: A unified vision language models and reinforcement learning framework\
    \ for safe autonomous driving. *arXiv preprint arXiv:2412.15544*, 2024.\n- <span\
    \ id=\"page-9-1\"></span>[2] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang\
    \ Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1:\
    \ Enhancing action prediction of gui agents by reinforcement learning, 2025.\n\
    - <span id=\"page-9-2\"></span>[3] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia.\
    \ Gui-r1 : A generalist r1-style vision-language action model for gui agents,\
    \ 2025.\n- <span id=\"page-9-3\"></span>[4] Hardy Chen, Haoqin Tu, Fali Wang,\
    \ Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an\
    \ early investigation into training r1-like reasoning large vision-language models,\
    \ 2025.\n- <span id=\"page-9-4\"></span>[5] Zhihong Shao, Peiyi Wang, Qihao Zhu,\
    \ Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu,\
    \ et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language\
    \ models. *arXiv preprint arXiv:2402.03300*, 2024.\n- <span id=\"page-9-5\"></span>[6]\
    \ Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking\
    \ and overthinking: An empirical study of reasoning length and correctness in\
    \ llms, 2025.\n- <span id=\"page-9-6\"></span>[7] Qiguang Chen, Libo Qin, Jiaqi\
    \ Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the capabilities of thought:\
    \ A reasoning boundary framework to quantify and optimize chain-ofthought, 2024.\n\
    - <span id=\"page-9-7\"></span>[8] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie\
    \ Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length\
    \ in llms, 2025.\n- <span id=\"page-9-8\"></span>[9] Xingyu Chen, Jiahao Xu, Tian\
    \ Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou,\
    \ Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think\
    \ that much for 2+3=? on the overthinking of o1-like llms, 2024.\n- <span id=\"\
    page-9-9\"></span>[10] Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu,\
    \ Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Adar1: From long-cot to\
    \ hybrid-cot via bi-level adaptive reasoning optimization, 2025.\n- <span id=\"\
    page-9-10\"></span>[11] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing\
    \ Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive\
    \ slow-thinking for large reasoning models, 2025.\n- <span id=\"page-9-11\"></span>[12]\
    \ An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\
    \ Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei\
    \ Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming\
    \ Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu,\
    \ Rui Men, Runji Lin, Tianhao Li,\n\nTingyu Xia, Xingzhang Ren, Xuancheng Ren,\
    \ Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\
    \ and Zihan Qiu. Qwen2.5 technical report. *arXiv preprint arXiv:2412.15115*,\
    \ 2024.\n\n- <span id=\"page-10-0\"></span>[13] Jiwen Zhang, Jihao Wu, Yihua Teng,\
    \ Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the\
    \ zoo: Chain-of-action-thought for gui agents. *arXiv preprint arXiv:2403.02713*,\
    \ 2024.\n- <span id=\"page-10-1\"></span>[14] Justin Johnson, Bharath Hariharan,\
    \ Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:\
    \ A diagnostic dataset for compositional language and elementary visual reasoning.\
    \ In *Proceedings of the IEEE conference on computer vision and pattern recognition*,\
    \ pages 2901–2910, 2017.\n- <span id=\"page-10-2\"></span>[15] Zhuowan Li, Xingrui\
    \ Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and\
    \ Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness\
    \ in visual reasoning. In *Proceedings of the IEEE/CVF conference on computer\
    \ vision and pattern recognition*, pages 14963–14973, 2023.\n- <span id=\"page-10-3\"\
    ></span>[16] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu,\
    \ Eric P Xing, and Liang Lin. Geoqa: A geometric question answering benchmark\
    \ towards multimodal numerical reasoning. *arXiv preprint arXiv:2105.14517*, 2021.\n\
    - <span id=\"page-10-4\"></span>[17] Kevin Qinghong Lin, Linjie Li, Difei Gao,\
    \ Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng\
    \ Shou. Showui: One vision-language-action model for gui visual agent. *arXiv\
    \ preprint arXiv:2411.17465*, 2024.\n- <span id=\"page-10-5\"></span>[18] Kanzhi\
    \ Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong\
    \ Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. *arXiv\
    \ preprint arXiv:2401.10935*, 2024.\n- <span id=\"page-10-6\"></span>[19] Wenyi\
    \ Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,\
    \ Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model\
    \ for gui agents. *arXiv preprint arXiv:2312.08914*, 2023.\n- <span id=\"page-10-7\"\
    ></span>[20] OpenAI. Gpt-4 technical report, 2023.\n- <span id=\"page-10-8\"></span>[21]\
    \ Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,\
    \ Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini:\
    \ a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*,\
    \ 2023.\n- <span id=\"page-10-9\"></span>[22] DeepSeek-AI, Daya Guo, Dejian Yang,\
    \ Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\
    \ Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong\
    \ Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei\
    \ Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai\
    \ Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo\
    \ Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang,\
    \ Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi\
    \ Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\
    \ Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\
    \ Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue\
    \ Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng\
    \ Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng\
    \ Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang,\
    \ R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\
    \ Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li,\
    \ Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang,\
    \ Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao\
    \ Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao\
    \ Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng\
    \ Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen\
    \ Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K.\
    \ Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng\
    \ Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi\
    \ Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou,\
    \ Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang\
    \ You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li,\
    \ Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,\
    \ Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan\n\nZhang, Zhewen\
    \ Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin\
    \ Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang,\
    \ and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via\
    \ reinforcement learning, 2025.\n\n- <span id=\"page-11-0\"></span>[23] John Schulman,\
    \ Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\
    \ optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.\n- <span id=\"\
    page-11-1\"></span>[24] OpenAI. Gpt-4v, 2023.\n- <span id=\"page-11-2\"></span>[25]\
    \ Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan\
    \ Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec.\
    \ Trl: Transformer reinforcement learning. <https://github.com/huggingface/trl>,\
    \ 2020.\n- <span id=\"page-11-3\"></span>[26] Rafael Rafailov, Archit Sharma,\
    \ Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\
    \ preference optimization: Your language model is secretly a reward model. *Advances\
    \ in Neural Information Processing Systems*, 36, 2024.\n- <span id=\"page-11-4\"\
    ></span>[27] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu\
    \ Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source\
    \ llm reinforcement learning system at scale. *arXiv preprint arXiv:2503.14476*,\
    \ 2025.\n- <span id=\"page-11-5\"></span>[28] Zichen Liu, Changyu Chen, Wenjun\
    \ Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding\
    \ r1-zero-like training: A critical perspective. *arXiv preprint arXiv:2503.20783*,\
    \ 2025.\n- <span id=\"page-11-6\"></span>[29] Jason Wei, Xuezhi Wang, Dale Schuurmans,\
    \ Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought\
    \ prompting elicits reasoning in large language models. *Advances in neural information\
    \ processing systems*, 35:24824–24837, 2022.\n- <span id=\"page-11-7\"></span>[30]\
    \ Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\
    \ and Yuan Cao. React: Synergizing reasoning and acting in language models. In\
    \ *International Conference on Learning Representations (ICLR)*, 2023.\n- <span\
    \ id=\"page-11-8\"></span>[31] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\
    \ Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan\
    \ Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. *arXiv\
    \ preprint arXiv:2303.11381*, 2023.\n- <span id=\"page-11-9\"></span>[32] Hao\
    \ Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral\
    \ Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement\
    \ learning. *arXiv preprint arXiv:2406.11896*, 2024.\n- <span id=\"page-11-10\"\
    ></span>[33] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi\
    \ Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training\
    \ llm web agents via self-evolving online curriculum reinforcement learning. *arXiv\
    \ preprint arXiv:2411.02337*, 2024.\n- <span id=\"page-11-11\"></span>[34] Izzeddin\
    \ Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck,\
    \ and Aleksandra Faust. A real-world webagent with planning, long context understanding,\
    \ and program synthesis. *arXiv preprint arXiv:2307.12856*, 2023.\n- <span id=\"\
    page-11-12\"></span>[35] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu\
    \ Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et\
    \ al. Kimi k1. 5: Scaling reinforcement learning with llms. *arXiv preprint arXiv:2501.12599*,\
    \ 2025.\n- <span id=\"page-11-13\"></span>[36] Zirui Zhao, Hanze Dong, Amrita\
    \ Saha, Caiming Xiong, and Doyen Sahoo. Automatic curriculum expert iteration\
    \ for reliable llm reasoning. *arXiv preprint arXiv:2410.07627*, 2024.\n- <span\
    \ id=\"page-11-14\"></span>[37] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu\
    \ Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning, 2025.\n\
    - <span id=\"page-11-15\"></span>[38] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin\
    \ Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.\
    \ Qwen2. 5-vl technical report. *arXiv preprint arXiv:2502.13923*, 2025.\n- <span\
    \ id=\"page-11-16\"></span>[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,\
    \ Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient\
    \ memory management for large language model serving with pagedattention. In *Proceedings\
    \ of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.\n-\
    \ <span id=\"page-11-17\"></span>[40] Christopher Rawles, Alice Li, Daniel Rodriguez,\
    \ Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset\
    \ for android device control. *arXiv preprint arXiv:2307.10088*, 2023.\n\n<span\
    \ id=\"page-12-0\"></span>[41] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan\
    \ Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient\
    \ fine-tuning of 100+ language models. *arXiv preprint arXiv:2403.13372*, 2024.\n\
    \n# Appendix\n\n# Contents\n\n| A | Motivation Experiments                   \
    \           | 15 |  |  |  |  |  |  |  |  |  |\n|---|-----------------------------------------------------|----|--|--|--|--|--|--|--|--|--|\n\
    | B | Rewards for Downstream Tasks                        |    |  |  |  |  | \
    \ |  |  |  |  |\n| C | Limitations                                         | \
    \   |  |  |  |  |  |  |  |  |  |\n| D | Broader Impact                       \
    \               |    |  |  |  |  |  |  |  |  |  |\n| E | Dataset             \
    \                                | 15 |  |  |  |  |  |  |  |  |  |\n| F | Setup\
    \                                               | 16 |  |  |  |  |  |  |  |  |\
    \  |\n| G | Experiments                                         | 17 |  |  | \
    \ |  |  |  |  |  |  |\n|   | G.1<br>TON on Math–GeoQA<br>.                   \
    \    | 17 |  |  |  |  |  |  |  |  |  |\n|   | G.2<br>TON on Counting–CLEVR   \
    \                     | 18 |  |  |  |  |  |  |  |  |  |\n|   | G.3<br>TON on Mobile\
    \ Agent–AITZ                     | 18 |  |  |  |  |  |  |  |  |  |\n|   | G.4<br>OOD\
    \ Performance of TON on CLEVR              | 19 |  |  |  |  |  |  |  |  |  |\n\
    |   | G.5<br>Different Thought Dropout Probabilities      | 20 |  |  |  |  | \
    \ |  |  |  |  |\n|   | G.6<br>Skip-thought Ratio on Different benchmarks   | 20\
    \ |  |  |  |  |  |  |  |  |  |\n|   | G.7<br>Prompt v.s. SFT on different benchmarks<br>.\
    \ | 21 |  |  |  |  |  |  |  |  |  |\n|   | G.8<br>Visualization Examples     \
    \                  | 22 |  |  |  |  |  |  |  |  |  |\n|   | G.9<br>Prompt for\
    \ AITZ task                         | 24 |  |  |  |  |  |  |  |  |  |\n|   | G.10\
    \ Reward for length                              | 25 |  |  |  |  |  |  |  | \
    \ |  |\n|   |                                                     |    |  |  |\
    \  |  |  |  |  |  |  |\n\n# <span id=\"page-14-0\"></span>A Motivation Experiments\n\
    \nTable [5](#page-14-5) presents the performance of the agent VQA with and without\
    \ the think source during the SFT stage, as well as with and without the think\
    \ format in the GRPO reward function. Using the think source results in higher\
    \ performance but longer output lengths, while excluding it leads to lower performance\
    \ with shorter outputs.\n\n<span id=\"page-14-5\"></span>Table 5: Qwen2.5-VL-3B\
    \ on the agent dataset (Android-in-the-zoo) with/ without think before the answer\
    \ by SFT and vanilla GRPO. acc is the test accuracy and len is the output length\
    \ of step-level.\n\n|          | SFT  |     | GRPO |     |\n|----------|------|-----|------|-----|\n\
    |          | acc  | len | acc  | len |\n| w think  | 0.67 | 181 | 0.74 | 88  |\n\
    | wo think | 0.39 | 29  | 0.58 | 29  |\n\n# <span id=\"page-14-1\"></span>B Rewards\
    \ for Downstream Tasks\n\nGeneral VQA: r = r<sup>f</sup> + rd, where r<sup>f</sup>\
    \ is the format reward and r<sup>f</sup> = 1 if the response follows the think\
    \ answer format, otherwise 0, r<sup>d</sup> is the concrete reward that r<sup>d</sup>\
    \ = 1 if the predicted answer is equal to the ground truth number.\n\nAgent VQA:\
    \ r = r<sup>f</sup> + r<sup>d</sup> + rc, where r<sup>f</sup> is the format reward\
    \ and r<sup>f</sup> = 1 if the response follows the think answer format, otherwise\
    \ 0, r<sup>d</sup> is the concrete reward that r<sup>d</sup> = 1 if the predicted\
    \ action type is equal to the ground truth action type,*e.g.,*click, press\\_home,\
    \ r<sup>c</sup> is the continues reward for the predicted coordinates when the\
    \ action type is click. In this paper, we use the normalized coordinates ranging\
    \ from 0-1 and set θ = 0.14 following [\\[18\\]](#page-10-5).\n\nMath VQA: r =\
    \ r<sup>f</sup> + rd, where r<sup>f</sup> is the format reward and r<sup>f</sup>\
    \ = 1 if the response follows the think answer format, otherwise 0, r<sup>d</sup>\
    \ is the concrete reward that r<sup>d</sup> = 1 if the predicted answer is equal\
    \ to the ground truth number.\n\n# <span id=\"page-14-3\"></span>C Limitations\n\
    \nDue to computational resources, our current work focuses on smaller-sized visual-language\
    \ models like 3B and 7B, the proposed method has not been evaluated on even larger\
    \ models (*e.g.,*235B). We implement TON on the open-domain VLMs; however, without\
    \ access to the source code of proprietary VLMs like GPT-4o, the proposed method\
    \ has not been implemented on them.\n\n# <span id=\"page-14-4\"></span>D Broader\
    \ Impact\n\nIn this paper, we propose a simple yet effective method TON, to cooperate\
    \ SFT and RL stages by thought dropout. We improve the vanilla GRPO's performance\
    \ by sampling minor code changes to teach the model to reason during the RL exploration\
    \ stage selectively. This enables a deeper understanding of RL in VLMs, inspiring\
    \ flexible injection of prior knowledge into the SFT stage instead of manually\
    \ creating rule-based rewards. For social impact, this work has a certain impact\
    \ on the RL research in the VLM and LLM.\n\n# <span id=\"page-14-2\"></span>E\
    \ Dataset\n\nGeneral VQA. The CLEVR dataset [\\[14\\]](#page-10-1) is designed\
    \ to generate complex multi-step questions based on synthetic images, assessing\
    \ a model's true reasoning ability. It is a diagnostic dataset that includes 100,000\
    \ rendered images and approximately one million automatically generated questions,\
    \ of which 853,000 are unique. The dataset features challenging questions involving\
    \ counting, comparison, logical reasoning, and memory storage, while the images\
    \ depict simple 3D shapes. In contrast to the original CLEVR dataset, Super-CLEVR\
    \ [\\[15\\]](#page-10-2) introduces more complex visual components and\n\noffers\
    \ better control over the factors contributing to domain shift. For our experiments,\
    \ we select a subset of 1,000 datasets that contain only counting problems for\
    \ training. We evaluate the model's performance on test sets by selecting 200\
    \ samples from CLEVR that were not seen in the training set, as well as 200 counting\
    \ problems from the out-of-distribution Super-CLEVR dataset.\n\nMath VQA. GeoQA\
    \ [\\[16\\]](#page-10-3) is a large-scale geometric question answering dataset\
    \ that contains 4,998 geometric problems collected from real math exams in Chinese\
    \ middle school. Each problem is accompanied by annotated programs illustrating\
    \ the solution process. While this dataset features multiple-choice questions,\
    \ we increase the difficulty in this paper by removing the answer choices and\
    \ requiring the model to generate the answers directly. We select a subset of\
    \ 1k problems that involve computing angles and side lengths for training and\
    \ test the model on this training set.\n\nGUI Agent. AITZ [\\[13\\]](#page-10-0)\
    \ is a dataset designed for the graph user interface (GUI) navigation task derived\
    \ from the large-scale mobile benchmark Android-in-the-wild (AITW [\\[40\\]](#page-11-17)).\
    \ It features a unique annotation called chain-of-action thought (CoAT), establishing\
    \ a connection between perception—specifically, the understanding of screen layouts\
    \ and UI elements—and cognition, which involves action decision-making. The AITZ\
    \ dataset includes 2,504 operational trajectories that encompass 18.6K real-world\
    \ intentions. Additionally, it is categorized into five subsets based on application\
    \ domains: General, Install, GoogleApps, Single, and WebShopping. We train the\
    \ model using the General domain with a dataset of randomly selected 1k examples\
    \ and evaluate its performance on the corresponding test sets, as well as on other\
    \ out-of-distribution domains.\n\n# <span id=\"page-15-0\"></span>F Setup\n\n\
    We use Llamafactory [\\[41\\]](#page-12-0) for the SFT stage with full parameters,\
    \ and the training time is no longer than 15 minutes for both Qwen2.5-VL-3B/7B\
    \ models. We set θ = 0.14 following [\\[18\\]](#page-10-5). We use vLLM [\\[39\\\
    ]](#page-11-16) and the zero1\\_no\\_optimizer GRPO settings to optimize further:\n\
    \n| Parameter                   | Value  |\n|-----------------------------|--------|\n\
    | cutoff_len                  | 2048   |\n| per_device_train_batch_size | 8  \
    \    |\n| gradient_accumulation_steps | 1      |\n| learning_rate            \
    \   | 1.0e-5 |\n| lr_scheduler_type           | cosine |\n| warmup_ratio     \
    \           | 0.1    |\n| epoch                       | 2      |\n\nTable 6: Training\
    \ Parameters for the first SFT of TON\n\nTable 7: Training Parameters for the\
    \ second GRPO stage of TON in general/agent\n\n| Parameter                   |\
    \ Value             |\n|-----------------------------|-------------------|\n|\
    \ max_prompt_length           | 4096              |\n| max_completion_length \
    \      | 2048              |\n| per_device_train_batch_size | 1              \
    \   |\n| gradient_accumulation_steps | 2                 |\n| learning_rate  \
    \             | 1e-6              |\n| lr_scheduler_type           | constant\
    \          |\n| bf16                        | true              |\n| β       \
    \                    | 0.04              |\n| gradient_checkpointing      | true\
    \              |\n| attn_implementation         | flash_attention_2 |\n| min_pixels\
    \                  | 3136              |\n| max_pixels                  | 501760\
    \            |\n| temperature                 | 1.0               |\n| num_generations\
    \             | 8                 |\n| step                        | 100     \
    \          |\n\n| Parameter                   | Value             |\n|-----------------------------|-------------------|\n\
    | max_prompt_length           | 4096              |\n| max_completion_length \
    \      | 2048              |\n| per_device_train_batch_size | 1              \
    \   |\n| gradient_accumulation_steps | 2                 |\n| learning_rate  \
    \             | 1e-6              |\n| lr_scheduler_type           | constant\
    \          |\n| bf16                        | true              |\n| β       \
    \                    | 0.04              |\n| attn_implementation         | flash_attention_2\
    \ |\n| min_pixels                  | 3136              |\n| max_pixels       \
    \           | 501760            |\n| temperature                 | 1.0       \
    \        |\n| num_generations             | 4                 |\n| step      \
    \                  | 300               |\n\nTable 8: Training Parameters for the\
    \ second GRPO stage of TON in math\n\n# <span id=\"page-16-1\"></span>G Experiments\n\
    \n# <span id=\"page-16-0\"></span>G.1 TON on Math–GeoQA\n\nFigure [7](#page-16-2)\
    \ & [8](#page-17-2) illustrate the progression of various variables throughout\
    \ the training process.\n\n<span id=\"page-16-2\"></span>![](_page_16_Figure_5.jpeg)\n\
    \nFigure 7: TON and GRPO visualization during the training process on Qwen2.5-VL-3B\
    \ on GeoQA.\n\n<span id=\"page-17-2\"></span>![](_page_17_Figure_0.jpeg)\n\nFigure\
    \ 8: TON and GRPO visualization during the training process on Qwen2.5-VL-7B on\
    \ GeoQA.\n\n### <span id=\"page-17-0\"></span>G.2 TON on Counting–CLEVR\n\nFigure\
    \ [9](#page-17-3) illustrates the progression of various variables throughout\
    \ the training process.\n\n<span id=\"page-17-3\"></span>![](_page_17_Figure_4.jpeg)\n\
    \nFigure 9: TON and GRPO visualization during the training process on Qwen2.5-VL-3B\
    \ on CLEVR.\n\n# <span id=\"page-17-1\"></span>G.3 TON on Mobile Agent–AITZ\n\n\
    Figure [10](#page-18-1) & [11](#page-18-2) illustrate the progression of various\
    \ variables throughout the training process.\n\n<span id=\"page-18-1\"></span>![](_page_18_Figure_0.jpeg)\n\
    \nFigure 10: TON and GRPO visualization during the training process on Qwen2.5-VL-3B\
    \ on AITZ.\n\n<span id=\"page-18-2\"></span>![](_page_18_Figure_2.jpeg)\n\nFigure\
    \ 11: TON and GRPO visualization during the training process on Qwen2.5-VL-7B\
    \ on AITZ.\n\n### <span id=\"page-18-0\"></span>G.4 OOD Performance of TON on\
    \ CLEVR\n\nTable [9](#page-19-3) compares the IID and OOD performance of TON and\
    \ vanilla GRPO. TON demonstrates superior performance in both IID and, particularly,\
    \ OOD scenarios under easy reasoning tasks, helping to avoid overfitting to the\
    \ training set of vanilla GRPO.\n\n|      | think | CLEVR<br>acc | Super-CLEVR<br>acc\
    \ |\n|------|-------|--------------|--------------------|\n| base | ✓     | 64\
    \           | 57.3               |\n| SFT  |       | 88.5         | 13.17    \
    \          |\n| GRPO | ✓     | 93.5         | 51.9               |\n| TON  | ours\
    \  | 98.5         | 62.79              |\n\n<span id=\"page-19-3\"></span>Table\
    \ 9: Qwen2.5-VL-3B on the IID domain CLEVR and OOD domain Super-CLEVR.\n\n###\
    \ <span id=\"page-19-2\"></span>G.5 Different Thought Dropout Probabilities\n\n\
    Figure [12](#page-19-4) illustrates the progression of various variables throughout\
    \ the training process under different dropout probabilities.\n\n<span id=\"page-19-4\"\
    ></span>![](_page_19_Figure_4.jpeg)\n\nFigure 12: GRPO visualization during the\
    \ training process on Qwen2.5-VL-3B on AITZ under dropout probabilities 20%, 50%,\
    \ 80%.\n\n### <span id=\"page-19-1\"></span>G.6 Skip-thought Ratio on Different\
    \ benchmarks\n\nFigure [13](#page-19-0) illustrates the skip-thought ratios under\
    \ TON throughout the training process under different VQA benchmarks.\n\n<span\
    \ id=\"page-19-0\"></span>![](_page_19_Figure_8.jpeg)\n\nFigure 13: Skip Ratio\
    \ of the output thinking during our TON training on three benchmarks.\n\n####\
    \ <span id=\"page-20-0\"></span>G.7 Prompt v.s. SFT on different benchmarks\n\n\
    Figure [14](#page-20-1) & [15](#page-20-2) & [16](#page-21-1) illustrate the progression\
    \ of various variables throughout the training process between injecting the skip-thought\
    \ during the prompt and the SFT stage.\n\n<span id=\"page-20-1\"></span>![](_page_20_Figure_2.jpeg)\n\
    \nFigure 14: hybrid prompt v.s. SFT visualization during the training process\
    \ on Qwen2.5-VL-3B on clevr.\n\n<span id=\"page-20-2\"></span>![](_page_20_Figure_4.jpeg)\n\
    \nFigure 15: hybrid prompt v.s. SFT visualization during the training process\
    \ on Qwen2.5-VL-3B on AITZ.\n\n<span id=\"page-21-1\"></span>![](_page_21_Figure_0.jpeg)\n\
    \nFigure 16: hybrid prompt v.s. SFT visualization during the training process\
    \ on Qwen2.5-VL-3B on GeoQA.\n\n#### <span id=\"page-21-0\"></span>G.8 Visualization\
    \ Examples\n\n![](_page_21_Figure_3.jpeg)\n\nFigure 17: Examples of TON on math\
    \ VQA and GUI agent VQA settings.\n\n<span id=\"page-22-0\"></span>\n\n| Counting\
    \ examples from CLEVR [14] |                                                 \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                           |                                                \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                   |  |  |\n\
    |-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|\n\
    |                                   |                                        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                    |                                       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            |\
    \  |  |\n| Question                          | How many items are there in the\
    \ im<br>age?                                                                 \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                            | How many items are there in the\
    \ image?                                                                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \       |  |  |\n| GRPO's outputs                    | <think>The image shows\
    \ three objects:<br>a large green cylinder from 1919, a<br>large teal sphere,\
    \ and a large golden<br>Both the teal and golden<br>sphere.<br>spheres appear\
    \ to be in a translucent<br>state, indicating that they are metallic.<br>The green\
    \ cylinder is plain and opaque,<br>without any reflective properties.<br>1.<br>Green\
    \ Cylinder: Transparent (metallic)<br>2. Teal Sphere: Transparent (metallic)<br>3.\
    \ Golden Sphere: Transparent (metal<br>lic) The question seems to ask for the<br>count\
    \ of items, and considering that all<br>three objects are distinguishable from<br>each\
    \ other as separate items in the im<br>age, the count is three. </think><br><answer>3</answer>\
    \ | <think> - I count six distinct objects in<br>the picture: a large green matte\
    \ cylinder,<br>a blue matte cylinder, a large green reflec<br>tive sphere, a large\
    \ blue matte cube, a small<br>blue matte cube, a light blue rubber object, a<br>shiny\
    \ silver cube, a large reflective blue ob<br>ject, a silver or dark green reflective\
    \ sphere,<br>a shiny purple metallic cylinder, and a blue<br>matte sphere. - I\
    \ observe the reflective sur<br>face properties of the objects around the<br>central\
    \ large green sphere. - I also note the<br>size and color variety of each object\
    \ as de<br>scribed by the visual cues. </think><br><answer>10</answer>       \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \      |  |  |\n| TON's outputs                     | <think>\\n\\n</think><br><answer>3</answer>\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                  | <think>Okay, let's check this out. The user<br>provided\
    \ a image with several objects and<br>their properties. Each entry starts with\
    \ \"A\"<br>followed by attributes like size, color, ma<br>terial, shape, rotation,\
    \ 3D coordinates, and<br>pixel coordinates.<br>First, I need to count how many\
    \ items there<br>are. Let me list each entry one by one.<br>1. \"A small purple\
    \ rubber sphere\" That's<br>the first item. 2. \"A large yellow rubber<br>cube\"\
    \ Second item. 3.<br>\"A small gray<br>rubber cube\" Third. 4. \"A large green<br>rubber\
    \ cylinder\" Fourth. 5. \"A small blue<br>metal cube\" Fifth.<br>6.<br>\"A small\
    \ green<br>metal sphere\" Sixth. 7. \"A large brown<br>metal cube\" Seventh. 8.\
    \ \"A small green<br>rubber cylinder\" Eighth.<br>Wait, that's eight distinct\
    \ entries. Each one<br>has unique combinations of attributes.<br>I<br>can't see\
    \ any duplicates. Each has different<br>values for color, size, material, rotation,\
    \ or<br>coordinates, so they're separate. So yes,<br>there are eight items in\
    \ total. </think><br><answer> 8 </answer> |  |  |\n\n<span id=\"page-22-1\"></span>Table\
    \ 10: Counting example from CLEVR [\\[14\\]](#page-10-1). Although the question\
    \ is simple, the two conditioned images differ significantly in difficulty: the\
    \ left image is clearly easier, while the right involves greater complexity due\
    \ to object occlusion. TON demonstrates selective activation of reasoning—engaging\
    \ thought only when needed—whereas GRPO generates reasoning traces for both cases\
    \ indiscriminately.\n\n### G.9 Prompt for AITZ task\n\n#### AITZ System Prompt\n\
    \nYou are an assistant trained to navigate the mobile phone. Given a task instruction,\
    \ a screen observation, and an action history sequence, output the next action\
    \ and wait for the next observation.\n\nHere is the action space:\n\n- 1. 'CLICK':\
    \ Click on an element, value is not applicable and the position [x,y] is required.\n\
    - 2. 'TYPE': Type a string into an element, value is a string to type and the\
    \ position is not applicable.\n- 3. 'SCROLL UP': Scroll up for the screen.\n-\
    \ 4. 'SCROLL DOWN': Scroll down for the screen.\n- 5. 'SCROLL LEFT': Scroll left\
    \ for the screen.\n- 6. 'SCROLL RIGHT': Scroll right for the screen.\n- 7. 'PRESS\
    \ BACK': Press for returning to the previous step, value and position are not\
    \ applicable.\n- 8. 'PRESS HOME': Press for returning to the home screen, value\
    \ and position are not applicable.\n- 9. 'PRESS ENTER': Press for submitting the\
    \ input content, value and position are not applicable.\n- 10. 'STATUS TASK COMPLETE':\
    \ Indicate the task is completed, value and position are not applicable.\n\nFormat\
    \ the action as a dictionary with the following keys: {'action': 'ACTION TYPE',\
    \ 'value': 'element', 'position': [x,y]}\n\nIf value or position is not applicable,\
    \ set it as 'None'. Position represents the relative coordinates on the screenshot\
    \ and should be scaled to a range of 0-1.\n\n#### Think Prompt\n\nPlease first\
    \ thinks about the reasoning process in the mind and then provides the user with\
    \ the action. The reasoning process and answer are enclosed within <think> </think>\
    \ and <action> </action> tags, respectively, i.e., <think> reasoning process here\
    \ </think><action> action here </action>\n\n### Overall Prompt\n\n{AITZ System\
    \ Prompt} {Think Prompt} Task: {Task} <image> Action history: {Action history}\
    \ \\*\\*Next action:\\*\\*\n\n### <span id=\"page-24-0\"></span>G.10 Reward for\
    \ length\n\nWe give the reward r<sup>l</sup> = 1 for the model if it outputs <think>\\\
    n\\n<\\think>, and otherwise 0. We find that the length reward remains 0 at the\
    \ first 100 steps, the whole training process visualization is as follows:\n\n\
    ![](_page_24_Figure_2.jpeg)\n\nFigure 18: Length penalty rewards and completion\
    \ length on AITZ datasets on Qwen2.5-VL-3B."
- title: "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework\n\
    \  for Pedagogical Visualization"
  abstract: 'While foundation models (FMs), such as diffusion models and large

    vision-language models (LVLMs), have been widely applied in educational

    contexts, their ability to generate pedagogically effective visual explanations

    remains limited. Most existing approaches focus primarily on textual reasoning,

    overlooking the critical role of structured and interpretable visualizations in

    supporting conceptual understanding. To better assess the visual reasoning

    capabilities of FMs in educational settings, we introduce EduVisBench, a

    multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem

    sets requiring visually grounded solutions, along with a fine-grained

    evaluation rubric informed by pedagogical theory. Our empirical analysis

    reveals that existing models frequently struggle with the inherent challenge of

    decomposing complex reasoning and translating it into visual representations

    aligned with human cognitive processes. To address these limitations, we

    propose EduVisAgent, a multi-agent collaborative framework that coordinates

    specialized agents for instructional planning, reasoning decomposition,

    metacognitive prompting, and visualization design. Experimental results show

    that EduVisAgent substantially outperforms all baselines, achieving a 40.2%

    improvement and delivering more educationally aligned visualizations.

    EduVisBench and EduVisAgent are available at

    https://github.com/aiming-lab/EduVisBench and

    https://github.com/aiming-lab/EduVisAgent.'
  url: http://arxiv.org/abs/2505.16832v1
  keywords: ''
  document: '# From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework
    for Pedagogical Visualization


    Haonian Ji<sup>1</sup>\* , Shi Qiu<sup>1</sup>\* , Siyang Xin<sup>1</sup>\* ,
    Siwei Han<sup>1</sup>\* Zhaorun Chen<sup>2</sup> , Dake Zhang<sup>3</sup> , Hongyi
    Wang<sup>3</sup> , Huaxiu Yao<sup>1</sup> <sup>1</sup>UNC-Chapel Hill, <sup>2</sup>University
    of Chicago, <sup>3</sup>Rutgers University


    ## Abstract


    While foundation models (FMs), such as diffusion models and large vision-language
    models (LVLMs), have been widely applied in educational contexts, their ability
    to generate pedagogically effective visual explanations remains limited. Most
    existing approaches focus primarily on textual reasoning, overlooking the critical
    role of structured and interpretable visualizations in supporting conceptual understanding.
    To better assess the visual reasoning capabilities of FMs in educational settings,
    we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features
    diverse STEM problem sets requiring visually grounded solutions, along with a
    fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis
    reveals that existing models frequently struggle with the inherent challenge of
    decomposing complex reasoning and translating it into visual representations aligned
    with human cognitive processes. To address these limitations, we propose Edu-VisAgent,
    a multi-agent collaborative framework that coordinates specialized agents for
    instructional planning, reasoning decomposition, metacognitive prompting, and
    visualization design. Experimental results show that EduVis-Agent substantially
    outperforms all baselines, achieving a 40.2% improvement and delivering more educationally
    aligned visualizations. EduVisBench and EduVisAgent are available at [github.com/aiming-lab/EduVisBench](https://github.com/aiming-lab/EduVisBench)
    and [github.com/aiming-lab/EduVisAgent.](https://github.com/aiming-lab/EduVisAgent)


    ## 1 Introduction


    *"To truly teach is not to tell the answer, but to illuminate the path."*


    While foundation models (FMs), such as diffusion models and large vision-language
    models (LVLMs), have been extensively adopted in educational domains [\(Chu et
    al.,](#page-8-0) [2025;](#page-8-0) [Wang et al.,](#page-9-0)


    <span id="page-0-0"></span>![](_page_0_Figure_8.jpeg)


    Figure 1: GPT-4o fails to illustrate its problem-solving with high-quality, logical,
    and explanatory visualization.


    [2024a\)](#page-9-0), including pedagogical agents providing automated classroom
    assistance and science learning agents offering textual explanations of problemsolving
    processes [\(Wu et al.,](#page-10-0) [2023\)](#page-10-0), their applications
    have predominantly focused on text-based interactions [\(Wu et al.,](#page-10-0)
    [2023;](#page-10-0) [Xu et al.,](#page-10-1) [2024\)](#page-10-1). However, in
    education, especially K-12 settings, creating compelling visualizations is crucial
    for cognitive comprehension and overall learning effectiveness [\(Presmeg,](#page-9-1)
    [2006\)](#page-9-1). Despite its importance, there is currently limited understanding
    of how FMs can effectively generate visually grounded elements (e.g., *diagrams*,
    *interactive education tools*, *illustrative graphics*) to support the pedagogical
    illustration of problem-solving processes.


    Currently, generating visually grounded elements for pedagogical reasoning poses
    several challenges: (1) decomposing complex reasoning into representable steps
    that align closely with human cognitive processes is non-trivial [\(Yang et al.,](#page-10-2)
    [2024;](#page-10-2) [Chen et al.,](#page-8-1) [2024d\)](#page-8-1); (2) precisely
    producing visual aids for each sub-step to optimally support learners is challenging
    [\(Hong et al.,](#page-8-2) [2025\)](#page-8-2); and (3) different educational
    domains require distinct visualization styles and formats, making consistent and
    adequate visual aid delivery difficult [\(Pandey and Ottley,](#page-9-2) [2025\)](#page-9-2).
    Addressing these obstacles first requires


    <sup>\*</sup>Equal contribution. Correspondence to: {haonianj, huaxiu}@cs.unc.edu


    a clear picture of how current FMs perform, so that future models can be purpose-built
    to close the gaps. Consequently, a comprehensive evaluation platform is critical
    for systematically assessing FMs on visual pedagogical reasoning.


    To bridge this gap, we introduce EduVisBench, a multi-domain, multi-level benchmark
    designed to evaluate the capacity of foundation models to generate pedagogically
    effective, step-by-step visual reasoning. EduVisBench comprises structured problem
    sets across diverse domains, each requiring multimodal-centric reasoning and solutions
    that prioritize visualization principles such as *interpretability*, *cognitive
    alignment*, and *instructional clarity* to achieve high evaluation score. To facilitate
    a detailed evaluation, we further develop a fine-grained rubric enabling multidimensional
    assessments of AI-generated visual outputs, focusing explicitly on pedagogical
    criteria such as *contextual relevance*, *visual clarity*, *multimodal coherence*,
    *reasoning support*, and *interactive engagement*.


    Utilizing this benchmark, we conduct extensive evaluations on a variety of FMs
    and agents. Our findings reveal that although current models achieve predominantly
    correct step-by-step textual analyses, they frequently fail to generate useful
    or faithful visualizations, as depicted in Figure [1.](#page-0-0) Specifically,
    our systematic analysis highlights recurring challenges including (1) semantic
    misalignments between textual explanations and visual components, (2) omissions
    of critical steps within rendered diagrams, and (3) structural inconsistencies
    in code-based visual outputs, collectively undermining accuracy, clarity, and
    interactivity.


    To address these limitations, we introduce a multi-agent collaborative framework,
    EduVis-Agent, designed to simulate the complete learning journey—from initial
    problem exposure to deep conceptual understanding. Specifically, a central planning
    agent orchestrates six specialized expert agents dedicated to *visualization design*,
    *cognitive scaffolding*, and *metacognitive regulation*. A synthesis module then
    integrates these expert outputs into interactive, personalized learning webpages
    tailored specifically to human learners. Experimental results demonstrate that
    our proposed method EduVisAgent achieves an average improvement of 40.2% than
    current SOTA method. This underscores the effectiveness of our approach—leveraging
    modular specialization and collaborative integration to produce robust and visually
    grounded learning solutions.


    <span id="page-1-0"></span>![](_page_1_Figure_4.jpeg)


    Figure 2: Dataset distribution of EduVisBench. Each domain encompasses various
    sub-domains, collectively covering 15 comprehensive pedagogical scenarios.


    # 2 EduVisBench Benchmark


    #### 2.1 Overview


    In this section, we introduce EduVisBench, a novel and challenging benchmark designed
    to evaluate the capability of models to generate logical and explanatory visualizations
    for educational purposes. As shown in Figure [2,](#page-1-0) EduVisBench comprises
    1,154 carefully curated STEM questions across three academic subjects and 15 distinct
    domains, organized into three levels of difficulty. In addition to assessing accuracy
    in step-by-step problem solving, EduVisBench places particular emphasis on a model''s
    ability to communicate the reasoning process clearly and visually—helping students
    understand problems through structured, interpretable visual outputs, as illustrated
    in Figure [3.](#page-2-0)


    Specifically, EduVisBench adopts a multimodal setting in which models are provided
    with both textual and visual inputs and are tasked with producing diverse output
    formats, including interactive web pages and visual diagrams. Beyond evaluating
    the correctness of final answers, we introduce a fine-grained evaluation framework
    that assesses the quality of visualizations across five key dimensions: (1) the
    logical sequencing of visual elements, (2) the structural richness of the visuals,
    (3) semantic alignment with the underlying subject matter, (4) the clarity and
    guidance provided for problemsolving, and (5) the level of interactivity and engagement.
    In the following subsections, we describe our dataset curation process and the
    design of the evaluation rubric in detail.


    #### 2.2 Dataset Curation


    EduVisBench is built from several high-quality public educational resources that
    we carefully cu-


    <span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Figure 3: Representative examples from EduVisBench, featuring questions from Maths,
    Chemistry, and Physics alongside their corresponding high-scoring visual explanations.
    These interactive visualizations, generated by our multi-agent system EduVisAgent,
    exemplify well-designed, pedagogically effective outputs for STEM problems.


    rated, translated, and adapted to support multimodal visualization learning tasks.
    Specifically, the chemistry questions are sourced from the C-MHChem-Benchmark
    [\(Zhang et al.,](#page-10-3) [2024a\)](#page-10-3), originally presented in Chinese
    and meticulously translated into English with careful attention to scientific
    accuracy and terminology. The physics questions are drawn from the high-school-physics
    [\(Ro](#page-9-3)[hith,](#page-9-3) [2023\)](#page-9-3) dataset, which includes
    a range of conceptual and quantitative exercises suitable for secondary-level
    learners. The mathematics component combines easy-level problems from the Illustrative
    Mathematics curriculum with mediumto hard-level questions selected from the MATH-500
    [\(Lightman et al.,](#page-9-4) [2023\)](#page-9-4) dataset. Furthermore, each
    domain encompasses diverse sub-domains, collectively covering 15 comprehensive
    pedagogical scenarios, as illustrated in Figure [2.](#page-1-0) All data sources
    were standardized into a unified format and consolidated to enable consistent
    and comprehensive evaluation across subjects.


    ### <span id="page-2-1"></span>2.3 Evaluation Metric


    In this subsection, we will detail the performance evaluation rubrics in EduVisBench.


    Evaluation Dimensions. To comprehensively evaluate the quality of generated visualizations
    in supporting student understanding and learning, we introduce a fine-grained
    scoring metric grounded in five pedagogically motivated dimensions: (1) Context
    Visualization: evaluates how clearly the vi-


    sualization situates the problem within a relevant context; (2) Diagram Design:
    assesses the clarity, accuracy, and effectiveness of the diagrams used to represent
    information; (3) Text–Graphic Integration: measures the coherence between textual
    explanations and visual elements, ensuring mutual interaction; (4) Thought Guidance:
    examines the extent to which the visualization supports reasoning processes and
    highlights critical thinking steps; (5) Interactivity: evaluates whether and how
    the visualization invites students engagement, reflection, or active manipulation.
    Each dimension captures a distinct aspect of effective multimedia learning, with
    detailed rubrics provided in Appendix [A.1](#page-11-0) to guide the scoring process.


    Rubrics and Evaluation Procedure. To ensure a systematic and replicable evaluation
    pipeline, we manually construct detailed scoring rubrics for each of the five
    evaluation dimensions, with clear criteria defined for every level on a 0–5 scale.
    The visual outputs generated by each model are then assessed using GPT-4o within
    a controlled evaluation environment. For static image outputs from textto-image
    models, the images are evaluated directly. For LVLMs, the generated SVG or HTML
    code is first rendered into its final visual form—either as a static image or
    an interactive webpage—before being evaluated. The detailed demonstration of our
    scoring procedure is in Figure [4.](#page-3-0)


    <span id="page-3-0"></span>![](_page_3_Figure_0.jpeg)


    Figure 4: Workflow for the EduVisBench benchmark evaluation. Models receive a
    visualization prompt and a question to generate visual outputs. All resulting
    visualizations undergo evaluation by GPT-4o across five dimensions to compute
    a total performance score.


    # 3 EduVisAgent


    Using the proposed EduVisBench benchmark, we systematically evaluate the performance
    of existing text-to-image models and LVLMs (see detailed results in Table [1](#page-5-0)
    in Section [4\)](#page-4-0). We find that most models perform poorly, with average
    scores below 50 on a 0–100 scale. This underperformance underscores the inherent
    challenge of decomposing complex reasoning and translating it into visual representations
    that align with human cognitive processes to effectively support education—a task
    that remains highly non-trivial.


    To address these challenges, we propose a multiagent system, EduVisAgent, inspired
    by pedagogical theories and designed to emulate the division of labor and collaborative
    reasoning found in expert instructional design. EduVisAgent consists of five specialized
    yet interdependent agents: a Task Planning Agent, which structures the instructional
    objective; a Conceptual Mapping Agent, which extracts and organizes key information;
    a Reasoning Decomposition Agent, which constructs step-by-step problem-solving
    logic; a Metacognitive Reviewer, which encourages summarization and learner reflection;
    and a Visualization Agent, which generates appropriate visual representations.
    This design introduces modularity and pedagogical


    <span id="page-3-1"></span>![](_page_3_Figure_5.jpeg)


    Figure 5: The structure of EduVisAgent.


    interpretability by embedding distinct instructional roles directly into the agent
    workflow. The overall operation of EduVisAgent proceeds in two stages: (1) instructional
    flow construction and (2) collaborative solution generation, as detailed below.


    # 3.1 Instructional Flow Construction


    The first stage of EduVisAgent focuses on formulating a well-structured instructional
    task based on the original problem. A key challenge lies in analyzing the underlying
    reasoning structure, identifying implicit logical dependencies, and associating
    each reasoning step with relevant conceptual knowledge. To address this, we employ
    the Task Planning Agent, which systematically organizes the problem into an instructional
    format suitable for multimodal visualization. Its main functions include: (1)
    breaking down the problem into coherent subgoals, (2) clarifying the reasoning
    expected at each step, (3) aligning each step with domain-specific principles
    or formulas, and (4) anticipating potential student misconceptions or cognitive
    needs. This structured formulation provides a pedagogically grounded foundation
    that guides the downstream agents in generating coherent, targeted, and educationally
    effective visual explanations.


    # 3.2 Collaborative Solution Generation


    In this stage, EduVisBench executes the instructional task constructed by sequentially
    activating a set of specialized agents, each responsible for completing a specific
    aspect of the task. As shown in Figure [5,](#page-3-1) these agents operate in
    a coordinated manner to enhance the coherence of instructional logic, improving
    the clarity of visual representation, and ensure alignment with educational objectives.
    Specifically:


    Conceptual Mapping Agent. This agent is responsible for extracting and organizing
    the core components of the input problem. Drawing on


    the Concrete–Representational–Abstract (CRA) instructional model [\(Nugroho and
    Jailani,](#page-9-5) [2019\)](#page-9-5), it classifies information into three
    categories: concrete entities, representational elements, and abstract constructs.
    The agent conducts fine-grained categorization and semantic summarization to support
    downstream visualization modules.


    Reasoning Decomposition Agent. This agent decomposes complex problems into manageable
    subcomponents and provides step-specific instructional guidance. It applies the
    memory-oriented FOPS strategy [\(Miller and Cohen,](#page-9-6) [2020\)](#page-9-6)—find
    the problem type (e.g., equation solving, conceptual reasoning, commonsense application,
    or graphical interpretation), organize the structure via equations or diagrams,
    plan the solution path, and solve the task. Based on the decomposed steps, the
    agent also identifies critical instructional points that require additional support,
    especially those that benefit from visual scaffolding or interactive guidance.
    Metacognitive Reviewer. Grounded in metacognitive theory [\(Schraw and Moshman,](#page-9-7)
    [1995\)](#page-9-7), this agent supports learners in monitoring their comprehension
    and reasoning processes. It generates reflective prompts that foster self-questioning
    and self-correction, encouraging learners to evaluate the soundness of their problem-solving
    approaches. Visualization Agent. This agent is responsible for constructing the
    "visual guidance" component of the instructional output. Instead of relying on
    decorative visuals, it emphasizes the use of abstract yet pedagogically effective
    representations—such as number lines, bar charts, schematic object illustrations,
    graphic organizers, sketch diagrams, and structured data tables. The agent ensures
    that each visualization is tightly aligned with the underlying abstract concept
    being taught. All visuals are rendered using the v0 [\(Vercel,](#page-9-8) [2025\)](#page-9-8)
    system for web-based deployment.


    # <span id="page-4-0"></span>4 Experiments


    This section outlines the experimental setup for benchmarking various foundation
    models on Edu-VisBench. We evaluate Diffusion Models, LVLMs, a specialized visualization
    agent (v0), and our proposed EduVisAgent. Our investigation seeks to address the
    following key questions: (1) How proficient are existing models at generating
    high-quality, explanatory visualizations within EduVisBench? (2) Can the proposed
    EduVisAgent system outperform current models? (3) What distinct performance patterns
    emerge across different model architectures, academic disciplines, and evaluation
    dimensions in EduVisBench?


    # 4.1 Experiment Setup


    Baseline Models. Our experimental evaluation encompasses a range of FMs, categorized
    as follows: (1) Image Generation Models: This category includes Flux.1-dev [\(Labs,](#page-9-9)
    [2024\)](#page-9-9), Stable Diffusion 3.5 Large (SD3.5) [\(IT Admin,](#page-8-3)
    [2024\)](#page-8-3), and Stable Diffusion XL Base 1.0 (SDXL) [\(Podell](#page-9-10)
    [et al.,](#page-9-10) [2023\)](#page-9-10). These models are tasked with generating
    static images directly from textual or visual inputs. (2) Large Vision-Language
    Models (LVLMs): We evaluate Deepseek-VL2 [\(Wu et al.,](#page-10-4) [2024\)](#page-10-4),
    GLM-4V-9B [\(GLM et al.,](#page-8-4) [2024\)](#page-8-4), MiniCPM-V2.6 [\(Yao
    et al.,](#page-10-5) [2024\)](#page-10-5), Mistral-Small-3.1-24B-Instruct-2503
    [\(Mistral AI,](#page-9-11) [2025\)](#page-9-11), Phi-3.5-Vision-Instruct [\(Abdin
    et al.,](#page-8-5) [2024\)](#page-8-5), Phi-4-Multimodal-Instruct [\(Abouelenin
    et al.,](#page-8-6) [2025\)](#page-8-6), Qwen2.5-VL-72B [\(Team,](#page-9-12)
    [2025\)](#page-9-12), GPT-4o [\(Hurst et al.,](#page-8-7) [2024\)](#page-8-7),
    Claude 3.7 Sonnet [\(Anthropic,](#page-8-8) [2025\)](#page-8-8), and Gemini 2.0
    Flash [\(Mallick and Kilpatrick,](#page-9-13) [2025\)](#page-9-13). These models
    are prompted to generate SVG or HTML code, which is then rendered into visual
    outputs for evaluation. (3) Specialized Visualization Agent: We also assess v0
    [\(Vercel,](#page-9-8) [2025\)](#page-9-8), an AI agent specifically designed
    to create interactive web pages based on instructional content.


    Evaluation Setups. During evaluation, all generated visualizations are standardized
    into image format. For interactive web pages containing buttons, an automated
    script navigates through all accessible sub-pages, capturing individual screenshots
    of each. Performance is assessed using the evaluation metric described in Section
    [2.3,](#page-2-1) where GPT-4o scores the visual outputs based on predefined rubrics,
    assigning a score from 0 to 5 for each of the five dimensions. The cumulative
    score (maximum 25 points) is then normalized to a 0–100 scale for standardized
    reporting and comparison.


    ## 4.2 Baseline Benchmarking


    The performance of all evaluated baseline models is detailed in Table [1.](#page-5-0)
    Across all evaluated models, the average scores indicate significant room for
    improvement. Diffusion Models generally exhibited the lowest performance, with
    average scores ranging from 13.8% (Flux.1-dev) to 21.8% (SDXL). This suggests
    that direct static image generation, while capable of producing visual elements,
    struggles substantially with the nuanced requirements of


    <span id="page-5-0"></span>


    | Method                      | Vis. Type | Easy | Maths<br>Medium | Hard                |
    Easy            | Physics<br>Medium | Hard | Easy | Chemistry<br>Medium | Hard
    | Avg  |

    |-----------------------------|-----------|------|-----------------|---------------------|-----------------|-------------------|------|------|---------------------|------|------|

    |                             |           |      |                 |                     |
    Diffusion Model |                   |      |      |                     |      |      |

    | Flux.1-dev                  | Image     | 13.8 | 13.4            | 13.2                |
    11.7            | 8.5               | 10.0 | 20.0 | 16.6                | 16.0
    | 13.8 |

    | SD3.5                       | Image     | 17.3 | 20.3            | 18.8                |
    16.8            | 13.0              | 12.0 | 22.8 | 21.7                | 34.0
    | 18.4 |

    | SDXL                        | Image     | 17.3 | 23.3            | 25.5                |
    18.9            | 15.4              | 24.0 | 33.6 | 30.2                | 24.0
    | 21.8 |

    | Large Vision Language Model |           |      |                 |                     |                 |                   |      |      |                     |      |      |

    | Deepseek VL2                | Webpage   | 20.3 | 17.1            | 15.7                |
    17.9            | 17.0              | 20.0 | 16.4 | 13.8                | 14.0
    | 17.5 |

    | GLM-4V-9B                   | Webpage   | 22.3 | 21.1            | 19.4                |
    24.5            | 21.5              | 24.0 | 22.3 | 21.5                | 16.0
    | 21.9 |

    | MiniCPM-V-2.6               | Webpage   | 24.1 | 17.3            | 15.5                |
    19.1            | 17.4              | 20.0 | 14.5 | 15.2                | 12.0
    | 19.3 |

    | Mistral-Small-3.1           | Webpage   | 29.1 | 31.6            | 32.2                |
    32.3            | 33.5              | 20.0 | 30.6 | 27.5                | 24.0
    | 30.2 |

    | Phi-3.5                     | Webpage   | 25.3 | 20.7            | 19.1                |
    21.2            | 19.5              | 12.0 | 20.0 | 18.6                | 20.0
    | 21.8 |

    | Phi-4                       | Webpage   | 26.1 | 25.1            | 22.9                |
    27.8            | 25.5              | 24.0 | 31.2 | 27.5                | 12.0
    | 26.4 |

    | Qwen2.5-VL-72B              | Webpage   | 24.3 | 18.1            | 15.8                |
    19.7            | 17.1              | 24.0 | 18.2 | 16.4                | 12.0
    | 20.0 |

    | Claude 3.7 Sonnet           | SVG       | 61.2 | 26.7            | 23.6                |
    18.5            | 16.9              | 14.0 | 47.5 | 47.2                | 18.0
    | 42.0 |

    | Claude 3.7 Sonnet           | Webpage   | 56.2 | 57.5            | 55.6                |
    44.8            | 42.6              | 24.0 | 61.1 | 60.6                | 64.0
    | 54.6 |

    | GPT-4o                      | Webpage   | 47.6 | 39.3            | 37.9                |
    25.7            | 24.2              | 24.0 | 34.3 | 32.6                | 36.0
    | 38.1 |

    | GPT-4o                      | SVG       | 36.1 | 19.7            | 19.5                |
    13.0            | 12.8              | 4.0  | 30.0 | 27.5                | 22.0
    | 26.3 |

    | Gemini 2.0 Flash            | Webpage   | 46.9 | 9.5             | 15.7                |
    31.7            | 26.5              | 24.0 | 32.0 | 25.8                | 30.0
    | 43.6 |

    |                             |           |      |                 | Visualization
    Agent |                 |                   |      |      |                     |      |      |

    | v0                          | Webpage   | 63.0 | 37.6            | 47.2                |
    53.3            | 58.5              | 52.0 | 74.7 | 52.8                | 68.0
    | 58.2 |


    Table 1: Performance of Diffusion Models, Large Vision Language Models and v0
    on EduVisBench.


    explanative and guiding visualizations for complex logical problems in our benchmark.


    LVLMs typically scored between 17.5% (Deepseek VL2) and 30.2% (Mistral-Small-3.1).
    Notable exceptions include Gemini 2.0 Flash (43.6%) and Claude 3.7 Sonnet; the
    latter''s significantly better performance with Webpages (54.6%) over SVG (42.0%).
    GPT-4o also showed a preference for Webpage generation (38.1%) over SVG (26.3%),
    suggesting that prompting advanced LVLMs for structured interactive webpages can
    yield more effective visual explanations. Nevertheless, even these top-tier LVLMs
    face considerable challenges in consistently meeting all of evaluation criteria.
    The visualization agent v0, specifically designed for webpage generation, achieved
    the highest average score among all baseline models at 58.2%. This result highlights
    the advantage of a specialized agent in this task over more general-purpose FMs.


    #### 4.3 Performance Analysis of EduVisBench


    Building upon the insights gained from the baseline evaluations, we assessed our
    proposed multi-agent system, EduVisAgent. The results, presented in Table [2](#page-6-0)
    demonstrate a substantial leap in performance for generating explanative and logical
    valuable visualizations for STEM problems. EduVisAgent achieved an impressive
    overall average score of 81.6%. Specifically, EduVisAgent surpasses the best-performing
    baseline v0 (58.2%), by a remark-


    able 23.4 percentage points. This constitutes an approximately 40.2% relative
    improvement, underscoring the efficacy of our multi-agent architecture and the
    integration of educational methodologies. Compared to the best performing LVLM
    (Claude 3.7 Sonnet Webpage at 54.6%) and the top diffusion model (SDXL at 21.8%),
    the advancement offered by EduVisAgent is even more pronounced. These results
    clearly indicate that the design principles underlying EduVisAgent, which incorporate
    a multi-agent structure and pedagogical strategies, effectively address many of
    the limitations observed in existing generative models.


    # 4.4 Case Analysis


    To further illustrate the limitations of existing baselines and how our approach
    addresses these challenges, we present two case studies in Figure [6.](#page-6-1)
    On the left, for a chemistry question, the GPT-4ogenerated solution lacks intuitive
    visualization of the chemical processes, resulting in fragmented information without
    visual guidance—reflected in a low score of just 28%. In contrast, EduVisAgent
    begins by displaying background images of the relevant chemical elements, activating
    students'' prior knowledge. It then contextualizes each of the four answer options
    with real-world scenarios, thereby enhancing students'' understanding of the underlying
    chemical transformations.


    Conversely, for the Carnot cycle efficiency physics problem (right side of Figure
    [6\)](#page-6-1), the Gem-


    <span id="page-6-0"></span>


    | Method      | Vis. Type | Easy | Medium    | Hard | Avg  |  |

    |-------------|-----------|------|-----------|------|------|--|

    |             |           |      | Maths     |      |      |  |

    |             |           | 90.2 | 64.5      | 65.0 |      |  |

    | EduVisAgent | Webpage   |      | Physics   |      | 81.6 |  |

    |             |           | 85.3 | 81.7      | 84.0 |      |  |

    |             |           |      | Chemistry |      |      |  |

    |             |           | 69.0 | 76.3      | 76.0 |      |  |


    (a) Performance of our EduVisAgent on EduVisBench. (b) Comparison of average score
    across all models.


    Table 2: Overall comparison of models: left is our EduVisAgent performance, right
    is the bar chart. EduVisAgent achieves the highest average score among all models.


    <span id="page-6-1"></span>![](_page_6_Figure_4.jpeg)


    Figure 6: Baseline models versus our EduVisAgent. These examples clearly demonstrate
    the often poor output quality of baseline models, contrasting sharply with the
    high-quality, effective visualizations produced by EduVisAgent.


    ini solution presents a single, flawed chart. Its depiction of 300K and 400K temperatures
    with identical heights introduces visual misinformation, failing to accurately
    represent data differences and thereby diminishing its pedagogical value. In stark
    contrast, EduVisAgent employs a multi-agent collaborative approach: it first generates
    a concrete factory scene to activate students'' working memory of the "heat engine"
    concept. Subsequently, it constructs an accurate Carnot cycle diagram and offers
    a step-by-step problem breakdown, fostering clear conceptual understanding. Crucially,
    EduVisAgent provides interactive visualization components, enabling users to dynamically
    adjust temperatures via sliders and observe real-time changes in heat engine efficiency.
    This interactive engagement significantly facilitates higher-order thinking skills.


    Overall, through coordinated multi-agent optimization of image design, instructional
    structure, and learning pathways, EduVisAgent significantly outperforms traditional
    single-model approaches in accuracy, guidance, and interactivity.


    ## 4.5 Fine-Grained Analysis on Five Evaluation Dimensions


    Figure [7](#page-7-0) reveals distinct performance profiles for eight well performed
    evaluated models. In Context Visualization and Diagram Design, most baselines,
    including SDXL, Claude 3.7, and v0, exhibit moderate to low scores, often struggling
    with providing rich situational cues or pedagogically sound visual structures,
    especially for complex problems. v0 and Claude show relatively better capabilities
    in Text-Graphic Integration and Thought Guidance compared to other FMs, which
    generally offer minimal support in these areas. However, all baseline models,
    including v0, are significantly limited in the Interactivity dimension, primarily
    due to their


    <span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)


    Figure 7: Fine-grained performance comparison across our five key evaluation dimensions.


    output format (static images/SVG or less dynamic webpages). In contrast, our EduVisAgent
    demonstrates consistently strong performance across all five dimensions. It particularly
    excels in creating rich context visualizations, well-structured diagram designs,
    and ensuring seamless text-graphic integration. Furthermore, EduVisAgent provides
    superior thought guidance and achieves notably high scores in Interactivity, areas
    where baseline models significantly lag. This comprehensive strength highlights
    EduVisAgent''s advanced ability to generate not just visualizations, but truly
    effective and interactive pedagogical tools.


    ## 5 Related Work


    LLM for Pedagogical Assistance. Foundation models (FMs), including diffusion models
    and large vision-language models (LVLMs), have been increasingly adopted in educational
    contexts [\(Chu](#page-8-0) [et al.,](#page-8-0) [2025;](#page-8-0) [Wang et al.,](#page-9-0)
    [2024a\)](#page-9-0) to support teaching and classroom interactions. EduAgent
    [\(Xu](#page-10-1) [et al.,](#page-10-1) [2024\)](#page-10-1) and Teachtune [\(Jin
    et al.,](#page-9-14) [2025\)](#page-9-14) enhance the problem-solving process
    through automated simulations of student-teacher dialogues, collaborative learning,
    and task-oriented reasoning. Agents such as SEFL [\(Zhang et al.,](#page-10-6)
    [2025\)](#page-10-6) and PROF [\(Nair et al.,](#page-9-15) [2024\)](#page-9-15)
    synthesize immediate, on-demand feedback to support large-scale instructional
    scenarios. Furthermore, domain-specific agents such as MathChat [\(Wu et al.,](#page-10-0)
    [2023\)](#page-10-0), NEW-TON [\(Wang et al.,](#page-10-7) [2023b\)](#page-10-7),
    and MEDCO [\(Wei](#page-10-8) [et al.,](#page-10-8) [2024\)](#page-10-8) further
    provide textual explanations tailored to scientific and medical education. While
    these systems address diverse pedagogical needs, their focus remains largely on
    text-based interactions [\(Wu et al.,](#page-10-0) [2023;](#page-10-0) [Xu et
    al.,](#page-10-1) [2024;](#page-10-1) [Cui et al.,](#page-8-9) [2024\)](#page-8-9),
    overlooking the critical role of visualization in fostering conceptual understanding
    and improving learning outcomes [\(Presmeg,](#page-9-1) [2006\)](#page-9-1). Despite
    its pedagogical importance, the capacity of FMs and agents to generate logical,
    explanatory visual illustrations remains underexplored. EduVisBench is the first
    comprehensive benchmark designed to systematically evaluate FMs'' ability to produce
    pedagogically effective, step-by-step visual reasoning, covering 15 diverse visually
    grounded educational scenarios with multi-level problem sets and multimodal-centric
    solutions, providing a rigorous platform for visual pedagogy assessment.


    LLM for Scientific Visualization. While some existing works have preliminarily
    explored the potential of FMs in supporting visual scaffolding [\(Podo et al.,](#page-9-16)
    [2024;](#page-9-16) [Chen et al.,](#page-8-10) [2024c;](#page-8-10) [Pandey](#page-9-2)
    [and Ottley,](#page-9-2) [2025;](#page-9-2) [Hong et al.,](#page-8-2) [2025\)](#page-8-2),
    they are typically fragmented, lack pedagogical grounding, and fail to generalize
    across diverse educational tasks [\(Wang et al.,](#page-9-17) [2023a;](#page-9-17)
    [Ku et al.,](#page-9-18) [2025\)](#page-9-18). For instance, Visual Sketchpad
    [\(Hu et al.,](#page-8-11) [2024\)](#page-8-11) attempts to illustrate problem-solving
    processes with sketches generated from code. However, these visuals are often
    low in quality, lack logical coherence, and fall short in explanatory depth [\(Wang
    et al.,](#page-10-9) [2025\)](#page-10-9). Other approaches like MatplotAgent
    [\(Yang](#page-10-2) [et al.,](#page-10-2) [2024\)](#page-10-2), PlotGen [\(Goswami
    et al.,](#page-8-12) [2025\)](#page-8-12), and OmniSVG [\(Yang et al.,](#page-10-10)
    [2025\)](#page-10-10) leverage plotting and SVG tools to produce more accurate,
    datagrounded visualizations. Still, these methods are limited in scope, often
    addressing only isolated steps rather than providing systematic, end-to-end visual
    explanations of multi-step problem-solving tasks [\(Vázquez,](#page-9-19) [2024;](#page-9-19)
    [Chen et al.,](#page-8-13) [2024a,](#page-8-13) [2025b\)](#page-8-14). To overcome
    these limitations, we propose a multiagent collaborative framework, EduVisAgent,
    that simulates the full learning journey—from initial problem exposure to deep
    conceptual understanding—by coordinating specialized agents to generate coherent,
    pedagogically aligned visualizations throughout the reasoning process.


    ## 6 Conclusion


    This paper addressed the challenge of generating pedagogically meaningful visual
    explanations with AI systems. We introduced EduVisBench, a benchmark revealing
    that existing models often produce inadequate visual outputs. To overcome this,
    we proposed EduVisAgent, a collaborative multi-agent framework. Experiments show
    EduVisAgent significantly outperforms all baselines, demonstrating the potential
    of agent-based systems for advancing educational visualization.


    ## References


    - <span id="page-8-5"></span>Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti
    Aneja, Ahmed Awadallah, Hany Hassan Awadalla, Nguyen Bach, Amit Bahree, Arash
    Bakhtiari, Harkirat Singh Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien
    Bubeck, Martin Cai, Caio C''esar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary,
    Parul Chopra, and 69 others. 2024. [Phi-3 technical report: A highly capa](https://api.semanticscholar.org/CorpusID:269293048)[ble
    language model locally on your phone.](https://api.semanticscholar.org/CorpusID:269293048)
    ArXiv, abs/2404.14219.

    - <span id="page-8-6"></span>Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson,
    Hany Hassan Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav
    Chaudhary, Congcong Chen, Dongdong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen,
    Yen-Chun Chen, Yi ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, and 54 others. 2025.
    [Phi-4-mini technical report: Compact yet](https://api.semanticscholar.org/CorpusID:276747153)
    [powerful multimodal language models via mixture](https://api.semanticscholar.org/CorpusID:276747153)[of-loras.](https://api.semanticscholar.org/CorpusID:276747153)
    ArXiv, abs/2503.01743.

    - <span id="page-8-19"></span>Cognition AI. 2024. Devin ai: World''s first ai
    software engineer.

    - <span id="page-8-8"></span>Anthropic. 2025. Claude 3.7 sonnet and claude code.
    [https://www.anthropic.com/news/claude-3-7](https://www.anthropic.com/news/claude-3-7-sonnet)
    [-sonnet](https://www.anthropic.com/news/claude-3-7-sonnet). Accessed: 2025-05-16.

    - <span id="page-8-15"></span>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James
    Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal
    Patwardhan, and 1 others. 2024. Mle-bench: Evaluating machine learning agents
    on machine learning engineering. arXiv preprint arXiv:2410.07095.

    - <span id="page-8-17"></span>Jiawen Chen, Jianghao Zhang, Huaxiu Yao, and Yun
    Li. 2025a. Celltypeagent: Trustworthy cell type annotation with large language
    models. arXiv preprint arXiv:2505.08844.

    - <span id="page-8-13"></span>Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou,
    Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang,
    and 1 others. 2024a. Mj-bench: Is your multimodal reward model really a good judge
    for text-to-image generation? arXiv preprint arXiv:2407.04842.

    - <span id="page-8-14"></span>Zhaorun Chen, Mintong Kang, and Bo Li. 2025b. Shieldagent:
    Shielding agents via verifiable safety policy reasoning. arXiv preprint arXiv:2503.22738.

    - <span id="page-8-16"></span>Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song,
    and Bo Li. 2024b. Agentpoison: Red-teaming llm agents via poisoning memory or
    knowledge bases. Advances in Neural Information Processing Systems, 37:130185–130213.

    - <span id="page-8-10"></span>Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu
    Yao, Bo Li, and Jiawei Zhou. 2024c. Halc: Object hallucination reduction via adaptive
    focal-contrast decoding. arXiv preprint arXiv:2403.00425.

    - <span id="page-8-1"></span>Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang,
    Xiang Li, Bhiksha Raj, and Huaxiu Yao.


    2024d. Autoprm: Automating procedural supervision for multi-step reasoning via
    controllable question decomposition. arXiv preprint arXiv:2402.11452.


    - <span id="page-8-0"></span>Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo
    Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S Yu, and 1 others.
    2025. Llm agents for education: Advances and applications. arXiv preprint arXiv:2503.11733.

    - <span id="page-8-9"></span>Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen,
    Gelei Deng, Huaxiu Yao, and Tat-Seng Chua. 2024. Fine-grained verifiers: Preference
    modeling as nexttoken prediction in vision-language alignment. arXiv preprint
    arXiv:2410.14148.

    - <span id="page-8-4"></span>Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui
    Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning
    Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang,
    Juanzi Li, and 37 others. 2024. [Chatglm:](https://arxiv.org/abs/2406.12793) [A
    family of large language models from glm-130b to](https://arxiv.org/abs/2406.12793)
    [glm-4 all tools.](https://arxiv.org/abs/2406.12793) Preprint, arXiv:2406.12793.

    - <span id="page-8-12"></span>Kanika Goswami, Puneet Mathur, Ryan Rossi, and Franck
    Dernoncourt. 2025. Plotgen: Multi-agent llm-based scientific data visualization
    via multimodal feedback. arXiv preprint arXiv:2502.00988.

    - <span id="page-8-18"></span>Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun
    Li, Hongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: A multi-modal multi-agent framework
    for document understanding. arXiv preprint arXiv:2503.13964.

    - <span id="page-8-2"></span>Jiayi Hong, Christian Seto, Arlen Fan, and Ross Maciejewski.
    2025. Do llms have visualization literacy? an evaluation on modified visualizations
    to test generalization in data interpretation. IEEE Transactions on Visualization
    and Computer Graphics.

    - <span id="page-8-11"></span>Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari
    Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay Krishna. 2024. Visual sketchpad:
    Sketching as a visual chain of thought for multimodal language models. arXiv preprint
    arXiv:2406.09403.

    - <span id="page-8-7"></span>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman,
    Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford,
    and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276.

    - <span id="page-8-3"></span>IT Admin. 2024. Introducing Stable Diffusion 3.5.
    [ht](https://stability.ai/news/introducing-stable-diffusion-3-5) [tps://stability.ai/news/introducing-sta](https://stability.ai/news/introducing-stable-diffusion-3-5)
    [ble-diffusion-3-5](https://stability.ai/news/introducing-stable-diffusion-3-5).
    Updated October 29, 2024; Accessed: 2025-05-20.

    - <span id="page-8-20"></span>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language
    models resolve real-world github issues? arXiv preprint arXiv:2310.06770.

    - <span id="page-9-14"></span>Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung
    Lee, Xu Wang, and Juho Kim. 2025. Teachtune: Reviewing pedagogical agents against
    diverse student profiles with simulated students. In Proceedings of the 2025 CHI
    Conference on Human Factors in Computing Systems, pages 1–28.

    - <span id="page-9-18"></span>Max Ku, Thomas Chong, Jonathan Leung, Krish Shah,
    Alvin Yu, and Wenhu Chen. 2025. Theoremexplainagent: Towards multimodal explanations
    for llm theorem understanding. arXiv preprint arXiv:2502.19400.

    - <span id="page-9-9"></span>Black Forest Labs. 2024. Flux. [https://github.com](https://github.com/black-forest-labs/flux)
    [/black-forest-labs/flux](https://github.com/black-forest-labs/flux).

    - <span id="page-9-4"></span>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
    Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and
    Karl Cobbe. 2023. Let''s verify step by step. arXiv preprint arXiv:2305.20050.

    - <span id="page-9-13"></span>Shrestha Basu Mallick and Logan Kilpatrick. 2025.
    Gemini 2.0: Flash, flash-lite and pro. [https:](https://developers.googleblog.com/en/gemini-2-family-expands/)
    [//developers.googleblog.com/en/gemini-2](https://developers.googleblog.com/en/gemini-2-family-expands/)
    [-family-expands/](https://developers.googleblog.com/en/gemini-2-family-expands/).
    Accessed: 2025-05-20.

    - <span id="page-9-23"></span>Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone,
    and Yue Wang. 2023. A language agent for autonomous driving. arXiv preprint arXiv:2311.10813.

    - <span id="page-9-6"></span>Chad M. Miller and Jonathan D. Cohen. 2020. [Metacognitive
    prompts in multimedia learning: A](https://doi.org/10.1007/s10648-020-09525-3)
    [meta-analysis.](https://doi.org/10.1007/s10648-020-09525-3) Educational Psychology
    Review, 32(3):979–1003.

    - <span id="page-9-11"></span>Mistral AI. 2025. Mistral small 3.1: Sota. multimodal.
    multilingual. apache 2.0. [https://mistral.ai/n](https://mistral.ai/news/mistral-small-3-1)
    [ews/mistral-small-3-1](https://mistral.ai/news/mistral-small-3-1). Accessed:
    2025-05-20.

    - <span id="page-9-15"></span>Inderjeet Nair, Jiaye Tan, Xiaotian Su, Anne Gere,
    Xu Wang, and Lu Wang. 2024. Closing the loop: Learning to generate writing feedback
    via language model simulated student revisions. arXiv preprint arXiv:2410.08058.

    - <span id="page-9-20"></span>Fan Nie, Lan Feng, Haotian Ye, Weixin Liang, Pan
    Lu, Huaxiu Yao, Alexandre Alahi, and James Zou. 2025. Weak-for-strong: Training
    weak metaagent to harness strong executors. arXiv preprint arXiv:2504.04785.

    - <span id="page-9-5"></span>Satria Nugroho and Jailani Jailani. 2019. [The effective](https://doi.org/10.18502/kss.v3i17.4620)[ness
    of concrete representational abstract approach](https://doi.org/10.18502/kss.v3i17.4620)
    [\(cra\) approach and problem solving approach on](https://doi.org/10.18502/kss.v3i17.4620)
    [mathematical representation ability at elementary](https://doi.org/10.18502/kss.v3i17.4620)
    [school.](https://doi.org/10.18502/kss.v3i17.4620) KnE Social Sciences.

    - <span id="page-9-2"></span>Saugat Pandey and Alvitta Ottley. 2025. Benchmarking
    visual language models on standardized visualization literacy tests. arXiv preprint
    arXiv:2503.16632.

    - <span id="page-9-10"></span>Dustin Podell, Zion English, Kyle Lacey, A. Blattmann,
    Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 2023. [Sdxl: Improving
    latent diffusion](https://api.semanticscholar.org/CorpusID:259341735) [models
    for high-resolution image synthesis.](https://api.semanticscholar.org/CorpusID:259341735)
    ArXiv, abs/2307.01952.

    - <span id="page-9-16"></span>Luca Podo, Muhammad Ishmal, and Marco Angelini.
    2024. Vi (e) va llm! a conceptual stack for evaluating and interpreting generative
    ai-based visualizations. arXiv preprint arXiv:2402.02167.

    - <span id="page-9-1"></span>Norma Presmeg. 2006. Research on visualization in
    learning and teaching mathematics. Handbook of research on the psychology of mathematics
    education, pages 205–235.

    - <span id="page-9-24"></span>Jianing Qiu, Kyle Lam, Guohao Li, Amish Acharya,
    Tien Yin Wong, Ara Darzi, Wu Yuan, and Eric J Topol. 2024. Llm-based agentic systems
    in medicine and healthcare. Nature Machine Intelligence, 6(12):1418–1420.

    - <span id="page-9-3"></span>M. Rohith. 2023. High school physics. [https://hugg](https://huggingface.co/datasets/mrohith29/high-school-physics)
    [ingface.co/datasets/mrohith29/high-schoo](https://huggingface.co/datasets/mrohith29/high-school-physics)
    [l-physics](https://huggingface.co/datasets/mrohith29/high-school-physics). Accessed:
    2025-05-15.

    - <span id="page-9-7"></span>Gregory Schraw and David Moshman. 1995. [Metacog](http://www.jstor.org/stable/23359367)[nitive
    theories.](http://www.jstor.org/stable/23359367) Educational Psychology Review,
    7(4):351–371.

    - <span id="page-9-22"></span>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,
    Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning
    text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.

    - <span id="page-9-21"></span>Chan Hee Song, Jiaman Wu, Clayton Washington, Brian
    M Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning
    for embodied agents with large language models. In Proceedings of the IEEE/CVF
    international conference on computer vision, pages 2998–3009.

    - <span id="page-9-25"></span>Giulio Starace, Oliver Jaffe, Dane Sherburn, James
    Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella,
    Wyatt Thompson, and 1 others. 2025. Paperbench: Evaluating ai''s ability to replicate
    ai research. arXiv preprint arXiv:2504.01848.


    <span id="page-9-12"></span>Qwen Team. 2025. [Qwen2.5-vl.](https://qwenlm.github.io/blog/qwen2.5-vl/)


    - <span id="page-9-19"></span>Pere-Pau Vázquez. 2024. Are llms ready for visualization?
    In 2024 IEEE 17th Pacific Visualization Conference (PacificVis), pages 343–352.
    IEEE.

    - <span id="page-9-8"></span>Vercel. 2025. v0: Ai chat interface for web automation.
    <https://v0.dev>. Accessed: 2025-05-20.

    - <span id="page-9-17"></span>Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim,
    and Yong Wang. 2023a. Llm4vis: Explainable visualization recommendation using
    chatgpt. arXiv preprint arXiv:2310.07652.

    - <span id="page-9-0"></span>Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen
    Liang, Jiliang Tang, Philip S Yu, and Qingsong Wen. 2024a. Large language models
    for education: A survey and outlook. arXiv preprint arXiv:2403.18105.

    - <span id="page-10-9"></span>Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng
    Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. 2025. Multimodal chain-of-thought reasoning:
    A comprehensive survey. arXiv preprint arXiv:2503.12605.

    - <span id="page-10-7"></span>Yi Ru Wang, Jiafei Duan, Dieter Fox, and Siddhartha
    Srinivasa. 2023b. Newton: Are large language models capable of physical reasoning?
    arXiv preprint arXiv:2310.07018.

    - <span id="page-10-14"></span>Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and
    Graham Neubig. 2024b. Agent workflow memory. arXiv preprint arXiv:2409.07429.

    - <span id="page-10-8"></span>Hao Wei, Jianing Qiu, Haibao Yu, and Wu Yuan. 2024.
    Medco: Medical education copilots based on a multiagent framework. arXiv preprint
    arXiv:2408.12496.

    - <span id="page-10-0"></span>Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li,
    Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023.
    Mathchat: Converse to tackle challenging math problems with llm agents. arXiv
    preprint arXiv:2306.01337.

    - <span id="page-10-4"></span>Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu,
    Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda
    Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan,
    Aixin Liu, and 8 others. 2024. [Deepseek-vl2: Mixture-of-experts vision](https://arxiv.org/abs/2412.10302)[language
    models for advanced multimodal under](https://arxiv.org/abs/2412.10302)[standing.](https://arxiv.org/abs/2412.10302)
    Preprint, arXiv:2412.10302.

    - <span id="page-10-1"></span>Songlin Xu, Xinyu Zhang, and Lianhui Qin. 2024.
    Eduagent: Generative student agents in learning. arXiv preprint arXiv:2404.07963.

    - <span id="page-10-10"></span>Yiying Yang, Wei Cheng, Sijin Chen, Xianfang Zeng,
    Jiaxu Zhang, Liao Wang, Gang Yu, Xingjun Ma, and Yu-Gang Jiang. 2025. Omnisvg:
    A unified scalable vector graphics generation model. arXiv preprint arXiv:2504.06263.

    - <span id="page-10-2"></span>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu
    Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, and 1 others.
    2024. Matplotagent: Method and evaluation for llm-based agentic scientific data
    visualization. arXiv preprint arXiv:2402.11453.

    - <span id="page-10-11"></span>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
    Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning
    and acting in language models. In International Conference on Learning Representations
    (ICLR).

    - <span id="page-10-5"></span>Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo
    Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, and 1 others.
    2024. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800.

    - <span id="page-10-17"></span>Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang,
    Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. 2024.
    Finmem: A


    performance-enhanced llm trading agent with layered memory and character design.
    In Proceedings of the AAAI Symposium Series, volume 3, pages 595–597.


    - <span id="page-10-16"></span>Puzhen Yuan, Angyuan Ma, Yunchao Yao, Huaxiu Yao,
    Masayoshi Tomizuka, and Mingyu Ding. 2025. Remac: Self-reflective and self-evolving
    multi-agent collaboration for long-horizon robot manipulation. arXiv preprint
    arXiv:2503.22122.

    - <span id="page-10-3"></span>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang
    Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei
    Zhang, Mao Su, Hansen Zhong, Yuqiang Li, and Wanli Ouyang. 2024a. [Chemllm: A
    chemical large language model.](https://arxiv.org/abs/2402.06852) Preprint, arXiv:2402.06852.

    - <span id="page-10-13"></span>Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi
    Jin. 2024b. Codeagent: Enhancing code generation with tool-integrated agent systems
    for realworld repo-level coding challenges. arXiv preprint arXiv:2401.07339.

    - <span id="page-10-6"></span>Mike Zhang, Amalie Pernille Dilling, Léon Gondelman,
    Niels Erik Ruan Lyngdorf, Euan D Lindsay, and Johannes Bjerva. 2025. Sefl: Harnessing
    large language model agents to improve educational feedback systems. arXiv preprint
    arXiv:2502.12927.

    - <span id="page-10-15"></span>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert
    Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, and
    1 others. 2023. Webarena: A realistic web environment for building autonomous
    agents. arXiv preprint arXiv:2307.13854.

    - <span id="page-10-12"></span>Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu
    Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng,
    and 1 others. 2025. Anyprefer: An agentic framework for preference data synthesis.
    arXiv preprint arXiv:2504.19276.


    # A Appendix


    # <span id="page-11-0"></span>A.1 Visualization disciplines


    Table [3](#page-11-1) illustrates the disciplines and types in our EduVisBench.


    <span id="page-11-1"></span>


    | Discipline  | Common Visualization Types                                                              |

    |-------------|-----------------------------------------------------------------------------------------|

    | Mathematics | Number lines, function graphs, and other formalized visual<br>tools.                    |

    | Physics     | Diagrams involving levers, rigid body motion, forces, and fields.                       |

    | Chemistry   | Molecular structures and schematic representations of standard<br>laboratory
    apparatus. |


    Table 3: Representative Visualization Types Across Academic Disciplines


    # A.2 Evaluation Metric


    Visual Scenario Design Guidance The category of "Visual Scenario Design Guidance"
    outlines different levels of visualizing mathematical concepts, progressing from
    basic text-only representations to highly integrated visual-text formats. Through
    five defined levels, the framework demonstrates how visual elements can enhance
    students'' understanding and engagement with abstract ideas, guiding instructional
    designers to gradually enrich scenarios, add annotations, and strengthen contextual
    connections—ultimately achieving the goal of visually presenting the full flow
    and conceptual structure of the content.The five levels of Visual Scenario Design
    Guidance are as follows:


    | Level   | Description                                                                                                                                                                                                                                                           |

    |---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Level 1 | The image contains no scenes or illustrations, presenting only text
    and formulas. It<br>lacks contextual visual cues, failing to spark interest or
    connect the concepts to real-life<br>situations.                                                                   |

    | Level 2 | The image includes a single static illustration or low-fidelity mockup
    with minimal<br>labeling that does not highlight variables or key objects, offering
    limited context and<br>poor immersion.                                                                     |

    | Level 3 | Multiple static schematic diagrams or sketch-style illustrations appear
    in the image,<br>labeling core objects, variables, and simple steps, providing
    basic visual guidance but<br>lacking layered coherence.                                                        |

    | Level 4 | The image integrates scenario illustrations, storyboard panels, and
    infographics to<br>present the process in multiple views and steps, with annotations
    and captions guiding<br>students through mapping abstract concepts to context.                               |

    | Level 5 | Storyboard-style illustrations and infographics are fused into a single
    image, including<br>overview, detailed close-ups, and key pathway diagrams with
    comprehensive annota<br>tions, allowing students to grasp the entire flow and
    conceptual network at a glance. |


    Table 4: Five Levels of Visual Scenario Design Guidance


    Visual Illustration Design The category of "Visual Illustration Design" describes
    progressive levels of visual elements used to support students'' systematic understanding
    of quantities and relationships. It ranges from no visual aids to complex integrated
    dashboards that deeply connect data and model structures. Through five levels,
    the framework guides designers to improve clarity, coherence, and contextual richness
    of visual illustrations, enhancing students'' analytic and comparative abilities.


    | Level   | Description                                                                                                                                                                                                                    |

    |---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Level 1 | The image contains no charts, axes, or flow diagrams—only text. Without
    embedded<br>visual tools, students cannot systematically organize or analyze quantities
    and relation<br>ships.                                         |

    | Level 2 | The image presents a static number line and colored bar chart with
    complete scales and<br>a legend, helping students gain a basic understanding of
    numerical changes. However,<br>it lacks comparison and contextual layering. |

    | Level 3 | The image presents a static number line and colored bar chart with
    complete scales and<br>legends, helping students grasp basic numerical changes
    visually, though comparison<br>and context layering are absent.              |

    | Level 4 | The image combines number lines, flowcharts, infographics, and arrow
    annotations;<br>multiple visuals are juxtaposed or overlaid to show processes
    and variable changes for<br>a coherent modeling view.                       |

    | Level 5 | The image presents a dashboard-style visualization integrating axes,
    bar charts, flow<br>diagrams, heatmaps, etc., with linked elements that deeply
    visualize data relationships<br>and model structure.                       |


    Table 5: Five Levels of Visual Illustration Design


    Text–Illustration Coordination The category of "Text–Illustration Coordination"
    describes levels of alignment and integration between textual content and visual
    elements within images. This progression ranges from complete disconnection to
    seamless fusion, enabling students to effectively map and synthesize text, formulas,
    and graphics. The framework guides designers in strengthening links between verbal
    and visual information to enhance comprehension and structural understanding.


    | Level   | Description                                                                                                                                                                                                |

    |---------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Level 1 | Text and illustrations in the image are completely disconnected, with
    no labels, legends,<br>or connectors—students cannot use visuals to understand
    text or formulas.                                     |

    | Level 2 | Text occasionally prompts "see diagram" or "refer to the illustration,"
    but the image<br>lacks legends or clear labels, so mapping between text and graphics
    remains ambiguous.                            |

    | Level 3 | Text descriptions and image elements share consistent numbering, color
    blocks, or<br>arrows linked to a simple legend, explaining core symbols and variables
    to support<br>initial mapping.                |

    | Level 4 | Text paragraphs are laid out alongside corresponding visuals within
    the same image,<br>with detailed legends and color-coded annotations enabling
    simultaneous reading and<br>mapping.                     |

    | Level 5 | Text, formulas, and legends are fully integrated in one image, using
    consistent colors,<br>numbering, and layered layout to achieve seamless text–graphic
    fusion for complete<br>structural understanding. |


    Table 6: Five Levels of Text–Illustration Coordination


    Learning Thought Guidance The category of "Learning Thought Guidance" describes
    the progressive inclusion of visualized problem-solving strategies and reflective
    cues in images. From presenting only problem statements to complex integrated
    dashboards, this framework guides designers to scaffold students'' strategic thinking
    and metacognitive reflection through visual tools, enabling deeper reasoning and
    transfer of learning.


    Interactivity and Personalized Support The category of "Interactivity and Personalized
    Support" outlines levels of incorporating feedback, hints, and tailored assistance
    into images, evolving from static presentations to dynamic, student-responsive
    visual supports. This framework encourages designers


    | Level   | Description                                                                                                                                                                                                     |

    |---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

    | Level 1 | The image offers no visualized problem-solving guidance, showing only
    the problem<br>statement and formulas, leaving students without strategic cues
    or reflection prompts.                                     |

    | Level 2 | The image embeds a simple flowchart or two title-style hints (e.g.,
    "Identify problem<br>type," "Check result"), but the flowchart is overly simplistic
    and hints lack hierarchical<br>detail.                  |

    | Level 3 | The image displays a step-by-step flowchart template with key thinking
    nodes and<br>self-check checkpoints, leaving annotation space for students to
    visually record their<br>reasoning.                        |

    | Level 4 | The image combines a near-transfer exercise with a comparative thought
    diagram,<br>visually highlighting strategy differences so students can apply existing
    reasoning to a<br>new context.                     |

    | Level 5 | The image fuses near- and far-transfer exercises, concept mind maps,
    and a reflection<br>panel into a dashboard-style layout, allowing students to
    review and extend their<br>problem-solving network visually. |


    Table 7: Five Levels of Learning Thought Guidance


    to embed interactive elements that adapt to learner needs, promoting engagement
    and personalized problem-solving.


    | Level   | Description                                                                                                                                                                                                                                  |  |  |

    |---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|

    | Level 1 | The image includes no feedback or support components—only a static
    problem state<br>ment and answer field—offering no hints, examples, or error cues
    and resulting in a<br>nonresponsive visual.                                             |  |  |

    | Level 2 | The image shows fixed hint boxes (e.g., "Hint: draw a number line,"
    "Hint: check<br>rounding"), but hints are not tailored to student responses, limiting
    personalized guid<br>ance.                                                         |  |  |

    | Level 3 | The image integrates multiple static correction tips and example solution
    modules<br>(common mistakes and standard approaches), which students can reference
    visually but<br>without intelligent recommendations.                            |  |  |

    | Level 4 | The image presents example solution workflows, text hints, and a common-errors<br>analysis
    section highlighted with color blocks and arrows, providing diverse visual<br>support
    in a single layout.                                         |  |  |

    | Level 5 | The image displays a comprehensive visual support panel with difficulty
    suggestions,<br>personalized hints, worked examples, and extension resource links,
    enabling students to<br>select tailored guidance directly from the visual layout.
    |  |  |


    Table 8: Five Levels of Interactivity and Personalized Support


    # <span id="page-13-0"></span>A.3 Similarity between GPT and Human Evaluation


    | Metric              | Chemistry | Math   | Physics | Average |

    |---------------------|-----------|--------|---------|---------|

    | Cosine Similarity ↑ | 0.9742    | 0.9557 | 0.9666  | 0.9655  |

    | MSE ↓               | 0.3895    | 0.7093 | 0.6118  | 0.5702  |


    Table 9: Cosine similarity and mean squared error across subjects. Math is the
    average of Math500 and Illustrative-Math, each with 50 samples.


    We evaluated the similarity between GPT-based evaluations and human evaluations
    to assess the reliability of GPT''s scoring capabilities. Specifically, we selected
    50 samples from each subject category (Chemistry, Math, and Physics) and had both
    GPT and human evaluators independently rate these samples. Our human evaluators
    were undergraduate students from top universities, ensuring high-quality and informed
    assessments. We measured agreement using two standard metrics: Cosine Similarity
    and Mean Squared Error (MSE). As shown in Table [9,](#page-13-0) the high Cosine
    Similarity scores (averaging 0.9655) and low MSE values (averaging 0.5702) across
    all subjects indicate that GPT''s scoring aligns closely with human judgment.
    These results demonstrate that GPT-based evaluation is highly reliable and sufficiently
    robust for practical usage, closely approximating human evaluative standards.


    #### A.4 Evaluation Prompt


    The instructional web page evaluation prompt is structured as follows:


    #### Evaluation Prompt


    As a professional evaluator of instructional web pages, your task is to determine
    whether the generated web page meets expectations across five specific categories.


    #### Instructions:


    - Assign an integer score from 0 to 5 for each of the five categories (1–5).

    - 0 = completely missing or extremely poor 5 = fully meets the highest standard

    - Evaluation should be based solely on the specified aspect: *{category}*. The
    definition of *{category}* is: *{description}*.

    - Do not include any explanation, justification, or additional commentary. Refusing
    to provide a score is not allowed.


    ### Evaluation Output Format


    {{RATING: {"1":score, "2":score, "3":score, "4":score, "5":score}}}


    # A.5 Additional Related Works


    LLM-based AI Agents. Recent advancements in LLM-based agents have led to the development
    of specialized architectures capable of long-horizon planning, tool use, and memory
    management across a range of real-world domains [\(Yao et al.,](#page-10-11) [2023;](#page-10-11)
    [Chan et al.,](#page-8-15) [2024;](#page-8-15) [Chen et al.,](#page-8-16) [2024b,](#page-8-16)
    [2025a;](#page-8-17) [Nie](#page-9-20) [et al.,](#page-9-20) [2025;](#page-9-20)
    [Han et al.,](#page-8-18) [2025;](#page-8-18) [Zhou et al.,](#page-10-12) [2025\)](#page-10-12).
    In software engineering, agents like Devin [\(AI,](#page-8-19) [2024\)](#page-8-19),
    CodeAgent [\(Zhang et al.,](#page-10-13) [2024b\)](#page-10-13), and SWE-agent
    [\(Jimenez et al.,](#page-8-20) [2023\)](#page-8-20) manage full development pipelines
    and perform iterative code debugging. In the domain of web automation, agents
    have been deployed for complex web navigation and interaction tasks [\(Wang et
    al.,](#page-10-14) [2024b;](#page-10-14) [Chen et al.,](#page-8-14) [2025b;](#page-8-14)
    [Zhou](#page-10-15) [et al.,](#page-10-15) [2023\)](#page-10-15). LLM-based agents
    have also demonstrated effectiveness in embodied settings such as robotic manipulation,
    autonomous driving, and embodied navigation [\(Song et al.,](#page-9-21) [2023;](#page-9-21)
    [Shridhar et al.,](#page-9-22) [2020;](#page-9-22) [Mao et al.,](#page-9-23) [2023;](#page-9-23)
    [Yuan et al.,](#page-10-16) [2025\)](#page-10-16). Beyond these, specialized agents
    have emerged in domains including healthcare [\(Qiu et al.,](#page-9-24) [2024\)](#page-9-24),
    finance [\(Yu et al.,](#page-10-17) [2024\)](#page-10-17), and academic research
    [\(Starace et al.,](#page-9-25) [2025\)](#page-9-25).


    In the educational domain, AI agents such as EduAgent [\(Xu et al.,](#page-10-1)
    [2024\)](#page-10-1) and Teachtune [\(Jin et al.,](#page-9-14) [2025\)](#page-9-14)
    simulate student-teacher dialogues, collaborative learning activities, and task-oriented
    reasoning to enhance problem-solving instruction. Agents like SEFL [\(Zhang et
    al.,](#page-10-6) [2025\)](#page-10-6) and PROF [\(Nair et al.,](#page-9-15) [2024\)](#page-9-15)
    generate on-demand feedback for large-scale educational settings, while domain-specific
    tools such as MathChat [\(Wu et al.,](#page-10-0) [2023\)](#page-10-0), NEWTON
    [\(Wang et al.,](#page-10-7) [2023b\)](#page-10-7), and MEDCO [\(Wei et al.,](#page-10-8)
    [2024\)](#page-10-8) provide textual explanations for scientific and medical learning.


    Despite these advancements, few works explore collaborative, multi-agent designs
    tailored for educational reasoning and visualization. EduVisAgent is the first
    systematic multi-agent framework that coordinates specialized agents for instructional
    planning, reasoning decomposition, metacognitive prompting, and visualization
    design, offering a comprehensive approach to support step-by-step pedagogical
    problem-solving.'
- title: "GUI-explorer: Autonomous Exploration and Mining of Transition-aware\n  Knowledge\
    \ for GUI Agent"
  abstract: 'GUI automation faces critical challenges in dynamic environments. MLLMs

    suffer from two key issues: misinterpreting UI components and outdated

    knowledge. Traditional fine-tuning methods are costly for app-specific

    knowledge updates. We propose GUI-explorer, a training-free GUI agent that

    incorporates two fundamental mechanisms: (1) Autonomous Exploration of

    Function-aware Trajectory. To comprehensively cover all application

    functionalities, we design a Function-aware Task Goal Generator that

    automatically constructs exploration goals by analyzing GUI structural

    information (e.g., screenshots and activity hierarchies). This enables

    systematic exploration to collect diverse trajectories. (2) Unsupervised Mining

    of Transition-aware Knowledge. To establish precise screen-operation logic, we

    develop a Transition-aware Knowledge Extractor that extracts effective

    screen-operation logic through unsupervised analysis the state transition of

    structured interaction triples (observation, action, outcome). This eliminates

    the need for human involvement in knowledge extraction. With a task success

    rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows

    significant improvements over SOTA agents. It requires no parameter updates for

    new apps. GUI-explorer is open-sourced and publicly available at

    https://github.com/JiuTian-VL/GUI-explorer.'
  url: http://arxiv.org/abs/2505.16827v1
  keywords: ''
  document: "# GUI-explorer: Autonomous Exploration and Mining of Transition-aware\
    \ Knowledge for GUI Agent\n\nBin Xie 1 , Rui Shao1†, Gongwei Chen1†, Kaiwen Zhou\
    \ 2 , Yinchuan Li 2 , Jie Liu 1 , Min Zhang 1 , Liqiang Nie 1\n\n<sup>1</sup>Harbin\
    \ Institute of Technology, Shenzhen, <sup>2</sup>Huawei Noah's Ark Lab\n\n###\
    \ Abstract\n\nGUI automation faces critical challenges in dynamic environments.\
    \ MLLMs suffer from two key issues: misinterpreting UI components and outdated\
    \ knowledge. Traditional fine-tuning methods are costly for app-specific knowledge\
    \ updates. We propose GUI-explorer, a trainingfree GUI agent that incorporates\
    \ two fundamental mechanisms: (1) Autonomous Exploration of Function-aware Trajectory.\
    \ To comprehensively cover all application functionalities, we design a Function-aware\
    \ Task Goal Generator that automatically constructs exploration goals by analyzing\
    \ GUI structural information (e.g., screenshots and activity hierarchies). This\
    \ enables systematic exploration to collect diverse trajectories. (2) Unsupervised\
    \ Mining of Transition-aware Knowledge. To establish precise screen-operation\
    \ logic, we develop a Transition-aware Knowledge Extractor that extracts effective\
    \ screen-operation logic through unsupervised analysis the state transition of\
    \ structured interaction triples (observation, action, outcome). This eliminates\
    \ the need for human involvement in knowledge extraction. With a task success\
    \ rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows significant\
    \ improvements over SOTA agents. It requires no parameter updates for new apps.\
    \ GUI-explorer is open-sourced and publicly available at [https:](https://github.com/JiuTian-VL/GUI-explorer)\
    \ [//github.com/JiuTian-VL/GUI-explorer](https://github.com/JiuTian-VL/GUI-explorer)\
    \ .\n\n### 1 Introduction\n\nAutomation in graphical user interfaces (GUIs) has\
    \ rapidly advanced [\\(Su et al.](#page-9-0) , [2024\\)](#page-9-0). This progress\
    \ is driven by foundational models like large language models (LLMs) [\\(Touvron\
    \ et al.](#page-9-1) , [2023](#page-9-1) ; [Achiam](#page-8-0) [et al.](#page-8-0)\
    \ , [2023](#page-8-0) ; [Yang et al.](#page-10-0) , [2024a\\)](#page-10-0) and\
    \ multimodal large language models (MLLMs) [\\(Hurst et al.](#page-8-1) , [2024](#page-8-1)\
    \ ; [Chen et al.](#page-8-2) , [2024](#page-8-2) ; [Shao et al.](#page-9-2) ,\
    \ [2024](#page-9-2) ; [Google](#page-8-3) ,\n\n<span id=\"page-0-3\"></span>![](_page_0_Figure_8.jpeg)\n\
    \nFigure 1: Comparison of GPT-4o and an user's interpretation of a UI element\
    \ in QQ Music[1](#page-0-0) . The red-bounded icon in the screenshot represents\
    \ the music recognition feature, but GPT-4o misidentified it. This highlights\
    \ the challenge of accurately interpreting UI elements in an ecosystem of diverse\
    \ apps with distinct designs.\n\n[2025](#page-8-3) ; [Shao et al.](#page-9-3)\
    \ , [2023](#page-9-3) ; [Li et al.](#page-9-4) , [2025a](#page-9-4) ; [Shen et\
    \ al.](#page-9-5) , [2024\\)](#page-9-5). These innovations enable agents [\\\
    (Zheng](#page-10-1) [et al.](#page-10-1) , [2024](#page-10-1) ; [Zhang et al.](#page-10-2)\
    \ , [2025](#page-10-2) ; [Wang et al.](#page-9-6) , [2024a](#page-9-6) ; [Li et\
    \ al.](#page-9-7) , [2025b](#page-9-7) ; [Ye et al.](#page-10-3) , [2024](#page-10-3)\
    \ ; [Li et al.](#page-9-8) , [2025c\\)](#page-9-8) to handle tasks. They require\
    \ no extensive fine-tuning or pretraining. This demonstrates their potential for\
    \ diverse applications.\n\nHowever, the practical deployment of these models faces\
    \ significant challenges. These challenges stem from the long-tail distribution\
    \ of app/website variants and their rapid iteration cycles. While core functionalities\
    \ might appear similar across platforms, critical design divergences exist. For\
    \ example: shopping cart features in Amazon.com and Temu [2](#page-0-1) share\
    \ similarities. In contrast, Pinduoduo[3](#page-0-2) (China's dominant e-commerce\
    \ platform) eliminates cart functionality entirely. This requires\n\n<sup>†</sup>\
    \ Corresponding authors. shaorui@hit.edu.cn, chengongwei@hit.edu.cn.\n\n<span\
    \ id=\"page-0-0\"></span><sup>1</sup> [https://play.google.com/store/apps/details?](https://play.google.com/store/apps/details?id=com.tencent.qqmusic)\
    \ [id=com.tencent.qqmusic](https://play.google.com/store/apps/details?id=com.tencent.qqmusic)\n\
    \n<span id=\"page-0-1\"></span><sup>2</sup> <https://www.temu.com>\n\n<span id=\"\
    page-0-2\"></span><sup>3</sup> <https://mobile.pinduoduo.com>\n\nsingle-item purchases\
    \ rather than batch checkout. Such inconsistencies extend beyond functionality\
    \ to interface semantics. As shown in Figure [1,](#page-0-3) even advanced MLLMs\
    \ such as GPT-4o [\\(Hurst et al.,](#page-8-1) [2024\\)](#page-8-1) can misinterpret\
    \ the button's actual functionality. Human users familiar with the app, however,\
    \ correctly interpret it through learned interaction patterns. Compounding this\
    \ challenge, apps/websites undergo frequent updates. Amazon Shopping alone released\
    \ 30 version iterations in 2024[4](#page-1-0) . This renders static model knowledge\
    \ obsolete. Retraining or fine-tuning (M)LLMs for every change proves prohibitively\
    \ expensive and latency-prone.\n\nIn this paper, we propose Autonomous Exploration\
    \ and Mining of Transition-aware Knowledge for GUI Agent (GUI-explorer). It synergizes\
    \ two key components: (1) Autonomous Exploration of Function-aware Trajectory.\
    \ To cover all potential functions of target applications, we design a Function-aware\
    \ Task Goal Generator. This module automatically constructs functionaware exploration\
    \ goals by analyzing structural information of the environment, including screenshots\
    \ and activity lists from APK files. Through systematic exploration, we obtain\
    \ diverse functionaware trajectories. (2) Unsupervised Mining of Transition-aware\
    \ Knowledge. To establish precise operation logic, we develop a Transitionaware\
    \ Knowledge Extractor. This component extracts effective operation logic through\
    \ unsupervised analysis of state transitions from structured interaction triples\
    \ (observation, action, outcome). This eliminates human involvement. Through multimodal\
    \ state modeling incorporating visual patterns and semantic patterns, the extractor\
    \ captures operation constraints and outcome dependencies, generating transition-aware\
    \ knowledge with explicit action-effect correlations. Finally, by performing visual-semantic\
    \ retrieval between current screen visuals and the knowledge vector store to construct\
    \ Dynamic Guidance, it achieves two goals: suppressing the misinterpretation of\
    \ UI components, and ensuring action proposals align with actual UI states. This\
    \ approach facilitates precise, goal-oriented prompt generation. These prompts\
    \ guide the agent in effectively understanding and interacting with GUI elements.\n\
    \nOur main contributions are listed below:\n\n• We propose GUI-explorer, a novel\
    \ training-\n\n<span id=\"page-1-0\"></span>4 [https://www.apkmirror.com/uploads/](https://www.apkmirror.com/uploads/?appcategory=amazon-shopping)\
    \ [?appcategory=amazon-shopping](https://www.apkmirror.com/uploads/?appcategory=amazon-shopping)\n\
    \nfree agent that integrates two mechanisms: (1) Autonomous exploration of functionaware\
    \ trajectory through environment-specific structural priors. (2) Unsupervised\
    \ mining of transition-aware knowledge that extracts atomic screen-operation logic\
    \ from raw interaction traces.\n\n- We conducted comprehensive evaluations of\
    \ GUI-explorer across AndroidWorld and SPA-Bench benchmarks, our agent achieves\
    \ 47.4% and 53.7% task success rates respectively, outperforming SOTA methods\
    \ by 2.6%∼11.7% improvement. Through ablation studies, we verified that our framework's\
    \ transition-aware knowledge integration approach reduces prior knowledge errors\
    \ by 16.0%.\n- We introduce a benchmark evaluating MLLMs' GUI understanding through\
    \ 500 curated samples across 43 applications. Results reveal critical limitations\
    \ in current models (15.2%∼22.8% prior knowledge inaccuracies).\n\n### 2 Related\
    \ Work\n\nGUI Agents Modern GUI agents leverage MLLMs to interpret interface states\
    \ and execute actions. SeeAct [\\(Zheng et al.,](#page-10-1) [2024\\)](#page-10-1)\
    \ pioneers GPT-4V [\\(OpenAI,](#page-9-9) [2023\\)](#page-9-9) for web task automation\
    \ through visual understanding and HTML-guided action grounding. MobileAgentV2\
    \ [\\(Wang et al.,](#page-9-6) [2024a\\)](#page-9-6) implements multi-agent collaboration\
    \ with memory units to track task progress and interface focus. M3A [\\(Rawles\
    \ et al.,](#page-9-10) [2024\\)](#page-9-10) integrates ReActstyle [\\(Yao et\
    \ al.,](#page-10-4) [2023\\)](#page-10-4) reasoning with Set-of-Mark (SoM) [\\\
    (Yang et al.,](#page-10-5) [2023\\)](#page-10-5) visual annotations for Android\
    \ device control, demonstrating zero-shot generalization across applications.\n\
    \nExploration & Knowledge-aware Agents Autonomous exploration mechanisms vary\
    \ in supervision requirements. AppAgent [\\(Zhang et al.,](#page-10-2) [2025\\\
    )](#page-10-2) requires manually designed exploration tasks for knowledge acquisition,\
    \ while AutoDroid [\\(Wen](#page-9-11) [et al.,](#page-9-11) [2024\\)](#page-9-11)\
    \ and MobileGPT [\\(Lee et al.,](#page-8-4) [2024\\)](#page-8-4) generates random\
    \ action sequences for environment interaction. DigiRL [\\(Zhou et al.,](#page-10-6)\
    \ [2024\\)](#page-10-6) employs reinforcement learning with Gemini-based [\\(Google,](#page-8-3)\
    \ [2025\\)](#page-8-3) trajectory filtering to collect successful demonstrations\
    \ as training data.\n\nKnowledge utilization strategies focus on experience retention\
    \ and retrieval. CAT [\\(Feng](#page-8-5)\n\n<span id=\"page-2-0\"></span>![](_page_2_Figure_0.jpeg)\n\
    \nFigure 2: Overview of GUI-explorer. (a) Automatically constructing function-aware\
    \ exploration goals by analyzing structural information from the GUI environment,\
    \ followed by systematic exploration to collect diverse functionaware trajectories.\
    \ (b) Extracting effective screen-operation logic through unsupervised analysis\
    \ of structured interaction triples (observation, action, outcome), enabling unsupervised\
    \ knowledge extraction. (c) Performing visual-semantic retrieval between screen\
    \ visuals and the knowledge vector store to construct Dynamic Guidance achieves\
    \ dual objectives: preventing UI misinterpretation and ensuring action proposals\
    \ align with actual UI states.\n\n[et al.,](#page-8-5) [2024\\)](#page-8-5) employs\
    \ retrieval-augmented generation with task-specific successful trajectories, though\
    \ limited to pre-collected demonstrations. Synapse [\\(Zheng et al.,](#page-10-7)\
    \ [2023\\)](#page-10-7) introduces trajectoryas-exemplar prompting with state\
    \ abstraction to improve cross-task generalization. ICAL [\\(Sarch](#page-9-12)\
    \ [et al.,](#page-9-12) [2024\\)](#page-9-12) abstracts interaction traces into\
    \ transferable knowledge through visual-language model summarization and human\
    \ feedback.\n\nWhile existing methods demonstrate progress, four critical limitations\
    \ persist: (1) Exploration efficiency suffers from random action generation or\
    \ manual task design; (2) Knowledge extraction relies on successful trajectories\
    \ or human curation, limiting scalability; (3) Static knowledge bases struggle\
    \ with rapidly evolving interfaces; (4) Binding knowledge to specific element\
    \ IDs restricts reuse to identical UIs.\n\n# 3 Autonomous Exploration and Mining\
    \ of Transition-aware Knowledge for GUI Agent\n\nAs illustrated in Figure [2,](#page-2-0)\
    \ GUI-explorer consists of two main components: autonomous exploration of function-aware\
    \ trajectory and unsupervised mining of transition-aware knowledge. Building upon\
    \ the dual components mentioned, we employ visualsemantic retrieval during the\
    \ agent's task execution to extract relevant knowledge based on the current observation.\
    \ This retrieval mechanism enables a dynamic knowledge integration process that\
    \ enhances the agent's decision-making capabilities. Specifically, we construct\
    \ task-specific guidance by synthesizing the retrieved knowledge with both the\
    \ current task goal and observational data. This guidance framework facilitates\
    \ sophisticated reasoning processes, allowing the agent to make more informed\
    \ decisions while navigating complex task environments.\n\n### <span id=\"page-2-1\"\
    ></span>3.1 Autonomous Exploration of Function-aware Trajectory\n\nThe core of\
    \ our method lies in autonomously generating diverse interaction trajectories\
    \ without human supervision. This exploration is grounded in environment-specific\
    \ structural priors. These priors suppress misinterpretations derived from MLLMs'\
    \ obsolete domain priors. Algorithm [1](#page-3-0) formalizes\n\nthis process\
    \ through two key components. First, anchor-guided task generation leverages interface\
    \ semantics. Second, depth-first exploration incorporates state restoration mechanisms.\n\
    \nGiven a target environment E, we first extract *Exploration Anchors*. These\
    \ are structural primitives derived from E's ground-truth architecture, as detailed\
    \ further in Appendix [D.](#page-11-0) For mobile apps, functional modules declared\
    \ in manifest files (e.g., \"PaymentActivity\"). These anchors serve as verifiable\
    \ constraints during task generation, preventing MLLMs from proposing actions\
    \ targeting nonexistent components. The Task\\_Generator function constructs prompts\
    \ (see Appendix [I.1\\)](#page-14-0) containing current observation o<sup>t</sup>\
    \ and valid anchors, then samples up to k candidate tasks from MLLM outputs.\n\
    \nThe exploration follows depth-first search (DFS) with configurable branching\
    \ factor b and depth d. This strategy eliminates the first state restoration overhead\
    \ when expanding child tasks. The elimination occurs because each branch naturally\
    \ inherits the terminal state of its parent task. This differs from breadth-first\
    \ search (BFS), which requires resetting to the parent state for each sibling\
    \ task expansion. Starting from initial state state0, each generated task initiates\
    \ an exploration branch. After executing a task for up to s steps via Task\\_Executor,\
    \ the environment rolls back to previous state state<sup>i</sup> . This mechanism\
    \ enables exhaustive traversal of interface pathways without manual reset. The\
    \ executor terminates exploration branches under two conditions: when receiving\
    \ an \"END\" action, or when reaching maximum steps. This balances thoroughness\
    \ with computational efficiency.\n\nThis design achieves two critical properties:\
    \ (1) Semantic Grounding: Anchors tether generated tasks to actual interface functions.\
    \ (2) Quadratic Coverage: Each d-depth exploration with branching factor b yields\
    \ O(b d ) distinct trajectories, systematically capturing combinatorial interaction\
    \ patterns.\n\n### 3.2 Unsupervised Mining of Transition-aware Knowledge\n\nThe\
    \ knowledge construction process focuses on mining atomic screen-operation logic.\
    \ These logic are derived from exploration trajectories. Let ξ = ⟨o1, a1, ...,\
    \ on, an⟩ denote an interaction trajectory. This trajectory is collected during\
    \ autonomous exploration. We extract transition-aware GUI knowledge through a\
    \ Transition-aware Knowledge Ex-\n\n### Algorithm 1: Autonomous Exploration of\
    \ Function-aware Trajectory\n\n<span id=\"page-3-0\"></span>\n\n| Input: Environment\
    \ E, max_branching_factor b,   |\n|-------------------------------------------------|\n\
    | max_depth d, max_steps s                        |\n| 1 Function Explore_DFS(E,\
    \ b, d, depth, task, s) |\n| Task_Executor(E, task, s);                      |\n\
    | if current_depth > d then                       |\n| return;               \
    \                          |\n| current_state ← E.get_current_state();       \
    \   |\n| child_tasks ← Task_Generator(E, b);             |\n| for i = 0 to length(child_tasks)\
    \ − 1 do         |\n| if i > 0 then                                   |\n| E.restore_to(current_state);\
    \                    |\n| Explore_DFS(E, b, d, depth +                    |\n\
    | 1, child_tasks[i], s);                          |\n| 11 Function Task_Generator(E,\
    \ k)                |\n| anchors ← E.app_functions;                      |\n|\
    \ p ← ConstructPrompt(E.observation, anchors);    |\n| return MLLM(p).sample_top_k(k);\
    \                 |\n| 15 Function Task_Executor(E, task, s)           |\n| for\
    \ round = 1 to s do                           |\n| action ← MLLM(task, E.observation);\
    \             |\n| /* We store the observation and                 |\n| action\
    \ for knowledge vector                     |\n| store construction<br>*/     \
    \                   |\n| if action == \"END\" then                         |\n\
    | return;                                         |\n| E.step(action);       \
    \                          |\n| 21 initial_state ← E.get_current_state();    \
    \   |\n| 22 tasks ← Task_Generator(E, b);                |\n| 23 foreach task\
    \ in tasks do                     |\n| Explore_DFS(E, b, d, 0, task, s);     \
    \          |\n| E.restore_to(initial_state);                    |\n|         \
    \                                        |\n\ntractor function Fextract. This\
    \ function operates on state-action transitions:\n\n$$F\\_{extract} : (o\\_i,\
    \ a\\_i, o\\_{i+1}) \\to \\{k\\_i : v\\_i\\} \\qquad (1)$$\n\nwhere o<sup>i</sup>\
    \ and oi+1 represent consecutive observations, a<sup>i</sup> denotes the action\
    \ executed, and {k<sup>i</sup> : vi} outputs a set of visual-semantic knowledge\
    \ entries. Each entry consists of: (1) k<sup>i</sup> : visual patch of the interacted\
    \ UI element, (2) v<sup>i</sup> : operational knowledge (e.g., \"Clicking this\
    \ button opens search history\").\n\nUnlike previous work [\\(Zheng et al.,](#page-10-7)\
    \ [2023;](#page-10-7) [Feng](#page-8-5) [et al.,](#page-8-5) [2024;](#page-8-5)\
    \ [Sarch et al.,](#page-9-12) [2024;](#page-9-12) [Qin et al.,](#page-9-13) [2025\\\
    )](#page-9-13), which requires successful trajectories for in-context learning\
    \ or fine-tuning, our approach has different requirements. Specifically, we only\
    \ need valid state transitions. Therefore, we implement a filtering mechanism\
    \ termed Transition Filtering to filter out invalid state transitions: Discard\
    \ transitions where o<sup>i</sup> ≈ oi+1. This similarity is measured via perceptual\
    \ hashing [\\(Marr and Hildreth,](#page-9-14) [1980\\)](#page-9-14). Such transitions\
    \ indicate ineffective actions. These occur in two scenarios: when a<sup>i</sup>\
    \ fails to alter the environment (invalid action) or when the environment fails\
    \ to respond (execution error).\n\n<span id=\"page-4-0\"></span>![](_page_4_Picture_0.jpeg)\n\
    \nFigure 3: Without transition-aware knowledge as reliable prior information,\
    \ MLLMs may fail to reason correctly due to outdated prior knowledge or diverse\
    \ GUI designs.\n\nThe knowledge vector store K is structured as a multi-modal\
    \ index:\n\n$$\\mathcal{K} = \\bigcup\\_{\\xi \\in \\Xi} \\bigcup\\_{t=1}^{|\\\
    xi|-1} F\\_{\\text{extract}}(o\\_t, a\\_t, o\\_{t+1}) \\qquad (2)$$\n\nwhere Ξ\
    \ denotes all exploration trajectories and |ξ| denotes the total steps of the\
    \ trajectory ξ.\n\nThis knowledge construction process enables Continuous Knowledge\
    \ Refinement. New explorations iteratively update K through:\n\n$$\\mathcal{K}\
    \ = \\begin{cases} \\mathcal{K} \\mid \\{ (k\\_{\\text{old}}, v\\_{\\text{old}})\
    \ \\} \\cup \\{ (k\\_{\\text{old}}, v\\_{\\text{old}} \\oplus v\\_{\\text{new}})\
    \ \\} & \\text{if } \\Phi\\\\ \\mathcal{K} \\cup \\{ (k\\_{\\text{new}}, v\\_{\\\
    text{new}}) \\} & \\text{otherwise} \\end{cases} \\tag{3}$$\n\nwhere K \\ {(kold,\
    \ vold)} denotes the removal of the original key-value pair from the knowledge\
    \ vector store, ⊕ represents the concatenation of knowledge, condition Φ is formally\
    \ defined as:\n\n$$\\begin{aligned} \\exists (k\\_{\\text{old}}, v\\_{\\text{old}})\
    \ & \\in \\mathcal{K} \\\\ \\text{s.t.} \\quad & \\begin{cases} \\cos \\left(\
    \ Emb(k\\_{\\text{nuc}}), Emb(k\\_{\\text{old}}) \\right) \\ge \\delta\\_k \\\\\
    \ \\cos \\left( Emb(v\\_{\\text{nuc}}), Emb(v\\_{\\text{old}}) \\right) \\le \\\
    delta\\_v \\end{cases} \\end{aligned} (4)$$\n\nwhere δ<sup>k</sup> and δ<sup>v</sup>\
    \ are similarity thresholds for key matching (≥ 0.99) and value merging (≤ 0.1)\
    \ respectively, cos(·) is cosine similarity, and Emb(·) is the embedding function.\
    \ This prevents redundant entries while capturing novel interface behaviors.\n\
    \nFigure [3](#page-4-0) demonstrates the importance of transition-aware knowledge.\n\
    \n#### 3.3 Dynamic Guidance for GUI Agent\n\nThe dynamic guidance mechanism connects\
    \ acquired Transition-aware Knowledge to real-time\n\n```\nAlgorithm 2: Dynamic\
    \ Guidance for\nGUI Agent\n  Input: Environment E, Instruction I,\n        Knowledge_Vector_Store\
    \ K,\n        Knowledge_Ranker Ranker, max_steps s\n1 Function Get_Guidance(obs,\
    \ I, K)\n2 annot_scr ← Get_Annotated_Screenshot(obs);\n3 ui_elements ← Extract_UI_Elements(obs);\n\
    4 all_knol ← ∅ ; // all_knowledge\n5 foreach ui_element in ui_elements do\n6 all_knol.append(Retrieve_Knowledge(K,\n\
    \           ui_element));\n7 prioritized_knol ← Ranker(I, all_knol);\n8 guidance\
    \ ← Create_Guidance_Prompt(I,\n       rioritized_knol, annot_scr);\n9 return guidance;\n\
    10 for idx = 1 to s do\n11 obs ← E.observation;\n12 operational_guid ← Get_Guidance(obs,\
    \ I, K);\n13 action ← MLLM(I, operational_guid, obs);\n14 if action == \"END\"\
    \ then\n15 break;\n16 E.step(action);\n```\ntask execution. This connection is\
    \ achieved through a ranking architecture. As detailed in Algorithm [2,](#page-4-1)\
    \ our approach uses a two-phase process. The first phase involves visual-semantic\
    \ knowledge retrieval. The second phase performs instructionaware prioritization.\n\
    \nKnowledge Ranking Formulation Given an instruction I and candidate knowledge\
    \ entries C = {k1, ..., kn}, we define the optimal knowledge ordering C ∗ through\
    \ pairwise utility comparison:\n\n$$\\mathcal{C}^\\* = \\underset{\\pi \\in \\\
    Pi(\\mathcal{C})}{\\text{arg}\\max} \\sum\\_{i=1}^{|\\mathcal{C}|-1} int(u(k\\\
    _{\\pi(i)}, I) \\ge u(k\\_{\\pi(i+1)}, I)) \\tag{5}$$\n\nwhere Π(C) denotes all\
    \ permutations of C, int(·) converts bool to integer (false as 0, true as 1),\
    \ and utility function u(k, I) measures the relevance between knowledge entry\
    \ k and instruction I. We implement u(·) through an MLLM-based pairwise comparator:\n\
    \n$$u(k\\_a, I) > u(k\\_b, I) \\Leftrightarrow f\\_{\\text{rank}}(g(I, k\\_a,\
    \ k\\_b)) = 1 \\quad (6)$$\n\nwhere g(·) constructs the ranking prompt (see Appendix\
    \ [I.3\\)](#page-14-1), and frank represents the MLLM's binary classification.\
    \ When the classification result is 1, it indicates k<sup>a</sup> is more helpful\
    \ than k<sup>b</sup> for this instruction. When the result is 2, it means k<sup>b</sup>\
    \ is more helpful than ka. This formulation enables efficient sorting through\
    \ a modified merge sort algorithm:\n\n$$\\text{Sort}(\\mathcal{C}, I) = \\begin{cases}\
    \ \\mathcal{C} & |\\mathcal{C}| \\le 1 \\\\ \\text{Merge}(\\text{Sort}(\\mathcal{C}\\\
    _L, I), \\text{Sort}(\\mathcal{C}\\_R, I), I) & \\text{otherwise} \\end{cases}\
    \ \\tag{7}$$\n\nThe merge operation recursively compares head elements from sorted\
    \ sublists using frank:\n\n$$\\mathsf{Merge}(A,B,I) = \\begin{cases} [a\\_0] \\\
    oplus \\mathsf{Merge}(A\\_1, B, I) & f\\_{\\text{rmk}}(g(I, a\\_0, b\\_0)) = 1\
    \ \\\\ A \\oplus B & A = \\emptyset \\lor B = \\emptyset \\\\ [b\\_0] \\oplus\
    \ \\mathsf{Merge}(A, B\\_1, I) & \\text{otherwise} \\end{cases} \\tag{8}$$\n\n\
    where a<sup>0</sup> and b<sup>0</sup> denote the first elements of lists A and\
    \ B respectively.\n\nOperational Guidance Generation At each execution step t,\
    \ the system: (1) Extracts UI elements U<sup>t</sup> from current observation\
    \ o<sup>t</sup> ; (2) Retrieves associated knowledge entries K<sup>t</sup> ⊆ K;\
    \ (3) Sorts entries via K<sup>∗</sup> <sup>t</sup> = Sort(K<sup>t</sup> , I);\
    \ (4) Constructs guidance prompt p<sup>t</sup> with relevant knowledge.\n\nAs\
    \ shown in Figure [2](#page-2-0) (c), the dynamic guidance mechanism enables precise\
    \ alignment between operational knowledge and real-time interface states.\n\n\
    ### 4 GUI-Knowledge Reasoning Benchmark\n\nWe introduce the GUI-Knowledge Reasoning\
    \ Benchmark (GUI-KRB). This benchmark evaluates MLLMs' accuracy in two areas:\
    \ prior knowledge accuracy and dynamic UI comprehension for mobile environments.\
    \ Existing benchmarks primarily focus on task completion. In contrast, GUI-KRB\
    \ assesses models' fundamental understanding of UI elements and their behaviors.\
    \ It contains 500 carefully curated samples spanning 43 applications across 8\
    \ categories. Appendix [B](#page-11-1) shows the proportion of apps in each category.\n\
    \n#### 4.1 Tasks and Metrics\n\nGUI-KRB includes two evaluation tasks: (1) Prior\
    \ Knowledge Assessment: Models must identify the functionality of specified UI\
    \ elements. They are given a single screenshot, its accessibility tree, and a\
    \ task context about this element. This task simulates the planning phase in GUI\
    \ automation. During planning, agents must understand element functionality before\
    \ acting. Success here indicates effective use of prior training knowledge. (2)\
    \ Dynamic Comprehension Assessment: Models analyze UI element functionality by\
    \ comparing preinteraction and post-interaction states within the task context\
    \ of this transition. These states include screenshots and accessibility trees.\
    \ This task evaluates reasoning about cause-effect logic in GUI interactions.\
    \ It simulates the knowledge extraction method we use in this paper.\n\nFor both\
    \ tasks, responses are evaluated against human-annotated keywords. A response\
    \ is consid-\n\nered correct if it contains at least 50% of expertidentified keywords.\
    \ This metric balances precision with flexibility for valid phrasings. (During\
    \ keyword labeling, we include up to 50% synonyms to accommodate diverse responses.)\n\
    \n#### 4.2 Annotation Process\n\nGUI-KRB was built through a rigorous multi-stage\
    \ process: (1) Trajectory Collection: We collected over 300 task execution trajectories\
    \ in a mobile environment. These trajectories contain more than 7,000 interaction\
    \ steps across diverse mobile applications. They capture authentic user interactions\
    \ in real-world scenarios. (2) Element Extraction: From these trajectories, we\
    \ extracted individual UI elements using bounding box information from accessibility\
    \ trees. To ensure diversity and remove redundancy, we eliminated duplicate elements\
    \ using perceptual hashing techniques [\\(Marr and Hil](#page-9-14)[dreth,](#page-9-14)\
    \ [1980\\)](#page-9-14). (3) Keyword Annotation: Human experts identified essential\
    \ keywords uniquely associated with each UI element's functionality. These keywords\
    \ capture both the element's immediate purpose and its broader role in the interface.\
    \ (4) Validation: The authors conducted a comprehensive review of all annotations,\
    \ verifying keyword accuracy and ensuring consistent annotation quality across\
    \ the dataset.\n\nThe final dataset provides triplets of target UI elements, their\
    \ corresponding screen states (before and after interaction), and expert-validated\
    \ keyword sets. Example annotations are provided in Appendix [C.](#page-11-2)\n\
    \n### 5 Experiments\n\n#### 5.1 Experimental Setup\n\n#### 5.1.1 Datasets\n\n\
    We evaluate GUI-explorer on two comprehensive, open-source benchmarks: MIT-licensed\
    \ SPA-Bench [\\(Chen et al.,](#page-8-6) [2025\\)](#page-8-6) and Apache-2.0-licensed\
    \ AndroidWorld [\\(Rawles et al.,](#page-9-10) [2024\\)](#page-9-10). Both benchmarks\
    \ emphasize real-world GUI and provide automated evaluation pipelines for rigorous\
    \ agent assessment.\n\nSPA-Bench SPA-Bench is a benchmark simulating daily smartphone\
    \ usage scenarios with 58 mainstream apps (e.g., Facebook and Gmail). It contains\
    \ three progressively challenging task levels (Level 1-3), where Level 3 represents\
    \ the most complex real-world workflows.\n\nAndroidWorld AndroidWorld is an Android\
    \ environment featuring 116 tasks across 20 real-world apps. The benchmark dynamically\
    \ generates task variants through randomized parameters (e.g., message content,\
    \ contact names, and calendar dates), creating millions of unique task instantiations.\n\
    \n#### 5.1.2 Implementation Details\n\nTo ensure fair evaluation across benchmarks,\
    \ we carefully selected base models according to their characteristics. For SPA-Bench\
    \ and AndroidWorld, we adopted GPT-4o [\\(Hurst et al.,](#page-8-1) [2024\\)](#page-8-1)\
    \ as the unified base model, which has been the de facto standard model in prior\
    \ works including, but not limited to, SPA-Bench [\\(Chen et al.,](#page-8-6)\
    \ [2025\\)](#page-8-6) and Aria-UI [\\(Yang et al.,](#page-10-8) [2024b\\)](#page-10-8),\
    \ eliminating performance variance caused by heterogeneous model capabilities.\
    \ In contrast, for GUI-KRB, we intentionally utilized the weakest-performing Qwen2-VL-72B-Instruct-GPTQ-Int4\
    \ [\\(Wang et al.,](#page-9-15) [2024b\\)](#page-9-15) as our base model, to rigorously\
    \ validate the robustness of our method.\n\nWe configured the exploration process\
    \ with a branching factor of 10, a maximum depth of 5, and a step limit of 30\
    \ for AndroidWorld and SPA-Bench. This setup facilitated the automated discovery\
    \ of over 1,300 knowledge items (detailed distribution in Appendix [H\\)](#page-14-2)\
    \ across 46 applications. For visual-semantic retrieval, we utilized google/siglip-so400m-patch14-384[5](#page-6-0)\
    \ as the embedding model. Hardware configurations are provided in Appendix [A.](#page-11-3)\n\
    \n#### 5.1.3 Comparative Baselines\n\nWe select three baselines with exploration\
    \ and knowledge extraction capabilities for comprehensive comparison. AppAgent\
    \ [\\(Zhang et al.,](#page-10-2) [2025\\)](#page-10-2) requires manually designed\
    \ exploration tasks to guide its interaction with GUI environments for knowledge\
    \ acquisition, whereas AutoDroid [\\(Wen](#page-9-11) [et al.,](#page-9-11) [2024\\\
    )](#page-9-11) eliminates task-specific human effort by autonomously generating\
    \ random action sequences to collect exploration trajectories. Both methods extract\
    \ structured text-based knowledge from raw textual observations during exploration.\
    \ DigiRL [\\(Zhou et al.,](#page-10-6) [2024\\)](#page-10-6) adopts a distinct\
    \ reinforcement learning framework to iteratively explore environments while utilizing\
    \ the Gemini [\\(Google,](#page-8-3) [2025\\)](#page-8-3) model to filter successful\
    \ trajectories as training data, enabling adaptive explo-\n\n<span id=\"page-6-0\"\
    ></span>5 [https://huggingface.co/google/](https://huggingface.co/google/siglip-so400m-patch14-384)\n\
    \n[siglip-so400m-patch14-384](https://huggingface.co/google/siglip-so400m-patch14-384)\n\
    \n<span id=\"page-6-1\"></span>\n\n| Agent                              | Input\
    \     |          | Base Model Task Success<br>Rate (%) |\n|------------------------------------|-----------|----------|-------------------------------------|\n\
    | AppAgent (Zhang et al., 2025)      | SoM       | GPT-4o   | 14.0           \
    \                     |\n| AutoDroid (Wen et al., 2024)       | a11y tree | GPT-4o\
    \   | 12.0                                |\n| CogAgent (Hong et al., 2024)  \
    \     | screen    | CogAgent | 0                                   |\n| DigiRL\
    \ (Zhou et al., 2024)         | screen    | DigiRL   | 0                     \
    \              |\n| M3A (Rawles et al., 2024)          | SoM       | GPT-4o  \
    \ | 42.0                                |\n| MobileAgentV2 (Wang et al., 2024a)\
    \ | SoM       | GPT-4o   | 20.0                                |\n| SeeAct (Zheng\
    \ et al., 2024)        | SoM       | GPT-4o   | 12.0                         \
    \       |\n| GUI-explorer (Ours)                | SoM       | GPT-4o   | 53.7\
    \                                |\n\nTable 1: Performance comparison on SPA-Bench\
    \ singleapp English Level 3 tasks. Results for the first 7 agents are from the\
    \ SPA-Bench [\\(Chen et al.,](#page-8-6) [2025\\)](#page-8-6). SoM [\\(Yang](#page-10-5)\
    \ [et al.,](#page-10-5) [2023\\)](#page-10-5) utilizes the bounding boxes (bbox)\
    \ recorded in the a11y tree to annotate UI elements with numerical labels in screenshots.\n\
    \nration with minimal human intervention. For completeness, we also report results\
    \ from additional baselines in their respective benchmark papers as performance\
    \ references.\n\n### 5.2 Experimental Results\n\nOur comprehensive evaluation\
    \ demonstrates GUIexplorer's superior performance across multiple dimensions.\
    \ As shown in Table [1,](#page-6-1) GUI-explorer achieves 53.7% task success rate\
    \ on SPA-Bench single-app English Level 3 tasks. This represents a 28.1% absolute\
    \ improvement over M3A, the previous state-of-the-art. Our transition-aware knowledge\
    \ mining approach proves highly effective in complex, real-world scenarios.\n\n\
    The AndroidWorld results in Table [3](#page-7-0) further validate GUI-explorer's\
    \ generalizability. Our agent achieves 47.4% success rate. This surpasses visioncentric\
    \ Aria-UI at 44.8%. It also outperforms multimodal M3A at 40.5%.\n\nThe GUI-KRB\
    \ evaluation reveals critical insights about MLLMs' GUI reasoning limitations.\
    \ GPT-4o shows an 18.2% prior knowledge error rate. These errors mainly stem from\
    \ the misinterpreting of UI components and outdated interface understanding. Our\
    \ method reduces these errors by 16.0% when applied to Qwen2-VL-72B-Instruct-GPTQ-Int4.\
    \ This demonstrates the effectiveness of transition-aware knowledge. The dynamic\
    \ comprehension assessment shows similar improvements. GUI-explorer-enabled models\
    \ achieve 13.4% lower error rates than base models.\n\n### 5.3 Analysis and Discussion\n\
    \nThe ablation study in Figure [4](#page-7-1) quantifies the impact of our key\
    \ components. Removing dynamic guidance construct by transition-aware knowledge\
    \ causes a 12.2% performance drop. This empha-\n\n<span id=\"page-7-2\"></span>\n\
    \n| App Category           | Retrieval Time<br>per Step (sec) | Ranking Time<br>per\
    \ Step (sec) | Reasoning Time<br>per Step (sec) | Total Time<br>per Step (sec)\
    \ | Ranking Cost per<br>Step (0.1USD) | Reasoning Cost<br>per Step (USD) | Total\
    \ Cost per<br>Step (USD) |\n|------------------------|----------------------------------|--------------------------------|----------------------------------|------------------------------|-----------------------------------|----------------------------------|------------------------------|\n\
    | Travel & Navigation    | 7.663                            | 33.084         \
    \                | 31.400                           | 72.147                 \
    \      | 0.017                             | 0.066                           \
    \ | 0.068                        |\n| Shopping & Finance     | 8.613         \
    \                   | 24.922                         | 36.622                \
    \           | 70.157                       | 0.013                           \
    \  | 0.063                            | 0.065                        |\n| News\
    \ & Reading         | 8.123                            | 17.317              \
    \           | 29.272                           | 54.712                      \
    \ | 0.008                             | 0.053                            | 0.053\
    \                        |\n| System Applications    | 6.955                 \
    \           | 31.083                         | 34.513                        \
    \   | 72.552                       | 0.016                             | 0.065\
    \                            | 0.067                        |\n| Productivity\
    \ & Tools   | 7.136                            | 28.091                      \
    \   | 28.382                           | 63.609                       | 0.016\
    \                             | 0.064                            | 0.066     \
    \                   |\n| Media & Entertainment  | 7.549                      \
    \      | 32.481                         | 30.586                           | 70.615\
    \                       | 0.017                             | 0.066          \
    \                  | 0.068                        |\n| Communication & Social\
    \ | 6.176                            | 25.662                         | 27.293\
    \                           | 59.130                       | 0.013           \
    \                  | 0.057                            | 0.058                \
    \        |\n| Food & Lifestyle       | 6.304                            | 9.511\
    \                          | 30.481                           | 46.296       \
    \                | 0.004                             | 0.041                 \
    \           | 0.042                        |\n| Overall                | 7.120\
    \                            | 28.462                         | 30.796       \
    \                    | 66.378                       | 0.015                  \
    \           | 0.062                            | 0.064                       \
    \ |\n\nTable 2: Per-Step Computational Overhead Analysis: Breakdown of time consumption\
    \ (seconds) and API costs (USD) across application categories. Note that Ranking\
    \ Cost per Step is presented in Dimes (0.1 USD) for better readability due to\
    \ its small magnitude.\n\n<span id=\"page-7-0\"></span>\n\n| Input | Base Model\
    \                                                                            \
    \     | Task Success<br>Rate (%)                                             \
    \                            |\n|-------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n\
    |       |                                                                    \
    \                        | 80.0                                              \
    \                                               |\n|       |                 \
    \                                                                           |\
    \ 37.1                                                                       \
    \                      |\n|       |                                          \
    \                                                  | 14.9                    \
    \                                                                         |\n\
    |       |                                                                    \
    \                        | 44.8                                              \
    \                                               |\n|       |                 \
    \                                                                           |\
    \ 15.7                                                                       \
    \                      |\n|       |                                          \
    \                                                  | 0.9                     \
    \                                                                         |\n\
    |       |                                                                    \
    \                        | 40.5                                              \
    \                                               |\n|       |                 \
    \                                                                           |\
    \ 34.5                                                                       \
    \                      |\n|       |                                          \
    \                                                  | 15.5                    \
    \                                                                         |\n\
    |       |                                                                    \
    \                        | 32.8                                              \
    \                                               |\n| SoM   | GPT-4o          \
    \                                                                           |\
    \ 47.4                                                                       \
    \                      |\n|       | screen<br>screen<br>SoM<br>screen<br>a11y\
    \ tree<br>screen<br>SoM<br>screen<br>SoM<br>screen | -<br>GPT-4o<br>GPT-4o<br>GPT-4o<br>GPT-4o<br>DigiRL<br>GPT-4o<br>GPT-4o<br>GPT-4-turbo<br>GPT-4o\
    \ |\n\nTable 3: Performance comparison on AndroidWorld.\n\n<span id=\"page-7-3\"\
    ></span>\n\n| Model                           | Prior Knowledge<br>Error Rate\
    \ (%) | Dynamic Comprehen<br>sion Rrror Rate (%) |\n|---------------------------------|-----------------------------------|------------------------------------------|\n\
    | Qwen2-VL (Wang et al., 2024b)   | 22.8                              | 19.8 \
    \                                    |\n| Qwen2.5-VL (Bai et al., 2025)   | 16.6\
    \                              | 14.0                                     |\n\
    | Gemini 2.0 Flash (Google, 2025) | 15.2                              | 11.2 \
    \                                    |\n| GPT-4o (Hurst et al., 2024)     | 18.2\
    \                              | 13.4                                     |\n\
    | GUI-explorer (w/o Ranker)       | 9.8                               | 6.8  \
    \                                    |\n| GUI-explorer                    | 6.8\
    \                               | 6.4                                      |\n\
    \nTable 4: Performance comparison on GUI-KRB. For all methods, we selected the\
    \ highest-performing models within device VRAM constraints: Qwen2-VL-72B-Instruct-GPTQ-Int4\
    \ for Qwen2-VL, and Qwen2.5-VL-7B-Instruct for Qwen2.5-VL.\n\nsizes the critical\
    \ role of transition-aware knowledge. Cross-Environment Guidance improves performance\
    \ by 4.3% compared to No Guidance. This demonstrates that our transition-aware\
    \ knowledge exhibits promising generalization capabilities. It effectively guides\
    \ agent reasoning even in previously unseen scenarios. The knowledge learned can\
    \ transfer across different UI environments.\n\nOur computational overhead analysis\
    \ appears in Table [2.](#page-7-2) It reveals practical tradeoffs. The ranking\
    \ component contributes 42.9% of time. This comes primarily from MLLM-based pairwise\
    \ comparisons. However, we use a merge sort implementation. This ensures O(n log\
    \ n) complexity. This keeps practical costs acceptable (0.0015 USD/step average).\
    \ Additionally, Table [4](#page-7-3) shows another benefit. The ranking component\
    \ reduced the error rate\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_8.jpeg)\n\
    \nFigure 4: Ablation study of operational guidance configurations on SPA-Bench:\
    \ (1) Baseline without dynamic guidance, (2) Guidance derived from cross-environment\
    \ exploration (AndroidWorld), (3) Guidance generated through in-environment exploration\
    \ (SPA-Bench).\n\n#### by 3% by prioritizing more relevant knowledge.\n\nThe GUI-KRB\
    \ results expose two fundamental limitations in current MLLMs. First, there are\
    \ persistent prior knowledge gaps. Even Gemini 2.0 Flash [\\(Google,](#page-8-3)\
    \ [2025\\)](#page-8-3) has a 15.2% error rate. Second, there is limited dynamic\
    \ reasoning capability.\n\nThe GUI-KRB Dynamic Comprehension task, equivalent\
    \ to transition-aware knowledge mining, achieved 86.6% accuracy with GPT-4o, indicating\
    \ comparable reliability in our GPT-4o-built Knowledge Vector Store.\n\n### 6\
    \ Conclusion\n\nWe present GUI-explorer, a GUI agent designed to address two key\
    \ challenges: misinterpretation of UI components and knowledge obsolescence. Our\
    \ approach achieves this through autonomous exploration and transition-aware knowledge\
    \ mining. Experimental results demonstrate our SOTA performance across major benchmarks.We\
    \ introduce the GUI-KRB benchmark, which reveals fundamental limitations in current\
    \ MLLMs' interface understanding capabilities. Our dynamic guidance mechanism\
    \ effectively mitigates these limitations.\n\n### Limitations\n\nWhile GUI-explorer\
    \ demonstrates significant advancements in GUI automation, several limitations\
    \ warrant discussion. First, our current implementation of exploration anchors\
    \ relies on mobile app manifest declarations (e.g., Android Activity components),\
    \ which limits direct applicability to web and desktop environments. Second, although\
    \ the current Knowledge Ranker takes only 28.5 seconds per step, it's still a\
    \ bit slow. Future work will focus on extending this approach to web and desktop\
    \ and speeding up Knowledge Ranker.\n\n### Ethics Statement\n\nOur work introduces\
    \ GUI-explorer, an autonomous agent for graphical user interface automation, and\
    \ raises several ethical considerations inherent to AIdriven interaction systems.\
    \ First, while our exploration process utilizes application screenshots and accessibility\
    \ metadata, we strictly employ opensource or publicly available applications,\
    \ ensuring no collection of private user data or infringement of intellectual\
    \ property rights.\n\nSecond, our reliance on large multimodal models introduces\
    \ potential risks of perpetuating societal biases embedded in their training data.\
    \ Though our transition-aware knowledge mechanism mitigates the misinterpretation\
    \ of UI components, we acknowledge that residual biases in element interpretation\
    \ could lead to unintended operational consequences. We strongly advocate for\
    \ human oversight in real-world deployments, particularly for sensitive applications\
    \ in healthcare or finance domains.\n\nThe computational costs associated with\
    \ our approach (average 66 seconds per interaction step) raise environmental concerns\
    \ regarding energy consumption. While our method eliminates the need for model\
    \ retraining—a significant carbon footprint contributor—future work must prioritize\
    \ efficiency optimizations to enable sustainable scaling.\n\nWe recognize potential\
    \ dual-use risks where autonomous GUI agents could be misused for malicious automation\
    \ (e.g., credential stuffing or click fraud), much like other AI technologies\
    \ can be used for creating deceptive presentations or face presentation attacks\
    \ [\\(Shao et al.,](#page-9-17) [2019,](#page-9-17) [2025\\)](#page-9-18).\n\n\
    Finally, our benchmark construction followed ethical annotation practices, with\
    \ contributors compensated at fair market rates and granted full rights to withdraw\
    \ their participation.\n\n### References\n\n- <span id=\"page-8-0\"></span>Josh\
    \ Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\
    \ Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\
    \ 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.\n- <span id=\"\
    page-8-9\"></span>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge,\
    \ Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi\
    \ Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren\
    \ Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo\
    \ Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-vl technical report. *arXiv\
    \ preprint arXiv:2502.13923*.\n- <span id=\"page-8-2\"></span>Gongwei Chen, Leyang\
    \ Shen, Rui Shao, Xiang Deng, and Liqiang Nie. 2024. Lion: Empowering multimodal\
    \ large language model with dual-level visual knowledge. In *Proceedings of the\
    \ IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages\
    \ 26540–26550.\n- <span id=\"page-8-6\"></span>Jingxuan Chen, Derek Yuen, Bin\
    \ Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu,\
    \ Shuai Wang, Kaiwen Zhou, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye HAO, Jun\
    \ Wang, and Kun Shao. 2025. Spa-bench: A comprehensive benchmark for smartphone\
    \ agent evaluation. In *The Thirteenth International Conference on Learning Representations*.\n\
    - <span id=\"page-8-5\"></span>Sidong Feng, Haochuan Lu, Jianqin Jiang, Ting Xiong,\
    \ Likun Huang, Yinglin Liang, Xiaoqin Li, Yuetang Deng, and Aldeida Aleti. 2024.\
    \ Enabling costeffective ui automation testing with retrieval-based llms: A case\
    \ study in wechat. In *Proceedings of the 39th IEEE/ACM International Conference\
    \ on Automated Software Engineering*, pages 1973–1978.\n\n<span id=\"page-8-3\"\
    ></span>Google. 2025. [Gemini 2.0 is now available to everyone.](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/)\n\
    \n- <span id=\"page-8-8\"></span>Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie,\
    \ Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2025. [Navigating the digital\
    \ world as humans do:](https://openreview.net/forum?id=kxnoqaisCT) [Universal\
    \ visual grounding for GUI agents.](https://openreview.net/forum?id=kxnoqaisCT)\
    \ In *The Thirteenth International Conference on Learning Representations*.\n\
    - <span id=\"page-8-7\"></span>Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng\
    \ Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et\
    \ al. 2024. Cogagent: A visual language model for gui agents. In *Proceedings\
    \ of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages\
    \ 14281–14290.\n- <span id=\"page-8-1\"></span>Aaron Hurst, Adam Lerer, Adam P\
    \ Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda,\
    \ Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. *arXiv preprint arXiv:2410.21276*.\n\
    - <span id=\"page-8-4\"></span>Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan\
    \ Wasi, Hojun Choi, Steve Ko, Sangeun Oh, and Insik\n\nShin. 2024. [Mobilegpt:\
    \ Augmenting llm with human](https://doi.org/10.1145/3636534.3690682)[like app\
    \ memory for mobile task automation.](https://doi.org/10.1145/3636534.3690682)\
    \ In *Proceedings of the 30th Annual International Conference on Mobile Computing\
    \ and Networking*, ACM MobiCom '24, page 1119–1133, New York, NY, USA. Association\
    \ for Computing Machinery.\n\n- <span id=\"page-9-4\"></span>Wei Li, Bing Hu,\
    \ Rui Shao, Leyang Shen, and Liqiang Nie. 2025a. Lion-fs: Fast & slow video-language\
    \ thinker as online video assistant. In *IEEE Conference on Computer Vision and\
    \ Pattern Recognition (CVPR)*.\n- <span id=\"page-9-7\"></span>Zaijing Li, Yuquan\
    \ Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. 2025b. Optimus-1:\
    \ Hybrid multimodal memory empowered agents excel in long-horizon tasks. *Advances\
    \ in neural information processing systems*, 37:49881–49913.\n- <span id=\"page-9-8\"\
    ></span>Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang\
    \ Nie. 2025c. Optimus-2: Multimodal minecraft agent with goal-observationaction\
    \ conditioned policy. In *2025 IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition (CVPR)*. IEEE.\n- <span id=\"page-9-14\"></span>David Marr and Ellen\
    \ Hildreth. 1980. Theory of edge detection. *Proceedings of the Royal Society\
    \ of London. Series B. Biological Sciences*, 207(1167):187– 217.\n\n<span id=\"\
    page-9-9\"></span>OpenAI. 2023. [Gpt-4v\\(ision\\) system card.](https://cdn.openai.com/papers/GPTV_System_Card.pdf)\n\
    \n- <span id=\"page-9-13\"></span>Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang,\
    \ Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang,\
    \ et al. 2025. Ui-tars: Pioneering automated gui interaction with native agents.\
    \ *arXiv preprint arXiv:2501.12326*.\n- <span id=\"page-9-10\"></span>Christopher\
    \ Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth\
    \ Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. 2024.\
    \ Androidworld: A dynamic benchmarking environment for autonomous agents. *arXiv\
    \ preprint arXiv:2405.14573*.\n- <span id=\"page-9-12\"></span>Gabriel Herbert\
    \ Sarch, Lawrence Jang, Michael J Tarr, William W Cohen, Kenneth Marino, and Katerina\
    \ Fragkiadaki. 2024. Vlm agents generate their own memories: Distilling experience\
    \ into embodied programs of thought. In *The Thirty-eighth Annual Conference on\
    \ Neural Information Processing Systems*.\n- <span id=\"page-9-17\"></span>Rui\
    \ Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen. 2019. Multi-adversarial discriminative\
    \ deep domain generalization for face presentation attack detection. In *Proceedings\
    \ of the IEEE/CVF conference on computer vision and pattern recognition*, pages\
    \ 10023– 10031.\n- <span id=\"page-9-3\"></span>Rui Shao, Tianxing Wu, and Ziwei\
    \ Liu. 2023. Detecting and grounding multi-modal media manipulation. In *Proceedings\
    \ of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages\
    \ 6904– 6913.\n- <span id=\"page-9-18\"></span>Rui Shao, Tianxing Wu, Liqiang\
    \ Nie, and Ziwei Liu. 2025. Deepfake-adapter: Dual-level adapter for deepfake\
    \ detection. *International Journal of Computer Vision*, pages 1–16.\n- <span\
    \ id=\"page-9-2\"></span>Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and\
    \ Ziwei Liu. 2024. Detecting and grounding multi-modal media manipulation and\
    \ beyond. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.\n\
    - <span id=\"page-9-5\"></span>Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan,\
    \ and Liqiang Nie. 2024. Mome: Mixture of multimodal experts for generalist multimodal\
    \ large language models. In *Advances in neural information processing systems*.\n\
    - <span id=\"page-9-0\"></span>Yu Su, Diyi Yang, Shunyu Yao, and Tao Yu. 2024.\
    \ [Lan](https://aclanthology.org/2024.emnlp-tutorials.3)[guage agents: Foundations,\
    \ prospects, and risks.](https://aclanthology.org/2024.emnlp-tutorials.3) In *Proceedings\
    \ of the 2024 Conference on Empirical Methods in Natural Language Processing:\
    \ Tutorial Abstracts*, pages 17–24, Miami, Florida, USA. Association for Computational\
    \ Linguistics.\n- <span id=\"page-9-19\"></span>Weiwei Sun, Lingyong Yan, Xinyu\
    \ Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren.\
    \ 2023. [Is ChatGPT good at search?](https://doi.org/10.18653/v1/2023.emnlp-main.923)\
    \ [investigating large language models as re-ranking](https://doi.org/10.18653/v1/2023.emnlp-main.923)\
    \ [agents.](https://doi.org/10.18653/v1/2023.emnlp-main.923) In *Proceedings of\
    \ the 2023 Conference on Empirical Methods in Natural Language Processing*, pages\
    \ 14918–14937, Singapore. Association for Computational Linguistics.\n- <span\
    \ id=\"page-9-1\"></span>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\
    \ Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,\
    \ Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\
    \ and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\
    \ *arXiv preprint arXiv:2302.13971*.\n- <span id=\"page-9-6\"></span>Junyang Wang,\
    \ Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang,\
    \ and Jitao Sang. 2024a. [Mobile-agent-v2: Mobile device op](https://proceedings.neurips.cc/paper_files/paper/2024/file/0520537ba799d375b8ff5523295c337a-Paper-Conference.pdf)[eration\
    \ assistant with effective navigation via multi](https://proceedings.neurips.cc/paper_files/paper/2024/file/0520537ba799d375b8ff5523295c337a-Paper-Conference.pdf)[agent\
    \ collaboration.](https://proceedings.neurips.cc/paper_files/paper/2024/file/0520537ba799d375b8ff5523295c337a-Paper-Conference.pdf)\
    \ In *Advances in Neural Information Processing Systems*, volume 37, pages 2686–\
    \ 2710. Curran Associates, Inc.\n- <span id=\"page-9-15\"></span>Peng Wang, Shuai\
    \ Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu,\
    \ Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men,\
    \ Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-vl: Enhancing\
    \ vision-language model's perception of the world at any resolution. *arXiv preprint\
    \ arXiv:2409.12191*.\n- <span id=\"page-9-16\"></span>Yiqin Wang, Haoji Zhang,\
    \ Jingqi Tian, and Yansong Tang. 2024c. Ponder & press: Advancing visual gui agent\
    \ towards general computer control. *arXiv preprint arXiv:2412.01268*.\n- <span\
    \ id=\"page-9-11\"></span>Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao\
    \ Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,\n\nYaqin Zhang, and Yunxin Liu.\
    \ 2024. Autodroid: Llmpowered task automation in android. In *Proceedings of the\
    \ 30th Annual International Conference on Mobile Computing and Networking*, pages\
    \ 543–557.\n\n- <span id=\"page-10-9\"></span>Yiheng Xu, Zekun Wang, Junli Wang,\
    \ Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong.\
    \ 2024. Aguvis: Unified pure vision agents for autonomous gui interaction. *arXiv\
    \ preprint arXiv:2412.04454*.\n- <span id=\"page-10-0\"></span>An Yang, Baosong\
    \ Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\
    \ Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\
    \ Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin\
    \ Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\
    \ Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su,\
    \ Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024a.\
    \ Qwen2.5 technical report. *arXiv preprint arXiv:2412.15115*.\n- <span id=\"\
    page-10-5\"></span>Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li,\
    \ and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual\
    \ grounding in gpt-4v. *arXiv preprint arXiv:2310.11441*.\n- <span id=\"page-10-8\"\
    ></span>Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and\
    \ Junnan Li. 2024b. Aria-ui: Visual grounding for gui instructions. *arXiv preprint\
    \ arXiv:2412.16256*.\n- <span id=\"page-10-4\"></span>Shunyu Yao, Jeffrey Zhao,\
    \ Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\
    \ Synergizing reasoning and acting in language models. In *International Conference\
    \ on Learning Representations (ICLR)*.\n- <span id=\"page-10-3\"></span>Qilang\
    \ Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. 2024. [Cat:\
    \ Enhancing](https://arxiv.org/abs/2403.04640) [multimodal large language model\
    \ to answer ques](https://arxiv.org/abs/2403.04640)[tions in dynamic audio-visual\
    \ scenarios.](https://arxiv.org/abs/2403.04640) *Preprint*, arXiv:2403.04640.\n\
    - <span id=\"page-10-2\"></span>Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng\
    \ Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2025. [Appagent: Multimodal\
    \ agents as smartphone](https://doi.org/10.1145/3706598.3713600) [users.](https://doi.org/10.1145/3706598.3713600)\
    \ In *Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems*,\
    \ CHI '25, New York, NY, USA. Association for Computing Machinery.\n- <span id=\"\
    page-10-1\"></span>Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024.\
    \ [Gpt-4v\\(ision\\) is a generalist web agent,](https://openreview.net/forum?id=piecKJ2DlB)\
    \ [if grounded.](https://openreview.net/forum?id=piecKJ2DlB) In *Forty-first International\
    \ Conference on Machine Learning*.\n- <span id=\"page-10-7\"></span>Longtao Zheng,\
    \ Rundong Wang, Xinrun Wang, and Bo An. 2023. Synapse: Trajectory-as-exemplar\
    \ prompting with memory for computer control. In *The Twelfth International Conference\
    \ on Learning Representations*.\n\n<span id=\"page-10-6\"></span>Yifei Zhou, Hao\
    \ Bai, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024.\
    \ Digirl: Training in-the-wild device-control agents with autonomous reinforcement\
    \ learning. In *Automated Reinforcement Learning: Exploring Meta-Learning, AutoML,\
    \ and LLMs*.\n\n### <span id=\"page-11-3\"></span>A Hardware Configurations\n\n\
    Hardware configurations were optimized for costeffectiveness: Most experiments\
    \ ran on a single NVIDIA GeForce RTX 4070 Laptop GPU (8GB VRAM). For GUI-KRB evaluations\
    \ involving open-source MLLMs, we scaled to two NVIDIA L40S GPUs (48GB VRAM) to\
    \ accommodate larger VRAM requirements.\n\n### <span id=\"page-11-1\"></span>B\
    \ GUI-KRB Benchmark Distributions\n\n<span id=\"page-11-4\"></span>![](_page_11_Figure_3.jpeg)\n\
    \nFigure 5: Distribution of apps in GUI-KRB.\n\nFigure [5](#page-11-4) shows the\
    \ distribution of the number of apps in GUI-KRB.\n\n# <span id=\"page-11-2\"></span>C\
    \ GUI-KRB Benchmark Sample Data\n\nThis section presents an example from the GUI-KRB\
    \ benchmark to illustrate its data structure, as shown in Figure [6.](#page-11-5)\
    \ Each sample consists of five main components. First, it contains screenshots\
    \ captured before and after the interaction with the target element to demonstrate\
    \ the visual state transition. Second, it includes the accessibility tree representation\
    \ of the interface. Third, the broader task context describes the necessary interaction\
    \ with the target element required to complete the task. Fourth, the transition-aware\
    \ knowledge associated with the element is documented but excluded from the test\
    \ input. Finally, for automated evaluation purposes, the sample includes evaluation\
    \ keywords (also excluded from test input) that incorporate synonyms and related\
    \ terms (such as \"modify\" and \"Main\") to accommodate various valid responses\
    \ and reduce false judgments during model assessment.\n\n# <span id=\"page-11-0\"\
    ></span>D Understanding Exploration Anchors\n\nAs introduced in Section [3.1,](#page-2-1)\
    \ Exploration Anchors are key entry points to application functionalities. These\
    \ are explicitly declared by develop-\n\n<span id=\"page-11-5\"></span>![](_page_11_Figure_10.jpeg)\n\
    \nFigure 6: A comprehensive sample from GUI-KRB benchmark illustrating: (1) before/after\
    \ screenshots of target element interaction, (2) accessibility tree representation,\
    \ (3) broader task context, (4) transition-aware knowledge (excluded from test\
    \ input), and (5) evaluation keywords with synonyms for robust assessment.\n\n\
    ers within the app's manifest file (e.g., Android's AndroidManifest.xml) and typically\
    \ correspond to *activities*, which represent individual screens or distinct functional\
    \ units within the app.\n\n# D.1 Extraction and Utilization\n\nExploration Anchors\
    \ are systematically extracted from the app's manifest file, which enumerates\
    \ all developer-declared activities. During the autonomous exploration process,\
    \ the agent utilizes these anchors to formulate intermediate navigational goals.\
    \ For example, if an anchor corresponding to a specific activity (e.g., an activity\
    \ named ShareInstagramStory) is not directly accessible\n\nfrom the agent's current\
    \ screen, the agent will iteratively generate sub-goals, such as navigating through\
    \ menus or interacting with UI elements, to eventually reach the target anchor\
    \ activity.\n\n### D.2 Illustrative Case Studies\n\nTo further clarify the concept\
    \ and utility of Exploration Anchors, consider the following examples from the\
    \ \"Retro Music\" application. Its manifest file declares activities including\
    \ ShareInstagramStory, DriveModeActivity, and RestoreActivity. If the agent begins\
    \ its exploration from a screen displaying \"Most played\" tracks, it might generate\
    \ intermediate goals such as:\n\n- 1. \"Tap 'Most played', then share the top\
    \ song to Instagram Story.\" (Targeting anchor: ShareInstagramStory)\n- 2. \"\
    Tap the settings icon, then navigate to Drive Mode and enable it.\" (Targeting\
    \ anchor: DriveModeActivity)\n\n# E Error Analysis\n\nIn this section, we categorize\
    \ and discuss three primary error types observed in our evaluation trajectories,\
    \ detailing their component-level manifestations and root causes.\n\n# E.1 Perceptual\
    \ Errors\n\nPerceptual errors occur when agents misinterpret visual or contextual\
    \ cues. For instance, in the SPA-Bench dictionary\\_merriam\\_webster\\_2 task,\
    \ the agent failed to recognize that a solid icon indicated an already \"saved\"\
    \ word. Instead, it redundantly clicked the \"save\" button, unintentionally unsaving\
    \ the word. Such errors often stem from limitations in grounding GUI elements\
    \ (e.g., distinguishing icon states like filled versus hollow) or parsing dynamic\
    \ UI hierarchies (e.g., overlays, scrolling content).\n\n# E.2 Reasoning Errors\n\
    \nReasoning errors arise from incomplete task decomposition or flawed step-by-step\
    \ logic. For example, in SPA-Bench contacts\\_2, the agent appended \"Three\"\
    \ to an existing last name instead of first deleting the original text, thereby\
    \ violating the task's implicit requirement to \"replace\" rather than \"modify\"\
    . These errors reflect limitations in the base model's ability to infer nuanced\
    \ constraints\n\n(e.g., distinguishing \"edit\" from \"overwrite\") or manage\
    \ multi-step dependencies (e.g., ensuring changes are saved before exiting).\n\
    \n# E.3 Missing Knowledge Errors\n\nMissing knowledge errors occur when agents\
    \ lack app-specific prior knowledge critical for task completion. For instance,\
    \ in SPA-Bench booking.com\\_5, the agent exhausted its step limit searching for\
    \ a \"currency\" setting under \"Preferences\" instead of the correct \"Payment\
    \ details\" section. This highlights challenges in efficiently navigating unfamiliar\
    \ UIs, particularly when key functionalities are nested within non-intuitive menus.\n\
    \n# F Comparison with Alternative Ranking Methods\n\nTo validate the necessity\
    \ and efficiency of our pairwise knowledge ranking module, we conducted comparative\
    \ experiments against three alternative ranking strategies: (1) Direct LLM judgment:\
    \ The LLM directly assesses the usefulness of each knowledge piece. (2) Confidence\
    \ scores: Ranking based on confidence scores assigned to knowledge. (3) Sliding-window\
    \ ranking [\\(Sun et al.,](#page-9-19) [2023\\)](#page-9-19): A sequential ranking\
    \ approach.\n\nWe evaluated these methods on the GUI-KRB benchmark using an identical\
    \ experimental setup (Qwen2-VL-72B-Instruct-GPTQ-Int4). The results\n\n<span id=\"\
    page-12-0\"></span>\n\n| Ranking Method                    | Prior Knowledge<br>Error\
    \ Rate (%) | Dynamic Comprehen<br>sion Error Rate (%) |\n|-----------------------------------|-----------------------------------|------------------------------------------|\n\
    | Direct LLM judgment               | 8.8                               | 8.8\
    \                                      |\n| Confidence score                 \
    \ | 7.6                               | 7.0                                  \
    \    |\n| Sliding-window (Sun et al., 2023) | 6.8                            \
    \   | 6.4                                      |\n| Ours                     \
    \         | 6.8                               | 6.4                          \
    \            |\n\nTable 5: Comparison of Different Knowledge Ranking Methods on\
    \ GUI-KRB. Error rates (%) are reported. Lower is better.\n\nin Table [5](#page-12-0)\
    \ demonstrate that both direct LLM judgment and confidence-based scoring yield\
    \ higher error rates than our proposed method. While slidingwindow ranking achieves\
    \ comparable error rates to our approach, it suffers from a time complexity of\
    \ O(n) due to its sequential traversal of knowledge items. In contrast, our merge-sort-inspired\
    \ pairwise comparison strategy allows for O(log n) time complexity through parallelizable\
    \ divide-andconquer iterations. This inherent parallelism makes our method significantly\
    \ more scalable for large knowledge sets. For instance, to reduce latency with\
    \ sliding-window ranking, one would need to\n\ncompromise sorting quality (e.g.,\
    \ by enlarging window or step sizes). Crucially, the computational overhead of\
    \ merge operations in our method is negligible, as LLM-based pairwise comparisons\
    \ (which incur seconds-level delays) dominate the overall runtime.\n\nFurthermore,\
    \ as shown in Table [4,](#page-7-3) disabling ranking increases the Prior Knowledge\
    \ error rate from 6.8% to 9.8% , highlighting the critical role of ranking in\
    \ performance.\n\n# G Generalization and Robustness Analysis\n\nThis section presents\
    \ additional experimental results to address the robustness of our autonomous\
    \ exploration approach with incomplete metadata and the generalization of our\
    \ framework to web environments.\n\n### G.1 Robustness to Incomplete Metadata\n\
    \nTo assess the performance of our autonomous exploration approach when application\
    \ metadata is incomplete or unavailable, we conducted supplementary experiments\
    \ where exploration task goal generation relied solely on screenshot data, without\
    \ access to manifest-derived structural information (Exploration Anchors as described\
    \ in Section [D\\)](#page-11-0). This scenario simulates conditions where only\
    \ visual information is accessible.\n\nWe tested this screenshot-only configuration\
    \ on the Retro Music and Broccoli applications, evaluating performance using the\
    \ corresponding task sets from the AndroidWorld benchmark. The results, compared\
    \ against a baseline without any exploration and our full method (which utilizes\
    \ manifest metadata), are presented in Table [6.](#page-13-0)\n\n<span id=\"page-13-0\"\
    ></span>\n\n| Method Configuration      | Task Success Rate (%) |  |\n|---------------------------|-----------------------|--|\n\
    | No exploration (Baseline) | 16.7                  |  |\n| Ours (Screenshots\
    \ only)   | 22.2                  |  |\n| Ours (Full method)        | 33.3   \
    \               |  |\n\nTable 6: Performance comparison on selected Android-World\
    \ tasks (Retro Music and Broccoli apps) under varying levels of metadata availability\
    \ for exploration task goal generation.\n\nThe results indicate that even when\
    \ restricted to using only screenshots for generating exploration goals, our method\
    \ achieves a 5.5% absolute improvement in task success rate over the nonexploratory\
    \ baseline. While the integration of full\n\nmetadata (Exploration Anchors) clearly\
    \ yields superior performance (33.3% success rate), these experiments confirm\
    \ that the core exploration strategy remains effective and provides benefits even\
    \ in metadata-deficient scenarios. This demonstrates a degree of robustness in\
    \ our approach, as the visual cues present in screenshots can still guide meaningful\
    \ exploration, albeit less efficiently than when supplemented with structural\
    \ priors from manifest files.\n\n### G.2 Generalization to Web Environments\n\n\
    While our method, particularly the Autonomous Exploration component described\
    \ in Section [3.1,](#page-2-1) leverages Android-specific structural information\
    \ (i.e., Exploration Anchors from manifest files) to guide exploration, it is\
    \ pertinent to investigate the potential for the acquired knowledge to generalize\
    \ to other platforms, such as web applications.\n\nTo this end, we conducted additional\
    \ experiments to evaluate the cross-platform applicability of the transition-aware\
    \ knowledge mined from Android environments. Specifically, we applied our framework,\
    \ using knowledge acquired solely from Android app exploration, to tasks in a\
    \ web environment. The evaluation was performed on the \"website\" split of the\
    \ Multimodal-Mind2Web test set, comprising 20 distinct samples. We compared the\
    \ performance of a baseline agent without any augmented knowledge against an agent\
    \ augmented with static knowledge derived from our Android exploration phase.\
    \ The results are presented in Table [7.](#page-13-1)\n\n<span id=\"page-13-1\"\
    ></span>\n\n| Knowledge Configuration            | Macro Element<br>Accuracy (%)\
    \ | Macro Step<br>Accuracy (%) |\n|------------------------------------|-------------------------------|----------------------------|\n\
    | No augmented knowledge (Baseline)  | 45.97                         | 42.31 \
    \                     |\n| + Static Android-derived knowledge | 47.26        \
    \                 | 43.05                      |\n\nTable 7: Performance on the\
    \ Multimodal-Mind2Web \"website\" test split (20 samples) with and without leveraging\
    \ static knowledge acquired from Android app exploration.\n\nThe results show\
    \ a modest improvement in both macro element accuracy (+1.29%) and macro step\
    \ accuracy (+0.74%) when leveraging knowledge acquired from Android applications.\
    \ This suggests that some fundamental aspects of UI interaction logic, such as\
    \ understanding hierarchical structures, common action sequences (e.g., \"select\
    \ then confirm\"), and visual-textual correlations, possess a degree of cross-platform\
    \ generalizability. While these improvements are not as substantial as those\n\
    \nobserved within the Android domain, they indicate that the core principles of\
    \ transition-aware knowledge are not strictly confined to mobile environments\
    \ and can offer some benefit in web contexts without any web-specific exploration\
    \ or finetuning.\n\nIt is important to note that web environments present unique\
    \ challenges, such as highly dynamic content, complex DOM structures, and browserspecific\
    \ interactions, which are not fully addressed by knowledge solely derived from\
    \ Android apps. Future work will focus on adapting the autonomous exploration\
    \ and knowledge mining mechanisms specifically for web and desktop platforms to\
    \ achieve more significant performance gains. However, these initial findings\
    \ provide encouraging evidence of the underlying generalizability of the learned\
    \ GUI interaction patterns.\n\n## <span id=\"page-14-2\"></span>H Distribution\
    \ of Transition-aware Knowledge\n\n<span id=\"page-14-3\"></span>![](_page_14_Figure_3.jpeg)\n\
    \nFigure 7: Distribution of transition-aware knowledge gained through autonomous\
    \ exploration.\n\nFigure [7](#page-14-3) shows the distribution of transitionaware\
    \ knowledge gained through autonomous exploration.\n\n### I Prompting Templates\
    \ of GUI-explorer\n\n### <span id=\"page-14-0\"></span>I.1 Prompting Template\
    \ of Function-aware Task Goal Generator\n\nGiven the screenshot of app name and\
    \ its available activities, generate a comprehensive list of practical user tasks\
    \ that:\n\n1. Start from the current screen shown in the screenshot 2. Can be\
    \ completed within 10-30 steps\n\n3. Utilize the app's full feature set based\
    \ on the activity list\n\n4. Are concrete and specific (like searching for a particular\
    \ item rather than just \"search\")\n\n5. Cover different user interaction patterns\
    \ (viewing, editing, sharing, etc.)\n\n6. Include both basic and advanced features\
    \ 7. Represent realistic user behaviors and goals\n\n8. Avoid excessive steps\
    \ on form-filling or scrolling pages\n\nImportant context: - App name: app name\
    \ - Package name: package name - Available activities (app screens/features):\
    \ activity list Format requirements:\n\n1. List only the tasks without explanations\
    \ or commentary\n\n2. Each task should be a single, clear directive 3. Use specific\
    \ examples (e.g., concrete search terms,\n\n- actions, settings) 4. Include the\
    \ expected outcome where relevant\n- 5. Tasks should follow this pattern: [Starting\
    \ action]\n\n+ [Specific steps] + [End goal]\n\nExample tasks from other apps\
    \ (for reference only): 1. Search for \"ocean waves\" white noise, then sort\n\
    \nresults by most played 2. Open the first recommended video, then post \"Great\
    \ content!\" as a comment\n\n3. Play the trending video, then add it to your \"\
    Watch Later\" playlist\n\n4. Navigate to the comments section of a featured video,\
    \ then like the top comment\n\nGenerate diverse tasks that would help a user explore\
    \ and utilize all major features visible in the screenshot and implied by the\
    \ activity list.\n\n# <span id=\"page-14-4\"></span>I.2 Prompting Template of\
    \ Unsupervised Mining of Transition-aware Knowledge\n\nObjective: Describe the\
    \ functionality of a specific UI element in a mobile app screenshot. Input:\n\n\
    - Two screenshots: Before and after interacting with a UI element\n- UI element\
    \ marked with a numeric tag in the top-left corner\n- Element number: numeric\
    \ tag of element\n- Broader task context: task description\n- Action taken: action\n\
    - UI Element Attributes: \"'\n\nui element attributes \"'\n\n- Requirements for\
    \ Functionality Description:\n- 1. Concise: 1-2 sentences\n- 2. Focus on general\
    \ function, not specific details\n- 3. Avoid mentioning the numeric tag 4. Use\
    \ generic terms like \"UI element\" or appropriate pronouns\n\nExample:\n\n- Incorrect:\
    \ \"Tapping the element #3 displays David's saved recipes in the results panel\"\
    \n\n- Correct: \"Tapping this element will initiates a search and displays matching\
    \ results\" Guidance:\n\n- Describe the core action and immediate result of interacting\
    \ with the UI element\n\n- Prioritize clarity and generality in the description\n\
    \n# <span id=\"page-14-1\"></span>I.3 Prompting Template of Knowledge Ranker\n\
    \nGiven the user instruction: task goal, determine which of the following two\
    \ knowledge entries is more useful. Respond ONLY with a integer value: 1 means\
    \ Knowledge A is strictly better. 2 means Knowledge B is strictly better. Knowledge\
    \ A: knowledge a Knowledge B: knowledge b Please provide your response:\n\n####\
    \ I.4 Prompting Template of Reasoning\n\n## Role Definition You are an Android\
    \ operation AI that fulfills user requests through precise screen interactions.\
    \ The current screenshot and the same screenshot with bounding boxes and labels\
    \ added are also given to you. ## Action Catalog Available actions (STRICT JSON\
    \ FORMAT REQUIRED): 1. Status Operations: - Task Complete: {\"action\\_type\"\
    : \"status\", \"goal\\_status\": \"complete\"} - Task Infeasible: {\"action\\\
    _type\": \"status\", \"goal\\_status\": \"infeasible\"} 2. Information Actions:\
    \ - Answer Question: {\"action\\_type\": \"answer\", \"text\": \"<answer\\_text>\"\
    } 3. Screen Interactions: - Tap Element: {\"action\\_type\": \"click\", \"index\"\
    : <visible\\_index>} - Long Press: {\"action\\_type\": \"long\\_press\", \"index\"\
    : <visible\\_index>} - Scroll: Scroll the screen or a specific scrollable UI element.\
    \ Use the 'index' of the target element if scrolling a specific element, or omit\
    \ 'index' to scroll the whole screen. \"action\\_type\": \"scroll\", \"direction\"\
    : <\"up\"|\"down\"|\"left\"|\"right\">, \"index\": <optional\\_target\\_index>\
    \ 4. Input Operations: - Text Entry: {\"action\\_type\": \"input\\_text\", \"\
    text\": \"<content>\", \"index\": <text\\_field\\_index>} - Keyboard Enter: {\"\
    action\\_type\": \"keyboard\\_enter\"} 5. Navigation: - Home Screen: {\"action\\\
    _type\": \"navigate\\_home\"} - Back Navigation: {\"action\\_type\": \"navigate\\\
    _back\"} 6. System Actions: - Launch App: {\"action\\_type\": \"open\\_app\",\
    \ \"app\\_name\": \"<exact\\_name>\"} - Wait Refresh: {\"action\\_type\": \"wait\"\
    } ## Current Objective User Goal: task goal ## Execution Context Action History:\
    \ history Visible UI Elements (Only interact with \\*visible=true elements): ui\
    \ elements ## Core Strategy 1. Path Optimization: - Prefer direct methods (e.g.,\
    \ open\\_app > app drawer navigation) - Always use the 'input\\_text' action for\
    \ entering text into designated text fields. - Verify element visibility ('visible=true')\
    \ before attempting any interaction (click, long\\_press, input\\_text). Do not\
    \ interact with elements marked 'visible=false'. - Use 'scroll' when necessary\
    \ to bring off-screen elements into view. Prioritize scrolling specific containers\
    \ ('index' provided) over full-screen scrolls if possible. 2. Error Handling Protocol:\
    \ - Switch approach after ≥ 2 failed attempts - Prioritize scrolling ('scroll'\
    \ action) over force-acting on invisible elements - If an element is not visible,\
    \ use 'scroll' in the likely direction (e.g., 'down' to find elements below the\
    \ current view). - Try opposite scroll direction if initial fails (up/down, left/right)\
    \ - If the 'open\\_app' action fails to correctly open the app, find the corresponding\
    \ app in the app drawer and open it. 3. Information Tasks: - MANDATORY: Use answer\
    \ action for questions - Verify data freshness (e.g., check calendar date) ##\
    \ Expert Techniques Here are some tips for you: knowledge ## Response Format STRICTLY\
    \ follow: Reasoning: [Step-by-step analysis covering: - Visibility verification\n\
    \n- History effectiveness evaluation\n- Alternative approach comparison\n\n- Consideration\
    \ of scrolling if needed] Action: [SINGLE JSON action from catalog] Generate response:\n\
    \n# J Prompting Templates of GUI-KRB\n\n### J.1 Prompting Template of Prior Knowledge\
    \ Task\n\nObjective: Describe the functionality of a specific UI element in a\
    \ mobile app screenshot. Input:\n\n- One screenshot: Before interacting with a\
    \ UI element - UI element marked with a numeric tag in the top-left corner\n\n\
    - Element number: numeric tag of element\n- Broader task context: task description\n\
    - UI Element Attributes:\n\n\"' ui element attributes\n\n\"'\n\nRequirements for\
    \ Functionality Description:\n\n- 1. Concise: 1-2 sentences\n- 2. Focus on general\
    \ function, not specific details 3. Avoid mentioning the numeric tag\n\n4. Use\
    \ generic terms like \"UI element\" or appropriate pronouns\n\nExample:\n\n- Incorrect:\
    \ \"Tapping the element #3 displays David's saved recipes in the results panel\"\
    \n\n- Correct: \"Tapping this element will initiates a search and displays matching\
    \ results\"\n\nGuidance:\n\n- Describe the core action and immediate result of\
    \ interacting with the UI element\n\n- Infer functionality based on the current\
    \ screen context - Prioritize clarity and generality in the description\n\n# J.2\
    \ Prompting Template of Dynamic Comprehension Task\n\nSame as Appendix [I.2.](#page-14-4)\n\
    \n# J.3 Prompting Templates for GUI-explorer (w/o Ranker)\n\n### J.3.1 Prompting\
    \ Template of Prior Knowledge Task\n\nObjective: Describe the functionality of\
    \ a specific UI element in a mobile app screenshot. Input:\n\n- One screenshot:\
    \ Before interacting with a UI element - UI element marked with a numeric tag\
    \ in the top-left\n\n- corner\n- Element number: numeric tag of element\n- Broader\
    \ task context: task description\n- UI Element Attributes: \"'\n\nui element attributes\n\
    \n\"'\n\n- Similar UI Elements' Functionalities (retrieved based on visual similarity):\
    \ \"'\n\n#### similar element functionalities\n\n\"' Requirements for Functionality\
    \ Description:\n\n- 1. Concise: 1-2 sentences\n- 2. Focus on general function,\
    \ not specific details 3. Avoid mentioning the numeric tag\n\n4. Use generic terms\
    \ like \"UI element\" or appropriate pronouns\n\n5. Consider similar elements'\
    \ functionalities as reference, but prioritize:\n\n- Current screen context\n\n\
    - UI element attributes - Task description\n\n6. Only incorporate relevant patterns\
    \ from similar elements if they align with the current context Example:\n\n- Incorrect:\
    \ \"Tapping the element #3 displays David's saved recipes in the results panel\"\
    \n\n- Correct: \"Tapping this element will initiates a search and displays matching\
    \ results\"\n\nGuidance:\n\n- Describe the core action and potential result of\
    \ interacting with the UI element\n\n- Infer functionality based on the current\
    \ screen context\n\n- Prioritize clarity and generality in the description - Use\
    \ similar elements' functionalities to validate and\n\nrefine your description,\
    \ not to simply copy them\n\n# J.3.2 Prompting Template of Dynamic Comprehension\
    \ Task\n\nObjective: Describe the functionality of a specific UI element in a\
    \ mobile app screenshot. Input:\n\n- Two screenshots: Before and after interacting\
    \ with a UI element\n\n- UI element marked with a numeric tag in the top-left\
    \ corner\n\n- Element number: numeric tag of element\n\n- Broader task context:\
    \ task description\n\n- UI Element Attributes: \"'\n\nui element attributes\n\n\
    \"' - Similar UI Elements' Functionalities (retrieved based on visual similarity):\n\
    \n\"' similar element functionalities\n\n\"' Requirements for Functionality Description:\n\
    \n1. Concise: 1-2 sentences\n\n2. Focus on general function, not specific details\n\
    \n3. Avoid mentioning the numeric tag\n\n4. Use generic terms like \"UI element\"\
    \ or appropriate pronouns\n\n5. Consider similar elements' functionalities as\
    \ reference, but prioritize:\n\n- Current screen context\n- UI element attributes\n\
    - Task description\n\n6. Only incorporate relevant patterns from similar elements\
    \ if they align with the current context Example:\n\n- Incorrect: \"Tapping the\
    \ element #3 displays David's saved recipes in the results panel\"\n\n- Correct:\
    \ \"Tapping this element will initiates a search and displays matching results\"\
    \n\nGuidance:\n\n- Describe the core action and immediate result of interacting\
    \ with the UI element\n\n- Infer functionality based on the current screen context\n\
    \n- Prioritize clarity and generality in the description - Use similar elements'\
    \ functionalities to validate and refine your description, not to simply copy\
    \ them\n\n# J.4 Prompting Templates of Prior Knowledge Task for GUI-explorer\n\
    \n### J.4.1 Prompting Template of Prior Knowledge Task\n\nObjective: Describe\
    \ the functionality of a specific UI element in a mobile app screenshot. Input:\n\
    \n- One screenshot: Before interacting with a UI element - UI element marked with\
    \ a numeric tag in the top-left corner\n\n- Element number: numeric tag of element\n\
    - Broader task context: task description\n- UI Element Attributes: \"'\n\nui element\
    \ attributes\n\n\"' - Similar UI Elements' Functionalities (ranked by relevance\
    \ to task description):\n\n\"' similar element functionalities\n\n\"' Note: Elements\
    \ are sorted by relevance, with most task-relevant functionalities listed first\n\
    \nRequirements for Functionality Description:\n\n1. Concise: 1-2 sentences\n\n\
    2. Focus on general function, not specific details\n\n3. Avoid mentioning the\
    \ numeric tag\n\n4. Use generic terms like \"UI element\" or appropriate pronouns\
    \ 5. Consider similar elements' functionalities as\n\nreference, with priority:\
    \ - Higher-ranked (more relevant) reference\n\nfunctionalities\n\n- Current screen\
    \ context\n- UI element attributes\n- Task description\n\n6. Only incorporate\
    \ relevant patterns from similar elements if they align with the current context\
    \ Example:\n\n- Incorrect: \"Tapping the element #3 displays David's saved recipes\
    \ in the results panel\"\n\n- Correct: \"Tapping this element will initiates a\
    \ search and displays matching results\"\n\nGuidance:\n\n- Describe the core action\
    \ and potential result of interacting with the UI element\n\n- Infer functionality\
    \ based on the current screen context - Prioritize clarity and generality in the\
    \ description\n\n- Pay special attention to higher-ranked similar functionalities\
    \ as they are more likely to be relevant - Use similar elements' functionalities\
    \ to validate and\n\nrefine your description, not to simply copy them\n\n# J.4.2\
    \ Prompting Template of Dynamic Comprehension Task\n\nObjective: Describe the\
    \ functionality of a specific UI element in a mobile app screenshot. Input:\n\n\
    - Two screenshots: Before and after interacting with a UI element\n\n- UI element\
    \ marked with a numeric tag in the top-left corner\n\n- Element number: numeric\
    \ tag of element\n- Broader task context: task description\n\n- UI Element Attributes:\
    \ \"'\n\nui element attributes\n\n\"' - Similar UI Elements' Functionalities (ranked\
    \ by\n\nrelevance to task description): \"'\n\nsimilar element functionalities\
    \ \"'\n\nNote: Elements are sorted by relevance, with most task-relevant functionalities\
    \ listed first\n\nRequirements for Functionality Description:\n\n1. Concise: 1-2\
    \ sentences\n\n2. Focus on general function, not specific details\n\n3. Avoid\
    \ mentioning the numeric tag\n\n4. Use generic terms like \"UI element\" or appropriate\
    \ pronouns\n\n5. Consider similar elements' functionalities as reference, with\
    \ priority:\n\n- Higher-ranked (more relevant) reference functionalities\n\n-\
    \ Current screen context\n\n- UI element attributes - Task description\n\n6. Only\
    \ incorporate relevant patterns from similar elements if they align with the current\
    \ context\n\nExample: - Incorrect: \"Tapping the element #3 displays David's saved\
    \ recipes in the results panel\"\n\n- Correct: \"Tapping this element will initiates\
    \ a search and displays matching results\"\n\nGuidance: - Describe the core action\
    \ and potential result of interacting with the UI element\n\n- Infer functionality\
    \ based on the current screen context\n\n- Prioritize clarity and generality in\
    \ the description\n\n- Pay special attention to higher-ranked similar functionalities\
    \ as they are more likely to be relevant\n\n- Use similar elements' functionalities\
    \ to validate and refine your description, not to simply copy them"
- title: "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in\n  Mathematical\
    \ Reasoning"
  abstract: 'Recent advances have demonstrated that integrating reinforcement learning

    with rule-based rewards can significantly enhance the reasoning capabilities of

    large language models, even without supervised fine-tuning. However, prevalent

    reinforcement learning algorithms such as GRPO and its variants like DAPO,

    suffer from a coarse granularity issue when computing the advantage.

    Specifically, they compute rollout-level advantages that assign identical

    values to every token within a sequence, failing to capture token-specific

    contributions and hindering effective learning. To address this limitation, we

    propose Key-token Advantage Estimation (KTAE) - a novel algorithm that

    estimates fine-grained, token-level advantages without introducing additional

    models. KTAE leverages the correctness of sampled rollouts and applies

    statistical analysis to quantify the importance of individual tokens within a

    sequence to the final outcome. This quantified token-level importance is then

    combined with the rollout-level advantage to obtain a more fine-grained

    token-level advantage estimation. Empirical results show that models trained

    with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five

    mathematical reasoning benchmarks. Notably, they achieve higher accuracy with

    shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base

    model.'
  url: http://arxiv.org/abs/2505.16826v1
  keywords: ''
  document: '# KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in
    Mathematical Reasoning


    Wei Sun1,2,Wen Yang1,2,Pu Jian1,2 , Qianlong Du<sup>1</sup> ,Fuwei Cui<sup>1</sup>
    ,Shuo Ren<sup>1</sup> ,Jiajun Zhang†1,2,3 1 Institute of Automation, Chinese Academy
    of Sciences <sup>2</sup>School of Artificial Intelligence, University of Chinese
    Academy of Sciences <sup>3</sup>Wuhan AI Research {sunwei2023,yangwen2023,jianpu2023,shuo.ren,fuwei.cui}@ia.ac.cn
    {qianlong.du,jjzhang}@nlpr.ia.ac.cn


    Project Page: <https://github.com/xiaolizh1/KTAE>


    ## Abstract


    Recent advances have demonstrated that integrating reinforcement learning with
    rule-based rewards can significantly enhance the reasoning capabilities of large
    language models, even without supervised fine-tuning. However, prevalent reinforcement
    learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse
    granularity issue when computing the advantage. Specifically, they compute rollout-level
    advantages that assign identical values to every token within a sequence, failing
    to capture token-specific contributions and hindering effective learning. To address
    this limitation, we propose Key-token Advantage Estimation (*KTAE*) - a novel
    algorithm that estimates fine-grained, token-level advantages without introducing
    additional models. KTAE leverages the correctness of sampled rollouts and applies
    statistical analysis to quantify the importance of individual tokens within a
    sequence to the final outcome. This quantified token-level importance is then
    combined with the rollout-level advantage to obtain a more fine-grained token-level
    advantage estimation. Empirical results show that models trained with GRPO+KTAE
    and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks.
    Notably, they achieve higher accuracy with shorter responses and even surpass
    R1-Distill-Qwen-1.5B using the same base model.


    ## 1 Introduction


    Notably, large reasoning language models (LRMs) like OpenAI o1 [\[1\]](#page-9-0)
    and DeepSeek R1 [\[2\]](#page-9-1) have demonstrated the capability to solve complex
    mathematical reasoning problems that challenge even human experts. This progress
    marks a significant step toward Artificial General Intelligence (AGI) [\[3,](#page-9-2)
    [4,](#page-9-3) [5\]](#page-9-4). These reasoning language models often exhibit
    behaviors such as self-reflection and self-verification within the reasoning chain,
    which are critical for enhancing reasoning accuracy. DeepSeek R1 aptly refers
    to the critical turning points that lead to improved performance as "aha moments".
    The emergence and cultivation of such moments are greatly facilitated by the application
    of reinforcement learning (RL) or distilled from more powerful LRMs [\[6,](#page-9-5)
    [7,](#page-9-6) [2\]](#page-9-1). For instance, DeepSeek applied RL directly to
    the base language model, using a simple rule-based reward function to encourage
    the model to explore and unlock its reasoning potential [\[8\]](#page-9-7) through
    self-exploration.


    <sup>†</sup>Corresponding author.


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    Figure 1: Model performance comparison. KTAE is a plug-and-play method that introduces
    no additional model. It provides token-level advantage estimation for existing
    RL algorithms such as GRPO and its variants. "GRPO+KTAE" and "DAPO+KTAE" denote
    GRPO and DAPO combined with KTAE respectively, both RL-tuned on the Qwen2.5-Math-7B
    model. Detailed results in Table [1.](#page-7-0)


    As a mainstream RL algorithm, Group Relative Policy Optimization (GRPO) [\[9\]](#page-9-8)
    differs from the Proximal Policy Optimization (PPO) [\[10\]](#page-9-9) by eliminating
    the need for a separate critic model. Instead, it estimates the advantage of each
    token using the rewards obtained from a set of generated rollouts. However, due
    to the absence of a critic model, GRPO computes a rollout-level advantage - assigning
    the same advantage value to every token within a single rollout. This limitation
    also persists in its improved variant, DAPO [\[11\]](#page-9-10). In practice,
    the importance of each token in a complete Chain-of-Thought (CoT) reasoning sequence
    varies, and we often observe that incorrect rollouts may only diverge from the
    correct reasoning path in the final steps. Consequently, applying a uniform advantage
    value across all tokens in a rollout lacks granularity and may hinder effective
    learning. Prior efforts have explored using process-level reward models to provide
    more fine-grained signals [\[12,](#page-9-11) [13,](#page-9-12) [14,](#page-9-13)
    [15,](#page-9-14) [16\]](#page-9-15). However, as highlighted by DeepSeek [\[2\]](#page-9-1),
    training fine-grained reward models is costly, difficult to scale, has limited
    capacity to provide accurate signals, and is prone to reward hacking [\[17\]](#page-10-0).


    To address these challenges, we propose the Key-token Advantage Estimation (KTAE)
    algorithm. KTAE introduces no additional models, and instead leverages the correctness
    of sampled rollouts and the occurrence of each token within them to construct
    a contingency table. Then, using statistical methods such as Fisher''s exact test
    and Information Gain (IG), it quantifies the strength of association between each
    token and correct rollout. Subsequently, by combining the token''s frequency and
    the reward assigned to its corresponding rollout, KTAE further quantifies the
    direction of this association''s contribution. Finally, these measures are combined
    (e.g., through multiplication) to yield a ''key-token value'' for each token.
    As shown in Figure [2,](#page-2-0) when a correct rollout is incorrectly classified
    as incorrect by the rule, KTAE can still highlight the positively contributing
    tokens through computing the key-token values. In contrast, GRPO assigns the same
    negative advantages to all tokens in such a case. Moreover, KTAE can effectively
    distinguish between tokens irrelevant to problem solving, such as ''First'' and
    ''denote'', and those highly relevant to problem solving, such as ''complement''
    and ''ratio''. Furthermore, KTAE is compatible with GRPO and DAPO. The resulting
    key-token values are then added to the rollout-level advantage computed by GRPO
    to obtain a more fine-grained token-level advantage estimate. As illustrated in
    Figure [1,](#page-1-0) integrating KTAE with either GRPO or DAPO yields improved
    performances on average across five major mathematical reasoning benchmarks. Moreover,
    KTAE not only improves test accuracy but also very effectively reduces response
    length without any length penalty reward, resulting in extremely high reasoning
    efficiency.


    In summary, the KTAE algorithm offers several advantages:


    1. KTAE provides more fine-grained advantage information without introducing extra
    models, resulting in lower training costs.


    - 2. KTAE quantifies the importance differences between tokens using statistical
    analysis methods, offering strong interpretability.

    - 3. KTAE''s key-token value is computed based on the correctness of the final
    answer and retains the original rollout-level advantage, making it less susceptible
    to reward hacking.

    - 4. KTAE helps the model to focus on key tokens and reduce the learning of irrelevant
    tokens, which can effectively reduce the response length.


    <span id="page-2-0"></span>![](_page_2_Figure_3.jpeg)


    Figure 2: Visualization of key-token-values computed by KTAE for a correct rollout
    whose final result was unparsable and thus received a final reward of 0. Red shading
    indicates negative token associations with producing a correct rollout, with darker
    red representing stronger negative influence; Green shading indicates positive
    associations.


    ## 2 Preliminary


    #### 2.1 Reinforcement Learning in LLM


    When applying Reinforcement Learning (RL) to language models, text generation
    is modeled as a token-level Markov Decision Process (MDP). At each timestep t,
    the state s<sup>t</sup> consists of the input prompt q and the previously generated
    tokens [o1, . . . , ot−1], i.e., s<sup>t</sup> = [q; o1, . . . , ot−1]. The policy
    π generates the next token o<sup>t</sup> as action at, and generation ends upon
    producing an endof-sequence token or reaching a maximum length T. The full sequence
    o = [o1, . . . , o<sup>T</sup> ] is then evaluated by a reward function R(q, o)
    = P<sup>|</sup>o<sup>|</sup> <sup>t</sup>=1 r(st, ot). The RL objective aims to
    maximize an entropy-regularized expected reward [\[18\]](#page-10-1):


    $$\mathcal{J}(\pi\_{\theta}) = \mathbb{E}\_{(q, o) \sim \pi} [R(q, o)] - \beta
    \mathbb{E}\_{q \sim D, s\_t \sim \pi\_{\theta}} [D\_{KL}(\pi\_{\theta}(\cdot |
    s\_t) || \pi\_{ref}(\cdot | s\_t))] \tag{1}$$


    Here, πref is a reference policy, DKL denotes the KL divergence, and β controls
    the penalty strength. This KL term, central to RLHF, discourages large shifts
    from the reference distribution to preserve fluency and diversity. In recent mathematical
    reasoning tasks [\[11,](#page-9-10) [19\]](#page-10-2), β is typically set to
    0.


    #### 2.2 GRPO


    Group Rollout Policy Optimization (GRPO) [\[9\]](#page-9-8) is simplified based
    on PPO and eliminates the need for a value model. Given an input q, GRPO samples
    G rollouts {o1, . . . , oG} from the old policy and computes their cumulative
    rewards R = {R1, . . . , RG}. These rewards are then used to estimate advantages
    Aˆ i,t, e.g., by comparing each R<sup>i</sup> to a baseline derived from R. The
    optimization objective for GRPO is defined as follows:


    $$\begin{aligned} \mathcal{L}\_{\text{GISO}}(\theta) &= \mathbb{E}\_{q \sim \mathcal{D},
    \{o\_i\}\_{i=1}^G \sim \pi\_{\theta\_{\text{old}}}(\cdot | q)}\\ &\left[ \frac{1}{G}
    \sum\_{i=1}^G \frac{1}{|o\_i|} \sum\_{t=1}^{|o\_i|} \left( \min\left(r\_{i,t}(\theta)\hat{A}\_{i,t},
    \text{clip}\left(r\_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon\right)\hat{A}\_{i,t}\right)
    - \beta D\_{\text{KL}}(\pi\_{\theta} || \pi\_{\text{nd}}) \right) \right] \end{aligned}
    (2)$$


    where ri,t(θ) = <sup>π</sup>θ(oi,t|q,oi,<t) πθold (oi,t|q,oi,<t) , and Aˆ i,t
    is the advantage estimate derived from the group rewards R, defined as Aˆ i,t
    = Ri−mean(R) std(R) . The clipping term with clip ratio ε [\[20\]](#page-10-3)
    aims to constrain the new policy within the trust region of the old policy, enhancing
    training stability. By eliminating the dependency on the value model Vϕ, GRPO
    aims to substantially reduce training costs while striving to maintain optimization
    effectiveness comparable to traditional PPO.


    #### 2.3 DAPO


    Dynamic Sampling Policy Optimization (DAPO) [\[11\]](#page-9-10) is an enhancement
    algorithm of GRPO, specifically tailored for tasks involving mathematical reasoning.
    To mitigate the phenomenon of entropy collapse, DAPO introduces the "Clip-Higher"
    method, which raises the upper bound of the clipping function. It incorporates
    "Dynamic Sampling" to prevent scenarios where all G sampled rollouts exhibit identical
    preference outcomes (e.g., all positive or all negative). A "Token-Level Policy
    Gradient Loss" is employed to stabilize the training process. Additionally, DAPO
    introduces "overlong reward shaping" to penalize excessively long responses, thereby
    preventing the model from falling into catastrophic repetition loops.


    $$\begin{split} \mathcal{J}\_{\text{DAPO}}(\theta) &= \begin{aligned} \mathbb{E}\_{q
    \sim \mathcal{D}, \{o\_{i}\}\_{i=1}^{G} \sim \pi\_{\theta\_{\text{dcl}}}(\cdot
    | q) \\ \left[ \frac{1}{\sum\_{i=1}^{G} |o\_{i}|} \sum\_{i=1}^{G} \sum\_{t=1}^{|o\_{i}|}
    \min\left(r\_{i,t}(\theta)\hat{A}\_{i,t}, \text{clip}\left(r\_{i,t}(\theta), 1
    - \varepsilon\_{\text{low}}, 1 + \varepsilon\_{\text{high}}\right) \hat{A}\_{i,t}\right)
    \right] \\ \text{s.t. } & 0 < \left| \{o\_{i} \mid \text{is\\_equivalent}(o, o\_{i})\}
    \right| < G, \end{aligned} \tag{3}$$


    where ri,t(θ) and Aˆ i,t are the same as GRPO. εlow and εhigh represent the upper
    and lower bounds of ri,t after decoupling.


    ## 3 KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation


    GRPO''s advantage estimation has a relatively coarse granularity. It assigns the
    same advantage value to every token within the same rollout. However, in tasks
    requiring complex reasoning steps, such as mathematical reasoning, the importance
    of different tokens within a rollout can vary significantly. To address this,
    we propose the KTAE (Key-tokens Advantage Estimation) algorithm. Without additional
    models, KTAE quantifies the importance of different tokens by analyzing the statistical
    associations within the set of sampled rollouts (correct vs. incorrect). It then
    integrates this quantified token importance with rollout-level advantage estimates
    (computed by GRPO) to produce fine-grained, token-level advantage estimations.
    In this section, we will introduce its calculation process in detail.


    #### 3.1 Building Token-Level Contingency Tables


    For a given problem, we sample a set of G rollouts {o1, . . . , oG}, each with
    a corresponding rulebased reward {R1, . . . , RG} indicating its correctness,
    following the same approach as GRPO. We divide these rollouts into a correct set
    x<sup>T</sup> and an incorrect set xF. For each token oij in the sampled rollouts
    o<sup>i</sup> , we examine its occurrence across all G rollouts and construct
    a 2 × 2 contingency table summarizing the counts of correct and incorrect rollouts
    that contain or do not contain oij . An example contingency table is shown in
    Figure [3.](#page-4-0)


    Here, G is the total number of rollouts. For token oij , we use the statistics
    a<sup>o</sup>ij , b<sup>o</sup>ij , c<sup>o</sup>ij , d<sup>o</sup>ij based on
    its occurrence across the rollouts sets. a<sup>o</sup>ij is the count of rollouts
    in x<sup>T</sup> where oij appears at least once, a<sup>o</sup>ij = P I(oij ∈
    xT); c<sup>o</sup>ij is the count of rollouts in x<sup>T</sup> where oij does
    not appear c<sup>o</sup>ij = sum(xT) − a<sup>o</sup>ij ; b<sup>o</sup>ij and d<sup>o</sup>ij
    are calculated in the same way. The total count: a<sup>o</sup>ij + b<sup>o</sup>ij
    + c<sup>o</sup>ij + d<sup>o</sup>ij = G. We then use the statistics from this
    contingency table to compute the association between the occurrence of oij and
    the policy sampling a correct rollout.


    #### 3.2 Quantifying Association Strength via Hypothesis Testing


    We begin by quantifying the association using hypothesis testing. We set the null
    hypothesis (H0) as: the occurrence of oij and the correctness of its rollout have
    no association. We use Fisher''s exact


    <span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)


    Figure 3: The outline of KTAE algorithm. It constructs a contingency table based
    on the correctness of the sampled rollouts, and then calculates the token-level
    advantage and adds it to the GRPO''s rollout-level advantage.


    test [\[21\]](#page-10-4) to compute the p-value, which is the probability of
    observing the current contingency table or a more extreme one, assuming H<sup>0</sup>
    is true. The formula for Fisher''s exact test p-value is:


    <span id="page-4-1"></span>

    $$Fisher(o\_{ij}) = \frac{\binom{a\_{ij} + b\_{o\_{ij}}}{a\_{o\_{ij}}} \binom{c\_{ij}
    + d\_{o\_{ij}}}{c\_{o\_{ij}}}}{\binom{N}{a\_{o\_{ij}} + c\_{o\_{ij}}}} = \frac{(a\_{o\_{ij}}
    + b\_{o\_{ij}})! (c\_{o\_{ij}} + d\_{o\_{ij}})! (a\_{o\_{ij}} + c\_{o\_{ij}})!
    (b\_{o\_{ij}} + d\_{o\_{ij}})!}{a\_{o\_{ij}}! b\_{o\_{ij}}! c\_{o\_{ij}}! d\_{o\_{ij}}!
    N!} \tag{4}$$


    In practice, this calculation is performed in log-space to handle large factorials
    (see appendix [D](#page-14-0) for details). A smaller p-value indicates stronger
    evidence against the null hypothesis, meaning a stronger association between the
    occurrence of token oij and rollout correctness. Since effective p-values are
    often concentrated in a small range, we employ a transformation function to quantify
    the association strength and amplify the impact of small p-values. We define the
    association score of Fisher''s test as:


    $$\mathcal{F}(o\_{ij}) = \begin{cases} e^{-2 \cdot Fisher(o\_{ij})} & \text{if
    } Fisher(o\_{ij}) \neq 1 \\ 0 & \text{if } Fisher(o\_{ij}) = 1 \end{cases} \tag{5}$$


    When p = 1 (complete no association), the score is 0; when p approaches 0 (strong
    association), the score approaches 1. Fisher''s exact test is chosen over chi-squared
    or G-tests because the latter provide less accurate p-values for small sample
    sizes N, while Fisher''s test offers an exact probability calculation even with
    small G (e.g., G = 8 or 16).


    #### 3.3 Quantifying Association Strength via Information Gain


    To complement the statistical test with an information-theoretic perspective,
    we compute the Information Gain (IG) between the occurrence of the token oij and
    rollout correctness. Let Y be a random variable representing rollout correctness,
    and X<sup>o</sup>ij be a variable indicating whether the token oij appears in
    a rollout. The entropy of rollout correctness H(Y ) is:


    <span id="page-4-2"></span>

    $$H(Y) = -\frac{a\_{o\_{ij}} + c\_{o\_{ij}}}{N} \log\_2\left(\frac{a\_{o\_{ij}}
    + c\_{o\_{ij}}}{N}\right) - \frac{b\_{o\_{ij}} + d\_{o\_{ij}}}{N} \log\_2\left(\frac{b\_{o\_{ij}}
    + d\_{o\_{ij}}}{N}\right) \tag{6}$$


    The conditional entropy of rollout correctness given whether token oij appears,
    H(Y |X<sup>o</sup>ij ), is:


    <span id="page-4-3"></span>

    $$\begin{split} H(Y|X\_{a\_{ij}}) &= \left(\frac{a\_{\boldsymbol{a}\_{ij}} + b\_{\boldsymbol{a}\_{ij}}}{N}\right)
    \left[ -\frac{a\_{\boldsymbol{a}\_{ij}}}{a\_{\boldsymbol{a}\_{ij}} + b\_{\boldsymbol{a}\_{ij}}}
    \log\_2\left(\frac{a\_{\boldsymbol{a}\_{ij}}}{a\_{\boldsymbol{a}\_{ij}} + b\_{\boldsymbol{a}\_{ij}}}\right)
    - \frac{b\_{\boldsymbol{a}\_{ij}}}{a\_{\boldsymbol{a}\_{ij}} + b\_{\boldsymbol{a}\_{ij}}}
    \log\_2\left(\frac{b\_{\boldsymbol{a}\_{ij}}}{a\_{\boldsymbol{a}\_{ij}} + b\_{\boldsymbol{a}\_{ij}}}\right)
    \right] \\ &+ \left(\frac{c\_{\boldsymbol{a}\_{ij}} + d\_{\boldsymbol{a}\_{ij}}}{N}\right)
    \left[ -\frac{c\_{\boldsymbol{a}\_{ij}}}{c\_{\boldsymbol{a}\_{ij}} + d\_{\boldsymbol{a}\_{ij}}}
    \log\_2\left(\frac{c\_{\boldsymbol{a}\_{ij}}}{c\_{\boldsymbol{a}\_{ij}} + d\_{\boldsymbol{a}\_{ij}}}\right)
    - \frac{d\_{\boldsymbol{a}\_{ij}}}{c\_{\boldsymbol{a}\_{ij}} + d\_{\boldsymbol{a}\_{ij}}}
    \log\_2\left(\frac{d\_{\boldsymbol{a}\_{ij}}}{c\_{\boldsymbol{a}\_{ij}} + d\_{\boldsymbol{a}\_{ij}}}\right)
    \right] \end{split} \tag{7}$$


    The Information Gain (IG) is defined as IG(oij ) = H(Y ) − H(Y |Xoij ).


    A higher IG value indicates that knowing whether the token oij appears reduces
    the uncertainty about rollout correctness more significantly, suggesting a stronger
    association with correctness. Otherwise, means the association is weaker. Through
    Fisher''s exact test and Information Gain, we have quantified the strength of
    association between the occurrence of token oij and rollout correctness (e.g.,
    via a linear combination h<sup>1</sup> · F(oij ) + h<sup>2</sup> · IG(oij )).


    #### 3.4 Quantifying Association Direction and Final Importance Score


    However, both F(oij ) and IG(oij ) can only quantify the strength of association
    between the occurrence of oij and the correct rollout, they cannot quantify the
    direction of this association (i.e., positive or negative association). For the
    detailed proof, see Appendix [E.](#page-15-0) To determine the direction of the
    association and further quantify token importance, we adapt the Term Frequency
    calculation idea from BM25 [\[22\]](#page-10-5) to compute standardized frequency
    scores for the token oij within the set of correct and incorrect rollouts.


    Specifically, we concatenate all correct rollouts into a single long sequence
    and all incorrect rollouts into another. We compute the term frequency (tf) of
    the token oij in these two concatenated sequences, denoted as tfT(oij ) and tfF(oij
    ). Based on tfs we compute standardized frequency scores:


    <span id="page-5-0"></span>

    $$TF\_{\rm TF}(o\_{ij}) = \frac{(k\_1 + 1) \cdot \text{tf}\_{\rm TF}(o\_{ij})}{k\_1(1
    - b + b \times \frac{l \text{entrge}}{l \text{en}\_{\rm avg}}) + \text{tf}\_{\rm
    TF}(o\_{ij})} \tag{8}$$


    Here, T/F refers to the correct or incorrect rollouts. *len*<sup>T</sup> and *len*<sup>F</sup>
    are the average lengths of the concatenated correct and incorrect rollouts, respectively,
    and *len*avg is the average rollout length across all G rollouts. k<sup>1</sup>
    and b are adjustable parameters controlling the influence of term frequency and
    length normalization (can be set empirically or tuned). Treating all correct/incorrect
    rollouts as single sequences reduces the impact of individual rollouts with extreme
    lengths.


    The token directional score D(oij ) combines a measure of effect size based on
    proportion differences and a measure based on standardized frequency score differences.
    We use Cohen''s h effect size (arcsin <sup>√</sup> <sup>x</sup> <sup>−</sup> arcsin
    <sup>√</sup>y) metric to measure the difference in the proportion of correct rollouts
    ( aoij aoij +coij ) versus incorrect rollouts ( <sup>b</sup>oij boij +doij ) where
    oij appears. Simultaneously, we consider the ratio difference of the standardized
    frequency scores. The final formula is:


    <span id="page-5-1"></span>

    $$D(o\_{ij}) = \left(\arcsin\sqrt{\frac{a\_{o\_{ij}}}{a\_{o\_{ij}} + c\_{o\_{ij}}}}
    - \arcsin\sqrt{\frac{b\_{o\_{ij}}}{b\_{o\_{ij}} + d\_{o\_{ij}}}}\right) + h\_3\left(\frac{TF\_{\mathbb{T}}(o\_{ij})}{TF\_{\mathbb{F}}(o\_{ij})}
    - \frac{TF\_{\mathbb{F}}(o\_{ij})}{TF\_{\mathbb{T}}(o\_{ij})}\right) \tag{9}$$


    This combination aims to capture different aspects of importance: when the token
    oij ''s frequency is similar in correct and incorrect rollouts(High frequency
    generic tokens), its importance might be better reflected by the probability difference
    in where it appears, hence the dominance of the arcsin square root proportion
    difference term ( Cohen''s h effect size); when the token oij ''s frequency differs
    significantly (especially for low-frequency but critical tokens), the frequency
    ratio better reflects its discriminative power, increasing the importance of the
    frequency score ratio term.


    Theoretically, both the Fisher score F(oij ) and the Information Gain IG(oij )
    are strictly greater than zero, while the directional score D(oij ) spans the
    full real range (−∞, +∞). To derive the final token-level relevance score, we
    multiply the magnitude of correlation (e.g., F(oij ) or IG(oij )) by the directionality
    score D(oij ), which reflects whether the token is positively or negatively associated
    with correct rollouts. Finally get key-token-value of oij is (h<sup>1</sup> ·
    F(oij )+h<sup>2</sup> · IG(oij ))·D<sup>o</sup>ij . Positive key-token-values
    represent positive association direction.


    To stabilize training and constrain the output range, we apply a sigmoid normalization
    to the resulting key-token-values. These normalized values are then added to the
    rollout-level advantage computed by GRPO, thereby producing the final token-level
    advantage:


    $$\hat{A}\_{o\_{ij}}^{KTAE} = \hat{A}\_{o\_i}^{GRPO} + \sigma((h\_1 \cdot \mathcal{F}(o\_{ij})
    + h\_2 \cdot IG(o\_{ij})) \cdot D\_{o\_{ij}}) - 0.5\tag{10}$$


    KTAE is an algorithm for estimating the advantage of tokens, which computes the
    key-token-value through the rollouts obtained from sampling. The complete implementation
    process is shown in Algorithm [1.](#page-6-0) It is orthogonal to the improvement
    strategy of DAPO, and can be combined with DAPO in addition. An schematic diagram
    is shown in Appendix [F.](#page-16-0)


    #### <span id="page-6-0"></span>Algorithm 1 Key-token Advantage Estimation(KTAE)


    - Input: Set of G rollouts {o1, . . . , oG} sampled from policy model, rule-based
    reward {R1, . . . , RG}, weighting parameter h1, h2, h<sup>3</sup>

    - 1: Calculate the rollout-level advantage of GRPO AˆGRP O

    - 2: Summarize all tokens in rollouts into a set O

    - 3: Divide the G rollouts into x<sup>T</sup> and x<sup>F</sup> sets according
    to the reward R obtained by each rollout

    - 4: for o in O do

    - 5: a = P I(oij ∈ xT), b = P I(oij ∈ xF), c = Len(xT) − a, d = Len(xF) − b

    - 6: Calculate F(o) according to Eq. [4,](#page-4-1) and calculate IG(o) according
    to Eq. [6](#page-4-2) and Eq. [7](#page-4-3)

    - 7: Weighted add F(o) and IG(o) to get quantized association strength h<sup>1</sup>
    · F(o) + h<sup>2</sup> · IG(o)

    - 8: Calculate the frequency of o in the correct and incorrect rollouts according
    to Eq. [8](#page-5-0)

    - 9: Calculate the quantized association direction according to Eq. [9](#page-5-1)

    - 10: Multiply the association direction and association strength to get the key-token-value
    of each token, and then add it to AˆGRP O o to get AˆKT AE o


    11: end for


    ## Output: AˆKT AE


    ## 4 Experiment


    Experiment Setting. Our validation and ablation experiments were conducted on
    the Qwen2.5- Math-1.5B [\[23\]](#page-10-6) base model and the comparison experiment
    with baseline methods is based on Qwen2.5-Math-7B base model, using math12k [\[12\]](#page-9-11)
    and its subset math-level3-5 respectively. See Appendix [G.1](#page-16-1) for
    specific details of dataset and benchmark, Appendix [G.3](#page-17-0) for implementation
    details and hyperparameters, and Appendix [J](#page-19-0) for prompt details.


    Method Validation Result. Experiments revealed several key performance trends
    (Fig. [4\)](#page-6-1). KTAE consistently enhances MATH500 test accuracy when
    integrated with GRPO and DAPO, respectively. Regarding mean response length, the
    addition of KTAE significantly reduced the response length for both algorithms
    compared to their original versions. We believe that achieving improved model
    performance while simultaneously reducing generation cost is more meaningful.
    In terms of generation entropy, GRPO+KTAE showed accelerated entropy decrease
    early on but stabilized at a higher level later, beneficial for mitigating entropy
    collapse [\[11\]](#page-9-10). For DAPO+KTAE, its entropy value was considerably
    higher than all other configurations and exhibited a continuous upward trend.
    While such high entropy contributes to increased sampling diversity and avoids
    entropy collapse, it may also introduce a potential risk of reduced training stability.


    <span id="page-6-1"></span>![](_page_6_Figure_17.jpeg)


    Figure 4: The metric curves of test accuracy, mean response length, and generation
    entropy of combining DAPO and GRPO with KTAE.


    Comparison with Baselines. In Table [1,](#page-7-0) the DAPO+KTAE-7B model achieved
    the highest average score across the 5 benchmarks, significantly outperforming
    others on MATH500. Both GRPO+KTAE and DAPO+KTAE achieved better performance than
    all baseline methods on AMC (See Appendix [G.2](#page-17-1) for more details about
    baselines). GRPO+KTAE showed performance improvements compared to the original
    GRPO on four out of five benchmarks, with only a slight decrease on AIME24 (Considering
    AIME24 has just 30 problems, this equates to only one fewer correct answer). Likewise,
    compared to the original DAPO, DAPO+KTAE''s performance improved or remained unchanged
    on four out of five benchmarks, experiencing a slight decrease only on OlympiadBench.
    This demonstrates the effectiveness of the KTAE algorithm. This performance was
    consistent with the 1.5B model, where our model even surpassed R1-Distill-Qwen-1.5B
    with the same base model.


    <span id="page-7-0"></span>Table 1: The zero-shot greedy pass@1 performance of
    the 1.5B and 7B models across five mathematical reasoning benchmarks. All the
    results above are of our reproduction. <sup>∗</sup> refers to OlympiadBench; †
    denotes the results from [\[24\]](#page-10-7). ‡ denotes the results from [\[25\]](#page-10-8),
    @8k refers the max response length.


    | 1.5B Models                  | AIME24 | MATH-500 | AMC  | Minerva | Olympiad∗
    | Avg  |

    |------------------------------|--------|----------|------|---------|-----------|------|

    | Qwen2.5-Math-1.5B-Instruct ‡ | 10.0   | 74.2     | 48.2 | 26.5    | 40.2      |
    39.8 |

    | Qwen2.5-Math-1.5B ‡          | 16.7   | 61.8     | 43.4 | 15.1    | 28.4      |
    33.1 |

    | R1-Distill-Qwen-1.5B@8k ‡    | 20.0   | 77.4     | 49.4 | 25.0    | 35.8      |
    41.5 |

    | Oat-Zero-1.5B [25]           | 20.0   | 74.4     | 50.6 | 23.9    | 37.0      |
    41.2 |

    | GRPO-1.5B                    | 16.7   | 76.0     | 51.8 | 22.1    | 36.3      |
    40.6 |

    | GRPO+KTAE-1.5B               | 26.7   | 75.4     | 41.0 | 27.2    | 38.2      |
    41.7 |

    | DAPO-1.5B                    | 16.7   | 77.6     | 47.0 | 25.7    | 39.0      |
    41.2 |

    | DAPO+KTAE-1.5B               | 20.0   | 77.6     | 50.6 | 29.0    | 40.0      |
    43.4 |

    | 7B Models                    | AIME24 | MATH-500 | AMC  | Minerva | Olympiad∗
    | Avg  |

    |                              |        |          |      |         |           |      |

    | Qwen2.5-Math-Instruct [26] † | 13.3   | 79.8     | 50.6 | 34.6    | 40.7      |
    43.8 |

    | Qwen2.5-Math †               | 13.3   | 57.6     | 45.0 | 14.7    | 23.7      |
    30.9 |

    | Eurus-2-7B-PRIME [27]        | 23.3   | 82.2     | 57.8 | 36.0    | 39.9      |
    47.8 |

    | Oat-Zero-7B [25]             | 36.7   | 79.8     | 61.4 | 30.5    | 41.8      |
    50.0 |

    | OpenReasoner-Zero-7B [19]    | 13.3   | 78.4     | 45.8 | 27.9    | 45.0      |
    41.7 |

    | SimpleRL-Zero-7B [28]        | 26.7   | 78.6     | 59.0 | 33.8    | 43.4      |
    48.3 |

    | GRPO-7B                      | 36.7   | 81.0     | 57.8 | 32.7    | 43.2      |
    50.3 |

    | GRPO+KTAE-7B                 | 33.3   | 82.4     | 65.1 | 33.8    | 43.7      |
    51.7 |

    | DAPO-7B                      | 36.7   | 81.8     | 60.2 | 34.5    | 45.3      |
    51.7 |


    Table [2](#page-8-0) demonstrates that our model can also significantly reduces
    the length of the response without any length penalty reward. This effect is particularly
    pronounced for the 7B parameter model, where the GRPO+KTAE model exhibits a considerably
    shorter generation lengths compared to the baseline methods. This indicates that
    the KTAE algorithm enables the model to concentrate more effectively on key tokens
    that are crucial for problem resolution, thereby curtailing the generation of
    redundant or non-essential tokens. That is to say, KTAE achieved the highest average
    score across the 5 benchmarks while using the least token budget, demonstrating
    the highest reasoning efficiency.


    Ablation Analysis. Figure [5](#page-8-1) shows the impact of each KTAE component.
    Removing any component consistently reduced test accuracy. Excluding IG had the
    largest negative effect on accuracy and produced the shortest sequences. In contrast,
    removing F or tf decreased accuracy while increasing sequence length, though lengths
    remained shorter than those of the GRPO baseline. For entropy, removing tf initially
    caused a notable increase. Importantly, while GRPO suffered from entropy collapse,
    KTAE avoided this. Overall, IG is key for accuracy and brevity, tf supports diversity
    and stability, and F contributes to overall performance. All components are essential
    in accuracy.


    Visualization Example. Beyond the example illustrated in Figure [2,](#page-2-0)
    we also observed several zero-reward rollouts. These rollouts are characterized
    by including the correct answer in their early stages, but subsequently generating
    a large number of redundant tokens, ultimately leading to the correct answer being
    obscured by the subsequent sequence. Appendix [H](#page-17-2) provides a such
    example, where a clear boundary can be distinctly observed, effectively separating
    the correct answer from the redundant tokens. This further validates KTAE''s accuracy
    in identifying key tokens. As shown in Appendix [K,](#page-19-1) we also observed
    the ''aha moment'' phenomenon [\[2\]](#page-9-1) during the KTAE training process.


    | 1.5B Models          | AIME24 | MATH-500 | AMC  | Minerva | Olympiad∗ | Avg    |

    |----------------------|--------|----------|------|---------|-----------|--------|

    | Oat-Zero-1.5B        | 1198   | 878      | 652  | 692     | 938       | 871.6  |

    | GRPO-1.5B            | 1299   | 635      | 908  | 731     | 958       | 906.2  |

    | GRPO+KTAE-1.5B       | 1187   | 884      | 617  | 663     | 890       | 848.2  |

    | DAPO-1.5B            | 1218   | 617      | 950  | 712     | 937       | 886.8  |

    | DAPO+KTAE-1.5B       | 1110   | 983      | 582  | 666     | 861       | 840.4  |

    | 7B Models            | AIME24 | MATH-500 | AMC  | Minerva | Olympiad∗ | Avg    |

    | Eurus-2-7B-PRIME     | 1498   | 685      | 1099 | 777     | 1077      | 1027.2
    |

    |                      |        |          |      |         |           |        |

    | Oat-Zero-7B          | 977    | 658      | 903  | 677     | 892       | 821.4  |

    | OpenReasoner-Zero-7B | 2300   | 1193     | 1901 | 1269    | 1871      | 1706.8
    |

    | SimpleRL-Zero-7B     | 1074   | 634      | 832  | 584     | 881       | 801    |

    | GRPO-7B              | 989    | 606      | 806  | 641     | 813       | 771.0  |

    | GRPO+KTAE-7B         | 941    | 563      | 741  | 577     | 771       | 718.6  |

    | DAPO-7B              | 1155   | 676      | 969  | 700     | 986       | 897.2  |


    <span id="page-8-0"></span>Table 2: The response length of the 1.5B and 7B models
    across five mathematical reasoning benchmarks. All the results above are of our
    reproduction. <sup>∗</sup> refers to OlympiadBench.


    <span id="page-8-1"></span>![](_page_8_Figure_2.jpeg)


    Figure 5: Training results after removing different components from KTAE.


    ## 5 Related Work


    Large Reasoning Models. Breakthroughs [\[1,](#page-9-0) [2,](#page-9-1) [3,](#page-9-2)
    [4,](#page-9-3) [29,](#page-10-12) [30\]](#page-10-13) in Large Language Models
    (LLMs) enable a new era of test-time scaling [\[31,](#page-10-14) [32\]](#page-10-15)
    and human-like, stepwise reasoning. DeepSeek R1 [\[2\]](#page-9-1) used pure RL
    to induce long Chain-of-Thought (CoT) and self-reflection. Following R1, subsequent
    work [\[11,](#page-9-10) [19,](#page-10-2) [28,](#page-10-11) [27,](#page-10-10)
    [25,](#page-10-8) [33\]](#page-10-16) explored RL training variants, mainly on
    smaller models. While R1''s paradigm was replicated [\[11,](#page-9-10) [19,](#page-10-2)
    [28,](#page-10-11) [27,](#page-10-10) [25\]](#page-10-8), exploring more fine-grained
    reward in GRPO is challenging. This work proposes token-level advantage estimation
    for GRPO and its variants.


    Reinforcement Learning. RL is key for sequential decision-making, using policy
    gradient methods. Early methods (e.g., REINFORCE [\[34\]](#page-10-17), DPO [\[35\]](#page-11-0))
    had high variance. TRPO [\[20\]](#page-10-3) and PPO [\[10\]](#page-9-9) improved
    stability with constrained/clipped updates, though PPO is costly. GRPO [\[9\]](#page-9-8)
    removed the Critic using group statistics. GRPO variants, like DAPO [\[11\]](#page-9-10)
    and Dr.GRPO [\[25\]](#page-10-8), built on this. However, GRPO and variants use
    uniform rollout advantage, ignoring token importance in reasoning. We propose
    Key-token Advantage Estimation, linking tokens to correctness statistically, for
    finer granularity. More related work in Appendix [I](#page-17-3)


    ## 6 Conclusion


    This paper introduces KTAE, an algorithm uses statistical analysis to quantify
    each token''s association to correct rollouts. By combining this with GRPO''s
    rollout-level advantage, KTAE computes token-level advantages, thereby providing
    more fine-grained optimization signals and significantly improving training effectiveness.
    It requires no new models, adds minimal computational overhead, and avoids reward
    hacking. KTAE can effectively identify the importance of different tokens in the
    rollout, making the model pay more attention to key tokens in the training process,
    showing excellent test performance utilizing the minimum token budget. Theoretically,
    the core idea of the KTAE can be applied to many other reasoning domains. Therefore,
    KTAE still holds significant potential.


    ## References


    - <span id="page-9-0"></span>[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson,
    Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney,
    et al. Openai o1 system card. *arXiv preprint arXiv:2412.16720*, 2024.

    - <span id="page-9-1"></span>[2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
    Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.
    Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
    *arXiv preprint arXiv:2501.12948*, 2025.

    - <span id="page-9-2"></span>[3] Kimi Team, Angang Du, Bofei Gao, Bowei Xing,
    Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao,
    et al. Kimi k1. 5: Scaling reinforcement learning with llms. *arXiv preprint arXiv:2501.12599*,
    2025.

    - <span id="page-9-3"></span>[4] Qwen Team. Qwq-32b: Embracing the power of reinforcement
    learning, March 2025.

    - <span id="page-9-4"></span>[5] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and
    Thang Luong. Solving olympiad geometry without human demonstrations. *Nature*,
    625(7995):476–482, 2024.

    - <span id="page-9-5"></span>[6] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern,
    Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. *arXiv preprint
    arXiv:2502.03387*, 2025.

    - <span id="page-9-6"></span>[7] Niklas Muennighoff, Zitong Yang, Weijia Shi,
    Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang,
    Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. *arXiv
    preprint arXiv:2501.19393*, 2025.

    - <span id="page-9-7"></span>[8] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz
    Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies
    behaviors learned in pretraining. *arXiv preprint arXiv:2504.07912*, 2025.

    - <span id="page-9-8"></span>[9] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
    Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath:
    Pushing the limits of mathematical reasoning in open language models. *arXiv preprint
    arXiv:2402.03300*, 2024.

    - <span id="page-9-9"></span>[10] John Schulman, Filip Wolski, Prafulla Dhariwal,
    Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*, 2017.

    - <span id="page-9-10"></span>[11] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng
    Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et
    al. Dapo: An open-source llm reinforcement learning system at scale. *arXiv preprint
    arXiv:2503.14476*, 2025.

    - <span id="page-9-11"></span>[12] Hunter Lightman, Vineet Kosaraju, Yuri Burda,
    Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever,
    and Karl Cobbe. Let''s verify step by step. In *The Twelfth International Conference
    on Learning Representations*, 2023.

    - <span id="page-9-12"></span>[13] Guoxin Chen, Minpeng Liao, Chengxi Li, and
    Kai Fan. Step-level value preference optimization for mathematical reasoning,
    2024.

    - <span id="page-9-13"></span>[14] Wei Sun, Qianlong Du, Fuwei Cui, and Jiajun
    Zhang. An efficient and precise training data construction framework for process-supervised
    reward model in mathematical reasoning, 2025.

    - <span id="page-9-14"></span>[15] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu,
    Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify
    and reinforce llms step-by-step without human annotations, 2024.

    - <span id="page-9-15"></span>[16] Lunjun Zhang, Arian Hosseini, Hritik Bansal,
    Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward
    modeling as next-token prediction, 2025.

    - <span id="page-10-0"></span>[17] Leo Gao, John Schulman, and Jacob Hilton. Scaling
    laws for reward model overoptimization, 2022.

    - <span id="page-10-1"></span>[18] John Schulman, Xi Chen, and Pieter Abbeel.
    Equivalence between policy gradients and soft q-learning, 2018.

    - <span id="page-10-2"></span>[19] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang,
    Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach
    to scaling up reinforcement learning on the base model. *arXiv preprint arXiv:2503.24290*,
    2025.

    - <span id="page-10-3"></span>[20] John Schulman, Sergey Levine, Pieter Abbeel,
    Michael Jordan, and Philipp Moritz. Trust region policy optimization. In *International
    conference on machine learning*, pages 1889–1897. PMLR, 2015.

    - <span id="page-10-4"></span>[21] Alan Agresti. A survey of exact inference for
    contingency tables. *Statistical science*, 7(1):131– 153, 1992.

    - <span id="page-10-5"></span>[22] Stephen E Robertson, Steve Walker, Susan Jones,
    Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. *Nist Special
    Publication Sp*, 109:109, 1995.

    - <span id="page-10-6"></span>[23] An Yang, Beichen Zhang, Binyuan Hui, Bofei
    Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang
    Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru
    Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.
    *arXiv preprint arXiv:2409.12122*, 2024.

    - <span id="page-10-7"></span>[24] Quy-Anh Dang and Chris Ngo. Reinforcement learning
    for reasoning in small llms: What works and what doesn''t. *arXiv preprint arXiv:2503.16219*,
    2025.

    - <span id="page-10-8"></span>[25] Zichen Liu, Changyu Chen, Wenjun Li, Penghui
    Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like
    training: A critical perspective. *arXiv preprint arXiv:2503.20783*, 2025.

    - <span id="page-10-9"></span>[26] An Yang, Beichen Zhang, Binyuan Hui, Bofei
    Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang
    Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via
    self-improvement. *arXiv preprint arXiv:2409.12122*, 2024.

    - <span id="page-10-10"></span>[27] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin
    Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et
    al. Process reinforcement through implicit rewards. *arXiv preprint arXiv:2502.01456*,
    2025.

    - <span id="page-10-11"></span>[28] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu,
    Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero
    reinforcement learning for open base models in the wild, 2025.

    - <span id="page-10-12"></span>[29] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah,
    Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar,
    Mojan Javaheripi, Neel Joshi, et al. Phi-4 reasoning technical report. *arXiv
    preprint arXiv:2504.21318*, 2025.

    - <span id="page-10-13"></span>[30] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang,
    Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang,
    Xiuyi Chen, et al. From system 1 to system 2: A survey of reasoning large language
    models. *arXiv preprint arXiv:2502.17419*, 2025.

    - <span id="page-10-14"></span>[31] Bradley Brown, Jordan Juravsky, Ryan Ehrlich,
    Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language
    monkeys: Scaling inference compute with repeated sampling, 2024.

    - <span id="page-10-15"></span>[32] Hritik Bansal, Arian Hosseini, Rishabh Agarwal,
    Vinh Q. Tran, and Mehran Kazemi. Smaller, weaker, yet better: Training llm reasoners
    via compute-optimal sampling, 2024.

    - <span id="page-10-16"></span>[33] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He,
    Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1:
    Curriculum sft, dpo and rl for long cot from scratch and beyond. *arXiv preprint
    arXiv:2503.10460*, 2025.

    - <span id="page-10-17"></span>[34] Ronald J Williams. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning*, 8:229–256,
    1992.

    - <span id="page-11-0"></span>[35] Rafael Rafailov, Archit Sharma, Eric Mitchell,
    Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36:53728–53741, 2023.

    - <span id="page-11-1"></span>[36] Jia LI, Edward Beeching, Lewis Tunstall, Ben
    Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert
    Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample,
    and Stanislas Polu. Numinamath. [\[https://huggingface.co/AI-MO/NuminaMath-CoT\]\(https://github.com/]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf))
    [project-numina/aimo-progress-prize/blob/main/report/numina\\_dataset.pdf\)]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)),
    2024.

    - <span id="page-11-2"></span>[37] Dan Hendrycks, Collin Burns, Saurav Kadavath,
    Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring
    mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*,
    2021.

    - <span id="page-11-3"></span>[38] Aitor Lewkowycz, Anders Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol
    Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with
    language models. *Advances in Neural Information Processing Systems*, 35:3843–3857,
    2022.

    - <span id="page-11-4"></span>[39] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng
    Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye,
    et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent
    ai. *Advances in Neural Information Processing Systems*, 37:19209–19253, 2024.

    - <span id="page-11-5"></span>[40] Guangming Sheng, Chi Zhang, Zilingfeng Ye,
    Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow:
    A flexible and efficient rlhf framework. *arXiv preprint arXiv: 2409.19256*, 2024.

    - <span id="page-11-6"></span>[41] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang,
    Mengdi Zhang, Jian Shao, and Yueting Zhuang. Inftythink: Breaking the length limits
    of long-context reasoning in large language models. *arXiv preprint arXiv:2503.06692*,
    2025.

    - <span id="page-11-7"></span>[42] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
    Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency
    improves chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.

    - <span id="page-11-8"></span>[43] Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan
    Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. Escape sky-high cost: Early-stopping
    self-consistency for multi-step reasoning.

    - <span id="page-11-9"></span>[44] Yuhuai Wu, Elman Mansimov, Roger B Grosse,
    Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning
    using kronecker-factored approximation. *Advances in neural information processing
    systems*, 30, 2017.

    - <span id="page-11-10"></span>[45] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei
    He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang,
    et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms.
    *arXiv preprint arXiv:2412.21187*, 2024.

    - <span id="page-11-11"></span>[46] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng
    He. Chain of draft: Thinking faster by writing less. *arXiv preprint arXiv:2502.18600*,
    2025.

    - <span id="page-11-12"></span>[47] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu
    Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie
    Chen, et al. Stop overthinking: A survey on efficient reasoning for large language
    models. *arXiv preprint arXiv:2503.16419*, 2025.

    - <span id="page-11-13"></span>[48] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng
    Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to
    skip steps? *arXiv preprint arXiv:2411.01855*, 2024.

    - <span id="page-11-14"></span>[49] Tingxu Han, Zhenting Wang, Chunrong Fang,
    Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. *arXiv
    preprint arXiv:2412.18547*, 2024.

    - <span id="page-11-15"></span>[50] Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena
    Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. Overthinking:
    Slowdown attacks on reasoning llms. *arXiv preprint arXiv:2502.02542*, 2025.

    - <span id="page-12-0"></span>[51] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu,
    Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought
    of llms via reinforcement learning. *arXiv preprint arXiv:2504.01296*, 2025.

    - <span id="page-12-1"></span>[52] Pranjal Aggarwal and Sean Welleck. L1: Controlling
    how long a reasoning model thinks with reinforcement learning. *arXiv preprint
    arXiv:2503.04697*, 2025.

    - <span id="page-12-2"></span>[53] Haotian Luo, Li Shen, Haiying He, Yibo Wang,
    Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing
    fine-tuning for o1-like reasoning pruning. *arXiv preprint arXiv:2501.12570*,
    2025.

    - <span id="page-12-3"></span>[54] Yuxiao Qu, Matthew YR Yang, Amrith Setlur,
    Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar.
    Optimizing test-time compute via meta reinforcement fine-tuning. *arXiv preprint
    arXiv:2503.07572*, 2025.

    - <span id="page-12-4"></span>[55] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao.
    Softcot: Soft chain-of-thought for efficient reasoning with llms. *arXiv preprint
    arXiv:2502.12134*, 2025.

    - <span id="page-12-5"></span>[56] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie
    Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length
    in llms. *arXiv preprint arXiv:2502.07266*, 2025.

    - <span id="page-12-6"></span>[57] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
    Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
    Asynchronous methods for deep reinforcement learning. In *International conference
    on machine learning*, pages 1928–1937. PmLR, 2016.

    - <span id="page-12-7"></span>[58] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
    Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing
    atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.

    - <span id="page-12-8"></span>[59] Scott Fujimoto, Herke Hoof, and David Meger.
    Addressing function approximation error in actor-critic methods. In *International
    conference on machine learning*, pages 1587–1596. PMLR, 2018.

    - <span id="page-12-9"></span>[60] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel,
    and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement
    learning with a stochastic actor. In *International conference on machine learning*,
    pages 1861–1870. Pmlr, 2018.

    - <span id="page-12-10"></span>[61] Timothy P Lillicrap, Jonathan J Hunt, Alexander
    Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
    Continuous control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971*,
    2015.

    - <span id="page-12-11"></span>[62] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin,
    Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: A simple, effective, and efficient
    reinforcement learning method for aligning large language models. *arXiv preprint
    arXiv:2310.10505*, 2023.


    ## Appendix


    | A | Limitations                                            | 15 |

    |---|--------------------------------------------------------|----|

    | B | Broader Impacts                                        | 15 |

    | C | Future Work                                            | 15 |

    | D | Calculating factorials using logarithmic space         | 15 |

    | E | Why quantify association direction?                    | 16 |

    | F | A schematic diagram of KTAE                            | 17 |

    | G | Implementation Detials                                 | 17 |

    |   | G.1<br>Dataset and Benchmark                           | 17 |

    |   | G.2<br>Baselines<br>.                                  | 18 |

    |   | G.3<br>Implementation Details and Hyperparameters<br>. | 18 |

    | H | Case Study                                             | 18 |

    | I | More Related Work                                      | 18 |

    |   | I.1<br>Large Reasoning Language Models<br>.            | 18 |

    |   | I.2<br>Reinforcement Learning<br>.                     | 20 |

    | J | Prompt                                                 | 20 |

    | K | Aha Moment                                             | 20 |

    | L | Dataset License                                        | 20 |


    ## <span id="page-14-1"></span>A Limitations


    Building upon GRPO, the KTAE algorithm introduces a more fine-grained token-level
    advantage, which guides the model to focus more on key tokens, thereby demonstrating
    significant performance improvement without introducing additional models. However,
    our experimental validation is primarily focused on models with 1.5B and 7B parameters,
    and the performance of KTAE on largerscale models has not yet been fully verified.
    Furthermore, while theoretically KTAE is applicable to any task beyond based on
    rule-based rewards, this paper only provides in-depth analysis and experimental
    validation on mathematical reasoning tasks, and its potential for application
    in broader domains requires further exploration.


    ## <span id="page-14-2"></span>B Broader Impacts


    Mathematics represents a pinnacle of human wisdom and serves as the foundation
    of many scientific disciplines. Our approach aims to empower large language models
    to tackle complex mathematical reasoning problems, bringing their capabilities
    closer to human expert-level intelligence. By doing so, we seek to advance the
    development of large language models across scientific fields and support human
    efforts in driving scientific progress. A common limitation among current large
    reasoning language models is that while their reasoning capabilities are enhanced,
    they tend to sacrifice some general abilities like summarization, abstracting,
    and translation. Consequently, these models may become specialized models primarily
    focused on the reasoning domain.


    ## <span id="page-14-3"></span>C Future Work


    The experiments presented in this paper primarily focus on tasks based on binary
    discrete rewards, such as in the fields of mathematics and code generation, where
    the reward is simply classified as either correct or incorrect. Future work will
    explore how to extend the core idea of KTAE to scenarios with multiclass discrete
    rewards, and even to those with continuous reward values.


    Furthermore, although KTAE quantifies the importance of each token to provide
    fine-grained optimization signals, the amount of information carried by a single
    token is often insufficient within a complete reasoning path. This implies that,
    even for human experts, it is difficult to determine whether the presence of a
    single token has a decisive impact on the final outcome (correct or incorrect),
    or whether its absence would necessarily lead to failure. Future work will aim
    to address or optimize this problem, for instance.


    ## <span id="page-14-0"></span>D Calculating factorials using logarithmic space


    The Gamma function is defined by the following integral:


    $$\Gamma(z) = \int\_0^\infty t^{z-1} e^{-t} \, dt \quad (\text{Re}(z) > 0).$$


    When z is a positive integer, this integral reduces directly to the factorial
    expression.


    Starting from the integral definition of the Gamma function:


    $$

    \Gamma(z+1) = \int\_0^\infty t^z e^{-t} \, dt.

    $$


    Let u = t z , dv = e <sup>−</sup><sup>t</sup>dt, then du = zt<sup>z</sup>−<sup>1</sup>dt,
    and v = −e −t . Applying integration by parts:


    $$

    \Gamma(z+1) = \left[ -t^z e^{-t} \right]\_0^\infty + z \int\_0^\infty t^{z-1}
    e^{-t} \, dt.

    $$


    As t → ∞, t z e <sup>−</sup><sup>t</sup> → 0; as t → 0, t z e <sup>−</sup><sup>t</sup>
    → 0 (since z > 0). Therefore, the boundary term vanishes, and we obtain:


    $$

    \Gamma(z+1) = z\Gamma(z).

    $$


    This recurrence relation is consistent with the factorial identity n! = n · (n
    − 1)!.


    Fisher''s exact test:


    $$Fisher(o\_{ij}) = \frac{\binom{a\_{o\_{ij}} + b\_{o\_{ij}}}{a\_{o\_{ij}}} \binom{c\_{o\_{ij}}
    + d\_{o\_{ij}}}{c\_{o\_{ij}}}}{\binom{N}{a\_{o\_{ij}} + c\_{o\_{ij}}}}$$


    Of which:


    $$\binom{a\_{o\_{ij}} + b\_{o\_{ij}}}{a\_{o\_{ij}}} = \frac{(a\_{o\_{ij}} + b\_{o\_{ij}})!}{a\_{o\_{ij}}!
    b\_{o\_{ij}}!} = e^{\left(\ln\Gamma(a\_{o\_{ij}} + b\_{o\_{ij}} + 1)\ln\Gamma(a\_{o\_{ij}}
    + 1) - \ln\Gamma(b\_{o\_{ij}} + 1)\right)}$$


    The above process converts factorial operations into addition and subtraction
    of the ln Γ function, which can be efficiently computed in parallel using ''torch.lgamma()''.
    This significantly improves both computational efficiency and numerical precision.


    ## <span id="page-15-0"></span>E Why quantify association direction?


    <span id="page-15-1"></span>


    | Appeared<br>Times        | a | b | Appeared<br>Times        | b | a |

    |--------------------------|---|---|--------------------------|---|---|

    | Not<br>Appeared<br>Times | c | d | Not<br>Appeared<br>Times | d | c |


    Figure 6: Example of contingency table after changing position.


    In the two contingency tables presented above in Figure [6,](#page-15-1) [a, b]
    and [c, d] have been interchanged. It is evident that, for these two tokens p,
    the direction of association with ''obtaining a correct rollout'' becomes completely
    opposite as a result of this interchange. However, it is noteworthy that the association
    metrics calculated by Equations [4,](#page-4-1) [6,](#page-4-2) and [7](#page-4-3)
    remain identical. The proof process is as follows. This indicates that methods
    such as Fisher''s exact test and Information Gain (IG) can only quantify the strength
    of association between the token and ''obtaining a correct rollout'', but fail
    to reveal its direction. Therefore, we propose the use of Equation [9](#page-5-1)
    to accurately quantify the directionality of this association.


    $$\begin{aligned} Fisher(o\_{11}) &= \frac{\binom{a+b}{a}\binom{c+d}{c}}{\binom{N}{a+c}}
    = \frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!N!} \\ Fisher(o\_{12}) &= \frac{\binom{b+a}{b}\binom{d+c}{d}}{\binom{N}{b+d}}
    = \frac{(b+a)!(d+c)!(b+d)!(a+c)!}{b!a!d!c!N!} \\ &= Finsler(o\_{11}) \end{aligned}$$


    $$\begin{split} IG(o\_{11}) &= -\frac{a+c}{N} \log\_2\left(\frac{a+c}{N}\right)
    - \frac{b+d}{N} \log\_2\left(\frac{b+d}{N}\right) \\ &- \left(\frac{a+b}{N}\right)
    \left[ -\frac{a}{a+b} \log\_2\left(\frac{a}{a+b}\right) - \frac{b}{a+b} \log\_2\left(\frac{b}{a+b}\right)
    \right] \\ &+ \left(\frac{c+d}{N}\right) \left[ -\frac{c}{c+d} \log\_2\left(\frac{c}{c+d}\right)
    - \frac{d}{c+d} \log\_2\left(\frac{d}{c+d}\right) \right] \\ &= -\frac{a+c}{N}
    \log\_2\left(\frac{a+c}{N}\right) - \frac{b+d}{N} \log\_2\left(\frac{b+d}{N}\right)
    \\ &+ \frac{a}{N} \log\_2\frac{a}{a+b} + \frac{b}{N} \log\_2\frac{b}{a+b} - \frac{c}{N}
    \log\_2\frac{c}{c+d} - \frac{d}{N} \log\_2\frac{d}{c+d} \end{split}$$


    $$\begin{split} IG(o\_{12}) &= -\frac{b+d}{N} \log\_2\left(\frac{b+d}{N}\right)
    - \frac{a+c}{N} \log\_2\left(\frac{a+c}{N}\right) \\ &\quad - \left(\frac{b+a}{N}\right)
    \left[ -\frac{b}{b+a} \log\_2\left(\frac{b}{b+a}\right) - \frac{a}{b+a} \log\_2\left(\frac{a}{b+a}\right)
    \right] \\ &\quad + \left(\frac{d+c}{N}\right) \left[ -\frac{d}{d+c} \log\_2\left(\frac{d}{d+c}\right)
    - \frac{c}{d+c} \log\_2\left(\frac{c}{d+c}\right) \right] \\ &= -\frac{b+d}{N}
    \log\_2\left(\frac{b+d}{N}\right) - \frac{a+c}{N} \log\_2\left(\frac{a+c}{N}\right)
    \\ &\quad + \frac{b}{N} \log\_2\frac{b}{b+a} + \frac{a}{N} \log\_2\frac{a}{b+a}
    - \frac{d}{N} \log\_2\frac{d}{d+c} - \frac{c}{N} \log\_2\frac{c}{d+c} \\ &= IG(o\_{11})
    \end{split}$$


    ## <span id="page-16-0"></span>F A schematic diagram of KTAE


    <span id="page-16-3"></span>![](_page_16_Figure_2.jpeg)


    Figure 7: An example of comparing KTAE with vanilla GRPO.


    As shown in Figure [7,](#page-16-3) policy model sampled 4 rollouts, among which
    only o3''s final answer was incorrect, while the remaining 3 rollouts all obtained
    correct results. Each square in the figure represents a token. The left side of
    the figure displays the advantage value calculated by GRPO for each token. It
    can be observed that not only are all correct rollouts assigned the same advantage
    value, but within the same correct rollout, every token is also assigned exactly
    the same advantage value. This reflects GRPO''s characteristic of performing evaluation
    at the rollout level. Building upon this, when we superimpose the key-token value
    calculated by KTAE (as shown on the right side of the figure), each token is quantified
    with a different importance score, thus significantly differentiating the contribution
    levels of various tokens within the rollout. This provides finer-grained optimization
    information compared to GRPO.


    ## <span id="page-16-2"></span>G Implementation Detials


    #### <span id="page-16-1"></span>G.1 Dataset and Benchmark


    For the initial validation phase of our method, we first utilized the widely-used
    MATH dataset, specifically the MATH12k[\[12\]](#page-9-11) subset as the training
    set and its corresponding MATH500 as the test set. We conducted experiments on
    the Qwen2.5-Math-1.5B Base model[\[23\]](#page-10-6), which successfully verified
    the effectiveness of our proposed KTAE method. Subsequently, to enable a more
    fair and comprehensive comparison with existing baseline methods, and following
    our initial validation of the method''s effectiveness, we decided to use a more
    challenging subset of the MATH dataset (specifically problems from Levels 3-5)
    as the training set, while still using MATH500 as the test set. Under this setup,
    we trained our model on the larger Qwen2.5-Math-7B Base model[\[23\]](#page-10-6).
    To comprehensively evaluate the mathematical reasoning capabilities of the KTAE-7B
    model, we selected five prominent and widely recognized benchmarks in the field
    of mathematical reasoning for testing: AIME24[\[36\]](#page-11-1), MATH-500[\[37,](#page-11-2)
    [12\]](#page-9-11), AMC[\[36\]](#page-11-1), Minerva[\[38\]](#page-11-3) and OlympiadBench[\[39\]](#page-11-4).


    #### <span id="page-17-1"></span>G.2 Baselines


    In the method validation phase, we aim to comprehensively evaluate the performance
    of our KTAE. To this end, we first compare them against the foundational GRPO[\[9\]](#page-9-8)
    and DAPO[\[11\]](#page-9-10) algorithms to quantify the performance gains introduced
    by our KTAE mechanism. Furthermore, to evaluate the model trained with our KTAE
    against existing reinforcement learning training techniques, we also selected
    the following representative approaches for comparison:1. Simple-RL-Zoo[\[28\]](#page-10-11):
    A baseline method trained on the Qwen2.5-Math-7B base model using the math-level3-5
    dataset, employing the basic GRPO algorithm and a rule-based reward. 2. PRIME-Zero[\[27\]](#page-10-10):
    An online process reward model (PRM) update method, characterized by its ability
    to enable online PRM updates using only policy rollouts and outcome labels through
    implicit process rewards. 3. OpenReasonerZero[\[19\]](#page-10-2): A zero-RL method
    based on the Qwen2.5-7B base model, which centrally applies the vanilla PPO algorithm.
    4. Oat-Zero [\[25\]](#page-10-8): Trained starting from the Qwen2.5Math-7B model
    and utilizing a rulebased reward. It employs an improved Dr.GRPO algorithm, which
    removes the standard deviation in GRPO advantage computation and token-level normalization
    in policy loss computation. These comparison methods encompass applications of
    basic RL algorithms, methods based on process rewards, and improved algorithms
    tailored for specific tasks (such as mathematical reasoning), aiming to evaluate
    the effectiveness and advancement of our methods from multiple perspectives.


    #### <span id="page-17-0"></span>G.3 Implementation Details and Hyperparameters


    Our KTAE-7B model was trained based on the Qwen2.5-Math-7B base model, employing
    a combined approach of DAPO and KTAE. The training utilized the VerL[\[40\]](#page-11-5)
    reinforcement learning framework for optimization. During the training process,
    the model inherited the maximum context length of 4096 from the base model. The
    specific training hyperparameters were configured as follows: The maximum generation
    length was set to 3072, and the maximum prompt length was set to 1024. The sum
    of these two values aligns with the model''s maximum context length. The learning
    rate was fixed at 1e-6. The training batch size was 1024 questions. The number
    of rollouts sampled per question (G) was set to 16. The sampling temperature was
    1.0. For the DAPO method, the clip low redio and clip high redio hyperparameters
    were set to (0.2, 0.28) in 1.5B models and (0.2, 0.24) in 7B models , respectively.
    The overlong buffer length was set to 512, and the length penalty coefficient
    was 1.0. The three hyperparameters h1, h2, and h<sup>3</sup> for the KTAE method
    were all set to 1.0, 2.0, 1.0, k<sup>1</sup> and b in Eq [8](#page-5-0) is set
    to 2.0, 0.5. To ensure reproducibility of the experimental results, all random
    seeds used were set to 0. Furthermore, for method validation or preliminary experiments,
    we conducted additional training on the Qwen2.5-Math-1.5B base model. The hyperparameters
    for this training were largely consistent with the 7B model configuration. All
    our experiments were performed on 8 NVIDIA H100 80G GPUs.


    ## <span id="page-17-2"></span>H Case Study


    Similar scenarios are observed in the other two examples in Figure [8.](#page-18-0)
    In these rollouts, the model also generated portions containing the correct answer,
    but subsequently produced additional or incorrect content, ultimately obscuring
    the correct result and leading to a zero reward. In such cases, the KTAE algorithm
    accurately identifies segments within the rollout that contain the correct answer
    and evaluates them as having a positive contribution, while assessing incorrect
    or distracting segments as having a negative contribution.


    ## <span id="page-17-3"></span>I More Related Work


    ## <span id="page-17-4"></span>I.1 Large Reasoning Language Models


    Recent breakthroughs [\[1,](#page-9-0) [2,](#page-9-1) [3,](#page-9-2) [4\]](#page-9-3)
    in Large Language Models (LLMs) have ushered in a new era of test-time scaling
    [\[31,](#page-10-14) [32,](#page-10-15) [41,](#page-11-6) [42,](#page-11-7) [43\]](#page-11-8),
    enabling models to simulate human-like, stepwise reasoning processes. OpenAI''s
    O1 [\[1\]](#page-9-0) introduces a profound paradigm shift, demonstrating that
    extending the length of each chain can significantly enhance model reasoning performance.
    DeepSeek R1 [\[2\]](#page-9-1) employed pure RL with rule-based reward, guiding
    LLMs toward the spontaneous emergence of long Chain-of-Thought (CoT) and self-reflection
    behaviors. This work established an RL training paradigm starting from a base
    model and open-sources both its training algorithm (GRPO) and model weights.


    <span id="page-18-0"></span>![](_page_18_Figure_0.jpeg)


    Figure 8: An example of visualization of KTAE calculation results.


    Following the success of RL training demonstrated by R1, subsequent efforts [\[11,](#page-9-10)
    [19,](#page-10-2) [28,](#page-10-11) [27,](#page-10-10) [25,](#page-10-8) [44\]](#page-11-9)
    have explored various RL training algorithms, predominantly focusing on the smaller
    Qwen2.5 series models. At the same time, a lot of work has been done to alleviate
    the overthinking [\[45,](#page-11-10) [46,](#page-11-11) [47,](#page-11-12) [48,](#page-11-13)
    [49,](#page-11-14) [50\]](#page-11-15) problem of LRM and the problem of too long
    generation length [\[51,](#page-12-0) [52,](#page-12-1) [53,](#page-12-2) [54,](#page-12-3)
    [55,](#page-12-4) [56\]](#page-12-5). While this line of work successfully replicated
    the RL training paradigm in open-source models, the exploration of more fine-grained
    reward signals in GRPO remains an open challenge. This work introduces a novel
    perspective on token-level advantage estimation to enable the seamless integration
    of GRPO and its variants.


    ## <span id="page-19-2"></span>I.2 Reinforcement Learning


    RL is key for sequential decision-making, using policy gradient methods (On-policy
    [\[20,](#page-10-3) [57,](#page-12-6) [44\]](#page-11-9), Off-policy [\[58,](#page-12-7)
    [59,](#page-12-8) [60,](#page-12-9) [61\]](#page-12-10)). Early methods (e.g.,
    REINFORCE [\[34\]](#page-10-17), DPO [\[35\]](#page-11-0), ReMax [\[62\]](#page-12-11))
    had high variance. Subsequent work focused on stabilization, leading to TRPO [\[20\]](#page-10-3)
    and PPO [\[10\]](#page-9-9), which use constrained updates and clipped objectives,
    respectively. PPO''s reliance on a separate Critic model increases computational
    cost. GRPO [\[9\]](#page-9-8) addressed this by removing the Critic and using
    group-level statistics. GRPO variants, such as DAPO [\[11\]](#page-9-10) (addressing
    scaling and entropy collapse) and Dr. GRPO [\[25\]](#page-10-8) (simplifying GRPO),
    have built upon this. However, GRPO and its variants assign uniform rollout-level
    advantage, overlooking token-specific importance in reasoning. To improve this
    granularity, we propose Key-token Advantage Estimation, utilizing statistical
    methods to quantify the association between individual tokens and rollout correctness.


    ## <span id="page-19-0"></span>J Prompt


    As shown in Figure [9,](#page-19-4) we use the same prompt template (Qwen-Math
    template) for both RL training and validation.


    # <span id="page-19-4"></span> *Prompt templates of RL training and Evalutaion*


    within \\boxed{}.<|im end|>\n<|im start |>user\n{queson}<|im end|>\n<|im


    start|>assistant\n


    Figure 9: Prompt template in RL training and validation.


    # <span id="page-19-1"></span>K Aha Moment


    <|im start|>system\nPlease reason step by step, and put your final answer As shown
    in Figure 10, the model''s output "we need to find the other solution." and solve
    the question successfully.It demonstrates a phenomenon similar to the ''Aha moment''
    mentioned in the Deepseek-R1 paper. However, GRPO did not show the ''aha moment''
    phenomenon and answered the question incorrectly. indicating that KTAE has developed
    a certain degree of self-reflection and error correction capabilities.


    # <span id="page-19-3"></span>L Dataset License


    Our training data is based on the MATH dataset [37], and we evaluate our model
    on AIME24 [\[36\]](#page-11-1), MATH-500 [37, 12], AMC [36], Minerva [38], and
    OlympiadBench [39]. We strictly adhere to the licenses associated with each dataset.


    <span id="page-20-0"></span>![](_page_20_Figure_0.jpeg)


    Figure 10: An example of ''Aha moment''.'
- title: Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce
  abstract: 'Model-based reinforcement learning (MBRL) offers an intuitive way to
    increase

    the sample efficiency of model-free RL methods by simultaneously training a

    world model that learns to predict the future. MBRL methods have progressed by

    largely prioritising the actor; optimising the world model learning has been

    neglected meanwhile. Improving the fidelity of the world model and reducing its

    time to convergence can yield significant downstream benefits, one of which is

    improving the ensuing performance of any actor it may train. We propose a novel

    approach that anticipates and actively seeks out high-entropy states using

    short-horizon latent predictions generated by the world model, offering a

    principled alternative to traditional curiosity-driven methods that chase

    once-novel states well after they were stumbled into. While many model

    predictive control (MPC) based methods offer similar alternatives, they

    typically lack commitment, synthesising multi step plans after every step. To

    mitigate this, we present a hierarchical planner that dynamically decides when

    to replan, planning horizon length, and the weighting between reward and

    entropy. While our method can theoretically be applied to any model that trains

    its own actors with solely model generated data, we have applied it to just

    Dreamer as a proof of concept. Our method finishes the Miniworld procedurally

    generated mazes 50% faster than base Dreamer at convergence and the policy

    trained in imagination converges in only 60% of the environment steps that base

    Dreamer needs.'
  url: http://arxiv.org/abs/2505.16787v1
  keywords: ''
  document: "# Gaze Into the Abyss - Planning to Seek Entropy When Reward is Scarce\n\
    \nAshish Sundar Department of Computer Science University of Exeter as1748@exeter.ac.uk\n\
    \nChunbo Luo Department of Computer Science University of Exeter c.luo@exeter.ac.uk\n\
    \nXiaoyang Wang Department of Computer Science University of Exeter x.wang7@exeter.ac.uk\n\
    \n### Abstract\n\nModel-based reinforcement learning (MBRL) offers an intuitive\
    \ way to increase the sample efficiency of model-free RL methods by simultaneously\
    \ training a world model that learns to predict the future. MBRL methods have\
    \ progressed by largely prioritising the actor; optimising the world model's learning\
    \ has been neglected meanwhile. Improving the world model's fidelity and reducing\
    \ its time to convergence can yield significant downstream benefits, one of which\
    \ is improving the ensuing performance of any actor it may train. We propose a\
    \ novel approach that anticipates and actively seeks out high-entropy states using\
    \ the world model's short-horizon latent predictions, offering a principled alternative\
    \ to traditional curiosity-driven methods that chase once-novel states well after\
    \ they were stumbled into. While many model predictive control (MPC) based methods\
    \ offer similar alternatives, they typically lack commitment, synthesising multi\
    \ step plans after every step. To mitigate this, we present a hierarchical planner\
    \ that dynamically decides when to replan, planning horizon length, and the weighting\
    \ between reward and entropy. While our method can theoretically be applied to\
    \ any model that trains its own actors with solely model generated data, we have\
    \ applied it to just Dreamer as a proof of concept. Our method finishes Miniworld's\
    \ procedurally generated mazes 50% faster than base Dreamer at convergence and\
    \ the policy trained in imagination converges in only 60% of the environment steps\
    \ that base Dreamer's policy needs.\n\n### 1 Introduction\n\nIn recent years,\
    \ reinforcement learning (RL) has achieved remarkable success across a variety\
    \ of domains, from mastering Go [\\[Silver et al., 2017a\\]](#page-10-0) and optimising\
    \ climate control [\\[Evans and Gao,](#page-10-1) [2016\\]](#page-10-1) to quadriped\
    \ locomotion [\\[Lee et al., 2020\\]](#page-10-2) and high speed drone racing\
    \ [\\[Kaufmann et al., 2023\\]](#page-10-3). However, these successes often rely\
    \ on dense reward signals and highly structured environments. In real-world applications\
    \ such as autonomous navigation, scientific exploration, and disaster response,\
    \ rewards are sparse, environments are partially observable, and they are stochastic.\
    \ In these conditions, achieving efficient exploration and good sample efficiency\
    \ remain unsolved.\n\nWhile shaping sparse rewards is a common solution, it is\
    \ often impractical; curiosity-driven exploration offers a promising alternative.\
    \ Contemporary high-performing curiosity-driven RL approaches [\\[Pathak et al.,\
    \ 2017,](#page-10-4) [Sekar et al., 2020,](#page-10-5) [Burda et al., 2018,](#page-10-6)\
    \ [Zhelo et al., 2018,](#page-10-7) [Houthooft et al., 2016,](#page-10-8) [Bellemare\
    \ et al., 2016\\]](#page-10-9) largely quantify novelty retrospectively by rewarding\
    \ agents for revisiting novel states that were chanced upon in previous episodes.\
    \ This retrospective approach limits real-time adaptability and is ineffective\
    \ in non-stationary environments, where past novelty estimates quickly become\
    \ obsolete. Anticipatory methods which predict the novelty of future states before\
    \ acting offer greater adaptability but are typically limited to ensemble-based\
    \ uncertainty estimations that fail in stochastic settings.\n\nA promising direction\
    \ to address these issues is model-based reinforcement learning (MBRL), where\
    \ models of the environment (world models [\\[Ha and Schmidhuber, 2018\\]](#page-10-10))\
    \ are concurrently trained to predict future observations. These world models\
    \ can also train policy networks by simulating a virtual environment in conjunction\
    \ with the policy network's actions. Beyond improving sample efficiency and ensuring\
    \ the actors always have access to on-policy data, world models inherently capture\
    \ environmental dynamics and can be leveraged to estimate both reward uncertainty\
    \ and transition uncertainty. Padding out the reward with reward uncertainty densifies\
    \ a sparse reward signal, while seeking out transition uncertainty as an additional\
    \ reward boosts training speed and stability. While world models are often treated\
    \ as tools for facilitating policy learning, we adopt the antithesis: policies\
    \ serve as mechanisms to accelerate and optimise world model training. By prioritizing\
    \ the model's epistemic gain during exploration, we ensure that policies operate\
    \ not just to maximize reward but to actively improve the model's understanding\
    \ of its environment. This reframing enables the world model to generalize more\
    \ effectively and support the training of robust downstream actors.\n\nPrior work\
    \ has also attempted to leverage model-based exploration but these approaches\
    \ suffer from key limitations. Look before you leap [\\[Wang et al., 2018\\]](#page-10-11)\
    \ encourages agents to seek lowentropy, high-reward states which is effective\
    \ only after training has converged. During training, this behavior leads to avoidance\
    \ of uncertain or novel states, stalling learning and ultimately causing policy\
    \ degradation. MaxEnt-Dreamer [\\[Svidchenko and Shpilman, 2021\\]](#page-10-12)\
    \ instead biases exploration toward high-entropy rollouts, but lacks any mechanism\
    \ for plan commitment. The agent replans at every step and frequently abandons\
    \ promising trajectories, undermining long-term exploration and reducing computational\
    \ efficiency. RAIF [\\[Nguyen et al., 2024\\]](#page-10-13) introduces a more\
    \ nuanced objective: to maximize posterior uncertainty while minimizing prior\
    \ uncertainty. However, this forces the agent to evaluate potential gains retrospectively,\
    \ resulting in its inability to react to novel states as they appear.\n\nIn this\
    \ work, we propose to augment Dreamer [\\[Hafner et al., 2020\\]](#page-10-14),\
    \ a prominent and efficient world model, with a planner to anticipate and seek\
    \ informative states that drive structured exploration. We also introduce a lightweight\
    \ PPO based hierarchical planner that uses model rollouts to dynamically select\
    \ between high-entropy and high-reward trajectories and dynamically decide when\
    \ to replan. While our experiments focus on Dreamer, our method is model-agnostic\
    \ and can be applied to any MBRL framework that trains policies entirely through\
    \ imagined trajectories. Although we do not explicitly combine our approach with\
    \ alternative models in this work, the mathematical formulation of our method\
    \ places no restrictions that would prevent doing so. However, we note that an\
    \ inherent limitation of our method is the reliance upon a world model that can\
    \ predict many states ahead with relatively low error rates. Further, we also\
    \ require that the actor be trained purely with world model generated states rather\
    \ than through experience replay, as we will bias collection of experiences towards\
    \ effective training of the world model, leading to a distributional shift in\
    \ the data relative to the actor's policy.\n\nOur contributions are:\n\n- Leveraging\
    \ world models' transition uncertainty to predicting short-horizon state entropy\
    \ and densify sparse rewards, accelerating training in MBRL.\n- Reformulating\
    \ the Dreamer KL minimization into an adversarial minmax objective to maximise\
    \ information gain, enabling intelligent entropy driven exploration to directly\
    \ optimise world model learning.\n- Introducing a reactive hierarchical planner\
    \ that dynamically selects between optimising entropy and reward while retaining\
    \ the flexibility to discard the current plan and replan a new trajectory.\n\n\
    The rest of the paper is structured as follows. In Section [2,](#page-2-0) we\
    \ review related work in intrinsic motivation, planning, and hierarchical RL.\
    \ In Section [3,](#page-3-0) we provide background on Dreamer and\n\nits training\
    \ formulation. Section [4](#page-4-0) introduces our entropy-seeking planner and\
    \ reactive hierarchical policy. Section [5](#page-6-0) details our experimental\
    \ setup, evaluates performance on a procedurally generated 3D maze environment.\
    \ Section [6](#page-8-0) concludes this work and outlines limitations and subsequent\
    \ directions for future work.\n\n# <span id=\"page-2-0\"></span>2 Related Work\n\
    \n### 2.1 Intrinsic Reward\n\nExisting approaches to intrinsic reward can be separated\
    \ into retrospective methods and anticipatory methods. Retrospective methods evaluate\
    \ experiences after they have occurred and assign intrinsic reward based on metrics\
    \ like prediction error [\\[Pathak et al., 2017\\]](#page-10-4), state novelty\
    \ [\\[Burda et al.,](#page-10-6) [2018\\]](#page-10-6), episodic novelty [\\[Badia\
    \ et al., 2020\\]](#page-10-15), or representation surprise [\\[Raileanu and Rocktäschel,](#page-10-16)\
    \ [2020\\]](#page-10-16). These methods encourage agents to revisit novel states\
    \ that were randomly discovered in past trajectories, and are typically implemented\
    \ using model-free algorithms such as PPO, IMPALA, and A3C.\n\nA known challenge\
    \ for retrospective curiosity is the white noise problem [\\[Burda et al., 2018\\\
    ]](#page-10-6), where agents are drawn to aleatoric uncertainty, stochastic but\
    \ uninformative features, that yield consistently high prediction error. Since\
    \ these rewards do not diminish with repeated exposure, agents may \"farm\" them,\
    \ especially in sparse-reward settings. Another issue, detachment [\\[Ecoffet\
    \ et al., 2019\\]](#page-10-17), occurs when agents fail to return to previously\
    \ discovered novel states as only the state is rewarded rather than the entire\
    \ process utilised to get to that state.\n\nAnticipatory methods instead attempt\
    \ to steer agents toward potentially novel states before they have been visited\
    \ by utilising short horizon predictions of epistemic uncertainty [\\[Shyam et\
    \ al., 2019,](#page-10-18) [Sekar](#page-10-5) [et al., 2020,](#page-10-5) [Chua\
    \ et al., 2018\\]](#page-10-19). When combined with reward conditioning, they\
    \ can better avoid aleatoric traps. However, these methods often require multi-step\
    \ planning with dedicated predictive models, introducing significant architectural\
    \ complexity and additional compute requirements. Overall, retrospective approaches\
    \ are more popular due to their simplicity, lower implementation cost, and compatibility\
    \ with standard model-free reinforcement learning algorithms—despite their known\
    \ limitations.\n\n### 2.2 Planning\n\nWhile most RL agents act greedily without\
    \ foresight, some of the most successful decision making AI systems explicitly\
    \ model future outcomes. For example, Monte Carlo Tree Search (MCTS) [\\[Coulom,](#page-11-0)\
    \ [2006\\]](#page-11-0) played a key role in AlphaGo [\\[Silver et al., 2016\\\
    ]](#page-11-1) and AlphaZero [\\[Silver et al., 2017b\\]](#page-11-2), but MCTS\
    \ relies on full observability and discrete action spaces, limiting its applicability\
    \ to real-world stochastic settings.\n\nPath integral control offers an alternative\
    \ framework for planning under uncertainty by sampling and reweighting trajectories\
    \ [\\[Gómez et al., 2014,](#page-11-3) [Williams et al., 2015\\]](#page-11-4),\
    \ but classical implementations are computationally expensive. Neural network\
    \ dynamics models alleviate this cost at the expense of precision and model calibration.\
    \ TD-MPC (Temporal Difference - Model Predictive Control) and TD-MPC2 [\\[Hansen\
    \ et al., 2022,](#page-11-5) [2023\\]](#page-11-6) apply Model Predictive Path\
    \ Integral (MPPI) style planning with Temporal Difference (TD) learning based\
    \ policy optimization. However, their planners drift the actions away from the\
    \ policy network, leading to distributional shift in the data and ensuing value\
    \ overestimation.\n\n# 2.3 Hierarchical Policies\n\nHierarchical reinforcement\
    \ learning (HRL) provides a structured approach for addressing longhorizon credit\
    \ assignment and decision-making by decomposing behavior into high-level and lowlevel\
    \ control operating at different temporal scales. The low-level policy selects\
    \ stepwise actions, while the high-level policy determines abstract actions or\
    \ goals over extended horizons. A prominent approach in this field is Option-Critic\
    \ (OC) [\\[Bacon et al., 2017\\]](#page-11-7), where low-level actions are composed\
    \ into options and executed. The critic is overloaded to determine when the actions\
    \ executed as part of an option begin to return rewards lower than the baseline,\
    \ at which point the option is terminated. HiPPO [\\[Li et al., 2019\\]](#page-11-8)\
    \ introduces a simpler alternative by running PPO at two predefined temporal\n\
    \nresolutions. However, this comes at the cost of flexibility, as it imposes fixed\
    \ planning intervals and prohibits dynamic replanning or early termination.\n\n\
    ### <span id=\"page-3-0\"></span>3 Preliminaries\n\nDreamer [\\[Hafner et al.,\
    \ 2019,](#page-11-9) [2020,](#page-10-14) [2023\\]](#page-11-10) learns a compact\
    \ latent dynamics model and performs policy optimization entirely in latent space.\
    \ At its core, Dreamer relies on a *Recurrent State Space Model* (RSSM) that factorizes\
    \ the environment into deterministic recurrent states and stochastic latent representations.\
    \ The RSSM is described by a series of formulations:\n\n> Recurrent model: h<sup>t</sup>\
    \ = fϕ(ht−1, zt−1, at−1) Transition predictor (prior): zˆ<sup>t</sup> ∼ pϕ(ˆz<sup>t</sup>\
    \ | ht) Representation model (posterior): z<sup>t</sup> ∼ qϕ(z<sup>t</sup> | ht,\
    \ xt) Predictors (Image, Reward, Discount): xˆt, rˆt, γˆ<sup>t</sup> ∼ pϕ(· |\
    \ ht, zt)\n\nHere, x<sup>t</sup> is the observation at time t, and h<sup>t</sup>\
    \ is the deterministic recurrent state. In the original Dreamer formulation, the\
    \ autoencoder loss (an auxiliary loss) is given by a logarithmic loss that is\
    \ applied through the image predictor as it reconstructs the image. In our implementation,\
    \ we change the loss function to a combination of mean squared error (MSE) and\
    \ a environment-specific reconstruction loss to accelerate convergence and improve\
    \ early-stage feature alignment. This modification accelerates convergence and\
    \ improves early-stage feature alignment by encouraging more stable and informative\
    \ representations, facilitating effective downstream training from the latent\
    \ space. Further details are provided in Appendix [A.](#page-12-0)\n\nThe RSSM\
    \ models the sequence of latent states z<sup>t</sup> through a combination of\
    \ different components, as detailed below. At each step, Dreamer generates a prior\
    \ latent state zˆ<sup>t</sup> from the deterministic recurrent state h<sup>t</sup>\
    \ via p(ˆz<sup>t</sup> | ht), and updates it into a posterior q(z<sup>t</sup>\
    \ | ht, xt) once the new observation x<sup>t</sup> has been received. The KL divergence\
    \ between the prior and posterior is minimized to train the model:\n\n<span id=\"\
    page-3-1\"></span>\n$$\\mathcal{L}\\_{\\text{KL}} = \\text{KL}\\left[q(z\\_t \\\
    mid h\\_t, x\\_t) \\parallel p(\\hat{z}\\_t \\mid h\\_t)\\right] \\tag{1}$$\n\n\
    Dreamer's policy network is trained using imagined trajectories generated by the\
    \ world model specifically the prior predictor. Training batches are constructed\
    \ by sampling random latent states from the replay buffer and rolling them out\
    \ using the current policy and the prior predictor. This ensures that policy training\
    \ remains effectively on-policy, regardless of the distribution of the sampled\
    \ states. Aside from stabilizing techniques such as Exponential Moving Average\
    \ (EMA), policy optimization in Dreamer largely relies on standard policy gradient\
    \ methods. For completeness, the policy loss is provided below.\n\n$$\\text{Actor:}\
    \ \\quad a\\_t \\sim \\pi\\_\\theta(a\\_t \\mid \\hat{z}\\_t) \\qquad \\text{Cristic:}\
    \ \\quad v\\_\\psi(R\\_t \\mid \\hat{z}\\_t) \\tag{2}$$\n\nThe loss of the critic\
    \ is given as:\n\n$$\\mathcal{L}(\\psi) = -\\sum\\_{t=1}^{T} \\ln p\\_{\\psi}(R\\\
    _t^{\\lambda} \\mid z\\_t) \\tag{3}$$\n\nwhere R<sup>λ</sup> t is the bootstrapped\
    \ return. The loss of the actor is given as:\n\n$$\\mathcal{L}(\\theta) \\sim\
    \ -\\sum\\_{t=1}^{T} \\left( \\frac{R\\_t^\\lambda - v\\_\\psi(z\\_t)}{S} \\right)\
    \ \\log \\pi\\_\\theta(a\\_t \\mid z\\_t) + \\eta \\,\\mathcal{H}\\left(\\pi\\\
    _\\theta(\\cdot \\mid z\\_t)\\right) \\tag{4}$$\n\nwhere S is:\n\n$$S \\sim \\\
    left( \\max(R\\_t^\\lambda) - \\min(R\\_t^\\lambda) \\right),$$\n\nWhile it is\
    \ clear that the policy network's training schema has been optimised, the buffer\
    \ that the world model trains from is populated by a naive ϵ-greedy actor. We\
    \ show in this work that filling the buffer with high entropy transitions can\
    \ lead to greater training efficiency and optimised exploration.\n\n### <span\
    \ id=\"page-4-0\"></span>4 Method\n\n#### 4.1 Entropy\n\nThe latent states in\
    \ Dreamer's world model are modelled as stochastic latent variables drawn from\
    \ a Gaussian distribution:\n\n$$\n\\hat{z}\\_t \\sim \\mathcal{N}(\\mu\\_t, \\\
    Sigma\\_t) \\tag{5}\n$$\n\nWe assume a diagonal Gaussian for tractability, so\
    \ the covariance matrix Σ<sup>t</sup> reduces to a vector of variances σ 2 t .\
    \ We use σp,i to refer to the standard deviation along dimension i of the prior.\n\
    \nTraining the model involves minimizing a KL divergence loss between the prior\
    \ and posterior distributions (Equation [\\(1\\)](#page-3-1)); the same KL divergence\
    \ loss can also be interpreted as the model's information gain (IG) about the\
    \ environment at timestep t [\\[Quinlan, 1986\\]](#page-11-11)\n\n$$\\text{IG}\
    \ = \\frac{1}{2} \\left( k \\frac{\\sigma\\_q^2}{\\sigma\\_p^2} + \\frac{||\\\
    mu\\_p - \\mu\\_q||^2}{\\sigma\\_p^2} - k + k \\log \\left( \\frac{\\sigma\\_p^2}{\\\
    sigma\\_q^2} \\right) \\right) \\tag{6}$$\n\nWhen the model has trained long enough\
    \ to become reasonably calibrated, the model should set σ<sup>p</sup> and µ<sup>p</sup>\
    \ such that any rise in µ<sup>p</sup> − µ<sup>q</sup> should be counterbalanced\
    \ by a proportional increase in σp. Conversely, µ<sup>p</sup> − µ<sup>q</sup>\
    \ ≪ 1 where σ<sup>p</sup> is low.\n\nOver time, the model naturally encourages\
    \ σ<sup>p</sup> to approach σ<sup>q</sup> from above but not fall below σq. This\
    \ behavior emerges because the magnitude of the gradient of information gain (or\
    \ KL divergence loss) with respect to σp, d IG d σ<sup>p</sup> \f, is greater\
    \ when <sup>σ</sup><sup>p</sup> < σ<sup>q</sup> than when <sup>σ</sup><sup>p</sup>\
    \ <sup>≥</sup> <sup>σ</sup>q. As a result, the model is more strongly driven to\
    \ increase σ<sup>p</sup> when it overestimates certainty, leading to a self-correcting\
    \ mechanism that helps maintain calibrated predictions and guards against overconfidence.\n\
    \nAs a consequence, the most reliable way to increase information gain is by increasing\
    \ σp, or where the model admits uncertainty. These are the places where learning\
    \ is most likely to happen.\n\nThe entropy of the state's distribution therefore\
    \ can be taken as an estimate for the model's uncertainty, as:\n\n$$H(\\sigma\\\
    _p) = \\frac{k}{2}\\log(2\\pi e) + \\sum\\_{i=1}^{k} \\log(\\sigma\\_{p,i})\\\
    tag{7}$$\n\nwhere k is the dimensionality of the state. Thus, the objective becomes\
    \ to maximise prior entropy:\n\n$$\\mathcal{J} = \\max H(\\sigma\\_p) \\tag{8}$$\n\
    \nThere are two primary failure modes for entropy-based exploration. The first\
    \ arises in environments with high aleatoric uncertainty, where a state has multiple\
    \ plausible successors due to stochasticity inherent to the environment. In these\
    \ cases, even a well-understood state s<sup>t</sup> may produce a high-entropy\
    \ predictive distribution despite all potential outcomes being familiar. Suppose\
    \ the environment's true transition function satisfies:\n\n$$p(s\\_{t+1} \\mid\
    \ s\\_t, a\\_t) > 0 \\quad \\text{for multiple distinct } s\\_{t+1}$$\n\nThe support\
    \ of the distribution is therefore given by:\n\n$$\\text{Support}\\left(p(s\\\
    _{t+1}\\mid s\\_t, a\\_t)\\right) = \\left\\{s\\_{t+1}^{(1)}, s\\_{t+1}^{(2)},\
    \ \\dots, s\\_{t+1}^{(n)}\\right\\}$$\n\nAn ideal model would represent each successor\
    \ with a low-variance Gaussian, forming a weighted mixture over plausible outcomes:\n\
    \n$$p^\\*(s\\_{t+1} \\mid s\\_t, a\\_t) = \\sum\\_{i=1}^n w\\_i \\cdot \\mathcal{N}(s\\\
    _{t+1} \\mid \\mu\\_i, \\sigma\\_i^2) \\tag{9}$$\n\nHere, each w<sup>i</sup> represents\
    \ the likelihood of observing the i-th successor state, satisfying P<sup>n</sup>\
    \ <sup>i</sup>=1 w<sup>i</sup> = 1. However, due to its unimodal predictive structure,\
    \ the prior predictor must approximate a multi-modal distribution — comprised\
    \ of several low-variance components — using a single Gaussian with inflated variance:\n\
    \n> pˆ(st+1 | st, at, h0:t) ∼ N (µ, σ<sup>2</sup> ), where σ <sup>2</sup> > σ<sup>2</sup>\
    \ i for all i\n\nHere h0:<sup>t</sup> denotes the history of recurrent hidden\
    \ states up to time t.\n\nIn this case, elevated entropy arises from aleatoric\
    \ uncertainty rather than epistemic uncertainty. To avoid over-exploration in\
    \ these regions, we condition planning not solely on entropy but jointly on both\
    \ entropy and predicted reward. The relative weighting of these objectives is\
    \ state- and contextdependent, varying across the episode. To balance this trade-off\
    \ effectively, we propose a hierarchical meta-policy that dynamically modulates\
    \ the emphasis on reward maximization versus entropy during trajectory planning,\
    \ as the optimal weighting may change through an episode.\n\nThe second failure\
    \ mode arises in environments with latent transitions that require specific, rarely\
    \ executed actions. In such cases, the ideal transition distribution remains:\n\
    \n$$p^\\*(s\\_{t+1} \\mid s\\_t, a\\_t) = \\sum\\_{i=1}^n w\\_i \\cdot \\mathcal{N}(s\\\
    _{t+1} \\mid \\mu\\_i, \\sigma\\_i^2) \\tag{10}$$\n\nHere, each mode corresponds\
    \ to a distinct possible outcome, with weights w<sup>i</sup> representing their\
    \ respective likelihoods. The weight associated with the common transitions, denoted\
    \ w<sup>C</sup> , satisfies w<sup>C</sup> ≫ w̸<sup>C</sup> , where w̸<sup>C</sup>\
    \ is the total weight of the rare transitions. If the agent has only encountered\
    \ the high-probability transitions, the learned model will be ignorant of the\
    \ rare outcomes and estimate:\n\n$$\n\\hat{p}(s\\_{t+1} \\mid s\\_t, h\\_{0:t})\
    \ \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad \\text{where } \\sigma^2 \\ll 1\n\
    $$\n\nIn these cases, a state's uncertainty may be chronically underestimated\
    \ and subsequently underexplored. This form of hidden epistemic uncertainty cannot\
    \ be resolved by naïvely increasing entropy-seeking behavior globally or by enabling\
    \ random exploration at all states. To mitigate such a problem, external semantic\
    \ hints are required. While this is outside the scope of the present method, we\
    \ hypothesize that using large language models (LLMs) to aid the SSM could enable\
    \ the world model to incorporate such information through question-answering or\
    \ tool use.\n\n#### 4.2 Reactive hierarchical planner\n\nBuilding on Dreamer's\
    \ ability to roll out imagined trajectories, we introduce a planning mechanism\
    \ that prioritizes high-entropy, high-reward trajectories to accelerate learning.\
    \ At each interaction step, the agent rolls out N candidate trajectories (256\
    \ in our work) using the underlying greedy policy. Each trajectory is scored based\
    \ on the entropy E<sup>t</sup> of predicted states and the expected reward Rt.\
    \ We define a combined objective, termed *endorphin*, D, as:\n\n$$D = \\frac{1}{H}\
    \ \\sum\\_{t}^{H+t-1} w\\_e E\\_t + (1 - w\\_e) R\\_t \\tag{11}$$\n\nwhere w<sup>e</sup>\
    \ ∈ [0, 1] controls the trade-off between exploration and exploitation over a\
    \ planning horizon H.\n\nThe trajectory with the highest endorphin is selected.\
    \ To decide whether to enact a plan, we introduce a stochastic hierarchical controller.\
    \ A meta-policy outputs a discrete planning probability p<sup>t</sup> ∈ {0.00,\
    \ 0.25, 0.50, 0.75, 1.00}, and planning (or replanning) occurs if a uniformly\
    \ sampled u<sup>t</sup> ∼ U(0, 1) satisfies u<sup>t</sup> < pt. This stochastic\
    \ mechanism introduces controlled noise into the planning process, similar to\
    \ noising actions. Should the model choose the binary values of 1 or 0, planning\
    \ becomes deterministic, allowing the meta policy to bypass the planning noise.\
    \ The intermediate probabilities are discretized to balance expressive control\
    \ with practical optimization. Introducing too many discrete decisions into an\
    \ RL policy — particularly at the meta level — can degrade sample efficiency and\
    \ destabilize training. Note that we do not include a cancel action as commonly\
    \ done [\\[Bacon et al., 2017\\]](#page-11-7), but instead the planner can choose\
    \ to replace the current plan with a new one.\n\nWe find in practice that the\
    \ planner tends to excessively replan, a finding echoed across the hierarchical\
    \ RL literature [\\[Klissarov et al., 2017\\]](#page-11-12) [\\[Chunduru and Precup,\
    \ 2022\\]](#page-11-13) [\\[Johnson and Weitzenfeld, 2025\\]](#page-11-14), so\
    \ we change the enaction expression from u<sup>t</sup> < p<sup>t</sup> to u<sup>t</sup>\
    \ < p<sup>2</sup> t to discourage excessive replanning.\n\nThe hierarchical controller\
    \ comprises two meta-policies:\n\n- Meta 1 outputs the planning horizon H and\
    \ entropy-reward weighting we.\n- Meta 2 outputs the planning probability pt.\n\
    \nImportantly, planning decisions are re-evaluated at every environment step,\
    \ allowing for flexible replanning without commitment if needed. Pseudocode for\
    \ our planning algorithm is given in Appendix [B.](#page-13-0)\n\nTo enable effective\
    \ training of the meta-policy that governs planning frequency, it is essential\
    \ that the agent is exposed to a sufficient diversity of planning behaviours early\
    \ in learning. In particular, if the planning probability distribution collapses\
    \ prematurely toward the extremes (i.e., always plan or never plan), the agent\
    \ receives an impoverished set of trajectory candidates, preventing the meta-policy\
    \ from learning when planning is actually beneficial. To mitigate this, we initialize\
    \ the planning probability network using He initialization [\\[He et al., 2015\\\
    ]](#page-11-15), followed by reordering the biases such that intermediate planning\
    \ probabilities are more likely when compared to the more extreme values.\n\n\
    # <span id=\"page-6-0\"></span>5 Experiments\n\nWe evaluate our approach in a\
    \ procedurally generated 3D environment designed to emulate the partial observability\
    \ and sparse rewards characteristic of real-world scenarios. The experiments aim\
    \ to assess whether anticipatory planning and entropy-driven behavior improve\
    \ exploration efficiency and downstream task performance.\n\n#### 5.1 Maze Explorer\n\
    \nWe use a 3D maze environment adapted from MiniWorld [\\[Chevalier-Boisvert et\
    \ al., 2023\\]](#page-11-16), where each episode presents a new random layout.\
    \ The agent receives RGB image observations and performs continuous actions to\
    \ locate three goal boxes. We introduce a *porosity* parameter that controls wall\
    \ density to vary exploration difficulty. Observations are augmented with a binary\
    \ spatial map that encodes visited regions and current orientation, providing\
    \ a simple episodic memory. The reward function combines exploration, proximity,\
    \ and goal rewards to encourage both coverage and task success. Full environment\
    \ details are in Appendix [C.](#page-13-1)\n\n# 5.2 Evaluation Methodology\n\n\
    Each agent is trained for 350,000 steps, a budget sufficient for Dreamer to converge.\
    \ Owing to the procedural nature of the environment and the use of online learning,\
    \ we evaluate generalization through training performance across diverse instantiations\
    \ rather than a fixed test set.\n\nWe compare our method against:\n\n- Dreamer:\
    \ A strong model-based baseline.\n- PPO: A widely used model-free method, given\
    \ 256 extra on-policy episodes for fair comparison.\n\nModel-free agents like\
    \ PPO are known to struggle in MiniWorld's mazes due to their limited memory and\
    \ planning capabilities [\\[Zha et al., 2021\\]](#page-11-17). Full PPO implementation\
    \ details are provided in Appendix [D.](#page-14-0)\n\nWe report:\n\nEpisode Length:\
    \ Steps required to complete the task (4096 if incomplete).\n\n- KL Divergence:\
    \ KL between posterior and prior at the end of each episode, reflecting information\
    \ gain.\n- Prior Entropy: Average entropy of the prior over sampled latent states,\
    \ indicating predictive uncertainty.\n\nAll curves use a rolling average and standard\
    \ deviation (window = 10% of data points).\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\
    \nFigure 1: Episode lengths across different porosity levels. Lower porosity increases\
    \ maze difficulty.\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_2.jpeg)\n\
    \nFigure 2: Prior entropies through different porosities, reflecting the model's\
    \ estimated uncertainty.\n\n#### 5.3 Experiments - Task Difficulty\n\nVarying\
    \ the porosity parameter controls maze difficulty. Visual examples of mazes at\
    \ different porosity levels are provided in Appendix [F.](#page-16-0) Figure [1](#page-7-0)\
    \ shows that our method maintains low episode lengths even in denser mazes, outperforming\
    \ both Dreamer and PPO. PPO fails across all settings, likely due to its lack\
    \ of memory and long-horizon reasoning. Dreamer's performance degrades under low\
    \ porosity, where rewards are harder to reach, while our method shows robust and\
    \ consistent performance. Our method also exhibits lower variance, suggesting\
    \ more consistent behavior across seeds.\n\nUnder the most difficult condition\
    \ (porosity = 0), where only a single path exists between the agent and three\
    \ goals, both our method and Dreamer perform worse. However, our approach still\
    \ outperforms Dreamer, achieving 20% shorter episode lengths on average, albeit\
    \ with higher variance due to the increased exploration burden.\n\nFigure [2](#page-7-1)\
    \ shows that our method maintains approximately 10% higher prior entropy across\
    \ training, indicating greater predictive uncertainty of the world model and a\
    \ broader exploration strategy.\n\nFigure [3](#page-8-1) shows KL divergence values\
    \ between the prior and posterior. Higher KL indicates that the world model is\
    \ encountering states it cannot yet predict, which can reflect either model failure\
    \ or valuable learning. Given that our method leads to better downstream performance,\
    \ we interpret this as a sign of active, informative learning. KL divergence here\
    \ functions as a min-max information gain signal: the agent seeks states that\
    \ challenge the model, thereby improving imagination-based training while increasing\
    \ KL divergence.\n\n#### 5.4 Ablation Study\n\nTo isolate the contributions of\
    \ individual components, we compare:\n\n<span id=\"page-8-1\"></span>![](_page_8_Figure_0.jpeg)\n\
    \nFigure 3: KL divergence across porosity levels. Higher porosity results in a\
    \ more marked difference between our method and base Dreamer.\n\n<span id=\"page-8-2\"\
    ></span>![](_page_8_Figure_2.jpeg)\n\n(a) Episode lengths during training for\
    \ different ablations. The full planner outperforms both the base Dreamer and\
    \ MPC-style variants, highlighting the benefit of plan commitment.\n\n(b) Ratio\
    \ of entropy to reward in D, as set by meta 1. A high value of 1 here refers to\
    \ all reward and no entropy and a low value of 0 refers to all entropy and no\
    \ reward. The value balances reward and entropy rather than collapsing to either\
    \ extreme.\n\nFigure 4: Ablation study results. (a) Training performance under\
    \ different planning variants. (b) Evolution of entropy-to-reward ratio during\
    \ training.\n\nBase Dreamer: The standard agent without planning.\n\nMPC Style:\
    \ To evaluate the value of our underlying planner, we modify the Meta 2 to replan\
    \ at every step, mimicking MPC behaviour. Meta 1 remains trainable and flexible.\n\
    \nFull Planner: Our full method where we do not restrict the Meta 2.\n\nWe run\
    \ all variants for 200,000 steps, which is sufficient for each to reach approximately\
    \ 90% of its final performance. This threshold was chosen to reduce training time\
    \ while still capturing meaningful differences in learning dynamics as performance\
    \ gains beyond this point typically occur much more slowly.\n\nThis demonstrates\
    \ the value of both the meta-policies and the planner, particularly the ability\
    \ to commit to plans over extended horizons. Figure [4b](#page-8-2) further shows\
    \ that the meta-policies do not exclusively prioritize reward or entropy. Instead,\
    \ they maintain a dynamic balance, suggesting that both signals contribute meaningfully\
    \ to effective planning, as reflected in PPO's policy gradients over a 15-step\
    \ horizon. Additional outputs from the meta-policies are provided in Appendix\
    \ G.\n\n### <span id=\"page-8-0\"></span>6 Discussion\n\nWe present a robust method\
    \ with fast convergence and a strong theoretical foundation to drive structured\
    \ exploration in MBRL, suggesting it can generalize across domains and model architectures.\
    \ While our experiments focus on a single environment and a single model, both\
    \ represent challenging\n\nbenchmarks — the environment requires sophisticated\
    \ exploration, and Dreamer is a state-of-the-art model. Future directions include\
    \ enhancing the meta-policies with explicit mechanisms for long-term credit assignment.\
    \ Improving this component to support temporally extended reasoning, while preserving\
    \ its reactivity, could further enhance the adaptability and performance of the\
    \ overall system. Extending this approach to more expressive world models, such\
    \ as Transformer State Space Models (TSSMs), and to more diverse domains is another\
    \ promising direction for future work.\n\n### References\n\n- <span id=\"page-10-0\"\
    ></span>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,\
    \ Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\
    \ et al. Mastering the game of go without human knowledge. *nature*, 550(7676):354–359,\
    \ 2017a.\n- <span id=\"page-10-1\"></span>Richard Evans and Jim Gao. Deepmind\
    \ ai reduces google data centre cooling bill by 40%, July 2016. URL [https://deepmind.google/discover/blog/](https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/)\
    \ [deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/](https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/).\
    \ Accessed: 2025-05-12.\n- <span id=\"page-10-2\"></span>Joonho Lee, Jemin Hwangbo,\
    \ Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion\
    \ over challenging terrain. *Science robotics*, 5(47):eabc5986, 2020.\n- <span\
    \ id=\"page-10-3\"></span>Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio,\
    \ Matthias Müller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone\
    \ racing using deep reinforcement learning. *Nature*, 620(7976):982–987, 2023.\n\
    - <span id=\"page-10-4\"></span>Deepak Pathak, Pulkit Agrawal, Alexei A Efros,\
    \ and Trevor Darrell. Curiosity-driven exploration by selfsupervised prediction.\
    \ In *International conference on machine learning*, pages 2778–2787. PMLR, 2017.\n\
    - <span id=\"page-10-5\"></span>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis,\
    \ Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised\
    \ world models. In *International conference on machine learning*, pages 8583–8592.\
    \ PMLR, 2020.\n- <span id=\"page-10-6\"></span>Yuri Burda, Harrison Edwards, Amos\
    \ Storkey, and Oleg Klimov. Exploration by random network distillation. *arXiv\
    \ preprint arXiv:1810.12894*, 2018.\n- <span id=\"page-10-7\"></span>Oleksii Zhelo,\
    \ Jingwei Zhang, Lei Tai, Ming Liu, and Wolfram Burgard. Curiosity-driven exploration\
    \ for mapless navigation with deep reinforcement learning. *arXiv preprint arXiv:1804.00456*,\
    \ 2018.\n- <span id=\"page-10-8\"></span>Rein Houthooft, Xi Chen, Yan Duan, John\
    \ Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing\
    \ exploration. *Advances in neural information processing systems*, 29, 2016.\n\
    - <span id=\"page-10-9\"></span>Marc Bellemare, Sriram Srinivasan, Georg Ostrovski,\
    \ Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and\
    \ intrinsic motivation. *Advances in neural information processing systems*, 29,\
    \ 2016.\n- <span id=\"page-10-10\"></span>David Ha and Jürgen Schmidhuber. World\
    \ models. *arXiv preprint arXiv:1803.10122*, 2018.\n- <span id=\"page-10-11\"\
    ></span>Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before\
    \ you leap: Bridging model-free and model-based reinforcement learning for planned-ahead\
    \ vision-and-language navigation. In *Proceedings of the European Conference on\
    \ Computer Vision (ECCV)*, pages 37–53, 2018.\n- <span id=\"page-10-12\"></span>Oleg\
    \ Svidchenko and Aleksei Shpilman. Maximum entropy model-based reinforcement learning.\
    \ *arXiv preprint arXiv:2112.01195*, 2021.\n- <span id=\"page-10-13\"></span>Viet\
    \ Dung Nguyen, Zhizhuo Yang, Christopher L Buckley, and Alexander Ororbia. R-aif:\
    \ Solving sparse-reward robotic tasks from pixels with active inference and world\
    \ models. *arXiv preprint arXiv:2409.14216*, 2024.\n- <span id=\"page-10-14\"\
    ></span>Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering\
    \ atari with discrete world models. *arXiv preprint arXiv:2010.02193*, 2020.\n\
    - <span id=\"page-10-15\"></span>Adrià Puigdomènech Badia, Pablo Sprechmann, Alex\
    \ Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín\
    \ Arjovsky, Alexander Pritzel, Andew Bolt, et al. Never give up: Learning directed\
    \ exploration strategies. *arXiv preprint arXiv:2002.06038*, 2020.\n- <span id=\"\
    page-10-16\"></span>Roberta Raileanu and Tim Rocktäschel. Ride: Rewarding impact-driven\
    \ exploration for procedurally-generated environments. *arXiv preprint arXiv:2002.12292*,\
    \ 2020.\n- <span id=\"page-10-17\"></span>Adrien Ecoffet, Joost Huizinga, Joel\
    \ Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration\
    \ problems. *arXiv preprint arXiv:1901.10995*, 2019.\n- <span id=\"page-10-18\"\
    ></span>Pranav Shyam, Wojciech Jaskowski, and Faustino Gomez. Model-based active\
    \ exploration. In ´ *International conference on machine learning*, pages 5779–5788.\
    \ PMLR, 2019.\n- <span id=\"page-10-19\"></span>Kurtland Chua, Roberto Calandra,\
    \ Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful\
    \ of trials using probabilistic dynamics models. *Advances in neural information\
    \ processing systems*, 31, 2018.\n- <span id=\"page-11-0\"></span>Rémi Coulom.\
    \ Efficient selectivity and backup operators in monte-carlo tree search. In *International\
    \ conference on computers and games*, pages 72–83. Springer, 2006.\n- <span id=\"\
    page-11-1\"></span>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent\
    \ Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda\
    \ Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural\
    \ networks and tree search. *nature*, 529(7587):484–489, 2016.\n- <span id=\"\
    page-11-2\"></span>David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis\
    \ Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan\
    \ Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a\
    \ general reinforcement learning algorithm. *arXiv preprint arXiv:1712.01815*,\
    \ 2017b.\n- <span id=\"page-11-3\"></span>Vicenç Gómez, Hilbert J Kappen, Jan\
    \ Peters, and Gerhard Neumann. Policy search for path integral control. In *Machine\
    \ Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD\
    \ 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14*, pages 482–497.\
    \ Springer, 2014.\n- <span id=\"page-11-4\"></span>Grady Williams, Andrew Aldrich,\
    \ and Evangelos Theodorou. Model predictive path integral control using covariance\
    \ variable importance sampling. *arXiv preprint arXiv:1509.01149*, 2015.\n- <span\
    \ id=\"page-11-5\"></span>Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal\
    \ difference learning for model predictive control. *arXiv preprint arXiv:2203.04955*,\
    \ 2022.\n- <span id=\"page-11-6\"></span>Nicklas Hansen, Hao Su, and Xiaolong\
    \ Wang. Td-mpc2: Scalable, robust world models for continuous control. *arXiv\
    \ preprint arXiv:2310.16828*, 2023.\n- <span id=\"page-11-7\"></span>Pierre-Luc\
    \ Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In *Proceedings\
    \ of the AAAI conference on artificial intelligence*, volume 31, 2017.\n- <span\
    \ id=\"page-11-8\"></span>Alexander C Li, Carlos Florensa, Ignasi Clavera, and\
    \ Pieter Abbeel. Sub-policy adaptation for hierarchical reinforcement learning.\
    \ *arXiv preprint arXiv:1906.05862*, 2019.\n- <span id=\"page-11-9\"></span>Danijar\
    \ Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:\
    \ Learning behaviors by latent imagination. *arXiv preprint arXiv:1912.01603*,\
    \ 2019.\n- <span id=\"page-11-10\"></span>Danijar Hafner, Jurgis Pasukonis, Jimmy\
    \ Ba, and Timothy Lillicrap. Mastering diverse domains through world models. *arXiv\
    \ preprint arXiv:2301.04104*, 2023.\n- <span id=\"page-11-11\"></span>J. Ross\
    \ Quinlan. Induction of decision trees. *Machine learning*, 1:81–106, 1986.\n\
    - <span id=\"page-11-12\"></span>Martin Klissarov, Pierre-Luc Bacon, Jean Harb,\
    \ and Doina Precup. Learnings options end-to-end for continuous action tasks.\
    \ *arXiv preprint arXiv:1712.00004*, 2017.\n- <span id=\"page-11-13\"></span>Raviteja\
    \ Chunduru and Doina Precup. Attention option-critic. *arXiv preprint arXiv:2201.02628*,\
    \ 2022.\n- <span id=\"page-11-14\"></span>Brendon Johnson and Alfredo Weitzenfeld.\
    \ Hierarchical reinforcement learning in multi-goal spatial navigation with autonomous\
    \ mobile robots. *arXiv preprint arXiv:2504.18794*, 2025.\n- <span id=\"page-11-15\"\
    ></span>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into\
    \ rectifiers: Surpassing human-level performance on imagenet classification. In\
    \ *Proceedings of the IEEE international conference on computer vision*, pages\
    \ 1026–1034, 2015.\n- <span id=\"page-11-16\"></span>Maxime Chevalier-Boisvert,\
    \ Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman\
    \ Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular &\
    \ customizable reinforcement learning environments for goal-oriented tasks. *CoRR*,\
    \ abs/2306.13831, 2023.\n- <span id=\"page-11-17\"></span>Daochen Zha, Wenye Ma,\
    \ Lei Yuan, Xia Hu, and Ji Liu. Rank the episodes: A simple approach for exploration\
    \ in procedurally-generated environments. *arXiv preprint arXiv:2101.08152*, 2021.\n\
    \n### <span id=\"page-12-0\"></span>A Task-Aware Reconstruction Loss, RSSM\n\n\
    Dreamer's reconstruction objective is trained jointly with its reward and transition\
    \ models. However, we find that the standard loss formulation often neglects sparse,\
    \ task-relevant visual features early in training. In particular, elements such\
    \ as single-colour goal boxes and proximity indicators are frequently underemphasized\
    \ until shaped by downstream reward gradients. This delayed encoding limits sample\
    \ efficiency and impedes the development of a semantically rich latent space.\n\
    \nTo address this, we propose a task-aware reconstruction loss built upon four\
    \ primary components, designed to upweight critical areas of the observation.\
    \ Let yˆ denote the reconstructed observation and y the ground truth. The total\
    \ loss is defined as:\n\n$$\\mathcal{L}\\_{\\text{rxcon}} = \\lambda\\_{\\text{MSE}}\
    \ \\cdot \\mathcal{L}\\_{\\text{MSE}} + \\lambda\\_{\\text{MAE}} \\cdot \\mathcal{L}\\\
    _{\\text{MAE}} + \\lambda\\_{\\text{SSIM}} \\cdot \\mathcal{L}\\_{\\text{SSIM}}\
    \ \\tag{12}$$\n\n• The MSE loss captures low-frequency structure.\n\n$$\\mathcal{L}\\\
    _{\\text{MSE}} = \\|\\hat{y} - y\\|\\_2^2 \\tag{13}$$\n\n• The MAE loss featuring\
    \ spatial and semantic weighting promotes fidelity in key regions.\n\n$$\\mathcal{L}\\\
    _{\\text{MAE}} = \\sum\\_{i} w\\_{i} \\cdot |\\hat{y}\\_{i} - y\\_{i}| \\tag{14}$$\n\
    \nwhere w<sup>i</sup> is a per-pixel weight based on semantic role. For instance:\n\
    \n- Goal box colors (red, green, blue): w = 16\n- Proximity bar region: w = 20\n\
    - Agent position and heading on the map: w = 35\n- The SSIM loss encourages high-frequency\
    \ perceptual fidelity.\n\n$$\\mathcal{L}\\_{\\text{SSIM}} = 1 - \\text{SSIM}(\\\
    hat{y}, y) \\tag{15}$$\n\nThe weights used internally are λMSE = 0.5, λMAE = 0.5,\
    \ and λSSIM = 1.0. These terms are applied independently across the image and\
    \ map channels of the observation, and their final sum forms the decoder's contribution\
    \ to the overall world model loss.\n\nQualitatively, we observe that adding these\
    \ terms enables consistent reconstruction of task-relevant visual semantics even\
    \ in early training. To ensure fairness in evaluation, we also extend the base\
    \ Dreamer model with this modified reconstruction loss during ablation and baseline\
    \ comparison experiments.\n\nWhile we modify the encoder's loss function to improve\
    \ the latent encoding and the speed with which a latent space is converged upon,\
    \ we do not modify the underlying Recurrent State-Space Model (RSSM) in any way\
    \ for the purposes of this study.\n\nDreamer models the environment using an RSSM,\
    \ utilising a Gated Recurrent Unit (GRU), a recurrent neural network, to capture\
    \ temporal dynamics that preserves context. The RSSM operates in latent space\
    \ which is produced by the encoder and refined by the decoder. The RSSM separates\
    \ the state into a deterministic component ht, which is the direct output of the\
    \ GRU, and a stochastic component z<sup>t</sup> which is produced by the prior\
    \ model when given ht. The decoder operates upon the posterior representation,\
    \ which processes the encoded representation with the prior's output.\n\nAt each\
    \ timestep t, RSSM updates the belief over latent states using:\n\n$$h\\_t = \\\
    text{GRU}(h\\_{t-1}, [z\\_{t-1}, a\\_{t-1}]) \\tag{16}$$\n\n$$z\\_t \\sim p(z\\\
    _t \\mid h\\_t) \\quad \\text{(prior)}\\tag{17}$$\n\n$$z\\_t \\sim q(z\\_t \\\
    mid h\\_t, e\\_t) \\quad \\text{(posterior)}\\tag{18}$$\n\nwhere e<sup>t</sup>\
    \ is the encoded observation and at−<sup>1</sup> is the previous action.\n\nThe\
    \ posterior q is only used during training for reconstruction and KL divergence\
    \ regularization. The prior p is used for imagination and planning. The concatenated\
    \ latent state s<sup>t</sup> = [ht, zt] is then passed to heads for reward prediction,\
    \ continuation, entropy estimation, and reconstruction.\n\nThe use of a GRU for\
    \ the deterministic hidden state h<sup>t</sup> makes the RSSM both lightweight\
    \ and computationally efficient. GRUs have lower inference time and fewer parameters\
    \ than more complex sequence models such as transformers. While GRUs operate with\
    \ linear computational complexity in the sequence length, transformers typically\
    \ incur quadratic complexity due to their full self attention mechanism. This\
    \ makes GRUs especially well suited for online model-based reinforcement learning,\
    \ where efficiency and speed are critical.\n\nBy combining temporal recurrence\
    \ with structured stochasticity, RSSM allows Dreamer to learn accurate and temporally\
    \ coherent world models, while remaining computationally efficient.\n\n### <span\
    \ id=\"page-13-0\"></span>B Planning Algorithm\n\nAlgorithm 1 Entropy Seeking\
    \ Anticipatory Planning\n\n- 1: Input: Observation o<sup>t</sup>\n- 2: Meta-policy\
    \ 1 selects planning horizon H and entropy weight w<sup>e</sup>\n- 3: Meta-policy\
    \ 2 computes discrete planning probability p<sup>t</sup> ∈ {0, 0.05, 0.25, 0.55,\
    \ 1} (via squaring sampled values from {0, 0.25, 0.5, 0.75, 1.0})\n- 4: Sample\
    \ u<sup>t</sup> ∼ U(0, 1)\n- 5: if u<sup>t</sup> < p<sup>t</sup> then\n- 6: Greedy\
    \ actor samples C candidate actions a1, . . . , a<sup>C</sup> from o<sup>t</sup>\n\
    - 7: for i = 1 to C do\n- 8: Roll out trajectory τ<sup>i</sup> of length H using\
    \ world model and greedy actor\n\n9: Compute D<sup>i</sup> = 1 H P<sup>H</sup>\
    \ <sup>t</sup>=1 weE (i) <sup>t</sup> + (1 − we)R (i) t\n\n- 10: Select trajectory\
    \ τbest = arg max D<sup>i</sup>\n- 11: Set plan to τbest\n\n12: Continue interacting\
    \ with environment; repeat planning check at next step\n\n### <span id=\"page-13-1\"\
    ></span>C Miniworld Environment and Reward Scheme\n\nWe extend the MiniWorld Maze\
    \ environment[\\[Chevalier-Boisvert et al., 2023\\]](#page-11-16) with several\
    \ task relevant augmentations. The maze environment is a 3 dimensional procedurally\
    \ generated maze of size 8x8 (using recursive backtracking) where the agent can\
    \ take continuous actions along three dimensions - forward/back (step size attenuated\
    \ if moving backwards to encourage progress), strafe left/right, turn left/right.\
    \ Each observation consists of a forward-facing RGB image of size (64x64x3). Each\
    \ episode ends when the time limit (4096) is reached, or the three goal boxes\
    \ have been found. Each training run samples a new maze structure every episode\
    \ to prevent memorization. No regions of the maze are sectioned off from the rest\
    \ of the maze and all the goal states are reachable.\n\nTo promote structured\
    \ exploration, we introduce a porosity parameter that controls wall density: with\
    \ probability p, wall segments are randomly removed during generation. This provides\
    \ a tunable complexity gradient for navigation tasks by creating variable maze\
    \ connectivity.\n\nAn auxiliary binary 2D map of size (64x64x3) that records agent\
    \ visitation over the course of an episode has been concatenated to the observation.\
    \ This map records visited coordinates as 1s whereas unvisited coordinates are\
    \ kept at 0. The position of the agent and the direction it is looking in is also\
    \ visible on the map. This serves as episodic spatial memory that enables agents\
    \ to reason about coverage and connect their actions to the current observation.\
    \ This mirrors plausible real-world capabilities that can be enacted through GPS\
    \ tracking or odometry.\n\nThe reward function consists of three components:\n\
    \nExploration Reward: A positive reward is granted when the agent visits a previously\
    \ unvisited cell in its binary exploration map. The reward magnitude is proportional\
    \ to the number of newly visited cells within a square region around the agent,\
    \ the size of which is controlled by the *blur* parameter, given by b. While this\
    \ reward introduces non-Markovian dynamics by incorporating visitation history,\
    \ the inclusion of a binary map in the observation allows memory-less model-free\
    \ agents such as PPO to perform effectively in this environment.\n\n$$\\text{calculation\
    \ reward} = \\begin{cases} \\frac{\\Delta\\_t}{b^2} & \\text{if } b > 1\\\\ \\\
    Delta\\_t & \\text{otherwise} \\end{cases}$$\n\nwhere ∆<sup>t</sup> = number of\
    \ newly explored cells at time t\n\nProximity Reward: A smoothly decaying signal\
    \ is emitted by each goal object, with exponentially scaled rewards given when\
    \ the agent is within an x-unit radius. This mimics real-world analogs such as\
    \ bluetooth signals or radio signals for search and rescue, animal noises for\
    \ ecological monitoring, or semantic hints for more advanced exploration. This\
    \ reward takes the form of two bars in the center of the image - if the agent\
    \ is near a goal box of a particular colour (red, green, or blue), the bars will\
    \ turn that colour with intensity varying with distance.\n\n$$\\text{Promimity\
    \ reward} = \\begin{cases} 0 & \\text{if } \\Delta < 0 \\text{ or } \\Delta >\
    \ 10\\\\ (10 - \\Delta)^2 \\cdot p\\_{mul} & \\text{otherwise} \\end{cases}$$\n\
    \n$$\\text{where } \\Delta = \\text{dist} - (r\\_{\\text{agent}} + r\\_{\\text{box}}\
    \ + s) \\text{ and } p\\_{mul} = 0.03$$\n\nGoal Reward: The agent gets a reward\
    \ for moving into a coloured box. It gets 50 per box and then 150 when it gets\
    \ the third box.\n\nThus the overall reward is composed of these three elements\
    \ summed onto the baseline of -10. The lower limit of reward gained in an episode\
    \ is −T where T is the time limit, and the upper limit is 0.\n\n# <span id=\"\
    page-14-0\"></span>D PPO Experiment Details\n\nWe use Ray RLlib's implementation\
    \ of PPO as our model-free baseline. PPO is a strong general-purpose algorithm,\
    \ but its performance tends to degrade in tasks with sparse rewards and partial\
    \ observability, such as MiniWorld's procedurally generated mazes [\\[Zha et al.,\
    \ 2021\\]](#page-11-17). As PPO is strictly on-policy, it cannot benefit from\
    \ the same random rollout pretraining used in model-based methods like Dreamer.\
    \ To ensure a fair comparison, we provide PPO with an additional 256 on-policy\
    \ training episodes instead of initialising it with previously collected random\
    \ walk data as we do with the model based methods.\n\nPPO receives the same augmented\
    \ observations as Dreamer, including the first-person RGB view and the auxiliary\
    \ spatial map. We also enable frame stacking and maintain RGB color to support\
    \ temporal and visual understanding during navigation.\n\n#### PPO-Specific Parameters\n\
    \n- PPO Steps: 1,398,576\n- Rollout Fragment Length: 512\n- Train Batch Size:\
    \ 16,384\n- SGD Minibatch Size: 2,048\n- Number of SGD Iterations: 10\n- Gradient\
    \ Clipping: 0.5\n- Learning Rate: 3 × 10<sup>−</sup><sup>5</sup>\n- Gamma (Discount):\
    \ 0.995\n- Clipping Parameter: 0.1\n- Entropy Coefficient: 0.001\n- Value Function\
    \ Loss Coefficient: 1.0\n- KL Target: 0.01\n\n# E Experiment Configuration\n\n\
    This appendix details the configuration used in all experiments. Our setup is\
    \ based on the Dreamer framework with extensions for hierarchical planning. We\
    \ include hyperparameters for the model architecture, training schedule, environment,\
    \ and planning modules to support full reproducibility.\n\nTo our surprise, MiniWorld\
    \ appears to be significantly more CPU-bound than GPU-bound, likely due to its\
    \ lightweight rendering and Python-based simulation. For example, 350,000 training\
    \ steps took 16 hours on a system with an NVIDIA 5090 GPU and an Intel i9-14900K\
    \ (24 cores, 6.0 GHz max), compared to 24 hours on an NVIDIA 4090 GPU paired with\
    \ a 13th Gen i7-13700K (16 cores, 5.4 GHz max), and 36 hours on an NVIDIA 4070\
    \ Ti GPU with an AMD Ryzen 9 5900X (12 cores, 4.8 GHz max). These differences\
    \ suggest that CPU core count and clock speed significantly influence throughput\
    \ in MiniWorld environments, even when powerful GPUs are available.\n\n### E.1\
    \ Environment\n\n- Environment Name: MiniWorld-MazeCA-v0\n- Time Limit: 4096 steps\
    \ per episode\n- Action Repeat: 1\n- Parallel Environments: 1\n- Porosity: Varied\
    \ across experiments; see Appendix F for visualizations\n\n#### E.2 Model Architecture\n\
    \n- Latent State: 512-dimensional deterministic + 32-dimensional discrete stochastic\n\
    - Recurrent Depth: 2 (GRU)\n- Encoder (Visual / Map): 5-layer MLPs with CNNs,\
    \ cnn\\_depth=64, kernel\\_size=4\n- Decoder: Decodes both image and map, uses\
    \ mae and symlog\\_mse losses\n- Activation Function: SiLU; normalization enabled\n\
    - Gradient Heads: decoder, reward, cont, entropy\n- Initial Distribution: Learned\n\
    \n#### E.3 Actor and Critic\n\n- Actor: 2-layer MLP, normal distribution, entropy\
    \ regularization 3e-4\n- Critic and Q-function: 2-layer MLPs, symlog\\_disc distribution\n\
    - Learning Rate: 3e-5, Gradient Clipping: 100.0\n\n#### E.4 Training Configuration\n\
    \n- Precision: 16-bit\n- Batch Size: 16\n- Batch Length: 64\n- Model Learning\
    \ Rate: 1e-4\n- Optimizer: Adam, eps=1e-8\n- Dataset Size: 1,048,576\n- Train\
    \ Ratio: 2\n- Train Steps per Update: 1\n\n#### E.5 Planning Behavior (Hierarchical)\n\
    \n- Planning Horizon: 15 steps\n- Plan Choices: 256\n- Meta-Policy Input Dimensions:\n\
    \t- Higher-level: 11272\n\t- Lower-level: 9734\n- Training Frequency: Every 32\
    \ steps\n- Learning Rate: 0.003\n- Entropy Regularization: 0.1\n\n#### E.6 Replay\
    \ Buffer and PPO-style Update\n\n- Buffer Size: 32,768\n- Minimum Samples Before\
    \ Training: 128\n- Clipping ϵ: 0.2\n- Discount Factor γ: 0.99\n- GAE λ: 0.95\n\
    \n#### E.7 Maze and Observation Configuration\n\n- Observation Size: 64 × 64 ×\
    \ 3\n- Domain Randomization: Disabled\n- Fluff Factor: 2\n- Map Input: Included\
    \ (include\\_maps=True)\n\n<span id=\"page-16-1\"></span>![](_page_16_Picture_0.jpeg)\n\
    \nFigure 5: Top-down maze layouts at selected porosity levels. Higher porosity\
    \ values remove more internal walls, increasing openness and reducing planning\
    \ difficulty.\n\n# <span id=\"page-16-0\"></span>F Maze Images\n\nTo visualize\
    \ the effect of varying porosity on maze complexity, we provide top-down views\
    \ of generated mazes at increasing porosity levels, see Fig. [5.](#page-16-1)\
    \ As porosity increases, more internal walls are removed, resulting in more open\
    \ environments. These top-down maps reflect the structural differences that influence\
    \ planning difficulty.\n\nTo contextualize the agent's perspective within these\
    \ mazes, we also provide an example of the full map layout and a corresponding\
    \ visual observation seen by the agent, as given in Figure [6.](#page-17-0)\n\n\
    # G Other Results\n\nIn this section, we present additional metrics and quantities\
    \ tracked during our experiments. While these results are not central to the main\
    \ text, they provide further insight into the model's internal behavior and performance\
    \ dynamics.\n\nFrom the porosity comparison study, we highlight the internal planning\
    \ parameters set by the meta-policy. Specifically:\n\n- The endorphin ratio between\
    \ reward and entropy is shown in Figure [7,](#page-17-1) with intra-episode standard\
    \ deviation in Figure [8.](#page-18-0) - The probability of replanning is shown\
    \ in Figure [9,](#page-18-1) with its intra-episode standard deviation in Figure\
    \ [10.](#page-18-2) - The planning horizon is shown in Figure [11,](#page-19-0)\
    \ with its intra-episode standard deviation in Figure [12.](#page-19-1)\n\nShaded\
    \ intervals in all graphs represent variation across different random seeds, while\
    \ intra-episode standard deviations are shown in dedicated plots.\n\nWe begin\
    \ by examining how the meta-policy adjusts the balance between reward and entropy\
    \ during planning. Figure [7](#page-17-1) illustrates the endorphin ratio, a proxy\
    \ for this trade-off, across different porosities and training steps. This ratio\
    \ remains relatively stable overall, though minor variations emerge depending\
    \ on environment difficulty and training stage. To understand how this behavior\
    \ fluctuates within individual episodes, Figure [8](#page-18-0) presents the intra-episode\
    \ standard deviation of the endorphin ratio. These deviations tend to decrease\
    \ as porosity increases, suggesting more consistent decision-making in easier\
    \ environments.\n\n<span id=\"page-17-0\"></span>![](_page_17_Picture_0.jpeg)\n\
    \n(a) Agent's visual observation (b) Agent's map observation\n\nFigure 6: Image\
    \ of what the agent perceives - the visual observation (left) and of the map observation\
    \ (right).\n\n<span id=\"page-17-1\"></span>![](_page_17_Figure_4.jpeg)\n\nFigure\
    \ 7: Endorphin ratios across different porosities. Although the ratios remain\
    \ relatively stable across porosities and training time, slight variation is observed.\n\
    \nNext, we explore the replanning probability, which captures how often the agent\
    \ updates its plan mid-episode. Figure [9](#page-18-1) shows the average replanning\
    \ probabilities over training for each porosity level. While the means are broadly\
    \ similar, a clear reduction in variability across seeds is observed at higher\
    \ porosities, reflecting more deterministic behavior under lower uncertainty.\
    \ Figure [10](#page-18-2) complements this by plotting the intra-episode standard\
    \ deviation of the replanning probability. Here, we observe a steady decline over\
    \ training, indicating growing confidence in the agent's planning under all conditions.\n\
    \nFinally, Figures [11](#page-19-0) and [12](#page-19-1) focus on the planning\
    \ horizon, which represents how far ahead the meta-policy attempts to predict.\
    \ As shown in Figure [11,](#page-19-0) planning horizons increase in more predictable\
    \ (high-porosity) environments, whereas more challenging settings result in slower\
    \ growth and greater inter-seed variance. The intra-episode standard deviations\
    \ in Figure [12](#page-19-1) remain relatively uniform across porosities but exhibit\
    \ a slight downward trend, reflecting the stabilizing influence of meta-policy\
    \ convergence during training.\n\nFigure [13](#page-19-2) presents the actor loss\
    \ across different porosities, indicating that our method does not significantly\
    \ affect the actor component itself, but instead the gains in performance are\
    \ driven by directly optimising the world model. Interestingly, a divergence between\
    \ the planned and unplanned variants becomes apparent toward the end of training\
    \ in the 0.2 porosity setting. This may suggest that, as the world model converges\
    \ and its predictive uncertainty decreases, the actor also stabilizes and its\
    \ loss declines. Additionally, the standard deviation of the actor loss is notably\
    \ lower for the planned agent in higher-porosity environments, which is likely\
    \ a consequence of reduced epistemic uncertainty in the world model.\n\nFigure\
    \ [14](#page-20-0) presents the length of a plan before replanning occurs. This\
    \ measure is generally stable across porosities, but the variance across seeds\
    \ is higher in more difficult (low porosity) settings. Interestingly, as\n\n<span\
    \ id=\"page-18-0\"></span>![](_page_18_Figure_0.jpeg)\n\nFigure 8: Intra-episode\
    \ standard deviations of endorphin ratios. The deviation decreases as porosity\
    \ increases, suggesting more stable planning under easier conditions.\n\n<span\
    \ id=\"page-18-1\"></span>![](_page_18_Figure_2.jpeg)\n\nFigure 9: Replanning\
    \ probabilities across porosities. While mean values remain comparable, variance\
    \ across seeds diminishes faster at higher porosities.\n\nshown in Figure [15,](#page-20-1)\
    \ the intra-episode standard deviation remains low, suggesting that while different\
    \ seeds may converge to distinct stable values, intra-run variability remains\
    \ small. The variation between seeds in Figure [15](#page-20-1) is higher in the\
    \ hardest setting, as noted in previous plots. This could be because of the world\
    \ model not reaching convergence as easily.\n\n<span id=\"page-18-2\"></span>![](_page_18_Figure_5.jpeg)\n\
    \nFigure 10: Intra-episode standard deviations of replanning probabilities. These\
    \ deviations drop quickly as meta-policies converge.\n\n<span id=\"page-19-0\"\
    ></span>![](_page_19_Figure_0.jpeg)\n\nFigure 11: Planning horizons across porosities.\
    \ Horizons increase as model fidelity improves, particularly for high-porosity\
    \ environments. In low-porosity settings, horizons rise more gradually, and variance\
    \ across seeds is higher.\n\n<span id=\"page-19-1\"></span>![](_page_19_Figure_2.jpeg)\n\
    \nFigure 12: Intra-episode standard deviations of planning horizons. The deviations\
    \ are relatively consistent across porosities, with a slight decline over training\
    \ as the meta policy converges.\n\n<span id=\"page-19-2\"></span>![](_page_19_Figure_4.jpeg)\n\
    \nFigure 13: Actor loss across porosities. The actor remains stable across planning\
    \ variants, with some divergence occurring later in training for 0.2 porosity.\n\
    \n<span id=\"page-20-0\"></span>![](_page_20_Figure_0.jpeg)\n\nFigure 14: Length\
    \ of plan before replanning across porosities. Stability is observed overall,\
    \ with higher inter-seed variation in low-porosity settings.\n\n<span id=\"page-20-1\"\
    ></span>![](_page_20_Figure_2.jpeg)\n\nFigure 15: Intra-episode standard deviation\
    \ of plan length before replanning. Values remain low across porosities, suggesting\
    \ consistent behavior within each run."
- title: "Fuzzy Information Evolution with Three-Way Decision in Social Network\n\
    \  Group Decision-Making"
  abstract: 'In group decision-making (GDM) scenarios, uncertainty, dynamic social

    structures, and vague information present major challenges for traditional

    opinion dynamics models. To address these issues, this study proposes a novel

    social network group decision-making (SNGDM) framework that integrates

    three-way decision (3WD) theory, dynamic network reconstruction, and linguistic

    opinion representation. First, the 3WD mechanism is introduced to explicitly

    model hesitation and ambiguity in agent judgments, thereby preventing

    irrational decisions. Second, a connection adjustment rule based on opinion

    similarity is developed, enabling agents to adaptively update their

    communication links and better reflect the evolving nature of social

    relationships. Third, linguistic terms are used to describe agent opinions,

    allowing the model to handle subjective, vague, or incomplete information more

    effectively. Finally, an integrated multi-agent decision-making framework is

    constructed, which simultaneously considers individual uncertainty, opinion

    evolution, and network dynamics. The proposed model is applied to a multi-UAV

    cooperative decision-making scenario, where simulation results and consensus

    analysis demonstrate its effectiveness. Experimental comparisons further verify

    the advantages of the algorithm in enhancing system stability and representing

    realistic decision-making behaviors.'
  url: http://arxiv.org/abs/2505.16781v1
  keywords: Social network group decision-making (SNGDM), Three-way decision (3WD),
    Consensus analysis.
  document: '#### I. INTRODUCTION


    I N multi-agent decision-making, information is often highly uncertain. Traditional
    centralized information fusion and decision-making systems often face problems
    such as isolated information and poor coordination. To improve local assessment
    capabilities and overall situation awareness, distributed information fusion and
    decision-making mechanisms based on multi-agent systems have attracted increasing
    attention in recent years [\[1\]](#page-10-0).


    Corresponding author: Qianlei Jia


    Qianlei Jia is with the School of Automation, Northwestern Polytechnical University,
    710072, China, (e-mail: jiaql@mail.nwpu.edu.cn).


    Xinliang Zhou is with the College of Computing and Data Science, Nanyang Technological
    Univeristy, 639798, Singapore, (e-mail: xinliang001@e.ntu.edu.sg).


    Ondrej Krejcar is with the FEEC, VSB Technical University of Ostrava, 708 00 Ostrava,
    Czech Republic, also with Skoda Auto University, 293 01 Mlada Boleslav, Czech
    Republic, and also with the Faculty of Informatics and Management, University
    of Hradec Kralove, 500 03 Hradec Kralove, Czech Republic (e-mail: ondrej.krejcar@uhk.cz).


    Enrique Herrera-Viedma is with the Andalusian Research Institute in Data Science
    and Computational Intelligence and the Department of Computer Science and Artificial
    Intelligence, University of Granada, 18071 Granada, Spain, (e-mail: viedma@decsai.ugr.es)


    This work was supported by the National Natural Science Foundation of China (No.
    62073266), by the grant PID2022-139297OB-I00 funded by MICIU/AEI/10.13039/501100011033,
    ERDF/EU, Grant C-ING-165-UGR23 funded by Consejer ´ıa de Universidad, Investigacion
    e Innovaci ´ on and by ´ ERDF Andalusia Program 2021-2027.


    Opinion dynamics models have been widely used as a fundamental framework for distributed
    information fusion in multi-agent systems. By simulating how individual agents
    update their information, these models effectively show collective behavior and
    the process of consensus formation in uncertain environment [\[2\]](#page-10-1).
    In [\[3\]](#page-10-2), the DeGroot and Hegselmann–Krause (HK) models were combined
    to propose a hybrid opinion dynamics (HOD) model in the large-scale group decisionmaking
    (LSGDM) framework. In [\[4\]](#page-10-3), a self-confidence evolution model,
    which encompassed the self-confidence levels of one''s group mates and the passage
    of time, was proposed. To consider the trust relationship in social network group
    decision making (SNGDM), a new discount-based uninorm trust propagation operator
    with 2-tuple linguistic was presented in [\[5\]](#page-10-4). Following the idea
    of the personalized individual semantics (PIS) model, the PIS-based linguistic
    opinions dynamics (PIS-LOD) model was proposed within the framework of bounded
    confidence effects [\[6\]](#page-10-5). In [\[7\]](#page-10-6), a novel opinion
    dynamics model called interval opinion dynamics with dynamic bounded confidence
    was proposed, in which agents expressed their opinions in numerical intervals
    and the bounded confidences varied within a specified range over time. By taking
    different numerical interval opinions and uncertainty tolerances into account,
    a numerical interval opinion dynamics model was proposed to investigate the process
    of forming collective opinions in a group of interacting agents [\[8\]](#page-10-7).
    Inspired by the DeGroot model, a novel group decision-making (GDM) model with
    opinions evolution was established, and a consensus model with minimum adjustments
    was provided to obtain the optimal adjusted initial opinions and collective consensus
    opinion [\[9\]](#page-10-8). Besides, an opinion similarity mixed (OSM) model
    and a structural similarity mixed (SSM) model were proposed in [\[10\]](#page-10-9),
    in which the strong and weak relations between individuals were considered. Based
    on the social network DeGroot (SNDG) model, the authors proved that the speed
    of consensus reaching was subject to the largest self-confidence level of opinion
    followers and the speed of consensus reaching was subject to the top two self-confidence
    levels of opinion leaders [\[11\]](#page-10-10). In [\[12\]](#page-11-0), a conflict
    resolution method considering the trust propagation, conflict detection, and alternatives
    selection was proposed.


    1


    As shown in the above analysis, opinion dynamics models have been widely applied
    across various fields. However, most of these models assume that agents must always
    make definite choices. For example, in the HK model, if the opinion difference
    between agents exceeds a given threshold, they will not interact or update their
    information. However, in many cases, the actual opinion difference may be only
    slightly larger than the threshold, yet it completely blocks information exchange,
    which is often unrealistic in practical situations. To overcome this limitation,
    this study incorporates the three-way decision (3WD) theory. 3WD has demonstrated
    unique advantages in various decision-making environments. In [\[13\]](#page-11-1),
    the update problem of 3WD with dynamic variation of scales in incomplete multi-scale
    information systems was studied, where the updating mechanisms of decision granules
    induced by the similarity relation were exploited through cut refinement and coarsening
    via attribute value taxonomies. Considering that the existing 3WD models could
    not effectively handle incomplete multiattribute decision-making (MADM) problems
    in real life, the preference of decision-makers for each alternative was considered
    and the concept of predecisions was introduced, whereby an incomplete fuzzy decision
    system (IFDS) was obtained in [\[14\]](#page-11-2). In [\[15\]](#page-11-3), a
    kind of 3WD-MADM with probabilistic dominance relations was proposed, where the
    two state sets were developed by virtue of probabilistic dominance classes. Also,
    a probabilistic linguistic 3WD method based on regret theory (RT), namely PL-TWDR,
    was proposed for MADM problems with probabilistic linguistic term sets (PLTSs),
    considering the effective rationality of a decisionmaker in complex decision environments
    [\[16\]](#page-11-4). In [\[17\]](#page-11-5), the concepts of positive, neutral,
    and negative alliances with two thresholds were first provided, and then three-way
    conflict analysis based on the Bayesian minimum risk theory was conducted, where
    examples were explored to show how the positive, neutral, and negative alliances
    were computed with a Pythagorean fuzzy loss function given by an expert.


    Moreover, the structure of social network is also essential in GDM. In [\[18\]](#page-11-6),
    a cooperation index was formulated to identify the non-cooperative behavior, based
    on which a noncooperative behavior transformation method was investigated and
    a bidirectional feedback mechanism was provided for SNGDM. In [\[19\]](#page-11-7),
    trust risks in the conflict-eliminating process (CEP) of SNGDM were addressed
    through third-party monitoring, and a trust risk analysis-based conflict-eliminating
    model for SNGDM was developed. In [\[20\]](#page-11-8), an interaction indicator
    was exploited to represent peer interaction effort (PIE), individual social cooperation
    networks (ISCNs) were constructed with the log-sigmoid transition technique, and
    a novel game-theoretic expert importance evaluation model guided by cooperation
    effects was finally proposed. To address the challenge that decision-makers in
    a community setting usually exhibited complex social preferences and intricate
    social interactions, a minimum cost consensus-based SNGDM approach considering
    altruism-fairness preferences and ordered trust propagation was designed in [\[21\]](#page-11-9).


    Despite the wide application of opinion dynamics models in distributed decision-making
    tasks, several key limitations remain. First, many classical models such as the
    DeGroot and HK frameworks assume that agents must always make definite decisions,
    typically accepting or rejecting a certain view. This binary assumption overlooks
    the hesitation and ambiguity that often arise in real-world environments, especially
    when information is incomplete or conflicting. Although the HK model introduces
    a confidence threshold to support selective interactions, the results are highly
    sensitive to small changes in this parameter, which reduces its robustness in
    dynamic settings. Second, most existing models do not incorporate a mechanism
    to explicitly handle indecision or uncertainty during the opinion updating process.
    As a result, agents are often forced to take positions even when the available
    information does not justify a clear choice. This can lead to premature convergence
    or extreme polarization. Third, the role of network structure is often simplified
    or treated as static, whereas in many practical scenarios, communication and influence
    networks evolve dynamically. Agents do not interact with all others equally; rather,
    their connections change based on factors such as opinion similarity, trust, or
    communication cost. Neglecting these structural dynamics limits the model''s ability
    to reflect realistic social interactions and adaptive coordination.


    Motivated by the challenges identified above, this paper proposes a novel information
    updating model. The main contributions of this study are as follows:


    - 1) Introducing 3WD to describe uncertainty in agent judgments. Unlike traditional
    opinion dynamics models that use binary decisions, this study applies the 3WD
    mechanism ("accept," "reject," and "defer") to better handle ambiguous or conflicting
    information. This helps prevent irrational decisions and premature consensus.

    - 2) Combining opinion dynamics with a dynamic social network reconstruction mechanism.
    A connection adjustment rule based on opinion similarity is proposed, allowing
    agents to adaptively update their communication links. This reflects the dynamic
    and selective nature of real-world social interactions.

    - 3) Using linguistic information to better match real-world scenarios. Instead
    of using only numerical values, agent opinions are described with linguistic terms.
    This improves the model''s ability to deal with vague, subjective, or incomplete
    information, making the decision-making process more practical to explain.

    - 4) Developing an integrated decision-making framework for multi-agent coordination.
    The framework considers uncertainty in individual decisions, opinion evolution
    in groups, and changes in network structure, which improves the system''s performance
    in complex environments.


    The paper''s structure is as follows: In Section II, the relevant concepts are
    introduced. In Section III, the new model is proposed. In Section IV, a series
    of examples are presented. In Section V, the comparison is conducted. The conclusion
    is given in Section VI.


    # II. PRELIMINARIES


    # *A. Linguistic Term Set*


    *Definition 1:* Let *H* = {*h<sup>i</sup>* |*i* = 0,1,··· ,2Φ,|Φ ∈ *N* <sup>∗</sup>}
    be finite and completely ordered linguistic term sets (LTSs). *h<sup>i</sup>*
    represents a possible value for a linguistic variable. *h<sup>i</sup>* and *h<sup>j</sup>*
    should satisfy [\[22\]](#page-11-10), [\[23\]](#page-11-11):


    - (1) *h<sup>i</sup>* ≤ *h<sup>j</sup>* if and only if *i* ≤ *j*;

    - (2) The negation operation *neg*(*hi*) = *h<sup>j</sup>* if *i*+ *j* = 2Φ;

    - (3) If *i* ≥ *j*, then max{*h<sup>i</sup>* ,*hj*} = *h<sup>i</sup>* ;

    - (4) If *i* ≥ *j*, then min{*h<sup>i</sup>* ,*hj*} = *h<sup>j</sup>* .


    #### *B. Linguistic Scale Functions (LSFs)*


    *Definition 2 [\[24\]](#page-11-12):* For the linguistic term *h<sup>j</sup>*
    in *H* = {*h<sup>i</sup>* |*i* = 0,1,··· ,2Φ,|Φ ∈ *N* <sup>∗</sup>}, *h<sup>j</sup>*
    represents a possible value for a linguistic variable. If θ ∈ [0, 1] is a numerical
    value, then the function *f* : *h<sup>j</sup>* → θ*<sup>j</sup>* , which conducts
    the mapping from *h<sup>j</sup>* to θ*<sup>j</sup>* (*j* = 0, 1, ··· , 2*t*),
    is defined as follows:


    $$\Theta\_j = \begin{cases} \begin{array}{c} \frac{d^l - d^{l-j}}{2d^l - 2}, \qquad
    0 \le j \le t \\\ \frac{d^l + d^{l-t} - 2}{2d^l - 2}, \qquad t < j \le 2t \end{array}
    \tag{1} $$


    #### *C. DeGroot Model*


    *Definition 3 [\[25\]](#page-11-13):* In the DeGroot model, the agents unconditionally
    trust the other agents. Suppose that *E* = {*e*1, *e*2,··· , *eN*} indicates the
    agents. *t* ∈ {1,2,··· ,*T*} indicates the discrete variable representing the
    number of rounds of opinion evolution. For *i* ∈ *N*, *yi*(*t*) ∈ [0,1] is the
    numeric opinion at time *t*. The weight offered by the agent *e<sup>i</sup>* to
    *e <sup>j</sup>* is expressed as ω*<sup>j</sup>* , where 0 ≤ ω*<sup>j</sup>* ≤
    1 and ∑ *N <sup>j</sup>*=1ω*<sup>j</sup>* = 1 for all *i* ∈ *N*. The opinions
    evolution rule is:


    $$\mathbf{y}\_{l}(t+1) = \sum\_{j=1}^{N} \mathbf{o}\_{j} \mathbf{y}\_{j}(t). \tag{2}$$


    #### *D. HK Bounded Confidence Model*


    *Definition 4 [\[26\]](#page-11-14), [\[27\]](#page-11-15):* Suppose that *E*
    = {*e*1, *e*2,··· , *ek*} indicates the agents. For *e<sup>i</sup>* ∈ *E*, *xi*(*t*)
    ∈ [0,1] is the numeric opinion at time *t*. *X* = {*x*1(*t*), *x*2(*t*),··· ,
    *xk*(*t*)} is the opinions of agents express at time *t*. ε indicates the confidence
    threshold value. If all agents hold the same confidence threshold ε, the HK model
    is homogeneous; otherwise, it is heterogeneous.


    *Step 1:* Determine the confidence set.


    Let *I*(*e<sup>i</sup>* ,*X*(*t*)) be the confidence set of *e<sup>i</sup>* at
    time *t*.


    $$I(e\_i, X(t)) = \{e\_j | |\mathbf{x}\_i(t) - \mathbf{x}\_j(t)| \le \varepsilon,
    e\_j \in E\}, \ i = 1, 2, \dots, k \text{ (3)}$$


    *Step 2:* Calculate the weights.


    When the agent *e<sup>i</sup>* update the opinion at time *t*, the weight of the
    agent *e<sup>j</sup>* is:


    $$w\_{ij}(t) = \begin{cases} \frac{1}{\#I(e\_i, Y(t))}, & \text{if } e\_j \in
    I(e\_i, Y(t)) \\ 0, & \text{otherwise} \end{cases} \tag{4}$$


    where #*I*(*e<sup>i</sup>* ,*Y*(*t*)) is the cardinality of *I*(*e<sup>i</sup>*
    ,*X*(*t*)).


    *Step 3:* Opinion evolution.


    Let *xi*(*t* +1) indicates the opinion of *e<sup>i</sup>* at time *t* +1, then


    $$\mathbf{x}\_{i}(t+1) = \mathbf{w}\_{i1}\mathbf{x}\_{1}(t) + \mathbf{w}\_{i2}\mathbf{x}\_{2}(t)
    + \dots + \mathbf{w}\_{ik}\mathbf{x}\_{k}(t) \tag{5}$$


    # *E. Social Network*


    *Definition 5 [\[28\]](#page-11-16)–[\[30\]](#page-11-17):* A social network is
    *G*(*V*,*E*). *V* = {*v<sup>t</sup>* , *t* ∈ *M*} expresses vertices. *E* = {*ets*,
    *t*,*s* ∈ *M*} indicates the edges. The edge *ets* = (*v<sup>t</sup>* , *vs*)
    indicates the relationship between *v<sup>t</sup>* and *v<sup>s</sup>* . *F* =
    (*fts*)*m*×*<sup>m</sup>* can describe the social network.


    $$f\_{ls} = \begin{cases} \ 1, & (\nu\_l, \nu\_s) \in E \\ 0, & otherwise \end{cases}
    \tag{6}$$


    *Definition 6 [\[31\]](#page-11-18), [\[32\]](#page-11-19):* For *G*(*V*,*E*),
    the following concepts are introduced.


    Density of social network:


    It is the ratio of the number of direct pairwise relationships to the possible
    number of direct relationships.


    In-degree *Cid*(*vt*) of the vertex *v<sup>t</sup>* :


    It is employed to describe the reputation of a node, which reflects the integration
    force.


    Out-degree *Cod*(*vt*) of the vertex *v<sup>t</sup>* :


    It is used to describe the communicative nature of a node, which reflects the
    radiation.


    The final centrality degree is γ = *Cid* (*vt*)+*Cod* (*vt*) 2(*m*−1)


    # *F. 3WD Model*


    *Definition 7 [\[33\]](#page-11-20):* The 3WD model includes two states *X* =
    {*C*,¬*C*} and three actions *H* = {*hA*,*hD*,*hR*}, where *C* and ¬*C* represent
    for a "good" state and a "bad" state, respectively. *hA*, *hD*, and *h<sup>R</sup>*
    represent the three domains in classifying an alternative *a<sup>i</sup>* , that
    is, *a<sup>i</sup>* ∈ *POS*(*C*), *a<sup>i</sup>* ∈ *BOU*(*C*), and *a<sup>i</sup>*
    ∈ *NEG*(*C*), respectively. Furthermore, *POS*(*C*), *BOU*(*C*), and *NEG*(*C*)
    correspond to the positive domain, the boundary domain and the negative domain,
    respectively. When *a<sup>i</sup>* ∈*C*, let λ*AP*, λ*DP*, and λ*RP* denote the
    losses of taking the actions *hA*, *hD*, and *hR*. Similarly, when *a<sup>i</sup>*
    ∈ ¬*C*, let λ*AN*, λ*DN*, and λ*RN* represent the losses of taking the actions
    *hA*, *hD*, and *hR*. Then, the loss functions of *a<sup>i</sup>* are given in
    Table I, where *C*(*P*) and ¬*C*(*P*) are represented by *P* and *N* standing
    for *C* and ¬*C*, respectively. For any alternative *a<sup>i</sup>* ∈ *A*, the
    equivalence class of *ai* is represented as [*a<sup>i</sup>* ]. *Pr*(*C* | [*a<sup>i</sup>*
    ]) and *Pr*(¬*C* | [*a<sup>i</sup>* ]) represent the conditional probability of
    the alternative *a<sup>i</sup>* belongs to and not belonging to *C*, respectively.
    Then, the expected losses *R*(*h<sup>I</sup>* | [*a<sup>i</sup>* ]) for the three
    actions (*I* = *A*;*D*;*R*) are:


    $$\begin{aligned} \mathcal{R}(h\_{A} \mid [a\_{i}]) &= \lambda\_{AP} Pr(C \mid
    [a\_{i}]) + \lambda\_{AN} Pr(\neg C \mid [a\_{i}]); \\ \mathcal{R}(h\_{D} \mid
    [a\_{i}]) &= \lambda\_{DP} Pr(C \mid [a\_{i}]) + \lambda\_{DN} Pr(\neg C \mid
    [a\_{i}]); \\ \mathcal{R}(h\_{R} \mid [a\_{i}]) &= \lambda\_{RP} Pr(C \mid [a\_{i}])
    + \lambda\_{RN} Pr(\neg C \mid [a\_{i}]). \end{aligned}$$


    TABLE I: Loss Functions of *a<sup>i</sup>* .


    |    | C(P) | ¬C(N) |

    |----|------|-------|

    | hA | λAP  | λAN   |

    | hD | λDP  | λDN   |

    | hR | λRP  | λRN   |


    Afterwards, we can obtain the following rules by using the Bayesian theory of
    minimum losses.


    - (*P*) If*R*(*h<sup>A</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>D</sup>* | [*a<sup>i</sup>*
    ]) and *R*(*h<sup>A</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>R</sup>* | [*a<sup>i</sup>*
    ]), decide *a<sup>i</sup>* ∈ *POS*(*C*);

    - (*B*) If*R*(*h<sup>D</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>A</sup>* | [*a<sup>i</sup>*
    ]) and *R*(*h<sup>D</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>R</sup>* | [*a<sup>i</sup>*
    ]), decide *a<sup>i</sup>* ∈ *BOU*(*C*);

    - (*N*) If*R*(*h<sup>R</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>A</sup>* | [*a<sup>i</sup>*
    ]) and *R*(*h<sup>R</sup>* | [*a<sup>i</sup>* ]) ≤ *R*(*h<sup>D</sup>* | [*a<sup>i</sup>*
    ]), decide *a<sup>i</sup>* ∈ *NEG*(*C*).


    ### III. LINGUISTIC 3WD OPINION DYNAMICS MODEL OVER


    .


    #### DYNAMIC SOCIAL NETWORKS


    To better model the interaction between opinion evolution and network structure
    in GDM, this paper proposes a coevolutionary model that combines the 3WD mechanism
    with a dynamic network update process. Traditional models often use a binary decision
    rule, where agents either fully accept or completely reject neighbors based on
    a fixed confidence bound. However, such a hard-boundary approach cannot fully
    reflect the diversity of real-world social behavior. In this work, we introduce
    the 3WD mechanism, which divides the opinion distance between two agents into
    three regions: if the distance is less than a strict acceptance threshold α, the
    neighbor is deterministically accepted; if the distance exceeds a rejection threshold
    β, the neighbor is rejected; and if the distance falls between α and β, the agent
    accepts the neighbor with a certain probability. This probability is controlled
    by an exponential decay function *p* = exp(−λ(*di j* − α)), where *di j* is the
    opinion distance and λ is a decay factor. This design reflects the psychological
    tendency that people are more likely to accept opinions that are closer to their
    own, while opinions farther away are less likely to be accepted. The 3WD mechanism
    allows the model to capture hesitation, selective trust, and partial agreement,
    which are common in real social interactions. In addition, to break the limitation
    of a static network, we introduce a dynamic update mechanism based on cognitive
    similarity. This mechanism allows the connections between agents to change over
    time, depending on their current opinions. Specifically, if two agents are not
    connected but their opinion distance is smaller than a threshold δadd, a link
    is added between them with probability *p*add. On the other hand, if two connected
    agents show a large opinion difference that exceeds δcut, their link may be removed
    with probability *p*cut. In this way, the network structure evolves according
    to the cognitive relationships among agents.


    We consider a group of *N* agents whose opinions are expressed using a finite
    linguistic term set *H* = {*h*0,*h*1,...,*h*2Φ}. Each term *h<sup>j</sup>* ∈ *H*
    is mapped to a real-valued scalar in the interval [0,1] via a predefined nonlinear
    function *f* : *H* → [0,1]. Let *xi*(*t*) ∈ *H* denote the linguistic opinion
    of agent *i* at time *t*, and let θ*i*(*t*) = *f*(*xi*(*t*)) be the corresponding
    numerical representation.


    #### Notation:


    - *H* = {*h*0,*h*1,...,*h*2Φ}: Linguistic term set

    - *f*(*hj*) = θ*<sup>j</sup>* : Mapping from *h<sup>j</sup>* to [0,1]

    - *xi*(*t*) ∈ *H*: Linguistic opinion of agent *i* at time *t*

    - θ*i*(*t*) = *f*(*xi*(*t*)): Numerical opinion

    - *Ai j*(*t*): Social adjacency matrix (1 if connected, 0 otherwise)

    - α, β: Three-way decision thresholds

    - λ: Probability decay parameter

    - δadd,δcut: Network update distance thresholds

    - *p*add, *p*cut: Rewiring probabilities


    *a) Initialization:* Each agent is randomly assigned an initial linguistic opinion
    *xi*(0) ∈ *H*, which is then mapped to its corresponding numeric value θ*i*(0)
    = *f*(*xi*(0)). The initial social network is represented by an adjacency matrix
    *A*(0), where *Ai j*(0) = 1 if agents *i* and *j* are connected, and *Ai j*(0)
    = 0 otherwise.


    *b) Opinion Updating:* At each discrete time step *t* = 1,2,...,*T*max, every
    agent *i* updates its opinion based on the opinions of its neighbors. Specifically,
    agent *i* evaluates each connected agent *j* (i.e., *Ai j*(*t*) = 1) by computing
    the opinion distance *di j*(*t*) = |θ*i*(*t*) − θ*j*(*t*)|. Then, a 3WD strategy
    is applied:


    - If *di j*(*t*) ≤ α, agent *j* is accepted as a trusted neighbor.

    - If *di j*(*t*) ≥ β, agent *j* is rejected.

    - If α < *di j*(*t*) < β, agent *j* is accepted with probability *p* = exp(−λ(*di
    j*(*t*)−α)).


    Let N*i*(*t*) denote the set of accepted neighbors of agent *i* at time *t*. If
    N*i*(*t*) is nonempty, agent *i* updates its numerical opinion by averaging:


    $$

    \theta\_l(t+1) = \frac{1}{|\mathcal{N}\_l''(t)|} \sum\_{j \in \mathcal{N}\_l''(t)}
    \theta\_j(t).

    $$


    If N*i*(*t*) is empty, then θ*i*(*t* + 1) = θ*i*(*t*) and the opinion remains
    unchanged. After the numerical update, the agent''s opinion is mapped back to
    the nearest linguistic term:


    $$\alpha\_{\boldsymbol{t}}(\boldsymbol{t}+1) = \arg\min\_{h\_{\boldsymbol{f}}
    \in H} |f(h\_{\boldsymbol{f}}) - \boldsymbol{\Theta}\_{\boldsymbol{t}}(\boldsymbol{t}+1)|.$$


    *c) Dynamic Network Rewiring:* Following the opinion update, the social network
    evolves based on the opinion differences among agents. For each pair of agents
    (*i*, *j*):


    - If *Ai j*(*t*) = 0 and |θ*i*(*t*) − θ*j*(*t*)| < δadd, a link is added with
    probability *p*add.

    - If *Ai j*(*t*) = 1 and |θ*i*(*t*)−θ*j*(*t*)| > δcut, the existing link is removed
    with probability *p*cut.


    The adjacency matrix is then updated to *A*(*t* +1) accordingly.


    *d) Termination Condition:* The iteration continues until the maximum opinion
    change between two successive steps is smaller than a convergence threshold ε,
    i.e.,


    $$\max\_{i} |\theta\_{i}(t+1) - \theta\_{i}(t)| < \varepsilon,$$


    or until the maximum number of iterations *T*max is reached.


    *e) Output:* The final linguistic opinion *x<sup>i</sup>* for each agent is obtained,
    representing the converged state or the state at termination.


    #### IV. EXAMPLE


    In recent years, with the development of multi-UAV cooperative technology, UAV
    swarm intelligence systems have demonstrated a high level of autonomy and efficient
    information-sharing capabilities across various fields. It is worth noting that
    this study is inspired by the research presented in [\[34\]](#page-11-21). As
    shown in Fig. 1, a group of *n* UAVs is cooperatively circumnavigating a group
    of *m* unknown targets on a horizontal plane. Taking post-earthquake rescue in
    mountainous areas as an example, multiple UAVs are often deployed to search for
    survivors across large regions. In this setting, the model based on collaborative
    decision-making and opinion dynamics is essential. Each UAV acts as an intelligent
    agent, and shares the linguistic information with the system. Algorithm 1: Linguistic
    3WD Opinion Dynamics over Dynamic Social Network


    Input: Linguistic term set *H*, mapping function *f* , initial network *A*(0),
    parameters α,β,λ,δadd,δcut, *p*add, *p*cut,*T*max, ε Output: Final linguistic
    opinions *x<sup>i</sup>* for all agents 1 Initialization: foreach *agent i* ∈
    {1,...,*N*} do <sup>2</sup> Randomly assign *xi*(0) ∈ *H*; <sup>3</sup> Compute
    θ*i*(0) = *f*(*xi*(0)); <sup>4</sup> for *t* = 0 to *T*max do <sup>5</sup> foreach
    *agent i* ∈ {1,...,*N*} do <sup>6</sup> Initialize neighbor set N*i*(*t*) ← /0;
    <sup>7</sup> foreach *j* ̸= *i and Ai j*(*t*) = 1 do <sup>8</sup> Compute *di
    j* = |θ*i*(*t*)−θ*j*(*t*)|; <sup>9</sup> if *di j* ≤ α then <sup>10</sup> N*i*(*t*)
    ← N*i*(*t*)∪ { *j*} ; // Accept 11 else <sup>12</sup> if *di j* ≥ β then <sup>13</sup>
    continue ; // Reject 14 else <sup>15</sup> Compute *p* = exp(−λ(*di j* −α)); <sup>16</sup>
    Sample *r* ∼ U (0,1); <sup>17</sup> if *r* < *p* then <sup>18</sup> N*i*(*t*)
    ← N*i*(*t*)∪ { *j*} ; // Accept with uncertainty <sup>19</sup> if N*i*(*t*) ̸=
    /0 then <sup>20</sup> θ*i*(*t* +1) = <sup>1</sup> <sup>|</sup>N*i*(*t*)<sup>|</sup>
    ∑ *j*∈N*i*(*t*) θ*j*(*t*); 21 else <sup>22</sup> θ*i*(*t* +1) = θ*i*(*t*) ; //
    No update <sup>23</sup> Map back to linguistic term:; <sup>24</sup> *xi*(*t* +1)
    = arg min *hj*∈*H* | *f*(*hj*)−θ*i*(*t* +1)|; // Step 5: Dynamic Network Rewiring
    <sup>25</sup> foreach *pair* (*i*, *j*) *with i* ̸= *j* do <sup>26</sup> Compute
    *di j*(*t*) = |θ*i*(*t*)−θ*j*(*t*)|; <sup>27</sup> if *Ai j*(*t*) = 0 *and di
    j*(*t*) < δ*add* then <sup>28</sup> Sample *r* ∼ U (0,1); <sup>29</sup> if *r*
    < *padd* then <sup>30</sup> Set *Ai j*(*t* +1) = *Aji*(*t* +1) = 1 ; // Add link
    <sup>31</sup> if *Ai j*(*t*) = 1 *and di j*(*t*) > δ*cut* then <sup>32</sup> Sample
    *r* ∼ U (0,1); <sup>33</sup> if *r* < *pcut* then <sup>34</sup> Set *Ai j*(*t*
    +1) = *Aji*(*t* +1) = 0 ; // Remove link <sup>35</sup> Check convergence:; <sup>36</sup>
    if max *i* |θ*i*(*t* +1)−θ*i*(*t*)| < ε then <sup>37</sup> break;


    ![](_page_4_Figure_2.jpeg)


    Fig. 1: Circumnavigation of *m* Targets by *n* UAVs (Red Center Represents the
    Geometric Centroid of *m* Targets) [\[34\]](#page-11-21).


    As these agents communicate, they update the evaluation information of the situation.


    To evaluate the proposed linguistic 3WD opinion dynamics model, we conduct simulations
    with a fixed number of agents and predefined model parameters. Specifically, the
    number of agents was set to *N* = 20. The linguistic term set consisted of 2Φ+1
    elements, with Φ = 3, and the non-linear mapping parameter was set to *a* = 2.
    The 3WD is controlled by two thresholds: the strict acceptance threshold α = 0.3,
    and the strict rejection threshold β = 0.6. Agents falling in the uncertain region
    (α,β) were accepted with a probability that decays exponentially with the distance
    from α, governed by a decay factor λ = 10. The opinion inertia parameter was set
    to ω = 0, meaning that agents fully adopt the weighted average of their accepted
    neighbors'' opinions without retaining their previous value. For the dynamic network
    update process, we adopted an additive threshold δadd = 0.15 and a cut-off threshold
    δcut = 0.45. Links were added or removed with probabilities *p*add = 0.5 and *p*cut
    = 0.5, respectively. The simulation ran for at most *T*max = 10 time steps, or
    until convergence was reached, defined by a maximum opinion change less than a
    tolerance of ε = 10−<sup>3</sup> .


    The initial linguistic opinions of the 20 agents at time step *t* = 0 are given
    by:


    | x1(0) = h0,  | x2(0) = h3,  | x3(0) = h6,  | x4(0) = h0,  |

    |--------------|--------------|--------------|--------------|

    | x5(0) = h0,  | x6(0) = h0,  | x7(0) = h5,  | x8(0) = h3,  |

    | x9(0) = h3,  | x10(0) = h3, | x11(0) = h5, | x12(0) = h1, |

    | x13(0) = h0, | x14(0) = h5, | x15(0) = h3, | x16(0) = h0, |

    | x17(0) = h4, | x18(0) = h3, | x19(0) = h4, | x20(0) = h3. |


    Fig. [2](#page-5-0) shows how the opinions of all agents change over time. We
    can see that several agents adjusted their opinions clearly in the early steps,
    which means that there is active communication and belief change at the beginning.
    Most of these changes happened within the first four iterations.


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    Fig. 2: Opinions of All Agents Across Iterations.


    <span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)


    Fig. 3: Final Linguistic Term for Each Agent.


    After that, the opinions became more stable. By the fifth iteration, most agents
    had nearly stopped changing. At the seventh iteration, all opinions stayed the
    same, which means the system reached a stable state and no more updates are needed.
    The final linguistic term for each agent is shown in Fig. [3.](#page-5-1) Fig.
    [4](#page-6-0) presents the evolution of the social network and agent opinions
    from iteration 0 to 7. In each subplot, nodes represent agents and edges represent
    social ties. Node colors indicate the linguistic term each agent holds at that
    time step. It can be seen that agents with similar opinions gradually cluster
    together as the simulation progresses. From iteration 5 onwards, the network evolves
    into two densely connected subgroups, each associated with an opinion. Fig. [5](#page-6-1)
    shows how the network structure changes over the iterations. The average degree
    increases quickly from about 1.9 to nearly 12, which means that agents are building
    more links with each other as iteration goes on. This suggests that agents are
    becoming more similar in their opinions, so new connections are more likely under
    the network update rule. Meanwhile, the number of isolated nodes drops from 12
    to 0 in the first two steps and stays at zero after that. This means that all
    agents are connected to the network early on, and the structure remains fully
    connected during the process. These results show that the network becomes more
    connected over time.


    # V. COMPARISON, ROBUSTNESS ANALYSIS, AND COMPLEXITY ANALYSIS


    # *A. Impact of Broad Acceptance Thresholds on Consensus Formation*


    In 3WD framework where [α,β] is the hesitation region, the choice of thresholds
    directly influences how likely agents are to exchange information. In Fig. [6,](#page-6-2)
    α is set to 0.2 and β to 0.4, which defines a narrow acceptance range. Under this
    setting, only a small number of opinion differences fall within the acceptance
    or hesitation zones, while most are rejected directly. As a result, agents are
    more likely to avoid opinion exchange, leading to more isolated updates and a
    wider spread in final linguistic term indices. This produces a more diverse and
    polarized results. In contrast, Fig. [3](#page-5-1) uses α = 0.3 and β = 0.6,
    expanding both the acceptance and hesitation regions. More agent pairs are allowed
    to interact, either certainly or probabilistically, which leads to smoother opinion
    integration. Therefore, smaller α and β reduce communication opportunities and
    promote diversity, while larger thresholds encourage interaction and support consensus
    formation.


    In Fig. [7,](#page-6-3) the thresholds are set to α = 0.5 and β = 0.9, which significantly
    enlarges both the hesitation and acceptance regions compared to the previous settings.
    Under this configuration, only opinion differences greater than 0.9 are directly
    rejected, so most agent pairs have a chance to exchange information. As shown
    in the figure, the majority of agents reach a high and consistent linguistic term
    index *h*3, while only a few remain at the lowest level *h*0. This result indicates
    that a wider acceptance rule allows more interaction among agents, helping them
    to integrate their opinions. Compared to smaller values of α and β, this setup
    is more likely to support global agreement.


    # *B. Impact of Network Scale on Opinion Evolution and Consensus Formation*


    Figs. [4,](#page-6-0) [8,](#page-7-0) and [9](#page-7-1) respectively illustrate
    the evolution process of the system when the number of agents is 20, 30, and 50,
    under the proposed model with 3WD mechanism and dynamic social network reconstruction
    rules. As the number of agents increases, the initially sparse social network
    gradually evolves into several closely connected subgroups. When *N* = 20, the
    connections between agents are relatively concentrated, and two consensus groups
    are eventually formed. When *N* = 30, the trend of network division becomes stronger,
    and some agents with extreme opinions form isolated small groups disconnected
    from the main network. In the large-scale system with *N* = 50, although the initial
    connections are very sparse, the main group gradually absorbs most agents through
    the self-adjusting structure of the network, and eventually forms a unified group
    with closer connections and more consistent opinions. This phenomenon fully demonstrates
    the effectiveness and advantages of the proposed algorithm. As the scale of the
    social network increases, differences between agents grow, and the uncertainty
    of connections rises. Traditional static network models are likely to face problems
    such as opinion polarization, information isolation, and failure to reach consensus.
    In contrast, our model introduces a dynamic linking


    <span id="page-6-0"></span>![](_page_6_Figure_0.jpeg)


    Fig. 4: Temporal Evolution of the Social Network and Linguistic Opinions.


    <span id="page-6-1"></span>![](_page_6_Figure_2.jpeg)


    Fig. 5: Network Connectivity Trends During Evolution.


    <span id="page-6-2"></span>![](_page_6_Figure_4.jpeg)


    Fig. 6: Final Linguistic Term for Each Agent.


    <span id="page-6-3"></span>![](_page_6_Figure_6.jpeg)


    Fig. 7: Final Linguistic Term for Each Agent.


    mechanism based on opinion distance and a "stay undecided" option from the 3WD
    theory, which flexibly handles agents near the decision boundary and significantly
    improves the system''s ability to reach consensus under complex conditions. As
    shown in the figures, this mechanism not only helps to reduce the trend of division
    caused by sparse networks and opinion differences but also enhances information
    flow and coordinated evolution within the group.


    # *C. Impact of Three-Way Decision Mechanism on Opinion Dynamics*


    To evaluate the effect of the proposed 3WD mechanism on the evolution of group
    opinions, we conducted a comparative study involving four scenarios. Fig. [10](#page-8-0)
    shows a comparison of convergence speed in four different scenarios. Each subplot
    represents a specific setting, where the horizontal axis is the number of iterations
    and the vertical axis shows the maximum change in opinions per step, denoted as
    δmax. This value is


    <span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)


    Fig. 8: Temporal Evolution of the Social Network and Linguistic Opinions.


    <span id="page-7-1"></span>![](_page_7_Figure_2.jpeg)


    Fig. 9: Temporal Evolution of the Social Network and Linguistic Opinions.


    used as the measurement of convergence speed in all four experiments, and it is
    calculated by the following formula:


    $$\delta\_{\max}(t) = \max\_{i} |\theta\_{i}(t) - \theta\_{i}(t-1)|$$


    Here, θ*i*(*t*) represents the opinion of agent *i* at *t*. This metric reflects
    the largest individual change at each step and helps evaluate how fast or smooth
    the opinion evolution process is. In the experiments, Scenario 1 and Scenario
    3 represent the cases with 40 and 60 agents respectively, where 3WD mechanism
    is applied using parameters α = 0.3 and β = 0.6. Scenario 2 and Scenario 4 use
    the same number of agents but do not include the 3WD mechanism, which is represented
    by α = β = 0.6, meaning no hesitation zone is added. From the figure, we can see
    that in Scenario 2 and Scenario 4 (without the mechanism), δmax drops quickly,
    and convergence is reached within 4 and 6 iterations respectively. The process
    is fast and smooth. In contrast, Scenario 1 and Scenario 3 (with the mechanism)
    show a slower convergence process, with a more gradual decrease in δmax and some
    fluctuations, especially in Scenario 3. Overall, the 3WD mechanism slows down
    the speed of opinion convergence, which results in a slower but more stable evolution.
    This effect becomes more visible when the number of agents increases. These results
    suggest that the 3WD mechanism achieves a balance between convergence speed and
    system stability.


    <span id="page-8-0"></span>![](_page_8_Figure_3.jpeg)


    Fig. 10: Convergence Speed Comparison Under Different 3WD Settings.


    # *D. Impact of* δ*add on Opinion Dynamics*


    Fig. [11](#page-9-0) shows the evolution of the social network over time steps
    *t* = 0,1,2,3,4,5. The connection threshold δadd in Section Example is increased
    from 0.15 to 0.35. This higher value for adding links allows agents with larger
    opinion differences to form new connections, making it easier for the network
    to reconnect. From the figure, we observe that the initial network is fragmented
    with several isolated nodes. As time increases, agents rapidly form new connections
    and the network quickly becomes denser. By *t* = 3, the system shows signs of
    global integration, and by *t* = 5, all agents are part of a highly connected
    network with relatively homogeneous opinions. This result suggests that increasing
    δadd facilitates faster structural convergence, reduces isolation, and promotes
    opinion merging across the entire network.


    # *E. Impact of* δ*cut on Opinion Dynamics*


    Fig. [12](#page-9-1) illustrates the evolution of the social network over time
    steps *t* = 0,1,2,...,7. In this example, we decreased the disconnection threshold
    δcut from 0.45 to 0.2. This makes it easier for existing links to be removed when
    the opinion distance between agents exceeds the threshold, increasing the likelihood
    of network fragmentation. As shown in the figure, although the network starts
    as a connected structure, it quickly begins to break into separate components.
    By *t* = 2, *e*3, *e*5, and *e*<sup>9</sup> become isolated from the main group.
    This pattern persists through later time steps, where the majority of agents form
    a dense cluster while the isolated ones remain disconnected. These results indicate
    that a lower δcut can reduce network stability and reinforce persistent social
    separation.


    # *F. Complexity Analysis*


    *1) Time Complexity:* In each iteration, the proposed model checks all possible
    neighbor pairs (*i*, *j*) to compute opinion distances and determine acceptance.
    In the worst case, each agent needs to compare with *N* − 1 others, resulting
    in a complexity of O(*N* 2 ) for this step. The dynamic rewiring step also involves
    checking all agent pairs and may update the network structure, which again takes
    O(*N* 2 ) time. The convergence check requires scanning all agents, which takes
    O(*N*) time. Therefore, each iteration has a total time complexity of O(*N* 2
    ), and with a maximum of *T*max iterations, the overall time complexity is O(*T*max
    ·*N* 2 ).


    *2) Space Complexity:* The main memory usage comes from storing the network and
    agent states. The adjacency matrix takes O(*N* 2 ) space. Each agent''s numerical
    opinion θ*i*(*t*) and linguistic opinion *xi*(*t*) together take O(*N*) space.
    In addition, the neighbor sets may each contain up to *N* elements, which leads
    to a worst-case memory usage of O(*N* 2 ). Therefore, the total space complexity
    is O(*N* 2 ).


    In summary, this algorithm models the information evolution with a reasonable
    level of computational complexity. Although the worst-case time and space complexity
    are both O(*N* 2 ), real-world social networks are usually sparse and locally
    connected, so the actual runtime is often much lower than the theoretical upper
    bound. Moreover, the model has a clear structure and is easy to implement in parallel.


    # *G. Potential Applications*


    *1) Brain-Computer Interface (BCI) systems:* BCI systems often involve non-invasive
    brain signals such as electroencephalography (EEG) and functional near-infrared
    spectroscopy (fNIRS). These signals usually have high variability, strong individual
    differences, and incomplete information. Traditional classification methods require
    making clear decisions for all states. However, in real-world applications, the
    user''s cognitive state may be "uncertain" or "hesitant." Introducing


    <span id="page-9-0"></span>![](_page_9_Figure_0.jpeg)


    Fig. 11: Temporal Evolution of the Social Network and Linguistic Opinions.


    <span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)


    Fig. 12: Temporal Evolution of the Social Network and Linguistic Opinions.


    the 3WD theory allows the brain signal recognition system to produce three types
    of outputs: "accept," "reject," and "defer." This keeps an intermediate state
    for handling vague information and helps avoid incorrect triggering. In addition,
    the mechanism of dynamically adjusting neural information channels can also inspire
    the design of adaptive models for individual brain region communication.


    ![](_page_10_Figure_1.jpeg)


    Fig. 13: Description of Brain Regions and EEG-Based Paradigms. (a) Four Regions
    of the Cortex. (b) Four Other Common EEG-Based Paradigms [\[35\]](#page-11-22).


    *2) Opinion Fusion in Smart City Governance:* In smart city governance, such as
    public policy making, intelligent traffic management, and urban risk warning,
    a large amount of public opinion needs to be collected and integrated. When there
    are differences in views, uncertain perceptions, and vague opinions, traditional
    opinion fusion models can easily lead to extreme results or distorted information.
    The proposed model allows individual opinions to be expressed in natural language
    and handles uncertain areas through deferred decisions. This makes the opinion
    fusion process more understandable and more tolerant of ambiguity.


    *3) Multi-Robot Coordination in Disaster Response and Agricultural Monitoring:*
    In scenarios such as post-disaster search and rescue and agricultural monitoring,
    multi-robot systems need to make collaborative decisions based on environmental
    sensing, their own abilities, and task requirements. Traditional assignment strategies
    often require each robot to immediately accept or reject a task. However, in uncertain
    environments or when task complexity is unclear, this approach may lead to poor
    task allocation or wasted resources. By introducing the 3WD theory, robots are
    allowed to delay task decisions, so they can make better choices when more information
    becomes available. At the same time, by adjusting the communication network dynamically,
    robots can improve their cooperation based on task similarity or path overlap,
    which increases the overall efficiency and robustness of the system.


    #### VI. CONCLUSION


    This paper proposes a group opinion update model that combines the 3WD mechanism
    with a dynamic network update process. The introduced 3WD mechanism sets a "hesitation
    interval" to expand the traditional binary rule of accepting or 11


    rejecting neighbors into a three-way probabilistic rule. This allows an agent
    to accept information from another agent with a certain probability when their
    opinion difference lies in an uncertain range. In addition, to overcome the limitations
    of a static network structure, the model includes a dynamic update mechanism based
    on cognitive similarity. By setting probabilistic rules for adding or removing
    links, the social relationships between agents can adjust in real time as their
    opinions evolve. The proposed algorithm is applied to a multi-UAV cooperative
    decision-making scenario. By simulating the information exchange and dynamic negotiation
    among UAVs, we demonstrate that the mechanism is effective in practical distributed
    intelligent systems. The experimental comparisons further verify the advantages
    of the proposed algorithm.


    As AI agents powered by large language models (LLMs) play an increasingly important
    role in group decision-making, how to combine the diverse solutions generated
    by different models has become a key issue. Future research can focus on conflict
    resolution between models, trust evaluation mechanisms, and integration strategies
    to improve the overall level of collaborative intelligence in the system. Also,
    we will add reinforcement learning or adaptive weighting based on past interactions
    to enable agents to evolve their decision strategies over time.


    #### REFERENCES


    - <span id="page-10-0"></span>[1] Z. Zhu, X. Zhang, M. Cao, F. Chiclana, and J.
    Wu, "A dynamic trust and prospect theory driven bilateral feedback mechanism for
    maximizing consensus income in social network group decision making," *Information
    Fusion*, vol. 117, p. 102904, 2025.

    - <span id="page-10-1"></span>[2] Y. Dong, Q. Zha, H. Zhang, and F. Herrera, "Consensus
    reaching and strategic manipulation in group decision making with trust relationships,"
    *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, vol. 51, no. 10,
    pp. 6304–6318, 2020.

    - <span id="page-10-2"></span>[3] Y. Shen, X. Ma, M. Deveci, E. Herrera-Viedma,
    and J. Zhan, "A hybrid opinion dynamics model with leaders and followers fusing
    dynamic social networks in large-scale group decision-making," *Information Fusion*,
    vol. 116, p. 102799, 2025.

    - <span id="page-10-3"></span>[4] X. Chen, Z. Ding, Y. Gao, H. Zhang, and Y. Dong,
    "Competitive resource allocation on a network considering opinion dynamics with
    self-confidence evolution," *Information Fusion*, vol. 114, p. 102680, 2025.

    - <span id="page-10-4"></span>[5] Y. Xing, S. Wang, Y. Dong, Y. Liu, and J. Wu,
    "The trust incentive mechanism by trust propagation to optimize consensus in social
    network group decision making," *Expert Systems with Applications*, vol. 257,
    p. 125111, 2024.

    - <span id="page-10-5"></span>[6] H. Liang, C.-C. Li, Y. Dong, and F. Herrera,
    "Linguistic opinions dynamics based on personalized individual semantics," *IEEE
    Transactions on Fuzzy Systems*, vol. 29, no. 9, pp. 2453–2466, 2020.

    - <span id="page-10-6"></span>[7] H. Liang, C.-C. Li, Y. Dong, and Y. Jiang, "The
    fusion process of interval opinions based on the dynamic bounded confidence,"
    *Information Fusion*, vol. 29, pp. 112–119, 2016.

    - <span id="page-10-7"></span>[8] Y. Dong, M. Zhan, Z. Ding, H. Liang, and F.
    Herrera, "Numerical interval opinion dynamics in social networks: Stable state
    and consensus," *IEEE Transactions on Fuzzy Systems*, vol. 29, no. 3, pp. 584–598,
    2019.

    - <span id="page-10-8"></span>[9] X. Chen, Z. Ding, Y. Dong, and H. Liang, "Managing
    consensus with minimum adjustments in group decision making with opinions evolution,"
    *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, vol. 51, no. 4,
    pp. 2299–2311, 2019.

    - <span id="page-10-9"></span>[10] Z. Wu, Q. Zhou, Y. Dong, J. Xu, A. H. Altalhi,
    and F. Herrera, "Mixed opinion dynamics based on degroot model and hegselmann–
    krause model in social networks," *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems*, vol. 53, no. 1, pp. 296–308, 2022.

    - <span id="page-10-10"></span>[11] Z. Ding, X. Chen, Y. Dong, S. Yu, and F. Herrera,
    "Consensus convergence speed in social network degroot model: The effects of the
    agents with high self-confidence levels," *IEEE Transactions on Computational
    Social Systems*, vol. 10, no. 5, pp. 2882–2892, 2022.

    - <span id="page-11-0"></span>[12] F. Pei, Y. Gao, A. Yan, M. Zhou, and J. Wu,
    "Conflict elimination based on opinion dynamics in fuzzy group decision-making,"
    *Expert Systems with Applications*, p. 124308, 2024.

    - <span id="page-11-1"></span>[13] C. Luo, T. Li, Y. Huang, and H. Fujita, "Updating
    three-way decisions in incomplete multi-scale information systems," *Information
    sciences*, vol. 476, pp. 274–289, 2019.

    - <span id="page-11-2"></span>[14] J. Zhan, J. Ye, W. Ding, and P. Liu, "A novel
    three-way decision model based on utility theory in incomplete fuzzy decision
    systems," *IEEE Transactions on Fuzzy Systems*, vol. 30, no. 7, pp. 2210–2226,
    2021.

    - <span id="page-11-3"></span>[15] W. Wang, J. Zhan, and C. Zhang, "Three-way
    decisions based multiattribute decision making with probabilistic dominance relations,"
    *Information Sciences*, vol. 559, pp. 75–96, 2021.

    - <span id="page-11-4"></span>[16] J. Zhu, X. Ma, L. Mart´ınez, and J. Zhan, "A
    probabilistic linguistic threeway decision method with regret theory via fuzzy
    c-means clustering algorithm," *IEEE Transactions on Fuzzy Systems*, vol. 31,
    no. 8, pp. 2821–2835, 2023.

    - <span id="page-11-5"></span>[17] G. Lang, D. Miao, and H. Fujita, "Three-way
    group conflict analysis based on pythagorean fuzzy set theory," *IEEE Transactions
    on Fuzzy Systems*, vol. 28, no. 3, pp. 447–461, 2019.

    - <span id="page-11-6"></span>[18] T. Gai, F. Chiclana, W. Jin, M. Zhou, and J.
    Wu, "A transformation method of non-cooperative to cooperative behavior by trust
    propagation in social network group decision making," *IEEE Transactions on Fuzzy
    Systems*, 2025.

    - <span id="page-11-7"></span>[19] M. Li, Y. Xu, X. Liu, F. Chiclana, and F. Herrera,
    "A trust risk dynamic management mechanism based on third-party monitoring for
    the conflict-eliminating process of social network group decision making," *IEEE
    Transactions on Cybernetics*, vol. 53, no. 6, pp. 3399–3413, 2022.

    - <span id="page-11-8"></span>[20] Z. Liu, T. Wen, Y. Deng, and H. Fujita, "Game-theoretic
    expert importance evaluation model guided by cooperation effects for social network
    group decision making," *IEEE Transactions on Emerging Topics in Computational
    Intelligence*, 2024.

    - <span id="page-11-9"></span>[21] Y. Feng, Y. Dang, J. Wang, J. Du, and F. Chiclana,
    "Minimum cost consensus-based social network group decision making with altruismfairness
    preferences and ordered trust propagation," *IEEE Transactions on Systems, Man,
    and Cybernetics: Systems*, 2024.

    - <span id="page-11-10"></span>[22] C.-C. Li, H. Liang, Y. Dong, F. Chiclana,
    and E. Herrera-Viedma, "Consistency improvement with a feedback recommendation
    in personalized linguistic group decision making," *IEEE Transactions on Cybernetics*,
    pp. 1–12, 2021.

    - <span id="page-11-11"></span>[23] L. A. Zadeh, "The concept of a linguistic
    variable and its application to approximate reasoning," *Information Sciences*,
    vol. 8, no. 3, pp. 199– 249, 1974.

    - <span id="page-11-12"></span>[24] J.-Q. Wang, P. Wang, J. Wang, H.-Y. Zhang,
    and X.-H. Chen, "Atanassov''s interval-valued intuitionistic linguistic multicriteria
    group decision-making method based on the trapezium cloud model," *IEEE Transactions
    on Fuzzy Systems*, vol. 23, no. 3, pp. 542–554, 2015.

    - <span id="page-11-13"></span>[25] Z. Wu, Q. Zhou, Y. Dong, J. Xu, A. H. Altalhi,
    and F. Herrera, "Mixed opinion dynamics based on degroot model and hegselmann–krause
    model in social networks," *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems*, vol. 53, no. 1, pp. 296–308, 2023.

    - <span id="page-11-14"></span>[26] Y. Li, M. Liu, J. Cao, X. Wang, and N. Zhang,
    "Multi-attribute group decision-making considering opinion dynamics," *Expert
    Systems with Applications*, vol. 184, p. 115479, 2021.

    - <span id="page-11-15"></span>[27] H. Liang, C.-C. Li, Y. Dong, and F. Herrera,
    "Linguistic opinions dynamics based on personalized individual semantics," *IEEE
    Transactions on Fuzzy Systems*, vol. 29, no. 9, pp. 2453–2466, 2021.

    - <span id="page-11-16"></span>[28] Y. Dong, Z. Ding, L. Mart´ınez, and F. Herrera,
    "Managing consensus based on leadership in opinion dynamics," *Information Sciences*,
    vol. 397, pp. 187–205, 2017.

    - [29] J. Wu and F. Chiclana, "A social network analysis trust–consensus based
    approach to group decision-making problems with interval-valued fuzzy reciprocal
    preference relations," *Knowledge-Based Systems*, vol. 59, pp. 97–107, 2014.

    - <span id="page-11-17"></span>[30] J. Wu, M. Cao, F. Chiclana, Y. Dong, and E.
    Herrera-Viedma, "An optimal feedback model to prevent manipulation behavior in
    consensus under social network group decision making," *IEEE Transactions on Fuzzy
    Systems*, vol. 29, no. 7, pp. 1750–1763, 2021.

    - <span id="page-11-18"></span>[31] A. W. Wolfe, "Social network analysis: Methods
    and applications," *Contemporary Sociology*, vol. 91, no. 435, pp. 219–220, 1995.

    - <span id="page-11-19"></span>[32] R.-X. Ding, X. Wang, K. Shang, and F. Herrera,
    "Social network analysis-based conflict relationship investigation and conflict
    degreebased consensus reaching process for large scale decision making using sparse
    representation," *Information Fusion*, vol. 50, pp. 251–272, 2019.

    - <span id="page-11-20"></span>[33] W. Wang, J. Zhan, and E. Herrera-Viedma, "A
    three-way decision approach with a probability dominance relation based on prospect
    theory for incomplete information systems," *Information Sciences*, vol. 611,
    pp. 199–224, 2022.

    - <span id="page-11-21"></span>[34] S. Huang, J. Shi, Q. Zhu, Z. Du, Y. Lyu, and
    Z. Liu, "Multiple uavs cooperatively circumnavigating a group of non-cooperative
    targets in a gps-free environment via a range-only distributed controller," *Aerospace
    Science and Technology*, vol. 158, p. 109924, 2025.

    - <span id="page-11-22"></span>[35] X. Z. Zhou, C. Liu, Y. Wu, Y. Ding, L. Zhai,
    K. Wang, Z. Jia, and Y. Liu, "A comprehensive survey on eeg-based emotion recognition:
    A graph-based perspective," 2024.


    ![](_page_11_Picture_24.jpeg)


    Qianlei Jia received the B.Eng. degree, M.Eng. degree, and Ph.D. degree from Northwestern
    Polytechnical University, Xi''an, China, in 2015, 2018, and 2022. He was a Research
    Fellow with the School of Mechanical and Aerospace Engineering, Nanyang Technological
    University (NTU), Singapore, from 2022 to 2024.


    His research interests include fuzzy theory, decision-making support systems,
    and information fusion.


    ![](_page_11_Picture_27.jpeg)


    Xinliang Zhou received the B.E. degree in Mechatronics Engineering from Beijing
    Jiaotong University, Beijing, China, in 2021, and the Ph.D. degree in Computer
    Science from Nanyang Technological University, Singapore, in 2025. His research
    interests include Brain-Computer Interfaces, Brain Foundation Models, and Interpretable
    Artificial Intelligence.


    ![](_page_11_Picture_29.jpeg)


    Ondrej Krejcar received the Ph.D. degree in technical cybernetics from Technical
    University of Ostrava, Ostrava, Czech Republic, in 2008. From 2016 to 2020, he
    was a Vice-Dean of science and research with the Faculty of Infor- matics and
    Management, University of Hradec Kralove (UHK), Hradec Kralove, Czech Republic.
    From 2020 to 2024, he was a Vice-Rector of science and creative activities with
    UHK. He is a Rector and a Full Professor at Skoda Auto University, Mlada Boleslav,
    Czech Republic, in 2024. At present, he is with the Center


    for Basic and Applied Research, Faculty of Informatics and Management, UHK. His
    H-index is 36 at Web of Science, where more than 250 IF journal articles are indexed
    in the JCR index with more than 4700 external citations (H-index is 42 at SCOPUS
    with more than 7900 citations in total). At the University of Hradec Kralove,
    he is a Guarantee of the Doctoral Study Program in Applied Informatics, where
    he is focusing on Lecturing on Smart Approaches to the Development of Information
    Sys- tems and Applications in Ubiquitous Computing Environments. His research
    interests include technical cybernetics, ubiquitous computing, control systems,
    smart sensors, wireless technology, biomedicine, image segmentation and recognition,
    and biometrics. Dr. Krejcar is on the editorial board of several journals indexed
    in Q1/Q2 at JCR. From 2014 to 2019, he was the Deputy Chairman of the Panel 7
    (Processing Industry, Robotics, and Electrical Engineering) of the Epsilon Program,
    Technological Agency of the Czech Republic. He was a Management Committee Member
    substitute at Project COST CA16226 from 2017 to 2023. He was a Vice-Leader and
    a Management Committee Member at WG4 at Project COST CA17136, from 2018 to 2024.
    From 2019 to 2024, he was a Regulator of the EEA/Norwegian Financial Mechanism,
    Czech Republic, and Technological Agency of the Czech Republic. From 2019 to 2024,
    he was the Chairman of the Program Committee of the KAPPA Program. From 2020 to
    2024, he was the Chairman of Panel 1 (Computer, Physical and Chemical Sciences)
    of the ZETA Program, Technological Agency of the Czech Republic. He was the 14th
    Top Peer Reviewer in Multidisciplinary in the World according to Publons in 2018
    and a Top Reviewer in the Global Peer Review Awards 2019 by Publons.


    ![](_page_12_Picture_0.jpeg)


    Enrique Herrera-Viedma (Fellow, IEEE) received the master''s degree in mathe-
    matics, and the Ph.D. degree in mathematics, from the University of Granada, Granada,
    Spain, 1988 and 1988, respectively. He is a Full Professor of Computer Science
    and AI with the University of Granada, Granada, Spain, from 2007, where he is
    the Vice-President for Research and Knowledge Transfer from June 2015. He has
    published more than 560 peer-reviewed scientific publications, including 350 papers
    in JCR journals (More than 200 in Q1, more than 50 in D1).


    According to Web of Science, his H-index is 85 (more than 23 000 citations received)
    and according to Google Scholar his H-index is 127 (more than 73 000 citations
    received). He currently has 50 Highly Cited Papers indexed in Web of Science.
    Prof. Herrera-Viedma has been identified as Highly Cited Researcher in both categories,
    Computer Science and Engineering by Clarivate Analytics, in the years 2014, 2015,
    2016, 2017, 2018, 2019, 2020, 2021, 2022, and 2023. He received the UGR Young
    Researcher Career Award in 2006, the several UGR excellence research paper awards
    in 2003, 2010, 2019, and 2020, the 2011 IEEE Transactions on Fuzzy Systems Outstanding
    Paper Award from the IEEE Computational Intelligence Society, the 2016 Andrew
    P. Sage Best Transactions Paper Award, which recognizes authors of the best paper
    among all papers published in the IEEE Transactions on Systems, Man, and Cybernetics:
    Systems, and the 2014-Best Paper Herbert Simon Award of journal International
    Journal of Information Technology and Decision Making. He has been the Vice-President
    for Publications in the IEEE System, Man and Cybernetic Society from 2019 to 2020
    and currently he is the New VP for Cybernetics till 2025. He is a Fellow of the
    Institute of Electrical and Electronic Engineers and also of International Fuzzy
    Systems Association for contributions to fuzzy decision making systems and linguistic
    modelling in 2021. He is also a member of the prestigious Spanish Think Tank Royal
    Institute Elcano supported by the large Spanish enterprises and the Spain Government.'
- title: "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A\n\
    \  Comprehensive Review"
  abstract: 'This paper presents a comprehensive synthesis of major breakthroughs
    in

    artificial intelligence (AI) over the past fifteen years, integrating

    historical, theoretical, and technological perspectives. It identifies key

    inflection points in AI'' s evolution by tracing the convergence of

    computational resources, data access, and algorithmic innovation. The analysis

    highlights how researchers enabled GPU based model training, triggered a data

    centric shift with ImageNet, simplified architectures through the Transformer,

    and expanded modeling capabilities with the GPT series. Rather than treating

    these advances as isolated milestones, the paper frames them as indicators of

    deeper paradigm shifts. By applying concepts from statistical learning theory

    such as sample complexity and data efficiency, the paper explains how

    researchers translated breakthroughs into scalable solutions and why the field

    must now embrace data centric approaches. In response to rising privacy

    concerns and tightening regulations, the paper evaluates emerging solutions

    like federated learning, privacy enhancing technologies (PETs), and the data

    site paradigm, which reframe data access and security. In cases where real

    world data remains inaccessible, the paper also assesses the utility and

    constraints of mock and synthetic data generation. By aligning technical

    insights with evolving data infrastructure, this study offers strategic

    guidance for future AI research and policy development.'
  url: http://arxiv.org/abs/2505.16771v1
  keywords: ''
  document: "#### I. INTRODUCTION\n\nArtificial intelligence (AI) technologies have\
    \ advanced rapidly and dramatically over the past decade, reshaping not only technical\
    \ methodologies but also the foundational paradigms of data processing. Early\
    \ developments in AI gained momentum primarily through improvements in computational\
    \ power. Over time, however, researchers increasingly relied on larger datasets\
    \ and more efficient algorithms to drive progress. Within this evolutionary trajectory,\
    \ distinct breakthroughs redirected the focus of AI research and revealed which\
    \ technologies and approaches could remain sustainable.\n\nForecasting the future\
    \ trajectory of AI extends beyond academic interest. It serves as a key driver\
    \ for strategic direction in industry, policy development, and career trajectories\
    \ across multiple fields. As the costs of acquiring data and maintaining compute\
    \ infrastructure continue to rise, researchers and decision-makers must identify\
    \ the forces behind the next transformative leap. Revisiting historical breakthroughs\
    \ does more than offer retrospective\n\n2 nd Ayse Yilmazer *Computer Engineering\
    \ Istanbul Technical University* Istanbul, Turkey yilmazerayse@itu.edu.tr\n\n\
    understanding it enables researchers to build grounded foresight by recognizing\
    \ recurring patterns of innovation. This study traces the history of AI breakthroughs\
    \ and investigates the structural shifts that enabled them. The analysis follows\
    \ three primary dimensions: computational resources (compute), the quantity and\
    \ nature of data (data), and algorithmic innovation. By applying principles from\
    \ statistical learning theory particularly concepts like sample complexity and\
    \ number of samples, the study explains why certain advances qualified as genuine\
    \ breakthroughs.\n\nThe paper also examines current challenges in the AI landscape.\
    \ As access to open data sources declines and privacy regulations tighten control\
    \ over private datasets, researchers face increasing constraints. In response,\
    \ the study evaluates emerging paradigms such as federated learning, privacy-enhancing\
    \ technologies (PETs), and data site infrastructures. It outlines a unified framework\
    \ to understand how ethical, technical, and policy considerations will shape the\
    \ future of AI infrastructure.\n\n#### II. STATISTICAL LEARNING THEORY AND THEORETICAL\
    \ FOUNDATIONS\n\nOne of the most fundamental ways to understand how and why artificial\
    \ intelligence (AI) systems learn involves evaluating this process through the\
    \ lens of Statistical Learning Theory (SLT). This framework offers a conceptual\
    \ and mathematical foundation for estimating the accuracy a model can achieve\
    \ based on the number of samples it receives. Two primary factors govern the quality\
    \ of learning: the number of samples and the sample complexity.\n\nSLT starts\
    \ with an intuitive premise: increasing the amount of data enhances a model's\
    \ capacity to learn. For instance, the MNIST dataset contains 60,000-labeled images\
    \ of handwritten digits [1]. If we could construct a dataset that covers all possible\
    \ 28×28 pixel permutations amounting to a space of 2^784 images the model would\
    \ effectively \"see\" every possible digit, thereby approaching a near-perfect\
    \ classifier. However, real-world conditions render this infeasible. Data remains\
    \ finite, so the system must infer patterns from examples it has never encountered.\
    \ This brings sample complexity into focus—the measure of data required by an\
    \ algorithm to reach a targeted performance threshold [2]. Algorithms with low\
    \ sample complexity achieve high performance with relatively few \"examples, whereas\
    \ those with higher complexity require significantly larger datasets to perform\
    \ comparably. Many studies in contemporary machine learning literature (e.g.,\
    \ NeurIPS and ICML papers) directly or indirectly aim to reduce sample complexity.\n\
    \nIn contrast, data efficiency refers to how well an algorithm learns from a limited\
    \ number of samples. Techniques such as dropout regularization or architectural\
    \ innovations like attention mechanisms improve data efficiency by enabling richer\
    \ inference from the same input. The Transformer architecture illustrates this\
    \ principle concretely. Whereas earlier models like RNNs and CNNs required high\
    \ sample complexity to capture sequential dependencies, the Transformer uses attention\
    \ alone to learn long-range relationships with fewer samples.\n\nSLT also emphasizes\
    \ the importance of data quality and diversity alongside data quantity. Large\
    \ and diverse datasets generally enhance generalization capability. However, if\
    \ the algorithm lacks efficiency in terms of sample complexity, these datasets\
    \ may prove insufficient or prohibitively costly for achieving desired outcomes.\
    \ A commonly cited heuristic in the field referred to as \"napkin math\" summarizes\
    \ this relationship as follows:\n\n#### AI Capability ≈ Number of Samples × Data\
    \ Efficiency\n\nThis formulation conveys a fundamental insight: more data typically\
    \ strengthens models, but better algorithms can reach high performance with less.\
    \ Compute power also plays a decisive role in this equation. With high-capacity\
    \ hardware, researchers can train on larger datasets. GPUs, in particular, support\
    \ both the generation of new examples (e.g., via video game simulations) and the\
    \ processing of those examples. This capability creates larger learning opportunities.\n\
    \nSLT further underscores two key dimensions of data: quantity and quality. Possessing\
    \ a sufficiently large and diverse dataset increases a model's capacity to generalize.\
    \ However this concept alone does not guarantee performance. If the underlying\
    \ algorithm exhibits high sample complexity, even vast quantities of data will\
    \ not suffice or will impose substantial costs.\n\nFigure 1 visualizes the relationship\
    \ between sample count and model accuracy. It compares algorithms with low, medium,\
    \ and high sample complexity. The curves illustrate how algorithms with lower\
    \ complexity reach higher accuracy with fewer examples, while those with high\
    \ complexity show slower performance gains.\n\n![](_page_1_Figure_7.jpeg)\n\n\
    *Figure 1: Relationship between Number of Samples and Model Accuracy*\n\nStatistical\
    \ Learning Theory offers more than just a conceptual lens—it provides researchers\
    \ with a practical framework to optimize AI development. It helps them choose\
    \ effective architectures, define performance limits, and align data requirements\
    \ accordingly. This perspective clarifies the significance of past breakthroughs\
    \ and identifies the parameters likely to drive the next wave of progress. By\
    \ grounding historical milestones in theoretical structure, SLT empowers researchers\
    \ to interpret progress through a coherent analytical framework. In the next section,\
    \ we explore these breakthroughs from this viewpoint.\n\n# III. HISTORICAL MILESTONES\
    \ IN AI BREAKTHROUGHS\n\nSome developments in the history of artificial intelligence\
    \ (AI) represent more than just technical achievements they signify paradigm shifts.\
    \ These breakthroughs either enabled learning at previously unattainable scales\
    \ or introduced new approaches that displaced incumbent methods. As illustrated\
    \ in Figure 2, each of these milestones highlighted below represents a notable\
    \ leap in AI capabilities. We categorize them across three fundamental dimensions:\
    \ compute capacity, data scale, and algorithmic efficiency. This framing allows\
    \ for a more nuanced understanding of why these developments served as turning\
    \ points.\n\n# *2009 – Deep Learning with GPUs (Raina & Ng)*\n\nIn 2009, Andrew\
    \ Ng and his team introduced the use of GPUs for training neural networks [3].\
    \ Until then, most models had relied on CPU-based training, which imposed significant\
    \ time and resource constraints, especially for large-scale models. Their work\
    \ demonstrated that it was feasible to train networks with over 100 million parameters\
    \ within reasonable timeframes. This marked a major leap in AI fueled by expanded\
    \ computational resources.\n\n# *2010 – Release of the ImageNet Dataset*\n\nStanford\
    \ University introduced the ImageNet dataset in 2010 [4], which, at the time,\
    \ stood as one of the largest and most comprehensive labeled image datasets in\
    \ the field. Containing millions of annotated images, ImageNet made comparative\
    \ benchmarking in visual recognition possible and actively encouraged algorithmic\
    \ progress. This development underscored the importance of data scale in driving\
    \ AI advancement.\n\n# *2012 – AlexNet and Dropout*\n\nThat same year, AlexNet's\
    \ success in the ImageNet competition redefined what deep learning could achieve\
    \ at scale. By training the model on GPUs and incorporating Dropout [5] as a regularization\
    \ technique, researchers demonstrated a practical method to curb overfitting while\
    \ improving generalization. Dropout, by randomly deactivating units during training,\
    \ encouraged the model to rely on distributed representations, which in turn reduced\
    \ its dependence on specific pathways and lowered the sample complexity. This\
    \ development not only improved performance without increasing data volume but\
    \ also illustrated how architectural simplicity and data efficiency could reinforce\
    \ each other. As a result, the community began to recognize that meaningful performance\
    \ gains could emerge not just from larger models or datasets, but from smarter\
    \ training techniques that made better use of what was already available.\n\n\
    ![](_page_2_Figure_1.jpeg)\n\n*Figure 2: AI Breakthrough Timeline*\n\n### *2013\
    \ – Word2Vec: Uncovering the Power of Simplicity*\n\nIn 2013, Thomas Mikolov and\
    \ colleagues introduced Word2Vec [6], a breakthrough in natural language processing.\
    \ The model became known for its capacity to represent semantic relationships\
    \ mathematically for instance below.\n\n#### King - Man + Woman = Queen\n\nHowever,\
    \ the true breakthrough lay not just in semantic modeling but also in the ability\
    \ to train on massive datasets using an extremely simplified architecture.\n\n\
    At the time, dominant approaches based on recurrent neural networks (RNNs) required\
    \ high sample complexity and came with steep computational costs. Mikolov purposefully\
    \ stripped down the architecture: he removed non-linear hidden layers, approximated\
    \ the output layer, and implemented the entire model in optimized C code. This\
    \ simplification yielded remarkable scalability, enabling the model to train on\
    \ trillions of words in mere minutes. Andrew Trask, during his participation at\
    \ ICML 2015, recalled witnessing Mikolov express the following motivation:\n\n\
    #### *\"My main goal was to simplify the algorithm so we could train on much more\
    \ data than with previous RNN-based approaches.\"*\n\nFrom a statistical learning\
    \ theory perspective, Word2Vec exemplifies a model with low sample complexity\
    \ that achieved high accuracy by scaling with data volume. The parallel threading\
    \ method used in its original implementation even violated conventional software\
    \ engineering norms by allowing simultaneous writes to shared memory yet the model's\
    \ structure tolerated such disorder, enabling fast and effective training.\n\n\
    Although Word2Vec may appear to be a standard case of algorithmic innovation,\
    \ its actual contribution lies in the fusion of a deliberately \"weak\" model\
    \ with vast quantities of data. In essence:\n\n#### Weak Model × Large Data =\
    \ High Performance\n\nThis episode reinforces a recurring pattern in AI: access\
    \ to massive and high-quality data often yields more substantial gains than sophisticated\
    \ algorithms alone.\n\n#### *2015 – DeepMind and Atari Games*\n\nThe combination\
    \ of Q-learning and convolutional neural networks in DeepMind's Atari project\
    \ demonstrated that reinforcement learning (RL) could facilitate the generation\
    \ of effectively infinite data [7]. The model learned from visual inputs derived\
    \ from gameplay, simulating experiences that would be difficult to replicate in\
    \ the physical world. This work established a new direction in synthetic data\
    \ generation and opened the door to the idea of learning from infinite virtual\
    \ environments.\n\n#### *2016 – AlphaGo: Algorithmic Intelligence Meets Infinite\
    \ Data Generation*\n\nAlphaGo introduced a new paradigm not only in gameplay strategy\
    \ but also in data generation, learning capacity, and decision-making architecture\
    \ [8]. Previous systems learned from a finite number of games played by human\
    \ experts. In contrast, AlphaGo redefined this trajectory. After pretraining on\
    \ expert games, the system began playing millions of games against itself using\
    \ a self-play strategy, thereby generating a virtually limitless training corpus.\
    \ This artificial data abundance enabled a depth of learning that real-world data\
    \ alone could not provide.\n\nThe system's success relied not only on data scale\
    \ but also on a multilayered algorithmic architecture. AlphaGo's decision-making\
    \ pipeline integrated three core components:\n\n*1) Monte Carlo Tree Search (MCTS):*\
    \ This algorithm explored potential moves as a branching tree, computing probabilities\
    \ and value estimates for each node. While it did not learn directly, it implemented\
    \ a highly efficient strategy to search for optimal actions.\n\n*2) Convolutional\
    \ Neural Networks (CNNs):* These networks interpreted visual board states to support\
    \ move prediction via:\n\n- Policy Network: \"Which move should I make in this\
    \ position?\"\n- Value Network: \"What is the win probability from this board\
    \ configuration?\"\n\n*3) Self-Generated Data:* Each self-played game produced\
    \ new examples and experiences, continuously enriching the training signal and\
    \ enabling the model to improve generalization and accuracy over time.\n\nWhat\
    \ set AlphaGo apart was not merely its algorithmic design but its ability to pair\
    \ these methods with a scalable simulation environment that generated training\
    \ data internally. Unlike traditional machine learning workflows that depend on\
    \ externally collected data, AlphaGo's environment served as both model and data\
    \ engine.\n\nAlphaGo marked a decisive moment in the history of AI for two reasons:\
    \ it pushed the boundaries of data generation and combined algorithmic depth with\
    \ modular, interconnected components. The breakthrough did not lie in compute\
    \ or architecture alone but in the cohesive alignment of data, algorithms, and\
    \ hardware. AlphaGo remains a rare instance where both \"more data\" and \"better\
    \ algorithms\" contributed synergistically to a transformative result.\n\n####\
    \ *2017 – Transformer*\n\nThe Transformer architecture, introduced in \"Attention\
    \ is All You Need,\" replaced complex RNN and CNN models with a streamlined yet\
    \ powerful alternative [9]. The architecture relied exclusively on attention mechanisms\
    \ to capture long-range dependencies, eliminating recurrence. This approach significantly\
    \ reduced sample complexity and enabled efficient training on massive datasets.\
    \ Researchers widely recognized Transformer not only for its architectural simplicity\
    \ but also for its ability to achieve high data efficiency at scale making it\
    \ a foundational innovation in modern AI.\n\n#### *2018–2020 – The GPT Series:\
    \ Generalization Through Scale and Data-Driven Breakthroughs*\n\nOpenAI's Generative\
    \ Pre-trained Transformer (GPT) series stands among the most compelling achievements\
    \ in deep learning. The core success of these models stemmed from their ability\
    \ to scale the Transformer architecture to handle immense data volumes. GPT models\
    \ adopted a twophase training strategy: first, large-scale self-supervised pretraining\
    \ on unlabeled text to develop general-purpose linguistic representations; then,\
    \ fine-tuning on taskspecific datasets using smaller, supervised corpora.\n\n\
    GPT-1 introduced this paradigm with a straightforward proposition [10]:\n\n\"\
    Labeled data is scarce, but unlabeled text is abundant. We first perform large-scale\
    \ self-supervised pretraining and then apply task-specific fine-tuning on small\
    \ datasets.\"\n\nThis strategy enabled the model to leverage massive data sources\
    \ effectively and achieve strong performance across a broad set of tasks. GPT-1\
    \ did not introduce a novel architecture but rather demonstrated how data scale\
    \ could elevate an existing one.\n\nGPT-2 followed the same principle [11]. It\
    \ trained on WebText a massive dataset drawn from millions of web pages. This\
    \ was not an architectural leap but a demonstration of how increasing data volume\
    \ could directly enhance model performance. More data translated into better generalization\
    \ and higher accuracy.\n\nGPT-3 emphasized scale even further. Although the authors\
    \ did not explicitly frame the achievement as a \"data breakthrough,\" [12] the\
    \ model's size and parameter count implicitly revealed its relationship with data.\
    \ GPT-3 outperformed earlier models not merely due to parameter growth but because\
    \ it could meaningfully utilize more and more diverse data. This progression showed\
    \ that scaling up a model also required a proportional increase in training data\
    \ to unlock additional learning capacity.\n\nAt this point, a critical balance\
    \ emerged. Enlarging model capacity without increasing data volume risked overfitting.\
    \ Success depended on scaling both the model and the data in tandem. The GPT series\
    \ demonstrated that harmony between architecture and data quantity was essential\
    \ not optional for effective learning at scale.\n\nGPT's overall success did not\
    \ arise from architectural novelty alone but from aligning data scale with model\
    \ capacity. This alignment enabled the models to perform zero-shot and few-shot\
    \ learning across a wide range of tasks. The series provided definitive evidence\
    \ that \"more data enables better generalization\" and showed how to translate\
    \ that principle into state-of-the-art performance in real-world AI applications.\n\
    \n#### *2022 – ChatGPT*\n\nBuilt on GPT-3.5, ChatGPT marked a pivotal moment in\
    \ both technical infrastructure and user experience. OpenAI integrated Reinforcement\
    \ Learning from Human Feedback (RLHF) to enable the model to generate more natural,\
    \ context-aware responses [13]. By deploying ChatGPT through a publicly accessible\
    \ conversational interface, OpenAI expanded the societal reach of AI and catalyzed\
    \ its cultural integration. This model embodied a balanced synthesis of compute,\
    \ data, and algorithmic design, positioning it as a multidimensional breakthrough.\n\
    \nViewed collectively, these breakthroughs reflect a consistent pattern where\
    \ algorithmic design, data availability, and compute capacity evolved in tandem.\
    \ Researchers leveraged GPU acceleration not only to expand model depth and parameter\
    \ counts, but also to accelerate data throughput, enabling iterative experimentation\
    \ at an unprecedented scale. As compute infrastructure matured, it facilitated\
    \ the emergence of training paradigms that aligned more closely with realworld\
    \ complexity supporting larger datasets, more nuanced architectures, and longer\
    \ training cycles. In this ecosystem, computational power shaped the way researchers\
    \ formulated problems, designed experiments, and optimized model performance across\
    \ diverse domains.\n\nSimilarly, the ImageNet milestone did not hinge on dataset\
    \ scale alone it also incorporated algorithmic interventions such as Dropout.\
    \ This simple yet powerful technique mitigated overfitting by injecting stochastic\
    \ noise into input and intermediate layers, thereby encouraging generalization.\
    \ In effect, Dropout produced a data augmentation effect by training the model\
    \ to behave as if it had encountered a wider diversity of examples. In other words,\
    \ Dropout amplified existing data representations through implicit diversification.\n\
    \nThe ascent of deep learning owes much to its architectural simplicity. While\
    \ probabilistic graphical models (PGMs) provide rich theoretical frameworks and\
    \ expressive capabilities, they demand significant computational resources that\
    \ limit practical scalability. In contrast, deep learning architectures align\
    \ more naturally with GPU acceleration, enabling faster execution and better adaptation\
    \ to high-volume data environments. This alignment allowed deep learning models\
    \ to take full advantage of the computational and data resources available during\
    \ the GPU-driven phase of AI development. Despite their conceptual depth, PGMs\
    \ found limited success in delivering scalable performance under realworld data\
    \ conditions, whereas deep learning systems demonstrated consistent gains through\
    \ streamlined design and efficient computation.\n\n![](_page_4_Figure_1.jpeg)\n\
    \n#### *Figure 3: Factors and Overlaps in AI Breakthroughs*\n\nA closer look at\
    \ major breakthroughs in AI reveals that most innovations arose from the interplay\
    \ of compute power, algorithmic design, and data availability. However, historical\
    \ analysis suggests that access to high-quality data consistently played a dominant\
    \ role. The Venn diagram in Figure 3 illustrates the conceptual intersections\
    \ where significant AI breakthroughs occurred. For example, models like GPT-3\
    \ and ChatGPT did not evolve solely due to advanced architectures or hardware\
    \ they relied fundamentally on access to vast corpora of textual data. Similarly,\
    \ the ImageNet era demonstrated how large-scale, labeled datasets could trigger\
    \ paradigm shifts.\n\nThis historical pattern suggests a critical insight: data\
    \ access has acted as the primary catalyst, while hardware and algorithms evolved\
    \ to accommodate the growing demands of data-intensive learning. We conclude that:\n\
    \n*\"Engineered intelligence may yield short-term gains, but long-term advancements\
    \ arise from scalable learning algorithms applied to increasingly powerful compute\
    \ and* \n\n*increasingly rich datasets.\"*\n\nTherefore, this study supports the\
    \ projection that the next significant breakthrough in AI will likely stem from\
    \ improved access to larger, higher-quality, and more inclusive data resources.\n\
    \nIV. WHERE WILL THE NEXT BREAKTHROUGH COME FROM?\n\nWhen we examine the major\
    \ AI breakthroughs of the past fifteen years, we find that two core drivers underpinned\
    \ most of them: the scaling of data and advances in computational power. While\
    \ researchers occasionally introduced structural innovations in algorithms, most\
    \ improvements focused on utilizing larger datasets more effectively. For instance,\
    \ the progress observed in the GPT series resulted not from radical architectural\
    \ change but from scalable data usage and expanded pretraining strategies.\n\n\
    As Andrew Trask, a leading AI researcher, emphasized [18], anyone aiming to anticipate\
    \ the next wave of breakthroughs must begin with this question:\n\n\"Over the\
    \ next 5–10 years, how can we increase the available data by a factor of 10 to\
    \ 1000?\"\n\nThis question transcends technical curiosity. It outlines a strategic\
    \ vision that will shape the future direction of AI research.\n\n### **Talent,\
    \ Hardware, or Data?**\n\nTraining researchers capable of building advanced algorithms\
    \ takes years. Pioneers like Geoffrey Hinton, Yann LeCun, and Yoshua Bengio shaped\
    \ the field through decades of cumulative contributions. However human talent\
    \ grows at a natural limit even a 10% annual increase would represent an optimistic\
    \ estimate. As such, breakthroughs rooted in algorithmic novelty will likely progress\
    \ too slowly to drive near-term leaps [14].\n\nSimilarly, compute performance\
    \ faces physical and economic ceilings. Moore's Law is approaching saturation,\
    \ and GPU throughput typically increases by just 2-4x per year. Although companies\
    \ like NVIDIA offer scalable cloud-based infrastructure, a 1000x increase in computational\
    \ throughput remains unlikely in the short term. Promising technologies like quantum\
    \ computing and analog computation have not yet reached operational or commercial\
    \ maturity [15].\n\n#### **The Key to the Next Breakthrough: Data Volume and Access**\n\
    \nGiven these constraints, future breakthroughs will most likely originate not\
    \ from better algorithms or hardware, but from data itself. However, success will\
    \ not depend solely on more data it will require new types of data, novel access\
    \ protocols, and the ability to analyze previously inaccessible or sensitive datasets.\n\
    \nApproaches such as federated learning, privacy-preserving computation, and synthetic\
    \ data generation offer promising pathways to unlock these new data regimes [16].\
    \ These techniques help researchers expand the usable data ecosystem while preserving\
    \ privacy and ensuring scalability.\n\nIn this light, we can state the following\
    \ with confidence:\n\nThe next significant breakthrough in AI will stem from leveraging\
    \ larger, more diverse, and more accessible datasets through well-designed utilization\
    \ strategies, rather than relying solely on advances in algorithmic power.\n\n\
    ## **A Policy and Strategy Perspective**\n\nThis insight extends beyond technical\
    \ forecasting. It offers concrete strategic guidance for both public and private\
    \ stakeholders. If governments and institutions implement data-centric public\
    \ policies such as open data standards, interoperable sharing protocols, and privacy-preserving\
    \ infrastructure then researchers will gain the tools to develop simpler yet powerful\
    \ models that reduce sample complexity.\n\nTherefore, the central thesis of this\
    \ article \"More data leads to stronger AI\" does not merely describe the past.\
    \ It articulates a core principle that will define the next frontier of AI development.\n\
    \n#### V. TRANSITIONING TOWARD DATA-CENTRIC APPROACHES IN AI\n\nArtificial intelligence\
    \ continues to evolve through a fundamental shift in its guiding paradigm. While\
    \ earlier advancements gained momentum through expanded computational capacity\
    \ and algorithmic improvements, current progress increasingly depends on data\
    \ as the central driving force. The effectiveness of a system now hinges on how\
    \ it harnesses computational resources and, more importantly, how it integrates\
    \ data into the learning process. This transition toward data-centric AI marks\
    \ a pivotal transformation one that defines the direction of future development\
    \ rather than offering a mere alternative.\n\nOpen data sources have begun to\
    \ diminish significantly. Many platforms such as Reddit, X (formerly Twitter),\
    \ and news websites have started restricting access to their content or closing\
    \ it off entirely. These restrictions directly limit the diversity and volume\
    \ of large-scale training datasets harvested from the web. Although synthetic\
    \ data generated through simulation environments (such as visual data from video\
    \ games) can prove effective in some domains, they fall short in fields that require\
    \ real-world complexity and contextual nuance such as natural language processing,\
    \ biomedical analysis, and socioeconomic modeling.\n\nIn this context, private\
    \ datasets such as hospital records, internal corporate documents, financial transactions,\
    \ and government archives represent the most valuable and meaningful data sources\
    \ for future AI breakthroughs. Yet accessing and utilizing these datasets introduces\
    \ not only technical but also ethical, legal, and political challenges. Regulatory\
    \ frameworks like the European Union's General Data Protection Regulation (GDPR),\
    \ Turkey's KVKK, and similar data protection laws have placed considerable constraints\
    \ on data sharing. In parallel, institutions face heightened concerns about competitive\
    \ risk, data leakage, and public trust, which further reinforce restrictive data\
    \ policies.\n\nAt this crossroads, the data-centric mindset redefines AI not as\
    \ a quest to accumulate more data at any cost, but as an effort to access data\
    \ ethically, securely, and under regulatory oversight. We now stand at the threshold\
    \ of a shift from the age of open data to the age of private data. The new imperative\
    \ is to center data in AI workflows without centralizing it physically. This requirement\
    \ compels the development of new technological infrastructures and responsible\
    \ access mechanisms.\n\n![](_page_5_Figure_5.jpeg)\n\n*Figure 4: Data Ecosystem\
    \ Evolution Diagram*\n\nFigure 4 illustrates this transformation. On the left\
    \ side of the diagram, open data repositories (e.g., publicly available datasets)\
    \ represent the past; on the right, private data vaults (e.g., hospital systems,\
    \ enterprise platforms) symbolize the future. The arrow connecting these domains\
    \ conveys the transition one that emphasizes secure, controlled, and privacy-preserving\
    \ access.\n\nThis transition demonstrates that AI must focus not only on the quantity\
    \ of data but also on its quality and contextual meaning. Open data often lacks\
    \ structure, completeness, or contextual richness. In contrast, private data tends\
    \ to be more consistent, labeled, and semantically dense factors that significantly\
    \ influence model performance. Table 1 presents a comparative view of open versus\
    \ private data sources:\n\n| Table 1: Comparative view of Open versus Private\
    \ data |  |\n|-------------------------------------------------------|--|\n| sources\
    \                                               |  |\n\n| Attribute          \
    \    | Open Data           | Private Data        |\n|------------------------|---------------------|---------------------|\n\
    |                        | Decreasing;         | Generally           |\n| Accessibility\
    \          | subject to platform | abundant within     |\n|                  \
    \      | restrictions        | specific domains    |\n|                      \
    \  | Varied; often       | Higher accuracy;    |\n| Data Quality           | noisy\
    \ and           | context-rich and    |\n|                        | unstructured\
    \        | labeled             |\n| Legal/Ethical<br>Risks | Low; minimal    \
    \    | High; demands       |\n|                        | compliance          |\
    \ strict adherence to |\n|                        | requirements        | privacy\
    \ regulations |\n| Access                 | Open or platform    | Strict; controlled\
    \  |\n| Control                | limited             | by data owners      |\n\
    \nAs a result, the data-centric AI paradigm calls for an interdisciplinary framework.\
    \ Engineering knowledge alone no longer suffices. Fields such as law, ethics,\
    \ public policy, and the life sciences must become active collaborators in shaping\
    \ AI systems of the future.\n\n#### VI. EMERGING SOLUTIONS FOR DATA PRIVACY AND\
    \ ACCESS IN AI\n\nData serves as the foundation of AI systems, yet its practical\
    \ value depends on the ability to use it effectively. In fields that involve sensitive\
    \ information—such as healthcare, finance, and enterprise operations—data usage\
    \ brings substantial challenges related to privacy, legal compliance, and cybersecurity.\
    \ As AI development progresses, its success increasingly depends on managing not\
    \ just data access, but also the context, location, and governance under which\
    \ that access occurs. In response, researchers have proposed a range of solutions\
    \ designed to address these challenges at their core.\n\n#### *Federated Learning*\n\
    \nFederated learning enables model training without transferring raw data to a\
    \ central server. Instead, each data holder (e.g., a hospital, financial institution,\
    \ or individual device) updates the model locally using its own data. It then\
    \ shares only the updated model parameters. A central coordinator aggregates these\
    \ local updates to produce a global model.\n\nThe process that demonstrated it\
    \ Figure 5, ensures that sensitive data never leaves its origin. It preserves\
    \ user privacy and facilitates compliance with legal frameworks by design.\n\n\
    ![](_page_6_Figure_1.jpeg)\n\n#### *Figure 5: Federated Learning Model Training\
    \ Process*\n\nThis approach proves especially effective in distributed yet privacy-sensitive\
    \ environments such as mobile devices, IoT systems, and hospital networks. However,\
    \ the success of federated learning depends on several factors: the heterogeneity\
    \ of data distributions, the quality of local model updates, and the integrity\
    \ of secure aggregation methods. As such, federated learning demands a new mindset\
    \ one that rethinks conventional algorithmic design.\n\n#### *Privacy-Enhancing\
    \ Technologies (PETs)*\n\nPETs encompass a suite of technologies designed to guarantee\
    \ privacy throughout the data processing pipeline. This category includes advanced\
    \ methods such as differential privacy, homomorphic encryption, and secure multi-party\
    \ computation (SMPC).\n\nHomomorphic encryption allows computations to run directly\
    \ on encrypted data. Users can extract insights without ever exposing the raw\
    \ content. SMPC enables multiple parties to perform joint computations without\
    \ revealing their individual datasets. These methods are no longer purely theoretical\
    \ constructs. In domains that demand stringent compliance such as healthcare,\
    \ defense, and finance researchers have begun deploying PETs as viable, real-world\
    \ solutions.\n\n## *DataSite Paradigm*\n\nThe DataSite Paradigm, developed by\
    \ Andrew Trask and the OpenMined team [19], reimagines how researchers access\
    \ data. Inspired by classical web architecture but redesigned with data privacy\
    \ as its core, this paradigm prevents direct downloads of raw datasets. Instead,\
    \ researchers send their code to the data site. After undergoing inspection and\
    \ approval by the data owner, the code executes locally on the server. Only the\
    \ computed output not the raw data returns to the researcher.\n\nThis method adheres\
    \ to the principle: \"Send the code to the data, not the data to the code.\" Rather\
    \ than transferring sensitive datasets, researchers send models or queries to\
    \ where the data resides. This structure preserves data locality while enabling\
    \ secure and accountable analysis.\n\nFrameworks like PySyft [20] support this\
    \ workflow, integrating PETs with functionalities such as remote execution, audit\
    \ logging, project governance, and automated policy enforcement. For instance,\
    \ when a researcher submits code to a data site, the system can automatically\
    \ verify compliance with predefined data policies removing the burden of manual\
    \ review from the data steward. This automation enhances both security and scalability.\n\
    \n#### *Network Logic and the DNS Analogy*\n\nData sites do not function in isolation\
    \ they belong to a broader data network. We compare this structure to the Domain\
    \ Name System (DNS) just as DNS resolves domain names into machine-readable addresses,\
    \ a data site network enables researchers to discover, access, and interact with\
    \ a decentralized array of private datasets.\n\nFigure 6 outlines the core operational\
    \ logic:\n\n- The data owner uploads the dataset to a local server and defines\
    \ access protocols.\n- The researcher submits analytical code to the data site.\n\
    - The code undergoes manual or automated review.\n- Upon approval, the system\
    \ executes the code on the local server.\n- The researcher receives either the\
    \ summary output or a distilled version of the model.\n\n#### *Algorithm 1: Data\
    \ Site Code Flow*\n\n| START                                           |\n|-------------------------------------------------|\n\
    | // 1. Data owner uploads the data to the system |\n| DataOwner.upload(data)\
    \                          |\n| DataOwner.defineAccessProtocol(protocol)     \
    \   |\n| // 2. Researcher submits their code             |\n| Researcher.submitCode(code)\
    \                     |\n| // 3. Code is reviewed                          |\n\
    | IF isCodeCompliant(code) == TRUE THEN           |\n| // 4. Code is executed\
    \ on the local server      |\n| result = executeLocally(code, data)          \
    \   |\n| // 5. Summary result is sent to the researcher  |\n| sendToResearcher(resultSummary(result))\
    \         |\n| ELSE                                            |\n| REJECT code\
    \ WITH explanation                    |\n| ENDIF                             \
    \              |\n| END                                             |\n\nThroughout\
    \ this process in Algorithm 1, the system logs every step, and no analysis proceeds\
    \ without the explicit approval of the data owner. This structure ensures a transparent,\
    \ auditable, and scalable framework.\n\n#### **Use Case: DataSite in Healthcare**\n\
    \nConsider a hospital that deploys a DataSite on its own infrastructure and uploads\
    \ patient health records into the system. The Ministry of Health sends the following\
    \ query to the DataSite:\n\n*result = df[(df[\"age\"] > 65) & (df[\"diagnosis\"\
    ] == \"heart attack\")].shape[0] / df.shape[0]*\n\nThis code calculates the proportion\
    \ of elderly patients diagnosed with heart attacks. Once the data owner reviews\
    \ and approves the code, the system executes it locally. The researcher receives\
    \ only the computed result such as \"12.4%\" without accessing any raw patient\
    \ data. This approach ensures data remains on-site, analysis proceeds securely,\
    \ and both ethical and legal compliance are maintained.\n\n![](_page_7_Figure_1.jpeg)\n\
    \n*Figure 6: Data Site Paradigm Flow Diagram*\n\nThis approach shown in Table\
    \ 2, provides a well-structured foundation for compliance reporting and supervisory\
    \ technologies in highly regulated sectors such as public administration, finance,\
    \ and healthcare. It enables organizations to conduct audits and data analyses\
    \ without ever extracting raw data. Institutions can thus preserve their data\
    \ sovereignty while contributing to scientific progress.\n\n*Table 2: Comparison\
    \ of Traditional and Modern Approaches*\n\n| Attribute                | Traditional<br>Centralized<br>Model\
    \          | Federated Learning /<br>DataSite Approach                      |\n\
    |--------------------------|----------------------------------------------|----------------------------------------------------------------|\n\
    | Data<br>Movement         | Transfers data<br>to a centralized<br>server | Keeps\
    \ data in place;<br>transfers code or<br>model weights     |\n| Privacy<br>Risk\
    \          | High (data may<br>be exposed)                | Low (raw data remains<br>local)\
    \                                |\n| Regulatory<br>Compliance | Often<br>problematic\
    \                         | Aligned with GDPR,<br>KVKK, and similar<br>frameworks\
    \          |\n| Control<br>Distribution  | Centralized<br>authority          \
    \           | Distributed; operates<br>with data owner's<br>explicit consent |\n\
    \nIn conclusion, solutions that balance data privacy and data access do not merely\
    \ represent technical optimizations they constitute foundational shifts that will\
    \ shape the future of AI. These frameworks not only ensure legal compliance but\
    \ also support scalable, trustworthy collaboration across institutions and even\
    \ national borders.\n\n### VII. MOCK AND SYNTHETIC DATA: PRACTICAL APPLICATIONS\
    \ AND LIMITATIONS\n\nUp to this point, we have argued that building artificial\
    \ intelligence systems involves much more than designing algorithms or training\
    \ models. The type, quality, and accessibility of data used in this process play\
    \ a decisive role in shaping a model's performance. However, in domains where\
    \ direct access to real data proves infeasible such as healthcare, finance, or\
    \ user-specific systems researchers and developers frequently turn to two practical\
    \ alternatives: mock data and synthetic data.\n\n**Mock Data:** Mock data refers\
    \ to artificial datasets that replicate the structural characteristics of real\
    \ data while containing randomly generated or anonymized values. Developers typically\
    \ use mock data to test system architectures, verify API endpoints, validate visualization\
    \ pipelines, or ensure user interfaces run without crashing. Common use cases\
    \ for mock data include:\n\n- Preventing system failures during early-stage software\
    \ testing,\n- Prototyping applications before real data becomes available,\n-\
    \ Observing interface behavior when data access is restricted.\n\nHowever, mock\
    \ data generally lacks semantic content and fails to reflect real-world dynamics.\
    \ For instance, mock ECG signals do not encode medically relevant patterns such\
    \ as heart rhythm, waveform morphology, or frequency. As a result, mock data has\
    \ limited application and primarily supports early development phases rather than\
    \ model training or evaluation.\n\n**Synthetic Data:** Synthetic data is artificially\
    \ generated by algorithms that model the statistical properties of a specific\
    \ data type. Although it mimics real-world structure, synthetic data does not\
    \ correspond to any actual user or event. When it maintains statistical similarity\
    \ to real data, synthetic data can effectively support training, validation, and\
    \ performance benchmarking.\n\nKey advantages of synthetic data include:\n\n-\
    \ The ability to approximate real-world outcomes,\n- Reduced privacy risk,\n-\
    \ Generation of diverse datasets for various use cases,\n- Shareability and publication\
    \ under ethical frameworks.\n\nFor example, if a hospital cannot release actual\
    \ patient records, researchers may create a synthetic dataset that matches the\
    \ statistical distributions of the hospital's data. This allows ongoing development\
    \ and testing of models without violating privacy. In domains like ECG signal\
    \ processing, MRI analysis, or biometric sensor data, mathematically modeled synthetic\
    \ signals make it possible to develop high-performing models while maintaining\
    \ ethical safeguards.\n\nNevertheless, synthetic data has several limitations:\n\
    \n It may fail to capture the full range of variance found in real-world events,\n\
    \n- Approaching hyper-realism can introduce reidentification risk,\n- Its generation\
    \ process can be computationally intensive and complex.\n\n| Attribute | Mock\
    \ Data         | Synthetic Data         |\n|-----------|-------------------|------------------------|\n\
    | Data      | Randomly          | Statistical properties |\n| Source    | generated,\
    \        | derived from real data |\n|           | template-based    |       \
    \                 |\n| Purpose   | Testing,          | Model training,       \
    \ |\n|           | prototyping,      | analysis, scenario     |\n|           |\
    \ system validation | simulation             |\n| Realism   | Low            \
    \   | Medium to high         |\n| Privacy   | Very low          | Moderate (if\
    \ overly    |\n| Risk      |                   | similar to real data)  |\n\n\
    *Table 3: Comparison of Mock Data and Synthetic Data*\n\nThe key distinction between\
    \ mock and synthetic data (Table 3) lies in their purpose and stage of application.\
    \ Developers use mock data during early development phases to bypass system-level\
    \ issues, while synthetic data supports model learning even in the absence of\
    \ real datasets providing higher functional value.\n\nToday, with the increasing\
    \ adoption of PETs and federated learning, synthetic data generation has become\
    \ a strategic method at the enterprise level. For instance, tools such as ECGSYN,\
    \ developed at MIT [17], generate synthetic ECG signals that mimic real cardiac\
    \ activity. These tools enable medical research to proceed without encountering\
    \ legal or ethical roadblocks.\n\n#### VIII. IMPLICATIONS FOR FUTURE AI INFRASTRUCTURE\
    \ AND RESEARCH DIRECTIONS\n\nThe evolution of artificial intelligence affects\
    \ far more than just system architectures. It also influences how developers build,\
    \ deploy, and govern AI systems. Future breakthroughs will require more than just\
    \ powerful algorithms or large datasets. They must emerge from infrastructures\
    \ that are secure, scalable, and politically sustainable.\n\n#### *The Source\
    \ of Future Breakthroughs: Data Quality and Security*\n\nTraining large-scale\
    \ models once posed the central challenge in AI. Today, the critical bottleneck\
    \ lies in accessing high quality, ethically usable data. With diminishing availability\
    \ of open data and increasing restrictions around institutional datasets, researchers\
    \ now face direct limitations. As a result, future breakthroughs will depend not\
    \ only on how much data systems can use, but also on how that data is accessed\
    \ and processed.\n\nTechnologies such as PETs, federated learning, and DataSite\
    \ architectures offer foundational responses to this challenge. Yet their success\
    \ hinges not only on technical feasibility but also on regulatory approval, institutional\
    \ readiness, and public trust. In this regard, public policy, civil society initiatives,\
    \ and the open-source movement will all play defining roles.\n\n# *Transformation\
    \ in Research and Industrial Practice*\n\nThese emerging infrastructures redefine\
    \ the research pipeline itself. Instead of downloading datasets and training models\
    \ on centralized servers, researchers must now adopt distributed, code-based access\
    \ methods. This shift requires a full rethinking of algorithm design, evaluation\
    \ procedures, and deployment strategies.\n\nIn industry, the same transformation\
    \ will drive the emergence of data-local, privacy-preserving business models.\
    \ Especially in sectors like healthcare, finance, and law where data sharing becomes\
    \ nearly impossible companies will need to run AI workloads within the data owner's\
    \ environment.\n\nThis trend will inevitably lead to new institutional investments.\
    \ Future AI ecosystems will depend on advanced GPU clusters, private data vaults,\
    \ federatedcompatible device networks, and PET-enabled software stacks. In the\
    \ long term, quantum computing and analog computation may further scale these\
    \ infrastructures and push the boundaries of what AI systems can achieve.\n\n\
    #### *Future Research Directions*\n\nFuture research in AI will likely concentrate\
    \ on three key domains:\n\n- Improving Federated Learning Algorithms: Researchers\
    \ will aim to enhance efficiency and stability for systems operating across heterogeneous\
    \ devices, non-iid data distributions, and limited compute environments.\n- Advancing\
    \ PET Frameworks: Work will focus on developing lighter, faster, and more deployable\
    \ variants of techniques such as homomorphic encryption, differential privacy,\
    \ and SMPC. This effort includes both algorithmic optimization and software-hardware\
    \ integration.\n- Enhancing Realism in Synthetic Data Generation: Researchers\
    \ will explore domain-specific generative models that mimic real data without\
    \ posing reidentification risks. Techniques such as GANs, VAEs, and physics-informed\
    \ simulators will lead this evolution.\n\nThe next generation of AI infrastructure\
    \ must prioritize data security, uphold ethical principles, operate in technically\
    \ distributed environments, comply with legal frameworks, and remain socially\
    \ transparent. Achieving this vision will require deep coordination across academia,\
    \ industry, and regulatory bodies ensuring that breakthroughs remain technically\
    \ impactful and socially sustainable.\n\n# IX. CONCLUSION\n\nThis article examined\
    \ the evolution of artificial intelligence over the past fifteen years from a\
    \ multidimensional perspective, systematically evaluating the core driving forces:\
    \ computational capacity, data volume, and algorithmic innovation. Each breakthrough,\
    \ when viewed through a historical lens, represented not just a technical advancement\
    \ but also a structural paradigm shift that redefined the trajectory of AI research.\n\
    \nThe journey that began in 2009 with the introduction of GPUs for deep learning,\
    \ continued through the data abundance enabled by ImageNet, the deep learning\
    \ revolution sparked by AlexNet, and the peak of algorithmic efficiency brought\
    \ by the Transformer architecture. Models like the GPT series and ChatGPT demonstrated\
    \ how selfsupervised learning on massive datasets could lead to transformative\
    \ results.\n\nThese developments highlight a key pattern: AI breakthroughs frequently\
    \ arose where abundant data aligned with advances in learning algorithm efficiency.\
    \ However, with open data resources dwindling and private data becoming increasingly\
    \ inaccessible, sustaining future breakthroughs will require a shift in data infrastructure\
    \ strategies.\n\nStrategic priorities for the future should focus on the following:\n\
    \n- Building ethical and policy frameworks that define who uses data, how, and\
    \ under what conditions ensuring transparency, accountability, and inclusiveness,\n\
    - Expanding collaboration protocols between researchers and institutions to facilitate\
    \ secure data access and sharing,\n- Evaluating algorithms not only on accuracy\
    \ but also on fairness, security, and sustainability.\n\nIn conclusion, the future\
    \ of AI does not rest solely on greater computational power or larger models.\
    \ It depends on better-organized, secure, ethical, and accessible data ecosystems.\
    \ Establishing these ecosystems will require multidisciplinary collaboration,\
    \ bringing together engineering, law, ethics, the social sciences, and public\
    \ policy as integral components. The most critical factor in steering the next\
    \ wave of AI breakthroughs will no longer be just \"how much data we have,\" but\
    \ \"how we use data ethically, securely, and in the right context.\"\n\n#### REFERENCES\n\
    \n- [1] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient-based learning\
    \ applied to document recognition,\" in Proceedings of the IEEE, vol. 86, no.\
    \ 11, pp. 2278-2324, Nov. 1998, doi: 10.1109/5.726791\n- [2] Shalev-Shwartz S,\
    \ Ben-David S. Understanding Machine Learning: From Theory to Algorithms. Cambridge\
    \ University Press; 2014.\n- [3] Raina, R., Madhavan, A., & Ng, A. Y. (2009).\
    \ Largescale deep unsupervised learning using graphics processors. In Proceedings\
    \ of the 26th Annual International Conference on Machine Learning (ICML '09) (pp.\
    \ 873–880).\n\nhttps://doi.org/10.1145/1553374.1553486\n\n- [4] Deng, J., Dong,\
    \ W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). ImageNet: A large-scale\
    \ hierarchical image database. In 2009 IEEE Conference on Computer Vision and\
    \ Pattern Recognition (pp. 248– 255). https://doi.org/10.1109/CVPR.2009.5206848\n\
    - [5] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification\
    \ with deep convolutional neural networks. In Advances in Neural Information Processing\
    \ Systems (NeurIPS) 25, 1097–1105. https://papers.nips.cc/paper\\_files/paper/2012/file/c39\
    \ 9862d3b9d6b76c8436e924a68c45b-Paper.pdf\n- [6] Mikolov, T., Chen, K., Corrado,\
    \ G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector\
    \ Space. arXiv preprint arXiv:1301.3781. https://arxiv.org/abs/1301.3781\n- [7]\
    \ Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,\
    \ D., & Riedmiller, M. (2015). Human-level control through deep reinforcement\n\
    \nlearning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n\n\
    - [8] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche,\
    \ G., & Hassabis, D. (2016). Mastering the game of Go with deep neural networks\
    \ and tree search. nature, 529(7587), 484-489. https://doi.org/10.1038/nature16961\n\
    - [9] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.\
    \ N., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural\
    \ Information Processing Systems (NeurIPS) 30, 5998– 6008. https://arxiv.org/abs/1706.03762\n\
    - [10]Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving\
    \ Language Understanding by Generative Pre-Training. OpenAI. [https://cdn.openai.com/research-covers/language](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)[unsupervised/language\\\
    \\_understanding\\\\_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n\
    - [11]Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).\
    \ Language Models are Unsupervised Multitask Learners. OpenAI. https://cdn.openai.com/better-languagemodels/language\\\
    _models\\_are\\_unsupervised\\_multitas k\\_learners.pdf\n- [12]Brown, T. B.,\
    \ Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P. & Amodei, D. (2020).\
    \ Language models are few-shot learners. In Advances in Neural Information Processing\
    \ Systems (NeurIPS) 33, 1877–1901. https://arxiv.org/abs/2005.14165\n- [13]Ouyang,\
    \ L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe,\
    \ R. (2022). Training language models to follow instructions with human feedback.\
    \ Advances in neural information processing systems, 35, 27730-27744.\n- [14]\
    \ LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553),\
    \ 436–444. https://doi.org/10.1038/nature14539\n- [15] Arute, F., Arya, K., Babbush,\
    \ R., Bacon, D., Bardin, J. C., Barends, R., ... & Martinis, J. M. (2019). Quantum\
    \ supremacy using a programmable superconducting processor. Nature, 574(7779),\
    \ 505– 510[. https://doi.org/10.1038/s41586-019-1666-5](https://doi.org/10.1038/s41586-019-1666-5)\n\
    - [16]Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji,\
    \ A. N., ... & Zhao, S. (2019). Advances and open problems in federated learning.\
    \ arXiv preprint arXiv:1912.04977. <https://doi.org/10.48550/arXiv.1912.04977>\n\
    - [17]McSharry PE, Clifford GD, Tarassenko L, Smith L. A dynamical model for generating\
    \ synthetic electrocardiogram signals. IEEE Transactions on Biomedical Engineering\
    \ 50(3): 289-294; March 2003.\n- [18]Adler, S., Hitzig, Z., Jain, S., Brewer,\
    \ C., Chang, W., DiResta, R., ... & Zick, T. (2024). Personhood credentials: Artificial\
    \ intelligence and the value of privacy-preserving tools to distinguish who is\
    \ real online. arXiv preprint arXiv:2408.07892.\n- [19]**OpenMined Foundation.**\
    \ (2024, November 28). *Datasite server documentation*. OpenMined. [https://docs.openmined.org/en/latest/components/data](https://docs.openmined.org/en/latest/components/datasite-server.html)\
    \ [site-server.html](https://docs.openmined.org/en/latest/components/datasite-server.html)\n\
    - [20]**OpenMined.** (2025, February). *PySyft (Version 0.9.5)* <https://github.com/OpenMined/PySyft>"
- title: "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use\n  Capabilities\
    \ in Large Language Models"
  abstract: 'As Large Language Models (LLMs) evolve from passive text generators to
    active

    reasoning agents capable of tool interaction, the Model Context Protocol (MCP)

    has emerged as a standardized framework for dynamic tool discovery and

    orchestration. Despite widespread industry adoption, existing evaluation

    methodologies fail to adequately assess tool utilization capabilities within

    this new paradigm. This paper introduces MCP-RADAR, the first comprehensive

    benchmark specifically designed to evaluate LLM performance in the MCP

    framework through a novel five-dimensional approach measuring: answer accuracy,

    tool selection efficiency, computational resource efficiency, parameter

    construction accuracy, and execution speed. Unlike conventional benchmarks that

    rely on subjective human evaluations or binary success metrics, MCP-RADAR

    employs objective, quantifiable measurements across multiple task domains

    including software engineering, mathematical reasoning, and general

    problem-solving. Our evaluations of leading commercial and open-source LLMs

    reveal distinctive capability profiles with significant trade-offs between

    accuracy, efficiency, and speed, challenging traditional single-metric

    performance rankings. Besides, we provide valuable guidance for developers to

    optimize their tools for maximum model compatibility and effectiveness. While

    focused on MCP due to its standardized approach, our methodology remains

    applicable across all LLM agent tool integration frameworks, providing valuable

    insights for both LLM developers and tool creators to optimize the entire

    LLM-tool interaction ecosystem. The implementation, configurations, and

    datasets used in our evaluation are publicly available at

    https://anonymous.4open.science/r/MCPRadar-B143.'
  url: http://arxiv.org/abs/2505.16700v1
  keywords: ''
  document: "# MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities\
    \ in Large Language Models\n\nXuanqi Gao<sup>∗</sup> Xi'an Jiaotong University\
    \ Xi'an, China gxq2000@stu.xjtu.edu.cn\n\nSiyi Xie<sup>∗</sup> Xi'an Jiaotong\
    \ University Xi'an, China xsy2225025620@stu.xjtu.edu.cn\n\nJuan Zhai University\
    \ of Massachusetts at Amherst Amherst, USA juanzhai@umass.edu\n\nShqing Ma University\
    \ of Massachusetts at Amherst Amherst, USA shiqingma@umass.edu\n\nChao Shen Xi'an\
    \ Jiaotong University Xi'an, China chaoshen@mail.xjtu.edu.cn\n\n### Abstract\n\
    \nAs Large Language Models (LLMs) evolve from passive text generators to active\
    \ reasoning agents capable of tool interaction, the Model Context Protocol (MCP)\
    \ has emerged as a standardized framework for dynamic tool discovery and orchestration.\
    \ Despite widespread industry adoption, existing evaluation methodologies fail\
    \ to adequately assess tool utilization capabilities within this new paradigm.\
    \ This paper introduces MCP-RADAR, the first comprehensive benchmark specifically\
    \ designed to evaluate LLM performance in the MCP framework through a novel five-dimensional\
    \ approach measuring: answer accuracy, tool selection efficiency, computational\
    \ resource efficiency, parameter construction accuracy, and execution speed. Unlike\
    \ conventional benchmarks that rely on subjective human evaluations or binary\
    \ success metrics, MCP-RADAR employs objective, quantifiable measurements across\
    \ multiple task domains including software engineering, mathematical reasoning,\
    \ and general problem-solving. Our evaluations of leading commercial and open-source\
    \ LLMs reveal distinctive capability profiles with significant trade-offs between\
    \ accuracy, efficiency, and speed, challenging traditional single-metric performance\
    \ rankings. Besides, we provide valuable guidance for developers to optimize their\
    \ tools for maximum model compatibility and effectiveness. While focused on MCP\
    \ due to its standardized approach, our methodology remains applicable across\
    \ all LLM agent tool integration frameworks, providing valuable insights for both\
    \ LLM developers and tool creators to optimize the entire LLM-tool interaction\
    \ ecosystem. The implementation, configurations, and datasets used in our evaluation\
    \ are publicly available at <https://anonymous.4open.science/r/MCPRadar-B143>.\n\
    \n<sup>∗</sup>These authors contributed equally to this work.\n\n# 1 Introduction\n\
    \nLarge Language Models (LLMs) are rapidly evolving from passive text generators\
    \ to active reasoning agents capable of interacting with external tools and APIs.\
    \ This transformation, driven by function calling capabilities, has recently culminated\
    \ in the introduction of the Model Context Protocol (MCP) - a standardized protocol\
    \ for LLM-tool interaction that enables dynamic discovery and orchestration of\
    \ tools based on task requirements. As MCP adoption accelerates across industry\
    \ leaders such as Anthropic [\\[1\\]](#page-9-0), OpenAI [\\[2\\]](#page-10-0),\
    \ and Alibaba [\\[3\\]](#page-10-1), there is a pressing need for standardized\
    \ evaluation frameworks to assess how effectively different models leverage this\
    \ new paradigm.\n\nThe Model Context Protocol represents a significant architectural\
    \ advancement over previous tool integration approaches. Unlike traditional function\
    \ calling, which requires predefined tool configurations and manual API connections,\
    \ MCP provides a unified framework where AI applications can dynamically discover,\
    \ select, and orchestrate tools through a standardized interface. This approach\
    \ not only simplifies development workflows but fundamentally transforms how LLMs\
    \ interact with external systems - enabling more flexible, context-aware tool\
    \ utilization that adapts to specific task requirements rather than following\
    \ rigid, predetermined pathways.\n\nExisting evaluation methodologies for LLMs\
    \ primarily focus on knowledge-based reasoning [\\[4,](#page-10-2) [5\\]](#page-10-3),\
    \ instruction following [\\[6,](#page-10-4) [7\\]](#page-10-5), and creative generation\
    \ [\\[8,](#page-10-6) [9\\]](#page-10-7), with limited attention to tool utilization\
    \ capabilities. Furthermore, available benchmarks typically rely on subjective\
    \ human evaluations or oversimplified binary success metrics that fail to capture\
    \ the nuanced aspects of effective tool use. This limitation is particularly problematic\
    \ for MCP evaluations, where models must not only select appropriate tools but\
    \ also construct valid parameters, efficiently orchestrate multi-step workflows,\
    \ and transform tool outputs into coherent responses - all while optimizing computational\
    \ resources.\n\n![](_page_1_Figure_4.jpeg)\n\n<span id=\"page-1-0\"></span>Figure\
    \ 1: Overview of MCP-RADAR.\n\nIn this paper, we introduce MCP-RADAR, the first\
    \ comprehensive benchmark specifically designed to evaluate LLM performance within\
    \ the MCP paradigm, as shown in [Figure 1.](#page-1-0) MCP-RADAR employs a novel\
    \ five-dimensional evaluation framework that quantifies model capabilities across:\
    \ (1) answer accuracy, (2) tool selection efficiency, (3) computational resource\
    \ efficiency, (4) parameter construction accuracy, and (5) execution speed. This\
    \ multi-dimensional approach enables researchers and practitioners to generate\
    \ intuitive radar visualizations that highlight the relative strengths and weaknesses\
    \ of different models across diverse task domains. Our primary objective is not\
    \ to establish definitive performance rankings, but rather to provide valuable\
    \ intuition for both LLM developers and MCP ecosystem contributors. For LLM developers,\
    \ MCP-RADAR offers detailed insights into model proficiencies and deficiencies\
    \ across multiple dimensions, guiding targeted improvements in areas such as tool\
    \ selection, parameter handling, and resource efficiency. For MCP developers,\
    \ our benchmark illuminates how different tool designs and implementation patterns\
    \ impact model performance, facilitating the creation of more model-friendly tools\
    \ and documentation. By bridging these communities through shared evaluation metrics,\
    \ we aim to foster collaborative optimization of the entire LLM-tool interaction\
    \ experience.\n\nMCP-RADAR differs from existing LLM benchmarks in several key\
    \ aspects. First, it focuses exclusively on tool-augmented capabilities within\
    \ the standardized MCP framework, allowing for fair comparison across models regardless\
    \ of their underlying architectures. Second, it employs strictly objective metrics\
    \ derived from quantifiable performance data rather than subjective human judgments,\
    \ enhancing reproducibility and reducing evaluation bias. Third, while our initial\
    \ implementation spans\n\nmultiple task domains (software engineering, mathematical\
    \ reasoning, and general problem-solving), the framework is designed with inherent\
    \ extensibility, allowing seamless integration of new domains, tasks, and tools\
    \ as the ecosystem evolves. Finally, it visualizes performance through intuitive\
    \ radar charts that enable rapid identification of model strengths and weaknesses\
    \ across multiple dimensions.\n\nWhile we use MCP as our focal point due to its\
    \ growing prominence and standardized approach, our evaluation methodology is\
    \ designed to be universally applicable across all LLM agent tool integration\
    \ frameworks. Whether tools are integrated via function calling, plugin architectures,\
    \ or other mechanisms, the fundamental capabilities required for effective tool\
    \ utilization remain consistent. Our framework measures these core capabilities\
    \ through objective, quantifiable metrics that transcend specific implementation\
    \ details, making it equally valuable for evaluating any toolaugmented language\
    \ model regardless of the underlying integration method.\n\nOur research results\
    \ indicate that tool use performance varies significantly across different models\
    \ and task domains, with important trade-offs between accuracy, efficiency, and\
    \ speed. Through evaluation of seven mainstream LLMs across three core task domains,\
    \ we found that model performance shows clear domain sensitivity, with no \"universal\
    \ champion\" existing. Large models generally perform better in mathematical reasoning\
    \ domains than in other areas, with an average accuracy of 0.78, while software\
    \ engineering and general problem-solving domains only achieved 0.42 and 0.28\
    \ respectively. Claude 3.7 and Gemini 2.5 Pro lead in accuracy dimensions, particularly\
    \ Gemini 2.5 Pro which reached 0.91 accuracy in the mathematics domain, the highest\
    \ among all models; however, in efficiency metrics, Gemini and Claude perform\
    \ relatively weakly, demonstrating different performance trade-off strategies.\
    \ GPT-4o displays a more balanced pentagon profile, and while not optimal in any\
    \ dimension, it has no obvious shortcomings; open-source models like DeepSeek-V3,\
    \ though lagging in accuracy, are the most resource-efficient models, offering\
    \ significant advantages in resource-constrained scenarios. Particularly noteworthy\
    \ is that all models show an expanding gap between intermediate process performance\
    \ (TFTD and FEP values) and accuracy in the general problem-solving domain, indicating\
    \ that models can partially correctly execute tool calling steps but struggle\
    \ to integrate these steps into correct final results, suggesting that current\
    \ LLMs still have significant room for improvement in tool chain integration capabilities\
    \ for open-ended tasks.\n\nFurthermore, our analysis reveals critical insights\
    \ about the relationship between model capabilities and tool design patterns.\
    \ We identify specific tool implementation strategies that consistently enhance\
    \ model performance across multiple dimensions, as well as common pitfalls that\
    \ create unnecessary barriers to effective tool utilization. These findings provide\
    \ valuable guidance for developers seeking to optimize their tools for maximal\
    \ model compatibility and effectiveness.\n\nOur main contributions include:\n\n\
    - The first systematic evaluation framework for tool-augmented language models\
    \ that employs purely objective, quantifiable metrics across multiple performance\
    \ dimensions.\n- MCP-RADAR, a comprehensive benchmark dataset spanning three critical\
    \ domains: software engineering, mathematical reasoning, and general problem-solving,\
    \ with built-in extensibility for additional domains.\n- Empirical evaluation\
    \ of leading commercial and open-source LLMs, revealing distinctive capability\
    \ profiles that challenge conventional single-metric performance rankings.\n-\
    \ Detailed analysis of performance patterns and their implications for both model\
    \ training and tool design, providing actionable insights for ecosystem optimization.\n\
    - Our implementation, configurations and collected datasets are available at [\\\
    [10\\]](#page-10-8).\n\n# 2 Related work\n\n### 2.1 Agent, tool use and MCP\n\n\
    The Model Context Protocol (MCP) for Large Language Models (LLMs) is a unified\
    \ interaction standard proposed by Anthropic to address systematic challenges\
    \ in tool invocation. Early LLMs such as the GPT family relied only on static\
    \ training data and were unable to access real-time information or interact with\
    \ external systems, resulting in limited applications.Wei et al. [\\[11\\]](#page-10-9)\
    \ demonstrated the role of structured reasoning in improving the performance of\
    \ LLMs, which provided the basis for subsequent tool invocation. With the increase\
    \ of complex scenarios such as multi-round dialog systems, developers try to connect\
    \ to external via API [\\[12\\]](#page-10-10) [\\[13\\]](#page-10-11) [\\[14\\\
    ]](#page-10-12) [\\[15\\]](#page-10-13) to external data sources. Use the tool-enhanced\
    \ LLM [\\[16\\]](#page-10-14) [\\[17\\]](#page-10-15) [\\[18\\]](#page-10-16)\
    \ to try to solve the web browsing [\\[19\\]](#page-11-0) [\\[20\\]](#page-11-1)\
    \ [\\[21\\]](#page-11-2) or code interpretation [\\[22\\]](#page-11-3) [\\[23\\\
    ]](#page-11-4) and other aspects of relevance, but schick et al. [\\[24\\]](#page-11-5)\
    \ points out that this \"peerto-peer\" integration leads to NxM issues, limiting\
    \ system expansion and increasing maintenance costs. While platforms such as Hugging\
    \ Face promote model sharing, and frameworks such as LangChain attempt to enhance\
    \ model capabilities through Tool Calling, these solutions do not address the\
    \ underlying problem. However, these solutions still fail to address the fundamental\
    \ problem. Hsieh et al. [\\[25\\]](#page-11-6)noting that these approaches still\
    \ lack a common context delivery mechanism. It is in this context that the Model\
    \ Context Protocol (MCP) was formally introduced and open sourced.\n\n#### 2.2\
    \ Tool/MCP use evaluation\n\nEvaluating the tool-use capabilities of large language\
    \ models has become an important direction of research in recent years. While\
    \ traditional LLM assessment frameworks focus on language comprehension capabilities,\
    \ with the development of tool-enhanced AI [\\[26\\]](#page-11-7) [\\[24\\]](#page-11-5),\
    \ assessment benchmarks specialized in tool-usage capabilities have become especially\
    \ necessary [\\[27\\]](#page-11-8) [\\[28\\]](#page-11-9) [\\[16\\]](#page-10-14).\
    \ The HELM framework proposed by Liang et al. [\\[29\\]](#page-11-10) attempts\
    \ to establish multidimensional evaluation criteria, but still does not focus\
    \ on the efficiency and effectiveness of tool interaction protocols. Existing\
    \ tool usage evaluation either struggles to handle more complex task scenarios\
    \ such as long context memory calls, multiple rounds or multi-tool call tests\
    \ [\\[30\\]](#page-11-11) [\\[16\\]](#page-10-14) [\\[27\\]](#page-11-8)[\\[22\\\
    ]](#page-11-3)[\\[15\\]](#page-10-13)[\\[14\\]](#page-10-12), or a single path\
    \ of standardized answers to task scenarios that do not match real-world user\
    \ needs [\\[31\\]](#page-11-12). At the same time, all of these evaluation systems\
    \ lack a systematic assessment of the model's ability to use a specific protocol\
    \ (e.g., MCP). At the practical application level, there is a lack of unified\
    \ evaluation criteria to measure the model's fitness with MCPs, the performance\
    \ impact of MCPs on large models, and the efficiency of multi-MCP collaboration.\
    \ In this paper, we introduce MCP-RADAR, a large-scale instruction benchmark,\
    \ to explore the performance of LLMs in a variety of real-world MCP usage scenarios,\
    \ and introduce mathematical, code, and generalized ability tests for MCP tools,\
    \ incorporating metrics such as accuracy, average round accuracy, and task processing\
    \ rate for a more comprehensive evaluation.\n\n### 3 MCP-RADAR\n\n#### 3.1 MCP-RADAR\
    \ Overview\n\nCurrent methods for evaluating large language models' tool utilization\
    \ capabilities often rely on single metrics or subjective judgment, making comprehensive\
    \ performance assessment difficult.\n\nWe introduce MCP-RADAR, a framework based\
    \ on three core principles: objective quantifiability, multidimensional complementarity,\
    \ and domain universality. MCP-RADAR assesses model performance across five key\
    \ dimensions:\n\nResult Accuracy (RA) Tool Call Process Accuracy (TCPA) First\
    \ Error Position (FEP) Computational Resource Efficiency (CRE) Response Time Efficiency\
    \ (RTE)\n\nWe've developed a benchmark test set covering software engineering,\
    \ mathematical reasoning, and general problem-solving domains. Our framework visualizes\
    \ model performance through radar charts, providing intuitive comparison of different\
    \ models' capabilities. Additionally, we design experiments to analyze model sensitivity\
    \ to tool documentation quality, offering empirical guidance for MCP ecosystem\
    \ optimization.\n\n### 3.2 Evaluation Dimensions\n\nMCP-RADAR employs five complementary\
    \ dimensions((RA), (DTSR), (FEP), (CRE) and (RTE)) to comprehensively evaluate\
    \ large language models' tool utilization capabilities in MCP environments, with\
    \ each dimension capturing distinct critical aspects of performance.\n\nThe detailed\
    \ information and calculation methods for these evaluation metrics can be found\
    \ in [Ap](#page-16-0)[pendix D.](#page-16-0)\n\n### 3.3 Benchmark Dataset\n\n\
    To comprehensively evaluate LLMs' tool utilization capabilities in MCP environments,\
    \ we have constructed a benchmark test set encompassing diverse tasks and tools.\
    \ This test set is meticulously designed to ensure broad domain coverage, task\
    \ complexity gradients, and tool description variants, enabling systematic analysis\
    \ of model performance.\n\n### 3.3.1 Task Domains and Distribution\n\nOur benchmark\
    \ test set contains 300 tasks across three core domains, with 100 tasks per domain:\n\
    \n- Software Engineering: This domain includes tasks related to code generation,\
    \ debugging, and code review, requiring models to interact with programming tools\
    \ and APIs.\n- Mathematical Reasoning: This domain encompasses tasks that involve\
    \ mathematical problem-solving, requiring models to utilize mathematical tools\
    \ and libraries.\n- General Problem Solving: This domain encompasses practical,\
    \ everyday tasks including web search, information gathering, research queries,\
    \ and other common real-world problems that require tool-assisted information\
    \ retrieval and processing.\n\nThese tasks are selected from high-quality open-source\
    \ datasets (such as GAIA [\\[32\\]](#page-11-13), GSM8k [\\[8\\]](#page-10-6),\
    \ MATH [\\[9\\]](#page-10-7), HumanEval [\\[33\\]](#page-11-14), MBPP [\\[34\\\
    ]](#page-12-0), etc.) and carefully reconstructed to ensure their suitability\
    \ for MCP tool usage evaluation. Each task undergoes standardized processing,\
    \ including clear inputs, expected outputs, and scoring criteria, thereby supporting\
    \ automated assessment and result verification. Each task undergoes multiple rounds\
    \ of validation to ensure quality:\n\n1. Initial design stage by domain experts\
    \ creating tasks and annotating standard answers;\n\n2. Independent verification\
    \ stage by two different reviewers completing tasks, confirming solution paths\
    \ and standard answer clarity;\n\n3. Consistency checks ensuring multiple reviewers\
    \ obtain identical results using the same tool set, eliminating ambiguous tasks;\n\
    \n4. Difficulty calibration processes ensuring consistency of difficulty levels\
    \ within domains\n\nThis rigorous process ensures high quality and reliability\
    \ of the benchmark test, providing a solid foundation for model evaluation. Each\
    \ task in the test set is equipped with clear inputs, expected outputs, and scoring\
    \ criteria, facilitating automated assessment.\n\n### 3.3.2 Task Complexity Grading\n\
    \nTo better evaluate large model capabilities, we classify tasks by complexity\
    \ from Level 1 (simplest) to Level 3 (most complex), based on the following criteria:\n\
    \nLevel 1 Tasks require single tool calls, simple parameters, no complex reasoning\n\
    \nLevel 2 Tasks require 2-4 tool calls, parameters needing moderate processing,\
    \ multi-step reasoning.\n\nLevel 3 Tasks require more than 4 tool calls, parameters\
    \ needing complex processing, deep reasoning and planning.\n\nThe distribution\
    \ of tasks by complexity in each domain is shown in [Figure 2.](#page-5-0)\n\n\
    ## 3.3.3 MCP Tool Analysis\n\nWe have collected 42 mainstream MCP, covering software\
    \ development tools (GitHub, VSCode, file systems, etc.), mathematical reasoning\
    \ tools (calculators, expression solvers, etc.), and general tools (search engines,\
    \ knowledge bases, etc.). These tools have all been applied in actual MCP ecosystems,\
    \ representing current mainstream tool usage scenarios.\n\n![](_page_5_Figure_0.jpeg)\n\
    \n<span id=\"page-5-0\"></span>Figure 2: Tasks distribution.\n\n![](_page_5_Figure_2.jpeg)\n\
    \nFigure 3: The distribution of tools.\n\nDomain #Tools Library 2 Repository 2\
    \ Calculation 5 ProjectManagement 4 WorkflowExecution 6 WebSearch 4 Terminal 8\
    \ File 7 Directory 4\n\nFigure 4: Tool statistics.\n\n# 4 Experiment\n\nNumber\
    \ of Tasks\n\n# 4.1 Setup\n\nWe designed a series of comparative experiments to\
    \ test the performance of different models across various task scenarios and tool\
    \ configurations. This section details the experimental setup, including model\
    \ selection, environment configuration, evaluation protocols, and data collection\
    \ methods.\n\nWe selected 7 representative large language models for evaluation,\
    \ covering current mainstream commercial closed-source models (Claude 3.7 Sonnet\
    \ [\\[35\\]](#page-12-1), GPT-4o [\\[36\\]](#page-12-2), GPT-4o-mini [\\[37\\\
    ]](#page-12-3), and Gemini 2.5 Pro [\\[38\\]](#page-12-4)) and open-source models\
    \ (Llama 3.3 70B [\\[39\\]](#page-12-5), DeepSeek V3 [\\[40\\]](#page-12-6), and\
    \ Qwen 3 245B [\\[41\\]](#page-12-7)). The model selection covers different architectures,\
    \ scales, and specialization degrees, enabling us to comprehensively compare the\
    \ performance differences of various models in the MCP environment. Each model\
    \ uses the latest version officially provided, ensuring the experimental results\
    \ represent the current technological level.\n\nAdditionally, we built a standardized\
    \ MCP test environment to ensure all models are evaluated under the same conditions.\
    \ MCP servers provide services through standard protocols, with each\n\n<span\
    \ id=\"page-6-0\"></span>\n\n| Task    | Model             | RA↑  | TFTD↑ | FEP↑\
    \ | CRE↓ | RTE↓  |\n|---------|-------------------|------|-------|------|------|-------|\n\
    | Math    | Claude 3.7        | 0.89 | 0.95  | 0.93 | 5925 | 6441  |\n|      \
    \   | Gemini 2.5 Pro    | 0.91 | 0.97  | 0.92 | 6187 | 4171  |\n|         | GPT-4o\
    \            | 0.85 | 0.92  | 0.90 | 5865 | 4725  |\n|         | GPT-4o-mini \
    \      | 0.72 | 0.88  | 0.86 | 5024 | 2845  |\n|         | Qwen3             |\
    \ 0.85 | 0.95  | 0.95 | 5365 | 11053 |\n|         | Deepseek-V3       | 0.59 |\
    \ 0.91  | 0.88 | 1813 | 2975  |\n|         | Llama3            | 0.64 | 0.80 \
    \ | 0.77 | 4838 | 3733  |\n| Coding  | Claude 3.7 Sonnet | 0.58 | 0.57  | 0.78\
    \ | 2529 | 4212  |\n|         | Gemini 2.5 Pro    | 0.54 | 0.55  | 0.69 | 2894\
    \ | 2424  |\n|         | GPT-4o            | 0.46 | 0.48  | 0.58 | 3481 | 3533\
    \  |\n|         | GPT-4o-mini       | 0.32 | 0.41  | 0.62 | 1180 | 2143  |\n|\
    \         | Qwen3             | 0.52 | 0.60  | 0.58 | 1182 | 4506  |\n|      \
    \   | Deepseek-V3       | 0.33 | 0.36  | 0.44 | 1205 | 1451  |\n|         | Llama3\
    \            | 0.21 | 0.22  | 0.24 | 1506 | 1471  |\n| General | Claude 3.7 Sonnet\
    \ | 0.33 | 0.72  | 0.72 | 5214 | 3539  |\n|         | Gemini 2.5 Pro    | 0.46\
    \ | 0.68  | 0.64 | 6255 | 4550  |\n|         | GPT-4o            | 0.27 | 0.31\
    \  | 0.44 | 5603 | 4450  |\n|         | GPT-4o-mini       | 0.13 | 0.29  | 0.33\
    \ | 4883 | 1077  |\n|         | Qwen3             | 0.28 | 0.42  | 0.40 | 4854\
    \ | 12135 |\n|         | Deepseek-V3       | 0.25 | 0.56  | 0.50 | 5183 | 1511\
    \  |\n|         | Llama3            | 0.22 | 0.35  | 0.28 | 2213 | 2014  |\n\n\
    Table 1: Comparison of model performance metrics across three domains of MCP-RADAR\n\
    \nserver offering several related tools. All servers are deployed in the same\
    \ hardware environment to ensure consistency in response time. We use Openrouter\
    \ as the model interface provider to ensure comparability of response times between\
    \ models, and maintain unchanged interface configurations during the experiment\
    \ to eliminate potential impacts from API changes.\n\n#### 4.2 Main Results\n\n\
    [Table 1](#page-6-0) shows the performance of 7 mainstream large language models\
    \ in the three core task domains of MCP-RADAR, including Result Accuracy (RA),\
    \ Dynamic Tool Selection Rate (DTSR), First Error Position (FEP), Computational\
    \ Resource Efficiency (CRE), and Response Time Efficiency (RTE). Lower values\
    \ for CRE and RTE indicate better efficiency.\n\nDomain Performance Variations.\
    \ Models demonstrate significant performance differences across domains. In the\
    \ mathematical reasoning domain, all models achieved the highest RA, averaging\
    \ 0.78, substantially higher than software engineering (0.42) and general problem-solving\
    \ domains (0.28). This indicates current LLMs have more reliable tool usage capabilities\
    \ in structured, well-defined mathematical tasks. Gemini 2.5 Pro achieved the\
    \ highest mathematics RA at 0.91, with Claude 3.7 following at 0.89. Similarly,\
    \ TCPA scores averaged 0.91 in mathematics versus 0.46 and 0.48 in other domains.\
    \ Qwen3 demonstrated exceptional stability in mathematical tasks (TCPA=0.95),\
    \ despite slightly lower final accuracy (RA=0.85).\n\nEfficiency Tradeoffs. Computational\
    \ efficiency varies dramatically between models, revealing potential accuracy-efficiency\
    \ tradeoffs. DeepSeek-V3 demonstrates superior efficiency (CRE=1813 in mathematics),\
    \ requiring significantly fewer tokens than Gemini 2.5 Pro (CRE=6187), but with\
    \ lower accuracy (RA=0.59). For response time, GPT-4o-mini excels across all domains\
    \ (RTE=2143 in software engineering; RTE=1077 in general problems), making it\
    \ suitable for real-time applications. Conversely, Qwen3 exhibits significantly\
    \ slower response times (RTE=12135 in general problems) despite competitive accuracy.\n\
    \nModel Performance Profiles. [Figure 5](#page-7-0) visualizes each model's multidimensional\
    \ performance characteristics. In mathematical reasoning, Gemini 2.5 Pro and Claude\
    \ 3.7 show strong performance in RA and TCPA dimensions, though Claude 3.7 demonstrates\
    \ weaker computational efficiency and response speed. GPT-4o displays a balanced\
    \ pentagon profile with no notable weaknesses. In contrast,\n\n![](_page_7_Figure_0.jpeg)\n\
    \n<span id=\"page-7-0\"></span>Figure 5: Model Performance Comparison Across Tasks.\
    \ Longer edges indicate superior performance in each metric.\n\nLlama3 shows higher\
    \ computational efficiency but lower accuracy. In the coding domain, all profiles\
    \ generally contract, with average RA dropping to 0.42, indicating increased difficulty\
    \ in tool usage. The general problem-solving domain reveals the most significant\
    \ model differences, with Gemini 2.5 Pro's RA (0.46) outperforming the second-place\
    \ Claude 3.7 (0.33) by approximately 40%.\n\nPerformance Tier Analysis. Based\
    \ on the radar charts, models can be classified into three performance tiers:\
    \ (1) Leading tier: Gemini 2.5 Pro and Claude 3.7, with highest accuracy but different\
    \ efficiency focuses; (2) Middle tier: GPT-4o, Qwen3, and GPT-4o-mini, each with\
    \ specific strengths and weaknesses; (3) Following tier: DeepSeek-V3 and Llama3,\
    \ lagging in accuracy but competitive in certain efficiency metrics. This classification\
    \ reflects the current development stages of LLMs in MCP environments.\n\n![](_page_7_Figure_4.jpeg)\n\
    \n<span id=\"page-7-1\"></span>Figure 6: Comprehensive Model Metrics. Longer edges\
    \ indicate superior performance in each metric.\n\nOpen-Source vs. Closed-Source\
    \ Comparison. [Figure 6](#page-7-1) contrasts model performance by type. Open-source\
    \ models (top) show distinct optimization patterns: Llama3 prioritizes response\
    \ time but has poor accuracy, Qwen3 excels in mathematical accuracy but struggles\
    \ with speed, and DeepSeek-V3 demonstrates superior computational efficiency.\
    \ Closed-source models (bottom) generally exhibit larger profile areas and greater\
    \ cross-domain stability. GPT-4o-mini and GPT-4o show balanced profiles, while\
    \ Claude 3.7 and Gemini 2.5 Pro demonstrate stronger accuracy at the expense of\
    \ efficiency.\n\nCross-Domain Generalization. The most significant difference\
    \ between model groups lies in profile stability across domains. Closed-source\
    \ models maintain more consistent performance when transitioning between domains,\
    \ while open-source models show more pronounced performance drops. For example,\
    \ Llama3's RA drops by 67% from mathematics (0.64) to programming (0.21), while\
    \ GPT-4o decreases by only 46% (from 0.85 to 0.46). This suggests closed-source\
    \ models have more robust generalization capabilities across diverse tool-use\
    \ scenarios.\n\n# 5 Discussion\n\nBased on the results of MCP-RADAR's comprehensive\
    \ assessment, we have come up with the following key insights and takeaways:\n\
    \n*There is no \"all-rounder\" model in the MCP environment now.* Although Claude\
    \ 3.7 and Gemini 2.5 Pro lead in overall performance, they show different patterns\
    \ of dominance in different domains. For example, Claude 3.7 had extremely high\
    \ First Error Position (FEP) and Result Accuracy (RA) in the math task, but relatively\
    \ average performance in the general problem domain. This suggests that the model\
    \ exhibits significant \"domain specialization\" in the MCP task. This domain\
    \ specificity may stem from two key factors: (1) differences in the proportion\
    \ of different domains in the pre-training data distribution; and (2) the model\
    \ architecture's intrinsic preference for specific types of reasoning. Notably,\
    \ all models generally outperformed the software engineering and generalized problem\
    \ solving domains in the mathematical domain, which may reflect the fact that\
    \ the deterministic nature of mathematical tasks is more in line with the capability\
    \ boundaries of current LLMs.\n\n*Fast first, then accurate.* The experimental\
    \ data show that instead of strictly pursuing full accuracy (high DTSR) of process\
    \ calls, it is better to allow the model to try and correct errors more quickly,\
    \ thus significantly reducing response time (improving RTE), especially in scenarios\
    \ with frequent user interactions. We observe that GPT-4o-mini, although lower\
    \ in accuracy than its full version, has a significant response time advantage\
    \ and may provide a better user experience in real-time interaction scenarios.\
    \ This provides important insights for MCP tool developers: when designing tool\
    \ interfaces and error handling mechanisms, they should prioritize fast attempts\
    \ and fast recovery paths rather than pursuing error-free execution.\n\n*Overemphasizing\
    \ the accuracy of each step may affect the overall progress of the task.* According\
    \ to the DTSR and FEP metrics, certain models (e.g., GPT-4o) showed high in-process\
    \ call accuracy, but did not show a significant advantage in first error occurrence\
    \ position (FEP). This suggests that in highly accurate short interactions, models\
    \ may instead be overly cautious or conservative, and more prone to early errors\
    \ or the need for frequent error correction for complex tasks. For LLM developers,\
    \ this means that the training goal should be to strike an appropriate balance\
    \ between \"local accuracy\" and \"global efficiency\". For example, by increasing\
    \ the model's tolerance for uncertainty, allowing possible solution paths to be\
    \ tried with a low degree of certainty rather than being abandoned prematurely.\n\
    \n*Error recovery mechanism is important.* Interestingly, high FEP values (later\
    \ errors) do not always lead to significant accuracy improvement. From the experimental\
    \ data, some models (e.g., DeepSeek-V3 in the software engineering domain) have\
    \ high FEP values, implying that the model's first error occurs at a later stage\
    \ of the task process, yet accuracy is not significantly improved. This finding\
    \ suggests that in MCP environments, late-stage error repair or fault-tolerance\
    \ mechanisms may have a greater impact on task success than simply delaying the\
    \ timing of errors. This raises important implications for LLM developers: the\
    \ focus should be on developing error recovery mechanisms, especially for complex\
    \ error states that may occur later in the task, rather than focusing solely on\
    \ avoiding errors.\n\nWe offer the following practical advice for different stakeholders:\n\
    \n### For MCP tool developers:\n\n- Design a robust error handling mechanism.\
    \ Our experimental results show that error handling ability is a key factor affecting\
    \ task success, especially in complex tasks. Error reports should distinguish\
    \ between parameter errors, execution errors, and result errors, enabling the\
    \ model to adopt appropriate strategies for different types of errors; and provide\
    \ undo or rollback operations to facilitate model recovery from errors.\n- Decomposition\
    \ of complex parameter structures into simple components. The experimental results\
    \ show that the accuracy of parameter construction decreases significantly for\
    \ all models as the parameter complexity increases. To reduce the difficulty of\
    \ model understanding, it is suggested that avoid deep nested objects, decompose\
    \ complex parameters into multiple flat parameters, limit the number of parameters,\
    \ and provide reasonable default values for non-essential parameters to reduce\
    \ the burden of model configuration.\n\n#### For LLM developers:\n\n- Strengthen\
    \ the model's error recovery ability. Current LLMs have significant deficiencies\
    \ in error recovery, especially when dealing with late task errors. Developers\
    \ should design specialized pre-training tasks to improve the model's ability\
    \ to parse and understand error messages, and to develop error recovery strategies\
    \ rather than simple retries.\n- Optimize inference link consistency. Developers\
    \ should improve the model's ability to maintain consistency over long sequences\
    \ and ensure that the training data contains sufficient samples of multiple rounds\
    \ of tool interactions.\n\n### For MCP application developers:\n\n- Adopting a\
    \ \"fast first, then accurate\" strategy in real-time interaction scenarios. Quickly\
    \ return preliminary results before progressively refining and correcting them,\
    \ and allow the model to continue interacting with the user while waiting for\
    \ time-consuming tools to be executed.\n- Design appropriate intermediate feedback\
    \ mechanisms for complex tasks. For example, visualize the current progress and\
    \ expected time remaining for multi-step tasks and allow users to confirm or redirect\
    \ at key decision points.\n- Introduction of tool-use memory and adaptive learning\
    \ mechanisms. For example, record the tool usage preferences and success patterns\
    \ of specific users, recommend effective tool combinations for new tasks based\
    \ on historical successes, or save the last successful tool invocation sequence\
    \ for similar tasks.\n\nWith these targeted recommendations, all participants\
    \ in the MCP ecosystem can work together to promote the practical application\
    \ value of tool-enhanced LLMs and drive the technology toward greater reliability,\
    \ efficiency and user-friendliness.\n\n# 6 Conclusion\n\nThis study presents MCP-RADAR,\
    \ the first multidimensional evaluation framework designed specifically for assessing\
    \ the tool usage capabilities of large language models in Model Context Protocol\
    \ environments. With five complementary dimensions, we go beyond the traditional\
    \ single-metric assessment to provide comprehensive insights into the tool-usage\
    \ capabilities of models. Based on these findings, we provide practical guidance\
    \ for all parties in the MCP ecosystem. MCP-RADAR provides a systematic methodology\
    \ for assessing and improving the ability of LLMs to utilize tools, revealing\
    \ the current state of the art and opportunities for improvement. With the development\
    \ of this critical capability, we believe that future LLMs will be able to utilize\
    \ tools more efficiently and accurately, providing reliable value for a wide range\
    \ of real-world application scenarios.\n\n# References\n\n<span id=\"page-9-0\"\
    ></span>[1] Introducing the Model Context Protocol. URL [https://www.anthropic.com/news/](https://www.anthropic.com/news/model-context-protocol)\
    \ [model-context-protocol](https://www.anthropic.com/news/model-context-protocol).\n\
    \n- <span id=\"page-10-0\"></span>[2] Model context protocol (MCP) - OpenAI Agents\
    \ SDK. URL [https://openai.github.io/](https://openai.github.io/openai-agents-python/mcp/)\
    \ [openai-agents-python/mcp/](https://openai.github.io/openai-agents-python/mcp/).\n\
    - <span id=\"page-10-1\"></span>[3] QwenLM/Qwen-Agent, . URL <https://github.com/QwenLM/Qwen-Agent>.\n\
    - <span id=\"page-10-2\"></span>[4] Dan Hendrycks, Collin Burns, Steven Basart,\
    \ Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive\
    \ Multitask Language Understanding, . URL [http:](http://arxiv.org/abs/2009.03300)\
    \ [//arxiv.org/abs/2009.03300](http://arxiv.org/abs/2009.03300).\n- <span id=\"\
    page-10-3\"></span>[5] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai\
    \ Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A Human-Centric\
    \ Benchmark for Evaluating Foundation Models. URL <http://arxiv.org/abs/2304.06364>.\n\
    - <span id=\"page-10-4\"></span>[6] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi\
    \ Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: Evaluating LLMs in Multi-turn\
    \ Interaction with Tools and Language Feedback. URL <http://arxiv.org/abs/2309.10691>.\n\
    - <span id=\"page-10-5\"></span>[7] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre\
    \ Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning\
    \ Text and Embodied Environments for Interactive Learning. URL <http://arxiv.org/abs/2010.03768>.\n\
    - <span id=\"page-10-6\"></span>[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\
    \ Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob\
    \ Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers\
    \ to Solve Math Word Problems. URL [http://arxiv.org/abs/](http://arxiv.org/abs/2110.14168)\
    \ [2110.14168](http://arxiv.org/abs/2110.14168).\n- <span id=\"page-10-7\"></span>[9]\
    \ Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric\
    \ Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving\
    \ With the MATH Dataset, . URL <http://arxiv.org/abs/2103.03874>.\n- <span id=\"\
    page-10-8\"></span>[10] Anonymized Repository - Anonymous GitHub. URL [https://anonymous.4open.science/](https://anonymous.4open.science/r/MCPRadar-B143/README.md)\
    \ [r/MCPRadar-B143/README.md](https://anonymous.4open.science/r/MCPRadar-B143/README.md).\n\
    - <span id=\"page-10-9\"></span>[11] Jason Wei, Xuezhi Wang, Dale Schuurmans,\
    \ Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought\
    \ prompting elicits reasoning in large language models. *Advances in neural information\
    \ processing systems*, 35:24824–24837, 2022.\n- <span id=\"page-10-10\"></span>[12]\
    \ Huanxi Liu, Jiaqi Liao, Dawei Feng, Kele Xu, and Huaimin Wang. Autofeedback:\
    \ An llm-based framework for efficient and accurate api request generation. *arXiv\
    \ preprint arXiv:2410.06943*, 2024.\n- <span id=\"page-10-11\"></span>[13] Yifan\
    \ Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang,\
    \ Cheng Li, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models\
    \ with real-world restful apis. *arXiv preprint arXiv:2306.06624*, 2023.\n- <span\
    \ id=\"page-10-12\"></span>[14] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu,\
    \ Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm:\
    \ Facilitating large language models to master 16000+ real-world apis. *arXiv\
    \ preprint arXiv:2307.16789*, 2023.\n- <span id=\"page-10-13\"></span>[15] Qiaoyu\
    \ Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun.\
    \ Toolalpaca: Generalized tool learning for language models with 3000 simulated\
    \ cases. *arXiv preprint arXiv:2306.05301*, 2023.\n- <span id=\"page-10-14\"></span>[16]\
    \ Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large\
    \ language model connected with massive apis. *Advances in Neural Information\
    \ Processing Systems*, 37: 126544–126565, 2024.\n- <span id=\"page-10-15\"></span>[17]\
    \ Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models.\
    \ *arXiv preprint arXiv:2205.12255*, 2022.\n- <span id=\"page-10-16\"></span>[18]\
    \ Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,\
    \ Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning\
    \ with large language models. *Advances in Neural Information Processing Systems*,\
    \ 36:43447–43478, 2023.\n- <span id=\"page-11-0\"></span>[19] Timo Schick and\
    \ Hinrich Schütze. Exploiting cloze questions for few shot text classification\
    \ and natural language inference. *arXiv preprint arXiv:2001.07676*, 2020.\n-\
    \ <span id=\"page-11-1\"></span>[20] Michal Spiegel and Aleš Horák. Webmap: Improving\
    \ llm web agents with semantic search for relevant web pages.\n- <span id=\"page-11-2\"\
    ></span>[21] Gobinda Chowdhury and Sudatta Chowdhury. Ai-and llm-driven search\
    \ tools: A paradigm shift in information access for education and research. *Journal\
    \ of Information Science*, page 01655515241284046, 2024.\n- <span id=\"page-11-3\"\
    ></span>[22] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa:\
    \ A dataset for llm question answering with external tools. *Advances in Neural\
    \ Information Processing Systems*, 36:50117–50143, 2023.\n- <span id=\"page-11-4\"\
    ></span>[23] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai,\
    \ Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating\
    \ llms as agents. *arXiv preprint arXiv:2308.03688*, 2023.\n- <span id=\"page-11-5\"\
    ></span>[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria\
    \ Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.\
    \ Toolformer: Language models can teach themselves to use tools. *Advances in\
    \ Neural Information Processing Systems*, 36: 68539–68551, 2023.\n- <span id=\"\
    page-11-6\"></span>[25] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii,\
    \ Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation\
    \ enables zero-shot tool-usage with large language models. *arXiv preprint arXiv:2308.00675*,\
    \ 2023.\n- <span id=\"page-11-7\"></span>[26] Zekun Wang, Ge Zhang, Kexin Yang,\
    \ Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan\
    \ Sim, Xiuying Chen, et al. Interactive natural language processing. *arXiv preprint\
    \ arXiv:2305.13246*, 2023.\n- <span id=\"page-11-8\"></span>[27] Qiantong Xu,\
    \ Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation\
    \ capability of open-source large language models. *arXiv preprint arXiv:2305.16504*,\
    \ 2023.\n- <span id=\"page-11-9\"></span>[28] Yaobo Liang, Chenfei Wu, Ting Song,\
    \ Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al.\
    \ Taskmatrix. ai: Completing tasks by connecting foundation models with millions\
    \ of apis. *Intelligent Computing*, 3:0063, 2024.\n- <span id=\"page-11-10\"></span>[29]\
    \ Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\
    \ Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\
    \ evaluation of language models. *arXiv preprint arXiv:2211.09110*, 2022.\n- <span\
    \ id=\"page-11-11\"></span>[30] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,\
    \ Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive\
    \ benchmark for tool-augmented llms. *arXiv preprint arXiv:2304.08244*, 2023.\n\
    - <span id=\"page-11-12\"></span>[31] Pei Wang, Yanan Wu, Zekun Wang, Jiaheng\
    \ Liu, Xiaoshuai Song, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang,\
    \ Junran Peng, et al. Mtu-bench: A multi-granularity tool-use benchmark for large\
    \ language models. *arXiv preprint arXiv:2410.11710*, 2024.\n- <span id=\"page-11-13\"\
    ></span>[32] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann\
    \ LeCun, and Thomas Scialom. GAIA: A benchmark for General AI Assistants. URL\
    \ [http://arxiv.org/abs/](http://arxiv.org/abs/2311.12983) [2311.12983](http://arxiv.org/abs/2311.12983).\n\
    - <span id=\"page-11-14\"></span>[33] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming\
    \ Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,\
    \ Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\
    \ Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,\
    \ Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\
    \ Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\
    \ Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\
    \ Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir\
    \ Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,\
    \ Jan Leike, Josh Achiam, Vedant Misra,\n\nEvan Morikawa, Alec Radford, Matthew\
    \ Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,\
    \ Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\
    \ Large Language Models Trained on Code. URL [http://arxiv.org/](http://arxiv.org/abs/2107.03374)\
    \ [abs/2107.03374](http://arxiv.org/abs/2107.03374).\n\n- <span id=\"page-12-0\"\
    ></span>[34] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\
    \ Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and\
    \ Charles Sutton. Program Synthesis with Large Language Models. URL <http://arxiv.org/abs/2108.07732>.\n\
    - <span id=\"page-12-1\"></span>[35] Claude 3.7 Sonnet and Claude Code. URL [https://www.anthropic.com/news/](https://www.anthropic.com/news/claude-3-7-sonnet)\
    \ [claude-3-7-sonnet](https://www.anthropic.com/news/claude-3-7-sonnet).\n- <span\
    \ id=\"page-12-2\"></span>[36] Hello GPT-4o. URL <https://openai.com/index/hello-gpt-4o/>.\n\
    - <span id=\"page-12-3\"></span>[37] GPT-4o mini: Advancing cost-efficient intelligence.\
    \ URL [https://openai.com/index/](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)\
    \ [gpt-4o-mini-advancing-cost-efficient-intelligence/](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/).\n\
    - <span id=\"page-12-4\"></span>[38] Gemini 2.5: Our most intelligent AI model.\
    \ URL [https://blog.google/technology/](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\
    \ [google-deepmind/gemini-model-thinking-updates-march-2025/](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/).\n\
    - <span id=\"page-12-5\"></span>[39] Llama 3.3 | Model Cards and Prompt formats.\
    \ URL [https://www.llama.com/docs/](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/)\
    \ [model-cards-and-prompt-formats/llama3\\\\_3/](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/).\n\
    - <span id=\"page-12-6\"></span>[40] Chars Yang. DeepSeek v3 - Advanced AI & LLM\
    \ Model Online. URL [https://deepseekv3.](https://deepseekv3.org/) [org/](https://deepseekv3.org/).\n\
    - <span id=\"page-12-7\"></span>[41] Qwen3: Think Deeper, Act Faster | Qwen, .\
    \ URL [https://qwenlm.github.io/blog/](https://qwenlm.github.io/blog/qwen3/) [qwen3/](https://qwenlm.github.io/blog/qwen3/).\n\
    \n# A Conversation Logs and Response Process\n\n#### Example-Code\n\nSystem Prompt:\
    \ You are a code assistant that MUST use available tools to solve code problems.\
    \ Never try to solve problems directly when tools can help.\n\nProcess:\n\nAnalyze\
    \ the problem Choose appropriate tool(s) Call tool(s) with correct parameters\
    \ Interpret results and use additional tools if needed Provide final answer Important:\n\
    \nAlways use available tools Show your tool-calling process Format answer as:\
    \ <answer>[YOUR FINAL ANSWER]</answer> Use appropriate number formats or LaTeX\
    \ for mathematical answers\n\nUser: Q1:\"Prompt\": \"def get\\_positive(l: list):\
    \ \"\"\"Return only positive numbers in the list. »> get\\_positive([-1, 2, -4,\
    \ 5, 6]) [2, 5, 6] »> get\\_positive([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])\
    \ [5, 3, 2, 3, 9, 123, 1] \"\"\"Please use MCP to break down the task to help\
    \ solve this problem. Write the solution and save it in a jsonl file named answer.jsonl\
    \ using the format {\"unique\\_id\": \"000000000000031\", \"Answer\": \"your\\\
    _code\\_here\"}\"\n\nAgent: <tool>break down the task</tool>\n\nStep 1: Code Implementation\n\
    \nStep 2: Format the Answer for JSONL\n\nStep 3: Save the Answer in a JSONL File\n\
    \nStep 4: Creating the Directory and Writing the File\n\n<tool>Create directory\
    \ structure</tool> <tool>Write the formatted answer to JSONL file</tool> finish!\n\
    \n### Example-General\n\nSystem Prompt: You are an assistant that MUST use available\
    \ tools to solve problems. Never try to solve problems directly when tools can\
    \ help.\n\nProcess:\n\nAnalyze the problem Choose appropriate tool(s) Call tool(s)\
    \ with correct parameters Interpret results and use additional tools if needed\
    \ Provide final answer Important:\n\nAlways use available tools Show your tool-calling\
    \ process Format answer as: <answer>[YOUR FINAL ANSWER]</answer> Use appropriate\
    \ number formats or LaTeX for mathematical answers\n\nUser: Q1: \"If we assume\
    \ all articles published by Nature in 2020 (articles, only, not book reviews/columns,\
    \ etc) relied on statistical significance to justify their findings and they on\
    \ average came to a p-value of 0.04, how many papers would be incorrect as to\
    \ their claims of statistical significance? Round the value up to the next integer.\"\
    \n\nAgent: Step 1: Search for the number of Nature articles published in 2020\n\
    \n<tool>search for information</tool> Step 2: calculate the expected number of\
    \ incorrect claims <answer>41</answer> finish!\n\n#### Example-Math\n\nSystem\
    \ Prompt: You are an math assistant that MUST use available tools to solve math\
    \ problems. Never try to solve problems directly when tools can help.\n\nProcess:\n\
    \nAnalyze the problem Choose appropriate tool(s) Call tool(s) with correct parameters\
    \ Interpret results and use additional tools if needed Provide final answer Important:\n\
    \nAlways use available tools Show your tool-calling process Format answer as:\
    \ <answer>[YOUR FINAL ANSWER]</answer> Use appropriate number formats or LaTeX\
    \ for mathematical answers\n\nUser: Q1: \"Find 12 7 · −29 3 − 12 7 · <sup>−</sup><sup>29</sup>\
    \ 3 .\"\n\nAgent: Step 1: Calculate the floor of -29/3 <tool>floor</tool> Step\
    \ 2: Calculate the ceiling of the expression <tool>ceiling</tool> <answer>2</answer>\
    \ finish!\n\n## B Replicated Experiment Results\n\nResult table [1](#page-6-0)\
    \ presents the average metrics across three experimental runs.\n\n| Task    |\
    \ Model          | RA↑  | TFTD↑ | FEP↑ | CRE↓ | RTE↓  |\n|---------|----------------|------|-------|------|------|-------|\n\
    | Math    | Claude 3.7     | 0.87 | 0.94  | 0.92 | 5890 | 6380  |\n|         |\
    \ Gemini 2.5 Pro | 0.89 | 0.96  | 0.90 | 6150 | 4120  |\n|         | GPT-4o  \
    \       | 0.84 | 0.93  | 0.91 | 5830 | 4780  |\n|         | GPT-4o-mini    | 0.71\
    \ | 0.87  | 0.84 | 5050 | 2820  |\n|         | Qwen3          | 0.83 | 0.94  |\
    \ 0.94 | 5320 | 11120 |\n|         | Deepseek-V3    | 0.57 | 0.90  | 0.87 | 1845\
    \ | 2950  |\n|         | Llama3         | 0.63 | 0.79  | 0.76 | 4870 | 3690  |\n\
    |         | Claude 3.7     | 0.56 | 0.56  | 0.76 | 2540 | 4170  |\n| Coding  |\
    \ Gemini 2.5 Pro | 0.52 | 0.54  | 0.67 | 2920 | 2450  |\n|         | GPT-4o  \
    \       | 0.45 | 0.47  | 0.57 | 3520 | 3560  |\n|         | GPT-4o-mini    | 0.30\
    \ | 0.40  | 0.60 | 1210 | 2170  |\n|         | Qwen3          | 0.51 | 0.59  |\
    \ 0.57 | 1200 | 4540  |\n|         | Deepseek-V3    | 0.32 | 0.35  | 0.43 | 1230\
    \ | 1470  |\n|         | Llama3         | 0.20 | 0.21  | 0.23 | 1530 | 1490  |\n\
    | General | Claude 3.7     | 0.31 | 0.70  | 0.70 | 5240 | 3580  |\n|         |\
    \ Gemini 2.5 Pro | 0.44 | 0.67  | 0.62 | 6290 | 4590  |\n|         | GPT-4o  \
    \       | 0.26 | 0.30  | 0.43 | 5640 | 4490  |\n|         | GPT-4o-mini    | 0.12\
    \ | 0.28  | 0.32 | 4910 | 1090  |\n|         | Qwen3          | 0.27 | 0.41  |\
    \ 0.39 | 4890 | 12180 |\n|         | Deepseek-V3    | 0.24 | 0.55  | 0.49 | 5220\
    \ | 1530  |\n|         | Llama3         | 0.21 | 0.34  | 0.27 | 2240 | 2030  |\n\
    \nTable 2: First run of model performance metrics (Experiment 1)\n\nTable 3: Second\
    \ run of model performance metrics (Experiment 2)\n\n| Task    | Model       \
    \   | RA↑  | TFTD↑ | FEP↑ | CRE↓ | RTE↓  |\n|---------|----------------|------|-------|------|------|-------|\n\
    | Math    | Claude 3.7     | 0.90 | 0.96  | 0.94 | 5950 | 6490  |\n|         |\
    \ Gemini 2.5 Pro | 0.92 | 0.98  | 0.93 | 6200 | 4220  |\n|         | GPT-4o  \
    \       | 0.86 | 0.91  | 0.89 | 5880 | 4680  |\n|         | GPT-4o-mini    | 0.73\
    \ | 0.89  | 0.87 | 5010 | 2860  |\n|         | Qwen3          | 0.86 | 0.96  |\
    \ 0.96 | 5390 | 10990 |\n|         | Deepseek-V3    | 0.60 | 0.92  | 0.89 | 1790\
    \ | 2990  |\n|         | Llama3         | 0.65 | 0.81  | 0.78 | 4810 | 3760  |\n\
    |         | Claude 3.7     | 0.59 | 0.58  | 0.79 | 2520 | 4240  |\n| Coding  |\
    \ Gemini 2.5 Pro | 0.55 | 0.56  | 0.70 | 2880 | 2400  |\n|         | GPT-4o  \
    \       | 0.47 | 0.49  | 0.59 | 3450 | 3510  |\n|         | GPT-4o-mini    | 0.33\
    \ | 0.42  | 0.63 | 1160 | 2120  |\n|         | Qwen3          | 0.53 | 0.61  |\
    \ 0.59 | 1170 | 4480  |\n|         | Deepseek-V3    | 0.34 | 0.37  | 0.45 | 1190\
    \ | 1440  |\n|         | Llama3         | 0.22 | 0.23  | 0.25 | 1490 | 1460  |\n\
    | General | Claude 3.7     | 0.34 | 0.73  | 0.73 | 5190 | 3510  |\n|         |\
    \ Gemini 2.5 Pro | 0.47 | 0.69  | 0.65 | 6230 | 4520  |\n|         | GPT-4o  \
    \       | 0.28 | 0.32  | 0.45 | 5580 | 4420  |\n|         | GPT-4o-mini    | 0.14\
    \ | 0.30  | 0.34 | 4860 | 1070  |\n|         | Qwen3          | 0.29 | 0.43  |\
    \ 0.41 | 4830 | 12100 |\n|         | Deepseek-V3    | 0.26 | 0.57  | 0.51 | 5160\
    \ | 1500  |\n|         | Llama3         | 0.23 | 0.36  | 0.29 | 2200 | 2000  |\n\
    \n| Task    | Model          | RA↑  | TFTD↑ | FEP↑ | CRE↓ | RTE↓  |\n|---------|----------------|------|-------|------|------|-------|\n\
    | Math    | Claude 3.7     | 0.90 | 0.95  | 0.93 | 5935 | 6453  |\n|         |\
    \ Gemini 2.5 Pro | 0.92 | 0.97  | 0.93 | 6211 | 4173  |\n|         | GPT-4o  \
    \       | 0.85 | 0.92  | 0.90 | 5885 | 4715  |\n|         | GPT-4o-mini    | 0.72\
    \ | 0.88  | 0.87 | 5012 | 2855  |\n|         | Qwen3          | 0.86 | 0.95  |\
    \ 0.95 | 5385 | 11049 |\n|         | Deepseek-V3    | 0.60 | 0.91  | 0.88 | 1804\
    \ | 2985  |\n|         | Llama3         | 0.64 | 0.80  | 0.77 | 4834 | 3749  |\n\
    | Coding  | Claude 3.7     | 0.59 | 0.57  | 0.79 | 2527 | 4226  |\n|         |\
    \ Gemini 2.5 Pro | 0.55 | 0.55  | 0.70 | 2882 | 2422  |\n|         | GPT-4o  \
    \       | 0.46 | 0.48  | 0.58 | 3473 | 3529  |\n|         | GPT-4o-mini    | 0.33\
    \ | 0.41  | 0.63 | 1170 | 2139  |\n|         | Qwen3          | 0.52 | 0.60  |\
    \ 0.58 | 1176 | 4498  |\n|         | Deepseek-V3    | 0.33 | 0.36  | 0.44 | 1195\
    \ | 1443  |\n|         | Llama3         | 0.21 | 0.22  | 0.24 | 1498 | 1463  |\n\
    | General | Claude 3.7     | 0.34 | 0.73  | 0.73 | 5212 | 3527  |\n|         |\
    \ Gemini 2.5 Pro | 0.47 | 0.68  | 0.65 | 6245 | 4540  |\n|         | GPT-4o  \
    \       | 0.27 | 0.31  | 0.44 | 5589 | 4440  |\n|         | GPT-4o-mini    | 0.13\
    \ | 0.29  | 0.33 | 4879 | 1071  |\n|         | Qwen3          | 0.28 | 0.42  |\
    \ 0.40 | 4842 | 12125 |\n|         | Deepseek-V3    | 0.25 | 0.56  | 0.50 | 5169\
    \ | 1503  |\n|         | Llama3         | 0.22 | 0.35  | 0.28 | 2199 | 2012  |\n\
    \nTable 4: Third run of model performance metrics (Experiment 3)\n\n### C Experimental\
    \ Procedure Formatting\n\n```\nMCP-Pool\n```\n\n```\n\"mcp_pool\": [\n{\n\"name\"\
    : \"FireCrawl\",\n\"description\": \"A Model Context Protocol (MCP) server implementation\
    \ that integrates with Firecrawl for\nweb scraping capabilities.\",\n\"tools\"\
    : [\n{\n\"tool_name\": \"firecrawl_search\",\n\"tool_description\": \"Search the\
    \ web and optionally extract content from search results.\",\n\"inputs\": [\n\
    {\n\"name\": \"query\",\n\"type\": \"string\",\n\"required\": true,\n\"description\"\
    : \"your search query\"\n}\n]\n}\n],\n\"run_config\": [\n{\n\"command\": \"npx\
    \ -y firecrawl-mcp\",\n\"env\": {\n\"FIRECRAWL_API_KEY\": \"your key\"\n},\n\"\
    port\": your port\n}\n]\n}\n```\n#### Eval-Result\n\n```\n{\n    \"unique_id\"\
    : \"*****\",\n    \"question\": \"Given a integer n(>0), make a pile of n levels\
    \ of ...\",\n    \"ground_truth\": \"def make_a_pile(n):\\n\n    return [n + 2*i\
    \ for i in range(n)]\",\n    \"prediction\": \"def make_a_pile(n):\\n\n    pile\
    \ = []\\n for i in range(n):\\n\n    if n % 2 == 0:\\n pile.append(n + 2*i)\\\
    n\n    else:\\n pile.append(n + 2*i)\\n\n    return pile\",\n    \"success\":\
    \ true,\n    \"tool_usage\": {\n      \"tool_calls\": [\n        {\"name\": \"\
    read_file\", \"arguments\": \"{\\\"path\\\":\\problem.jsonl\\\"}\"},\n       \
    \ {\"name\": \"write_file\", \"arguments\": \"{\\\"path\\\":\\answer.jsonl\\\"\
    }\"}\n      ],\n      \"total_tool_count\": 2,\n      \"tool_names\": [\"read_file\"\
    , \"write_file\"]\n    },\n    \"token_usage\": {\n      \"prompt_tokens\": 820,\n\
    \      \"completion_tokens\": 610,\n      \"total_tokens\": 1430\n    }\n}\n```\n\
    ### <span id=\"page-16-0\"></span>D Others\n\nEvaluation dimensions' detailed\
    \ introduction: Result Accuracy (RA). Result Accuracy is a fundamental metric\
    \ measuring a model's task completion success, defined as:\n\n$$RA = \\frac{N\\\
    _{success}}{N\\_{total}}\\tag{1}$$\n\nwhere Nsuccess represents the number of\
    \ tasks successfully completed by the model, and Ntotal represents the total number\
    \ of tasks requiring tool use. The criterion for successful completion is that\
    \ the model's final answer matches the standard answer, allowing for minor expression\
    \ variants. Unlike other dimensions, RA focuses exclusively on the task's final\
    \ result rather than process quality. While simple, this metric is indispensable\
    \ as a baseline measurement standard because, in practical applications, the correctness\
    \ of the final result is typically users' primary concern, regardless of the process.\
    \ Although RA is a fundamental measurement standard, it alone cannot reflect the\
    \ model's efficiency or robustness in tool utilization processes.\n\nDynamic Tool\
    \ Selection Rate (DTSR). DTSR evaluates the accuracy of each call round in the\
    \ model's tool utilization process, calculated as:\n\n$$DTSR = 1 - \\frac{\\sum\\\
    _{i=1}^{N\\_{total}} R\\_i}{N\\_{total}} \\tag{2}$$\n\nwhere R<sup>i</sup> represents\
    \ the proportion of erroneous tool calls in i-th task. Unlike RA, which focuses\
    \ solely on final results, DTSR reflects the quality of model behavior throughout\
    \ the entire interaction process; even if the final result is correct, erroneous\
    \ calls during the process will reduce the DTSR score. This metric is particularly\
    \ important for evaluating model performance in complex multi-step tasks, as it\
    \ reveals the model's ability to maintain long-term tool usage consistency.\n\n\
    First Error Position (FEP). FEP measures the position of a model's first error\
    \ in the workflow, assessing overall task resilience:\n\n$$FEP = \\frac{\\sum\\\
    _{i=1}^{N\\_{total}} D\\_i}{N\\_{total}} \\tag{3}$$\n\nwhere D<sup>i</sup> represents\
    \ the proportional depth of the first erroneous tool call in the i-th task. For\
    \ successful tasks, D<sup>i</sup> = 1, indicating an error-free entire process;\
    \ for failed tasks, higher D<sup>i</sup> values indicate later error occurrence,\
    \ suggesting the model can correctly execute longer tool call sequences. Unlike\
    \ DTSR, which focuses on overall error rates, FEP concentrates on the timing of\
    \ errors, particularly important for evaluating model stability in long-sequence\
    \ reasoning. In complex tasks, early errors are typically more fatal than later\
    \ ones, and FEP effectively distinguishes this difference.\n\nComputational Resource\
    \ Efficiency (CRE). CRE measures the model's efficient utilization of computational\
    \ resources during task execution, calculated based on token consumption:\n\n\
    $$CRE = \\frac{T\\_b}{T\\_a} \\tag{4}$$\n\nwhere T<sup>b</sup> represents the\
    \ average token consumption of a benchmark model completing similar tasks, and\
    \ T<sup>a</sup> represents the actual token consumption of the evaluated model.\
    \ CRE values greater than 1 indicate higher efficiency than the benchmark, while\
    \ values less than 1 indicate lower efficiency. This metric is particularly important\
    \ because, in production environments, computational efficiency directly affects\
    \ operational costs and response times.\n\nResponse Time Efficiency (RTE). RTE\
    \ measures time efficiency from user input to final response:\n\n$$RTE = \\frac{t\\\
    _{baseline}}{t\\_{actual}}\\tag{5}$$\n\nwhere tbaseline represents the average\
    \ response time of a benchmark model completing similar tasks, and tactual represents\
    \ the actual time for the model to generate a complete response. Similar to CRE,\
    \ RTE values greater than 1 indicate faster performance than the benchmark, while\
    \ values less than 1 indicate longer response times. In practical applications,\
    \ response time directly affects user experience, especially in scenarios requiring\
    \ real-time interaction.\n\nTo intuitively display model performance distribution\
    \ across the five dimensions, we normalize the metrics to the [0, 1] interval\
    \ and generate radar chart representations. Our primary goal is not to provide\
    \ a singular ranking but to reveal different models' strengths and weaknesses\
    \ through multidimensional visualization, guiding selection for specific application\
    \ scenarios. This multidimensional evaluation framework allows MCP-RADAR to transcend\
    \ simple binary \"task completed or not\" assessments, deeply understanding model\
    \ behavioral characteristics, efficiency performance, and resource consumption\
    \ in tool usage processes, providing fine-grained feedback for developers and\
    \ researchers to optimize model and tool design."
- title: 'SPaRC: A Spatial Pathfinding Reasoning Challenge'
  abstract: 'Existing reasoning datasets saturate and fail to test abstract, multi-step

    problems, especially pathfinding and complex rule constraint satisfaction. We

    introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000

    2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,

    requiring step-by-step planning with arithmetic and geometric rules. Humans

    achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best

    reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).

    Models often generate invalid paths (>50% of puzzles for o4-mini), and

    reasoning tokens reveal they make errors in navigation and spatial logic.

    Unlike humans, who take longer on hard puzzles, models fail to scale test-time

    compute with difficulty. Allowing models to make multiple solution attempts

    improves accuracy, suggesting potential for better spatial reasoning with

    improved training and efficient test-time scaling methods. SPaRC can be used as

    a window into models'' spatial reasoning limitations and drive research toward

    new methods that excel in abstract, multi-step problem-solving.'
  url: http://arxiv.org/abs/2505.16686v1
  keywords: ''
  document: "Introduction This guide provides the rules and instructions for annotating\
    \ SPaRC puzzles. We want to compare whether there are patterns/similarities that\
    \ make the puzzles difficult for humans or LLMs to solve. Therefore, we have created\
    \ 6 test sets of 50 puzzles each. These sets contain puzzles with all possible\
    \ combinations of rules and grid sizes.\n\nTask Annotate all the samples in the\
    \ dataset in as little time as possible per puzzle. Each puzzle is solvable, but\
    \ if you can't think of a solution after a reasonable amount of time (5-10 minutes),\
    \ you can click Show Solution or Skip Puzzle to continue. Once you have completed\
    \ all 50 puzzles, please e-mail the annotated file back to us. By sending the\
    \ file back to us, you agree that we can publish your annotations anonymously.\
    \ This includes solve time, required attempts, and solution path.\n\nRules The\
    \ line must connect Start with End with a continuous path without using the same\
    \ cell twice. It must also follow all rules defined by the puzzle. A visual explanation\
    \ of the rules can be seen in Figure [26.](#page-28-2)\n\n- Gaps: The line CANNOT\
    \ traverse a cell marked by a Gap.\n- Dots: The line MUST pass through a cell\
    \ marked by a Dot.\n- Stone: All stones within a single region created by the\
    \ line must be the same color. Different colored squares MUST be separated into\
    \ different regions by the line.\n- Stars: Each star must be paired with EX-ACTLY\
    \ one other element of the same color in a region. Other colors are ignored.\n\
    - Triangles: The line must touch EXACTLY the number of edges specified by the\
    \ triangle\n\ncount (edges are top, right, bottom, left of the cell).\n\n- Polyshapes\
    \ (Poly): The region containing this symbol must be shaped EXACTLY like the defined\
    \ polyshape. The shape must fit entirely within the region's boundaries. If multiple\
    \ positive polyshapes are in one region, the region's shape must accommodate their\
    \ combined, non-overlapping forms (like Tetris pieces).\n- Negative Polyshapes\
    \ (Ylop): The negative polyshape can only be placed on top of already placed normal\
    \ polyshapes. The negative polyshapes must fit on the grid, but can allow overlap\
    \ between normal polyshapes or placement of polyshapes that extend beyond the\
    \ area defined by the line. If the negative polyshapes exactly cancel the normal\
    \ polyshapes, there is no restriction on the grid shape anymore. A negative polyshape\
    \ only counts as valid if it is used.\n\nExample Dataset You can use the following\
    \ dataset to experiment and get familiar with the puzzles and all rules:\n\n#\
    \ Link redacted for anonymity.\n\n# Important Hints\n\n- The annotation state\
    \ gets saved even when closing the window, but to be safe, also always download\
    \ the current annotated dataset when you stop annotating.\n- If you reload the\
    \ page, don't overwrite the existing data.\n\n# N Acknowledgment of AI Usage\n\
    \nAI Usage card based on [Wahle et al.](#page-10-9) [\\(2023\\)](#page-10-9).\n\
    \n| AI Usage Card        |                                                   \
    \                                                                      |     \
    \                                                                            \
    \                 |                                                          \
    \                                            |\n|----------------------|-------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n\
    | PROJECT<br>DETAILS   | PROJECT NAME<br>SPaRC: A Spatial<br>Pathfinding Reasoning\
    \                                                               | DOMAIN<br>Paper\
    \                                                                            \
    \      | KEY APPLICATION<br>Dataset                                          \
    \                                 |\n| CONTACT(S)           | Challenge<br>NAME(S)<br>Lars\
    \ Benedikt Kaesberg                                                          \
    \                | EMAIL(S)<br>l.kaesberg@uni<br>goettingen.de               \
    \                                       | AFFILIATION(S)<br>University Göttingen\
    \                                                               |\n| MODEL(S)\
    \             | MODEL NAME(S)<br>ChatGPT<br>Gemini<br>Claude                 \
    \                                                           | VERSION(S)<br>4o,\
    \ 4.5, o3<br>2.5 pro<br>3.7 sonnet                                           \
    \    |                                                                       \
    \                               |\n| LITERATURE<br>REVIEW | FINDING LITERATURE<br>ChatGPT<br>Gemini\
    \                                                                            \
    \     | FINDING EXAMPLES<br>FROM KNOWN<br>LITERATURE OR ADDING<br>LITERATURE FOR<br>EXISTING\
    \ STATEMENTS  | COMPARING<br>LITERATURE                                      \
    \                                        |\n| WRITING              | GENERATING\
    \ NEW TEXT<br>BASED ON<br>INSTRUCTIONS                                       \
    \                                  | ASSISTING IN<br>IMPROVING OWN<br>CONTENT\
    \ OR<br>PARAPHRASING<br>RELATED WORK<br>ChatGPT<br>Gemini | PUTTING OTHER<br>WORKS\
    \ IN<br>PERSPECTIVE                                                          \
    \   |\n| CODING               | GENERATING NEW<br>CODE BASED ON<br>DESCRIPTIONS\
    \ OR<br>EXISTING CODE<br>ChatGPT<br>Gemini<br>Claude                      | REFACTORING\
    \ AND<br>OPTIMIZING EXISTING<br>CODE<br>ChatGPT<br>Gemini<br>Claude          \
    \          | COMPARING ASPECTS<br>OF EXISTING CODE                           \
    \                                     |\n| ETHICS               | WHY DID WE USE\
    \ AI<br>FOR THIS PROJECT?<br>Efficiency / Speed<br>Expertise Access          \
    \                              | WHAT STEPS ARE WE<br>TAKING TO MITIGATE<br>ERRORS\
    \ OF AI?<br>-                                    | WHAT STEPS ARE WE<br>TAKING\
    \ TO MINIMIZE<br>THE CHANCE OF HARM<br>OR INAPPROPRIATE<br>USE OF AI?<br>- |\n\
    |                      | THE CORRESPONDING AUTHORS VERIFY AND AGREE WITH THE MODIFICATIONS\
    \ OR GENERA<br>TIONS OF THEIR USED AI-GENERATED CONTENT |                    \
    \                                                                            \
    \  |                                                                         \
    \                             |\n| AI Usage Card v2.0   |                    \
    \                                                                            \
    \                         | https://ai-cards.org                             \
    \                                                | PDF   BibTeX              \
    \                                                                           |\n\
    \n![](_page_31_Picture_0.jpeg)\n\nGenerated with [citeassist.uni-goettingen.de](https://citeassist.uni-goettingen.de/)\
    \ [\\(Kaesberg et al.,](#page-9-25) [2024\\)](#page-9-25)\n\n# **BibTeX Entry**\n\
    \n```\n@techreport{kaesberg2025,\n  author={Kaesberg, Lars Benedikt and Wahle,\
    \ Jan Philip and Ruas, Terry and Gipp, Bela},\n  title={SPaRC: A Spatial Pathfinding\
    \ Reasoning Challenge},\n  year={2025},\n  month={05}\n}\n```\n# **Online Access**\n\
    \n**CiteAssist** [https://citeassist.uni-goettingen.de/preprint/1c30ada3-ead0-44a5-](https://citeassist.uni-goettingen.de/preprint/1c30ada3-ead0-44a5-97f9-6a7dfe39ba22)\
    \ [97f9-6a7dfe39ba22](https://citeassist.uni-goettingen.de/preprint/1c30ada3-ead0-44a5-97f9-6a7dfe39ba22)"
- title: "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive\n  Programming"
  abstract: 'While recent research increasingly emphasizes the value of human-LLM

    collaboration in competitive programming and proposes numerous empirical

    methods, a comprehensive understanding remains elusive due to the fragmented

    nature of existing studies and their use of diverse, application-specific human

    feedback. Thus, our work serves a three-fold purpose: First, we present the

    first taxonomy of human feedback consolidating the entire programming process,

    which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a

    novel programming dataset specifically designed for human-LLM collaboration,

    meticulously annotated to enable large-scale simulated human feedback and

    facilitate costeffective real human interaction studies. Third, we introduce

    ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM

    competitive programming. With ELABORATION, we pinpoint strengthes and

    weaknesses of existing methods, thereby setting the foundation for future

    improvement. Our code and dataset are available at

    https://github.com/SCUNLP/ELABORATION'
  url: http://arxiv.org/abs/2505.16667v1
  keywords: ''
  document: "# ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming\n\
    \nXinwei Yang♠♡, Zhaofeng Liu♢, Chen Huang♠♡, Jiashuai Zhang♠, Tong Zhang ♠♡,\
    \ Yifan Zhang △, Wenqiang Lei♠♡∗\n\n♠ Sichuan University ♢ Tianjin University\
    \ of Science and Technology ♡ Engineering Research Center of Machine Learning\
    \ and Industry Intelligence, Ministry of Education, China △ Vanderbilt University\
    \ xinwei\\_yang@stu.scu.edu.cn {scu.zhangtong, huangc.scu}@gmail.com wenqianglei@scu.edu.cn\n\
    \n## Abstract\n\nWhile recent research increasingly emphasizes the value of human-LLM\
    \ collaboration in competitive programming and proposes numerous empirical methods,\
    \ a comprehensive understanding remains elusive due to the fragmented nature of\
    \ existing studies and their use of diverse, application-specific human feedback.\
    \ Thus, our work serves a three-fold purpose: First, we present the first taxonomy\
    \ of human feedback consolidating the entire programming process, which promotes\
    \ fine-grained evaluation. Second, we introduce ELABORA-TIONSET, a novel programming\
    \ dataset specifically designed for human-LLM collaboration, meticulously annotated\
    \ to enable large-scale simulated human feedback and facilitate costeffective\
    \ real human interaction studies. Third, we introduce ELABORATION, a novel benchmark\
    \ to facilitate a thorough assessment of human-LLM competitive programming. With\
    \ ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby\
    \ setting the foundation for future improvement. Our code and dataset are available\
    \ at [https:](https://github.com/SCUNLP/ELABORATION) [//github.com/SCUNLP/ELABORATION](https://github.com/SCUNLP/ELABORATION).\n\
    \n## 1 Introduction\n\nCompetitive programming presents a formidable challenge,\
    \ as it requires mastery of four key stages: 1) the precise understanding of complex\
    \ problems, 2) the strategic planning of efficient solutions, 3) the generation\
    \ of effective code within strict constraints, 4) and the rigorous debugging of\
    \ their implementations [\\(Cormen et al.,](#page-9-0) [2022;](#page-9-0) [Huang\
    \ et al.,](#page-9-1) [2023b;](#page-9-1) [Dale and Weems,](#page-9-2) [2014\\\
    )](#page-9-2). To mitigate this challenge, there has been a growing interest in\
    \ utilizing large language models (LLMs) for automatic competitive programming\
    \ tasks [\\(Nijkamp et al.,](#page-10-0) [2022;](#page-10-0) [Li et al.,](#page-10-1)\
    \ [2023a;](#page-10-1) [Roziere et al.,](#page-10-2) [2023;](#page-10-2) [Guo](#page-9-3)\
    \ [et al.,](#page-9-3) [2024;](#page-9-3) [Ridnik et al.,](#page-10-3) [2024;](#page-10-3)\
    \ [Lozhkov et al.,](#page-10-4) [2024;](#page-10-4) [Liu et al.,](#page-10-5)\
    \ [2024\\)](#page-10-5), assisting individuals in CS education and technical interview\
    \ preparation. However, these models have not yet demonstrated remarkable performance\
    \ for practical utility [\\(Yan](#page-11-0) [et al.,](#page-11-0) [2023;](#page-11-0)\
    \ [Li et al.,](#page-10-6) [2023b;](#page-10-6) [Jain et al.,](#page-9-4) [2024\\\
    )](#page-9-4).\n\nDriven by this concern, recent research has shifted from relying\
    \ solely on LLMs to explore Human-LLM Competitive Programming, a humanin-the-loop\
    \ paradigm that leverages multi-turn human feedback to enhance LLM efficacy [\\\
    (Shi et al.,](#page-10-7) [2024;](#page-10-7) [Chae et al.,](#page-9-5) [2024;](#page-9-5)\
    \ [Zheng et al.,](#page-11-1) [2024\\)](#page-11-1). However, existing research\
    \ have been somewhat fragmented, with studies employing various, scattered and\
    \ application-specific human feedback. This fragmentation hinders a comprehensive\
    \ understanding of effective Human-LLM collaboration in competitive programming\
    \ [\\(Shi et al.,](#page-10-7) [2024\\)](#page-10-7). For instance, [Mozannar\
    \ et al.](#page-10-8) [\\(2023\\)](#page-10-8) and [Wang et al.](#page-10-9) [\\\
    (2024\\)](#page-10-9) focus on suggesting solution strategies, while [Zheng et\
    \ al.](#page-11-1) [\\(2024\\)](#page-11-1) concentrate on conversational error\
    \ identification. These approaches overlook the potential advantages of human\
    \ guidance in areas such as problem comprehension, solution planning. *A comprehensive\
    \ benchmark is therefore needed to evaluate the effectiveness and characteristics\
    \ of human-LLM collaboration across the entire competitive programming process.*\n\
    \nTo this end, we introduce ELABORATION, a novel benchmark featuring a comprehensive\
    \ evaluation protocol to facilitate a thorough assessment. This protocol incorporates\
    \ a taxonomy of human feedback spanning the entire competitive programming process,\
    \ and a new human-LLM programming dataset to support the evaluation implementations.\
    \ Specifically, our evaluation protocol builds upon existing works [\\(Gao et\
    \ al.,](#page-9-6) [2024;](#page-9-6) [Huang](#page-9-7)\n\n<sup>∗</sup> Corresponding\
    \ author.\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_0.jpeg)\n\nFigure\
    \ 1: Illustration of ELABORATION evaluation. A human feedback taxonomy, structuring\
    \ the entire programming process into four stages, enables stage-specific evaluation.\n\
    \n[et al.,](#page-9-7) [2024a;](#page-9-7) [Chen et al.,](#page-9-8) [2023\\)](#page-9-8),\
    \ using a conversational human-LLM interaction where textual human feedback is\
    \ integrated into each code generation turn. As illustrated in Figure [1,](#page-1-0)\
    \ a novel taxonomy structures this human feedback, ensuring coverage across the\
    \ entire competitive programming process: problem comprehension, solution planning,\
    \ code generation, and debugging. This allows ELABORATION to incorporate human\
    \ feedback at each stage and comprehensively assess its effectiveness. Moreover,\
    \ to facilitate the evaluation implementation, we introduce ELABORATIONSET, the\
    \ first competitive programming dataset specifically designed for human-LLM collaboration.\
    \ This dataset comprises 8,320 problems from Codeforces and AtCoder, meticulously\
    \ annotated to enable large-scale simulated human feedback and facilitate cost-effective\
    \ real human interaction studies (cf. Table [1\\)](#page-1-1). As such, ELABORATION\
    \ provides a robust and comprehensive framework for analyzing human-LLM competitive\
    \ programming, paving the way for future advancements in this field.\n\n<span\
    \ id=\"page-1-1\"></span>\n\n| Dataset                                       \
    \  | Easy | Middle                | Difficult |  |  |  |  |\n|-------------------------------------------------|------|-----------------------|-----------|--|--|--|--|\n\
    | Basic Problem Information                       |      |                   \
    \    |           |  |  |  |  |\n| Time Period                                \
    \     |      | Oct. 2011 ~ Nov. 2024 |           |  |  |  |  |\n| #Problems  \
    \                                     | 3642 | 2098                  | 2580  \
    \    |  |  |  |  |\n| Avg. #Test Cases                                | 14.4 |\
    \ 14.5                  | 14.2      |  |  |  |  |\n| Annotations for Human Interaction\
    \ (per Problem) |      |                       |           |  |  |  |  |\n| Avg.\
    \ #Statement Clarifications                  | 8.1  | 10.9                  |\
    \ 12.1      |  |  |  |  |\n| Avg. #Algorithm Knowledge Summaries             |\
    \ 2.4  | 3.0                   | 3.8       |  |  |  |  |\n| Avg. #Ground Truth\
    \ Solutions                    | 4.8  | 4.9                   | 4.8       |  |\
    \  |  |  |\n| Interaction Records with Real Humans            |      |       \
    \                |           |  |  |  |  |\n| #Problems                      \
    \                 | 100  | 100                   | 100       |  |  |  |  |\n|\
    \ Avg. #Turns (#Human Feedback)                   | 3.4  | 5.1               \
    \    | 6.9       |  |  |  |  |\n| Avg. #Human-Annotated LLM Code Errors      \
    \     | 1.3  | 1.5                   | 2.0       |  |  |  |  |\n\nTable 1: ELABORATIONSET\
    \ Dataset statistics. Further details and examples are provided in Appendix [A.](#page-11-2)\n\
    \nWith ELABORATION, we evaluate strengths and weaknesses of existing methods using\
    \ both LLMbased user simulators and real human participants. Our findings demonstrate\
    \ limited capacity of LLM alone for solving competitive programming problems,\
    \ particularly those of high difficulty or unseen ones (-9.2%, on average). Notably,\
    \ human-LLM collaboration significantly improves task performance (+7.0%, on average),\
    \ particularly during the coding stage, although this requires efficient resource\
    \ management. Real human experiments further highlight the complementary strengths\
    \ of human and LLM bug identification, leading to a powerful synergy. In this\
    \ paper, ELABORATION stands as a valuable resource to provide guidance and insight\
    \ into benchmarking human-LLM competitive programming for future improvements.\
    \ In conclusion, our contributions are as follows:\n\n- We introduce ELABORATION,\
    \ a novel benchmark for Human-LLM competitive programming, which features a comprehensive\
    \ evaluation protocol to facilitate a thorough assessment.\n- We present the first\
    \ taxonomy of human feedback consolidating the entire programming process into\
    \ four stages, enabling ELABORATION to evaluate task effectiveness at each stage.\n\
    - We introduce ELABORATIONSET, a novel programming dataset specifically designed\
    \ for human-LLM collaboration. It comprises 8,320 problems, meticulously annotated\
    \ to enable largescale simulated human feedback and facilitate cost-effective\
    \ real human interaction studies.\n- With ELABORATION, we evaluate pros and cons\
    \ of existing methods using both LLM-based user simulators and real human participants,\
    \ providing guidance and insight for future improvements.\n\n| Competitive Programming\
    \                            |           | Basic Problem Information   |     \
    \                |                     | Annotations for Human Interaction | \
    \                         |                           | Real Human Interaction\
    \ |                                 |\n|----------------------------------------------------|-----------|-----------------------------|---------------------|---------------------|-----------------------------------|--------------------------|---------------------------|------------------------|---------------------------------|\n\
    | Benchmark                                          | Task Type | Contamination<br>Annotation\
    \ | Stage<br>Annotation | Compile<br>Feedback | Clarify<br>Problem           \
    \     | Algorithmic<br>Knowledge | Ground Truth<br>Solutions | Bug<br>Annotation\
    \      | Human-LLM<br>Multi-turn Records |\n| APPS(Hendrycks et al., 2021a)  \
    \                    | Automatic | ✗                           | ✗           \
    \        | ✗                   | ✗                                 | ✗       \
    \                 | ✓                         | ✗                      | ✗   \
    \                            |\n| CODE-CONTESTS(Li et al., 2022)             \
    \        | Automatic | ✗                           | ✗                   | ✗ \
    \                  | ✗                                 | ✗                   \
    \     | ✓                         | ✓<br>✗                 | ✗               \
    \                |\n| XCODEEVAL(Khan et al., 2023)                       | Automatic\
    \ | ✗                           | ✓<br>✗              | ✗                   |\
    \ ✗                                 | ✗                        | ✓           \
    \              | ✓<br>✗                 | ✗                               |\n\
    | CODESCOPE(Yan et al., 2023)                        | Automatic | ✗         \
    \                  | ✓<br>✗              | ✓                   | ✓           \
    \                      | ✗                        | ✓<br>✗                   \
    \ | ✗                      | ✗                               |\n| KareCoder(Huang\
    \ et al., 2024c)                     | Automatic | ✓<br>✗                    \
    \  | ✗                   | ✗                   | ✗                           \
    \      | ✓                        | ✓                         | ✗            \
    \          | ✗                               |\n| TACO(Li et al., 2023b)     \
    \                        | Automatic | ✗                           | ✗       \
    \            | ✗                   | ✗                                 | ✗   \
    \                     | ✗                         | ✗                      | ✗\
    \                               |\n| USCAOBENCH(Shi et al., 2024)            \
    \           | Automatic | ✗                           | ✓<br>✗              |\
    \ ✗                   | ✗                                 | ✗                \
    \        | ✓                         | ✗                      | ✗            \
    \                   |\n| LIVECODEBENCH(Jain et al., 2024)                   |\
    \ Automatic | ✓                           | ✓<br>✗              | ✗          \
    \         | ✗                                 | ✗                        | ✓ \
    \                        | ✗                      | ✗                        \
    \       |\n| OpenCoderInterpreter(Zheng et al., 2024) Human-LLM |           |\
    \ ✗                           | ✗                   | ✓                   | ✗\
    \                                 | ✗                        | ✓<br>✗        \
    \            | ✓<br>✗                 | ✗                               |\n| ELABORATION\
    \ (ours)                                 | Human-LLM | ✓                     \
    \      | ✓                   | ✓                   | ✓                       \
    \          | ✓                        | ✓                         | ✓        \
    \              | ✓                               |\n\nTable 2: Difference between\
    \ ELABORATION and existing benchmarks. Only OpenCoderInterpreter and ours are\
    \ specifically designed for human-LLM competitive programming. Here, '✓✗' indicates\
    \ partial support.\n\n# 2 Related Work\n\nOur research focuses on human-LLM competitive\
    \ programming, offering a comprehensive literature review and highlighting our\
    \ novel contributions.\n\nCompetitive Programming. Competitive programming challenges\
    \ participants to solve complex algorithmic problems under strict time and memory\
    \ constraints [\\(Dale and Weems,](#page-9-2) [2014\\)](#page-9-2). Each problem\
    \ begins with a detailed statement outlining the requirements and input/output\
    \ specifications [\\(Becker](#page-8-0) [et al.,](#page-8-0) [2023\\)](#page-8-0).\
    \ Unlike other programming tasks that focus on real-world applications, maintainability,\
    \ readability, and collaboration [\\(Passos et al.,](#page-10-12) [2011;](#page-10-12)\
    \ [Gallmeister,](#page-9-11) [1995;](#page-9-11) [Martin,](#page-10-13) [2003\\\
    )](#page-10-13), competitive programming demands precise problem comprehension,\
    \ efficient algorithmic design, accurate code implementation, and thorough debugging\
    \ to produce a solution that passes rigorous testing within the specified time\
    \ and memory limits [\\(Huang et al.,](#page-9-1) [2023b;](#page-9-1) [Dale and\
    \ Weems,](#page-9-2) [2014;](#page-9-2) [Jain et al.,](#page-9-4) [2024\\)](#page-9-4).\n\
    \nHuman-LLM Competitive Programming. While the success of LLMs in other domains\
    \ [\\(Zhang et al.,](#page-11-3) [2025;](#page-11-3) [Huang et al.,](#page-9-12)\
    \ [2025\\)](#page-9-12) has fueled the application to automate competitive programming,\
    \ recent benchmarks reveal limitations in their ability to solve expert-level\
    \ problems [\\(Hendrycks et al.,](#page-9-13) [2021b;](#page-9-13) [Li et al.,](#page-10-10)\
    \ [2022;](#page-10-10) [Zheng et al.,](#page-11-4) [2023;](#page-11-4) [Yan et\
    \ al.,](#page-11-0) [2023;](#page-11-0) [Jain et al.,](#page-9-4) [2024\\)](#page-9-4),\
    \ even with compiler feedback (e.g., an error message) [\\(Yang et al.,](#page-11-5)\
    \ [2024;](#page-11-5) [Phung et al.,](#page-10-14) [2023;](#page-10-14) [Tian\
    \ et al.,](#page-10-15) [2024\\)](#page-10-15). This suggests limited practical\
    \ utility when relying solely on LLMs for this complex task. Consequently, research\
    \ is increasingly focusing on human-LLM competitive programming, which leverages\
    \ multiturn human feedback to enhance LLM performance. However, current methods\
    \ often restrict human feedback to providing (pseudo-)code [\\(Mozannar](#page-10-8)\
    \ [et al.,](#page-10-8) [2023;](#page-10-8) [Wang et al.,](#page-10-9) [2024\\\
    )](#page-10-9) or debugging assistance [\\(Zheng et al.,](#page-11-1) [2024;](#page-11-1)\
    \ [Shi et al.,](#page-10-7) [2024\\)](#page-10-7), neglecting the broader potential\
    \ of human guidance\n\nacross the entire programming process. This leads to fragmented\
    \ understanding of the effectiveness and characteristics of human-LLM competitive\
    \ programming, and further motivates our work.\n\nHuman Feedback Simulation in\
    \ Human-LLM Competitive Programming. Evaluating any interactive systems is inherently\
    \ labor-intensive [\\(Huang](#page-9-14) [et al.,](#page-9-14) [2023a\\)](#page-9-14).\
    \ Therefore, human simulators are commonly used in this field. While rule-based\
    \ simulators have been employed [\\(Mozannar et al.,](#page-10-8) [2023;](#page-10-8)\
    \ [Zheng et al.,](#page-11-1) [2024\\)](#page-11-1), their limited realism and\
    \ comprehensiveness fall short of capturing the nuanced aspects of human feedback\
    \ in competitive programming, which requires deep problem understanding, algorithmic\
    \ knowledge, adaptive problem-solving, and error correction skills [\\(Robins\
    \ et al.,](#page-10-16) [2003;](#page-10-16) [Pless,](#page-10-17) [2011;](#page-10-17)\
    \ [Lee,](#page-10-18) [2018\\)](#page-10-18). The emergence of LLMbased simulators\
    \ offers a more realistic alternative, enhancing both simulation and evaluation\
    \ reliability [\\(Zheng et al.,](#page-11-1) [2024;](#page-11-1) [Mozannar et\
    \ al.,](#page-10-8) [2023\\)](#page-10-8). In line with these studies, our evaluation\
    \ protocol also leverages LLM-based simulators to mimic human feedback at each\
    \ programming stage. Crucially, we augment our benchmark with real human participants,\
    \ providing a more grounded assessment.\n\n## 3 ELABORATION Benchmark\n\nOur ELABORATION\
    \ benchmark evaluates human-LLM competitive programming using a novel protocol\
    \ that incorporates a comprehensive taxonomy of human feedback, covering all stages\
    \ of the process, and a new human-LLM programming dataset. Evaluation Protocol\
    \ Overview. Our evaluation protocol accommodates both real human programmers and\
    \ user simulators to provide feedback at each stage of the competitive programming\
    \ process, as illustrated in Figure [1.](#page-1-0) For simplicity, we refer to\
    \ both as \"humans\" unless otherwise noted. In this human-LLM competitive programming\
    \ process, each LLM interacts iteratively with a human, generating intermediate\
    \ results and receiving feedback until a correct solution is produced or a maximum\
    \ number of iterations is reached. A correct solution is defined as code that\
    \ passes all test cases within the specified time and memory limits.\n\n## 3.1\
    \ Human Feedback Taxonomy\n\nTo support comprehensive benchmark, we establish\
    \ a taxonomy of human feedback, informed by analyses of real-world human interactions\
    \ [\\(Robins](#page-10-16) [et al.,](#page-10-16) [2003;](#page-10-16) [Fincher,](#page-9-15)\
    \ [1999\\)](#page-9-15) and established competitive programming practices [\\\
    (Cormen et al.,](#page-9-0) [2022;](#page-9-0) [Huang et al.,](#page-9-1) [2023b;](#page-9-1)\
    \ [Dale and Weems,](#page-9-2) [2014\\)](#page-9-2). This taxonomy consolidates\
    \ the entire programming process into the following primary stages.\n\n- Problem\
    \ Comprehension. LLMs require a thorough understanding of the problem statement.\
    \ To facilitate this, human feedback can provide crucial requirements and specifications.\
    \ For example, specifying edge cases that need to be handled (*e.g., handling\
    \ empty input arrays*), summarizing the functionalities that the code needs to\
    \ implement (*e.g., return the median value*), or highlighting the key constraints\
    \ and objectives (*e.g., solution must run in O(nlogn) time*).\n- Solution Planning.\
    \ LLMs engage in solution planning by selecting appropriate algorithms. To facilitate\
    \ this, human feedback can suggest effective algorithms, provide justifications,\
    \ and even supply complete and accurate pseudocode. For example, a human might\
    \ suggest using Dijkstra algorithm for a shortest path problem, explaining its\
    \ suitability for weighted graphs and providing the pseudocode for its implementation.\n\
    - Code Generation. LLMs must generate complete, compilable code. In this case,\
    \ human feedback can suggest solution strategies to improve the generated code\
    \ by, for example, suggesting a more efficient data structure (*e.g., a stack*)\
    \ and explicitly coding algorithm implementation details (*e.g., using a binary\
    \ heap-based priority queue and a stack for Dijkstra algorithm*).\n- Code Debugging.\
    \ LLMs must pass the complete set of test cases[1](#page-3-0) . In this case,\
    \ humans assist in identifying errors until all unseen test cases are passed (*e.g.,\
    \ pinpointing logic flaws leading to infinite loops*). Current, most exiting methods\
    \ limit their focus at this stage and provide conversational feedback for error\
    \ identification [\\(Zheng](#page-11-1) [et al.,](#page-11-1) [2024;](#page-11-1)\
    \ [Shi et al.,](#page-10-7) [2024\\)](#page-10-7).\n\n## 3.2 ELABORATIONSET Dataset\n\
    \nTo facilitate our evaluation, we created ELABO-RATIONSET, a high-quality human-LLM\
    \ programming dataset. It comprises 8,320 problems from Codeforces[2](#page-3-1)\
    \ and AtCoder[3](#page-3-2) between October 2011 and November 2024, meticulously\
    \ annotated to enable both large-scale simulated human feedback and cost-effective\
    \ studies using real human participants across all stages of the programming process.\
    \ See Table [1](#page-1-1) and Figure [5](#page-12-0) for illustration.\n\nProblem\
    \ Information Collection. Our dataset is collected in a three-step process: First,\
    \ our automatic HTML scrapers[4](#page-3-3) extract all necessary information\
    \ from Codeforces and AtCoder, including problem statements, input/output formats,\
    \ test case examples, dates, tags, and difficulty levels. Second, because not\
    \ all code problems provide test cases, we utilize GPT-4o to generate them where\
    \ needed, following the approach of [Li et al.](#page-10-6) [\\(2023b\\)](#page-10-6);\
    \ [Jain](#page-9-4) [et al.](#page-9-4) [\\(2024\\)](#page-9-4) and then manually\
    \ check their validation. Third, the final dataset is split by date for our later\
    \ contamination-free evaluation (i.e., evaluating the performance on unseen data).\n\
    \nAnnotations for Human Interaction. To mitigate the labor-intensive and expertise-dependent\
    \ nature of human problem-solving in competitive programming, ELABORATIONSET incorporates\
    \ fully accurate, static annotations for each problem. This provides a reliable\
    \ reference for humans and facilitates cost-effective solutions for human-LLM\
    \ collaboration. Specifically, annotations include: problem statement clarifications\
    \ (requirements and specifications of each problem); algorithm-specific knowledge\
    \ summaries (required algorithms to solve each problem and their definitions and\
    \ pseudocodes); and ground truth solutions (see Figure [5\\)](#page-12-0). This\
    \ resource enables human reviewers to make informed feedback decisions and allows\
    \ for the simulation of human participants with varying levels of expertise by\
    \ adjusting the granularity of the provided feedback. Notably, all annotations,\
    \ except ground truth solutions, undergo a two-stage process: initial LLM generation\
    \ followed by manual review to ensure quality. Ground truth solutions are sourced\
    \ directly from the respective programming platforms. Refer to Appendix [A.2.2](#page-12-1)\
    \ for details.\n\n<span id=\"page-3-0\"></span><sup>1</sup> It could use the compiler\
    \ output to refine its code when necessary.\n\n<span id=\"page-3-1\"></span><sup>2</sup>\
    \ <https://codeforces.com/>\n\n<span id=\"page-3-2\"></span><sup>3</sup> <https://atcoder.jp/>\n\
    \n<span id=\"page-3-3\"></span><sup>4</sup> Scraper code will be released along\
    \ with our dataset.\n\n<span id=\"page-4-1\"></span>![](_page_4_Figure_0.jpeg)\n\
    \n# 4 Benchmark Experiments\n\n## 4.1 Experiment Setup\n\nHuman Simulators. Our\
    \ benchmark incorporates LLM-based user simulators for large-scale evaluation,\
    \ employing *O1-Mini* to ensure realistic human simulation. In particular, we\
    \ include the following two participant groups representing a range of programming\
    \ expertise. By this means, we assess the effectiveness of the evaluated methods\
    \ across a range of programming abilities and to understand how well the methods\
    \ cater to different levels of user expertise. Notably, novice programmers are\
    \ excluded due to their limited capacity to provide valuable feedback for LLM\
    \ improvement.\n\n- Student Programmer (Intermediate Skill Level) possess more\
    \ than basic programming knowledge but lack the deep expertise. Following established\
    \ practices in human programmer simulation [\\(Zheng et al.,](#page-11-1) [2024\\\
    )](#page-11-1), the *O1-Mini* is prompted to provide feedback based on its internal\
    \ knowledge.\n- Teacher Programmer (Expert Level) possess a high level of programming\
    \ skill and experience. Unlike student programmer, this simulator leverages the\
    \ complete ELABORATIONSET dataset to ensure expert-level performance.\n\nHuman\
    \ Participants. Our experiments also incorporate real human participants to gain\
    \ practical insights. Refer to Section [4.4](#page-6-0) for details.\n\nLLM Models.\
    \ We benchmark thirteen LLMs, encompassing both closed-source and open-source\
    \ models of varying sizes. This include O1-Mini [\\(OpenAI,](#page-10-19) [2024b\\\
    )](#page-10-19), GPT-4o [\\(OpenAI,](#page-10-20) [2024a\\)](#page-10-20), GPT-4-Turbo\
    \ [\\(OpenAI,](#page-10-21) [2023\\)](#page-10-21), Gemini-1.5-pro [\\(Team](#page-10-22)\
    \ [et al.,](#page-10-22) [2024\\)](#page-10-22), Claude-3.5 [\\(cla,](#page-8-1)\
    \ [2024\\)](#page-8-1), CodeLlama [\\(Roziere et al.,](#page-10-2) [2023\\)](#page-10-2),\
    \ Deepseek-Coder [\\(Guo et al.,](#page-9-3) [2024\\)](#page-9-3), Qwen2.5-Coder\
    \ [\\(Hui et al.,](#page-9-16) [2024\\)](#page-9-16).\n\nEvaluation Metrics. Following\
    \ established practice [\\(Belz et al.,](#page-9-17) [2021;](#page-9-17) [Yan\
    \ et al.,](#page-11-0) [2023;](#page-11-0) [Shi et al.,](#page-10-7) [2024;](#page-10-7)\
    \ [Jain et al.,](#page-9-4) [2024\\)](#page-9-4), we utilize the Pass@k\n\n(k=1,3,5)[5](#page-4-0)\
    \ metric [\\(Chen et al.,](#page-9-18) [2021\\)](#page-9-18) to evaluate overall\
    \ performance, with Pass@1 holding particular importance due to its relevance\
    \ to real-world applications. To exclude the influence of potentially memorized\
    \ solutions from the training corpus, we also employ a contamination-free evaluation,\
    \ focusing on problems released after the LLMs' respective cutoff dates.\n\nImplementation\
    \ Details. Our evaluation implementation proceeds through the forementioned four\
    \ stages, with iterative human feedback provided until the human is satisfied\
    \ with the LLM's response or a maximum iteration limit is reached. Finegrained\
    \ evaluation involves assessing LLM performance at each stage by comparing their\
    \ outputs (e.g., summarized problem requirements and specifications, algorithm\
    \ selection with justification, and pseudocode) against the annotated ground truth\
    \ in our dataset. Code generation and debugging are evaluated based on final code\
    \ performance, with error analysis conducted using either human participants or\
    \ simulators. In our experiments, we utilize nucleus sampling, with a maximum\
    \ of 10 iterations per stage. See Appendix [B](#page-13-0) for more details.\n\
    \n## 4.2 Overall Performance (RQ1)\n\nThis section benchmarks the performance\
    \ of human-LLM competitive programming, assessing both overall performance and\
    \ performance within specific problem categories. We report the results in Table\
    \ [3](#page-5-0) and draw the following observations.\n\n*Are LLMs qualified competitive\
    \ programmers?* – They demonstrate limited capacity for solving competitive programming\
    \ problems, particularly those of high difficulty or unseen ones. As shown in\
    \ Table [3,](#page-5-0) model performance exhibits a positive correlation with\
    \ parameter size (larger models generally perform better), with the recently released\
    \ O1-Mini achieving the best results, with a pass@1 score of 59.3% on unseen problems.\
    \ However, this effectiveness is limited to simpler programming problems. Performance\
    \ across all LLMs, including those specifically designed for coding tasks, degrades\
    \ significantly as problem difficulty increases, with the average pass@1 score\
    \ is merely 3.4% on unseen hard problems, rendering them alone unsuitable for\
    \ real-world applications. Furthermore, performance deteriorates even further\
    \ in contamination-free evaluations, as illustrated in Table [3](#page-5-0) and\
    \ Figure [2,](#page-4-1) with an average drop of 9.3%\n\n<span id=\"page-4-0\"\
    ></span><sup>5</sup>Given space limit, results with k=3,5 are in Appendix [C.](#page-14-0)\n\
    \n<span id=\"page-5-0\"></span>\n\n| Model (Cut-off Date Release Date)       |\
    \             |              | Contamination Evaluation (%) |              | Contamination-free\
    \ Evaluation (%) |              |              |              |\n|-----------------------------------------|-------------|--------------|------------------------------|--------------|-----------------------------------|--------------|--------------|--------------|\n\
    |                                         | Easy        | Middle       | Hard\
    \                         | Overall      | Easy                              |\
    \ Middle       | Hard         | Overall      |\n| O1-Mini (2023-12   2024-09)\
    \             | 88.1        | 70.3         | 41.7                         | 66.7\
    \         | 80.6                              | 66.6         | 30.8         |\
    \ 59.3         |\n| GPT-4o (2023-11   2024-05)              | 80.4        | 50.5\
    \         | 20.8                         | 50.6         | 74.1               \
    \               | 31.7         | 10.3         | 38.7         |\n| + Student Programmer\
    \ Feedback           | 83.1        | 53.1         | 24.3                     \
    \    | 53.5         | 76.2                              | 34.8         | 15.1\
    \         | 42.0         |\n| + Teacher Programmer Feedback           | 87.7 \
    \       | 66.1         | 38.2                         | 64.0         | 80.1  \
    \                            | 42.9         | 23.3         | 48.8         |\n\
    | GPT-4-Turbo (2023-05   2023-11)         | 70.5        | 40.6         | 8.7 \
    \                         | 39.9         | 65.2                              |\
    \ 27.3         | 5.8          | 32.8         |\n| + Student Programmer Feedback\
    \           | 75.5        | 46.1         | 12.1                         | 44.6\
    \         | 70.8                              | 33.2         | 8.8          |\
    \ 37.6         |\n| + Teacher Programmer Feedback           | 83.2        | 58.8\
    \         | 20.1                         | 54.0         | 75.3               \
    \               | 39.8         | 14.3         | 43.1         |\n| Gemini-1.5-pro\
    \ (2023-11   2024-02)      | 81.2        | 48.2         | 22.0               \
    \          | 50.5         | 73.2                              | 32.8         |\
    \ 9.3          | 38.4         |\n| + Student Programmer Feedback           | 84.0\
    \        | 50.1         | 25.1                         | 53.0         | 75.5 \
    \                             | 35.0         | 13.1         | 41.2         |\n\
    | + Teacher Programmer Feedback           | 89.1        | 65.6         | 36.6\
    \                         | 63.8         | 81.0                              |\
    \ 40.2         | 24.2         | 48.5         |\n| Claude-3.5 (2024-03   2024-06)\
    \          | 78.0        | 51.3         | 16.2                         | 48.5\
    \         | 74.5                              | 34.3         | 5.4          |\
    \ 38.1         |\n| + Student Programmer Feedback           | 82.2        | 55.0\
    \         | 24.1                         | 53.8         | 76.6               \
    \               | 37.1         | 7.9          | 40.5         |\n| + Teacher Programmer\
    \ Feedback           | 87.0        | 66.7         | 33.4                     \
    \    | 62.4         | 83.1                              | 44.2         | 16.5\
    \         | 47.9         |\n| Avg.                                    | 77.5 \
    \       | 47.7         | 16.9                         | 47.4         | 71.8  \
    \                            | 31.5         | 7.7          | 37.0         |\n\
    | + Student Programmer Feedback           | 81.2 (+3.7) | 51.1 (+3.4)  | 21.4\
    \ (+4.5)                  | 51.2 (+3.8)  | 74.8 (+3.0)                       |\
    \ 35.0 (+3.5)  | 11.2 (+3.5)  | 40.3 (+3.3)  |\n| + Teacher Programmer Feedback\
    \           | 86.8 (+9.3) | 64.3 (+16.6) | 32.1 (+15.2)                 | 61.1\
    \ (+13.7) | 79.9 (+8.1)                       | 41.8 (+10.3) | 19.6 (+11.9) |\
    \ 47.1 (+10.1) |\n|                                         |             |  \
    \            | ∼7B Scale                    |              |                 \
    \                  |              |              |              |\n| CodeLlama-7B\
    \ (2023-01   2024-01)        | 30.3        | 5.9          | 0.5              \
    \            | 12.2         | 15.2                              | 2.1        \
    \  | 0.3          | 5.9          |\n| + Student Programmer Feedback          \
    \ | 36.7        | 10.3         | 2.2                          | 16.4         |\
    \ 24.2                              | 3.1          | 1.4          | 9.6      \
    \    |\n| + Teacher Programmer Feedback           | 48.6        | 17.8       \
    \  | 6.9                          | 24.4         | 35.9                      \
    \        | 8.4          | 4.7          | 16.3         |\n| Deepseek-Coder-6.7B\
    \ (2023-09   2023-11) | 40.6        | 15.4         | 1.8                     \
    \     | 19.3         | 21.4                              | 7.0          | 0.7\
    \          | 9.7          |\n| + Student Programmer Feedback           | 46.3\
    \        | 18.8         | 4.3                          | 23.1         | 27.8 \
    \                             | 11.3         | 2.0          | 13.7         |\n\
    | + Teacher Programmer Feedback           | 58.6        | 27.8         | 8.2 \
    \                         | 31.5         | 39.2                              |\
    \ 24.2         | 6.1          | 23.2         |\n| Qwen2.5-Coder-7B (2024-06  \
    \ 2024-11)    | 61.2        | 22.4         | 4.9                          | 29.5\
    \         | 48.6                              | 9.3          | 0.5          |\
    \ 19.5         |\n| + Student Programmer Feedback           | 70.1        | 26.6\
    \         | 6.7                          | 34.5         | 53.8               \
    \               | 12.3         | 2.3          | 22.8         |\n| + Teacher Programmer\
    \ Feedback           | 76.3        | 35.5         | 11.3                     \
    \    | 41.0         | 57.8                              | 21.6         | 5.9 \
    \         | 28.4         |\n| Avg.                                    | 44.0 \
    \       | 14.6         | 2.4                          | 20.3         | 28.4  \
    \                            | 6.1          | 0.5          | 11.7         |\n\
    | + Student Programmer Feedback           | 51.0(+7.0)  | 18.6(+4.0)   | 4.4(+2.0)\
    \                    | 24.7(+4.4)   | 35.3(+6.9)                        | 8.9(+2.8)\
    \    | 1.9(+1.4)    | 15.4(+3.7)   |\n| + Teacher Programmer Feedback        \
    \   | 61.2(+17.2) | 27.0(+12.4)  | 8.8(+6.4)                    | 32.3(+12.0)\
    \  | 44.3(+15.9)                       | 18.1(+12.0)  | 5.6(+5.1)    | 22.6(+10.9)\
    \  |\n|                                         |             |              |\
    \ ∼13B Scale                   |              |                              \
    \     |              |              |              |\n| CodeLlama-13B (2023-01\
    \   2024-01)       | 35.8        | 7.3          | 1.7                        \
    \  | 14.9         | 23.5                              | 3.0          | 0.3   \
    \       | 8.9          |\n| + Student Programmer Feedback           | 40.3   \
    \     | 12.1         | 2.9                          | 18.4         | 26.3    \
    \                          | 9.8          | 1.4          | 12.5         |\n| +\
    \ Teacher Programmer Feedback           | 44.2        | 19.9         | 5.8   \
    \                       | 23.3         | 29.8                              | 14.6\
    \         | 3.1          | 15.8         |\n| Qwen2.5-Coder-14B (2024-06   2024-11)\
    \   | 70.8        | 28.7         | 7.7                          | 35.7       \
    \  | 58.3                              | 15.1         | 2.2          | 25.2  \
    \       |\n| + Student Programmer Feedback           | 75.9        | 33.5    \
    \     | 10.2                         | 40.0         | 61.2                   \
    \           | 18.9         | 4.1          | 28.1         |\n| + Teacher Programmer\
    \ Feedback           | 80.1        | 41.5         | 14.2                     \
    \    | 45.3         | 66.3                              | 24.3         | 6.8 \
    \         | 32.5         |\n| Avg.                                    | 53.3 \
    \       | 18.0         | 4.7                          | 25.3         | 40.9  \
    \                            | 9.1          | 1.3          | 17.1         |\n\
    | + Student Programmer Feedback           | 58.1 (+4.8) | 22.8 (+4.8)  | 6.6 (+1.9)\
    \                   | 29.2 (+3.9)  | 43.8 (+2.9)                       | 14.4\
    \ (+5.3)  | 2.8 (+1.5)   | 20.3 (+3.2)  |\n| + Teacher Programmer Feedback   \
    \        | 62.2 (+8.9) | 30.7 (+12.7) | 10.0 (+5.3)                  | 34.3 (+9.0)\
    \  | 48.1 (+7.2)                       | 19.5 (+10.4) | 5.0 (+3.7)   | 24.2 (+7.1)\
    \  |\n|                                         |             |              |\
    \ ∼34B Scale                   |              |                              \
    \     |              |              |              |\n| CodeLlama-34B (2023-01\
    \   2024-01)       | 38.1        | 7.9          | 3.1                        \
    \  | 16.4         | 25.0                              | 5.1          | 1.0   \
    \       | 10.4         |\n| + Student Programmer Feedback           | 42.0   \
    \     | 12.3         | 4.0                          | 19.4         | 26.1    \
    \                          | 8.4          | 2.3          | 12.3         |\n| +\
    \ Teacher Programmer Feedback           | 49.2        | 18.8         | 6.2   \
    \                       | 24.7         | 32.2                              | 13.0\
    \         | 4.4          | 16.5         |\n| Deepseek-Coder-33B (2023-09   2023-11)\
    \  | 63.9        | 23.7         | 4.2                          | 30.6        \
    \ | 50.6                              | 10.4         | 1.2          | 20.7   \
    \      |\n| + Student Programmer Feedback           | 74.8        | 28.7     \
    \    | 7.0                          | 36.8         | 55.8                    \
    \          | 13.3         | 3.1          | 24.0         |\n| + Teacher Programmer\
    \ Feedback           | 78.9        | 40.1         | 12.3                     \
    \    | 43.8         | 68.8                              | 20.4         | 5.5 \
    \         | 31.6         |\n| Qwen2.5-Coder-32B (2024-06   2024-11)   | 77.3 \
    \       | 41.3         | 9.0                          | 42.5         | 70.1  \
    \                            | 20.3         | 3.2          | 31.2         |\n\
    | + Student Programmer Feedback           | 80.4        | 45.3         | 11.0\
    \                         | 45.6         | 72.0                              |\
    \ 23.1         | 4.0          | 33.0         |\n| + Teacher Programmer Feedback\
    \           | 85.1        | 53.4         | 15.8                         | 51.4\
    \         | 76.8                              | 30.1         | 7.6          |\
    \ 38.0         |\n| Avg.                                    | 59.8        | 24.3\
    \         | 5.4                          | 29.8         | 48.6               \
    \               | 11.9         | 1.8          | 20.8         |\n| + Student Programmer\
    \ Feedback           | 65.7(+5.9)  | 28.8(+4.5)   | 7.3(+1.9)                \
    \    | 33.9(+4.1)   | 51.3(+2.7)                        | 14.9(+3.0)   | 3.1(+1.3)\
    \    | 23.1(+2.3)   |\n| + Teacher Programmer Feedback           | 71.1(+11.3)\
    \ | 37.4(+13.1)  | 11.4(+6.0)                   | 40.0(+10.2)  | 59.3(+10.7) \
    \                      | 21.2(+9.3)   | 5.8(+4.0)    | 28.7(+7.9)   |\n| Average\
    \ over All LLMs                   | 60.7        | 28.6         | 8.4         \
    \                 | 32.6         | 50.0                              | 16.5  \
    \       | 3.4          | 23.3         |\n| + Student Programmer Feedback     \
    \      | 65.9(+5.2)  | 32.7(+4.1)   | 11.2(+2.8)                   | 36.6(+4.0)\
    \   | 53.9(+3.9)                        | 20.0(+3.5)   | 5.5(+2.1)    | 26.4(+3.1)\
    \   |\n| + Teacher Programmer Feedback           | 72.3(+11.6) | 42.7(+14.1) \
    \ | 17.4(+9.0)                   | 44.1(+11.5)  | 60.5(+10.5)                \
    \       | 27.0(+10.5)  | 10.2(+6.8)   | 32.6(+9.3)   |\n\nTable 3: Pass@1 scores\
    \ across various LLMs and varying levels of human feedback expertise. Since O1-Mini\
    \ is expensive and recently released, experiments with it have been deferred.\
    \ Refer to Appendix [C](#page-14-0) for more results.\n\non unseen problems compared\
    \ to seen ones. This suggests that a substantial portion of LLM performance may\
    \ stem from memorization of the training dataset[6](#page-5-1) , a issue warrants\
    \ further investigation.\n\n*How effective can human feedback be in assisting\
    \ LLMs with competitive programming challenges?* – Human-LLM collaboration significantly\
    \ enhances LLM performance, demonstrating the crucial role of human feedback.\
    \ As shown in Table [3,](#page-5-0) the integration of human participation throughout\
    \ the programming process, creating a human-LLM competitive programming framework,\
    \ resulted in significant performance gains across various LLMs, problem difficulties,\
    \ and levels of\n\nhuman expertise. Interestingly, such performance gain is consistently\
    \ observed regardless of data contamination, definitively demonstrating the power\
    \ of human-LLM collaboration in solving complex programming challenges. This human-LLM\
    \ collaboration approach resulted in an average increase of 9.3% in the Pass@1\
    \ score when teacher programmers provided feedback on unseen problems, and an\
    \ average increase of 11.5% when they offered feedback on seen problems. Similarly,\
    \ student programmers contributed to an average improvement of 3.1% in the Pass@1\
    \ score for unseen problems and 4.0% for seen problems. However, the integration\
    \ of human feedback necessitates a corresponding investment of human effort, a\
    \ topic explored further in the following section.\n\n<span id=\"page-5-1\"></span><sup>6</sup>This\
    \ memorization isn't simply rote learning; LLMs still produce some correct answers\
    \ on unseen problems.\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_0.jpeg)\n\
    \nFigure 3: Stage-specific evaluation averaged over various LLMs. While coding-stage\
    \ feedback is most beneficial, it also incurs higher token usage.\n\n<span id=\"\
    page-6-2\"></span>\n\n| Stage               | Easy | Middle | Hard |\n|---------------------|------|--------|------|\n\
    | Comprehension Stage | 0.96 | 0.93   | 0.90 |\n| Planning Stage      | 0.72 |\
    \ 0.53   | 0.41 |\n\nTable 4: Fine-grained evalution at comprehension and planning\
    \ stages. We report averaged comprehension accuracy of summarized requirements\
    \ and specifications, and average planning accuracy of algorithm selection. Refer\
    \ to Appendix [C.3](#page-14-1) for nuanced understanding.\n\n## 4.3 Finer-grained\
    \ Analysis (RQ2)\n\nThis section delves into the detailed characteristics of human-LLM\
    \ competitive programming, with specical focus on the task performance and cost\
    \ efficiency across various stages.\n\n*At what stage of the programming process\
    \ is human feedback most beneficial?* – During the coding stage, even on problems\
    \ with no data contamination. Figures [3](#page-6-1) illustrate the effectiveness\
    \ of human feedback at different stages of competitive programming, both with\
    \ and without data contamination, as measured by the average improvement in Pass@1.\
    \ Regardless of data contamination, The results indicate that human feedback is\
    \ consistently least effective during the comprehension stage and most effective\
    \ during the coding stage, indicating that LLMs readily understand problem statements\
    \ (cf. Table [4,](#page-6-2) high performance at comprehension stage) but struggle\
    \ to generate correct code. Taking Table [5](#page-6-3) for example, when tackling\
    \ the classic 8-queens problem, the LLM frequently makes initialization errors\
    \ and omits checks for queen conflicts. In this case, targeted human feedback\
    \ during the coding stage can effectively mitigate these issues. Crucially, the\
    \ minimal improvement observed with debugging-stage feedback highlights the importance\
    \ of providing guidance throughout the entire process, underscoring our contributions.\
    \ *What are characteristics of different types of programmer feedback?* – While\
    \ detailed, expert feedback yields greater benefits, its higher cost necessitates\
    \ efficient use of human resources. As\n\n#### <span id=\"page-6-3\"></span>Teacher\
    \ Programmer Feedback\n\nTo implement the 8-queens problem, start by initializing\
    \ the board representation, usually as a one-dimensional array of length 8, with\
    \ initial values set to a placeholder (like -1). Prepare auxiliary functions to\
    \ verify the legality of queen placements and be ready to store potential solutions.\
    \ When placing a queen in each row i from 0 to row -1, return False if there is\
    \ a conflict with any previously placed queen in the same column or on either\
    \ diagonal (the main diagonal from top-left to bottom-right or the secondary diagonal\
    \ from top-right to bottom-left). Then, ensure that you assign the corresponding\
    \ value in the array to the column number. Finally, if the row number equals the\
    \ number of queens, return the array.\n\n#### Student Programmer Feedback\n\n\
    When implementing the 8-queens problem, initialize the board representation, typically\
    \ as a one-dimensional array of length 8, set initial values to a placeholder,\
    \ prepare auxiliary functions to check the legality of queen placements, and be\
    \ ready to store potential solutions.\n\nTable 5: Coding-stage feedback comparison\
    \ on 8 queens problem. Teacher feedback is more detailed with specific placeholder\
    \ value, iterative placement strategy, and explicit backtracking, etc.\n\nillustrated\
    \ in Figures [3,](#page-6-1) teacher programmers generally achieve higher Pass@1\
    \ improvement than student programmers, attributable to the more detailed and\
    \ nuanced nature of their feedback. However, this improvement comes at a significant\
    \ cognitive cost. For example, given the classic 8-queens problem, the student\
    \ programmer feedback might miss several crucial details compared to teacher feedback\
    \ (cf. Table [5\\)](#page-6-3), such as specific placeholder value, iterative\
    \ placement strategy, and explicit backtracking. Following previous studies [\\\
    (Owoi](#page-10-23)[cho et al.,](#page-10-23) [2023;](#page-10-23) [Wu et al.,](#page-11-6)\
    \ [2024\\)](#page-11-6), we further calculate the average number of tokens in\
    \ human feedback, identifying a substantial token overhead (indicated by the dashed\
    \ line in the figures), particularly during the coding stage. While human participants\
    \ in collaborative programming may be willing to invest time, the high cost necessitates\
    \ more efficient methods for LLM integration of human feedback. Currently, a preliminary\
    \ cost-benefit analysis (by Pass@1/#token) suggests that planning-stage feedback\
    \ might be more cost-effective than currently implemented. Therefore, future research\
    \ within the community should prioritize the development of cost-effective methods\
    \ for integrating human feedback to address this challenge.\n\n## <span id=\"\
    page-6-0\"></span>4.4 Collaborating with Real Humans (RQ3)\n\nWith ELABORATIONSET,\
    \ we benchmark existing methods using real human programmers to gain practical\
    \ insights into their characteristics.\n\nSetup. Five computer science graduate\
    \ students\n\n<span id=\"page-7-2\"></span>\n\n| Debug     | Difficulty | Error\
    \ Identification |        | Problem Resolution (P@1) |         |  |\n|-----------|------------|----------------------|--------|--------------------------|---------|--|\n\
    | Type      | Level      | Precision            | Recall | Original          \
    \       | + Debug |  |\n|           | Easy       | 0.34                 | 0.56\
    \   | 0.66                     | 0.73    |  |\n| Automatic | Middle     | 0.22\
    \                 | 0.36   | 0.27                     | 0.33    |  |\n| Debug\
    \     | Hard       | 0.14                 | 0.28   | 0.06                    \
    \ | 0.08    |  |\n|           | Overall    | 0.23                 | 0.40   | 0.33\
    \                     | 0.38    |  |\n|           | Easy       | 0.92        \
    \         | 0.78   | 0.73                     | 0.92    |  |\n| Human     | Middle\
    \     | 0.80                 | 0.72   | 0.33                     | 0.65    | \
    \ |\n| Debug     | Hard       | 0.72                 | 0.64   | 0.08         \
    \            | 0.29    |  |\n|           | Overall    | 0.81                 |\
    \ 0.71   | 0.38                     | 0.62    |  |\n\nTable 6: Analysis of GPT-4\
    \ Turbo error identification and resolution with automatic and human debugging.\n\
    \nare employed in this study. Following [Shi et al.](#page-10-7) [\\(2024\\)](#page-10-7);\
    \ [Tian et al.](#page-10-15) [\\(2024\\)](#page-10-15), they only provide textual\
    \ feedback identifying syntactic and semantic errors (cf. Table [7\\)](#page-7-0)\
    \ rather than direct code editing. The collaborative process continues until a\
    \ correct solution is found or a maximum of 10 iterations is reached. Considering\
    \ human labor, we focus humans on the debugging stage[7](#page-7-1) using a subset\
    \ of 300 randomly selected unseen problems from ELAB-ORATIONSET. For the LLM,\
    \ we consider GPT-4- Turbo (due to its balance of strong performance and reasonable\
    \ cost). we allows GPT-4-Turbo to refine its solution based on both compiler feedback\
    \ and simulator feedback played by O1-Mini (We term this process as Automatic\
    \ Debug), which reduces the human workload for bug identification. Finally, we\
    \ conduct a post-experiment review, where bugs within all generated codes are\
    \ meticulously annotated. This supplementary dataset will be made publicly available\
    \ along with our dataset. Refer to Appendix [B.1](#page-13-1) for details.\n\n\
    *How valuable is human-LLM collaboration from a practical perspective?* – Humans\
    \ play a vital role in identifying bugs and improving LLM performance. Table [6](#page-7-2)\
    \ reveals that automatic debug struggles to accurately identify bugs, achieving\
    \ only 23% precision and 40% recall, resulting in a mere 5% improvement in Pass@1\
    \ performance. In contrast, incorporating human bug identification significantly\
    \ improved results, yielding 81% precision and 71% recall, and a substantial 24%\
    \ increase in Pass@1 performance, demonstrating the critical role of human intervention.\n\
    \n*How do human and LLM bug detection differ?* – They have complementary strengths,\
    \ creating a powerful synergy. We conduct in-depth debug analysis and report the\
    \ results in Table [7.](#page-7-0) Our experiments show that GPT-4 Turbo generates\
    \ significantly more semantic bugs than syntactic ones,\n\n<span id=\"page-7-0\"\
    ></span>\n\n| Bug<br>Category | Bug Type                | Original | + Automatic<br>Debug\
    \ | + Human<br>Debug |\n|-----------------|-------------------------|----------|----------------------|------------------|\n\
    |                 | Function Related Errors | 11       | 4                   \
    \ | 1                |\n|                 | Operation Errors        | 3      \
    \  | 1                    | 0                |\n| Syntactic       | Structure\
    \ Errors        | 4        | 1                    | 0                |\n| Bugs\
    \            | Declaration Errors      | 8        | 2                    | 0 \
    \               |\n|                 | Import Errors           | 7        | 2\
    \                    | 0                |\n|                 | Overall       \
    \          | 33       | 10                   | 1                |\n|         \
    \        | Control Flow Errors     | 58       | 46                   | 30    \
    \           |\n|                 | Reference Errors        | 17       | 16   \
    \                | 3                |\n| Semantic        | Calculation Errors\
    \      | 23       | 25                   | 4                |\n| Bugs        \
    \    | Incomplete Errors       | 142      | 99                   | 55        \
    \       |\n|                 | Logical Direction Error | 87       | 33       \
    \            | 12               |\n|                 | Suboptimal Errors     \
    \  | 23       | 20                   | 19               |\n|                 |\
    \ Overall                 | 350      | 239                  | 123            \
    \  |\n\nTable 7: Bug statistics for GPT-4 Turbo: with and without feedback. Bug\
    \ description are in Table [17.](#page-30-0)\n\nespecially incomplete and logically\
    \ flawed errors. While automatic debugging effectively addresses most syntactic\
    \ errors (nearly all when combined with human debugging), it struggles with semantic\
    \ errors. Human debugging significantly improves the resolution of these semantic\
    \ errors, particularly those involving references, calculations, incompleteness,\
    \ and logical flaws. This highlights the complementary strengths of humans and\
    \ LLMs, each identifying different types of errors [\\(Rosen](#page-10-24)[feld\
    \ et al.,](#page-10-24) [2018\\)](#page-10-24), and underscores the human-LLM\
    \ collaboration for more accurate outputs.\n\n*How effectively does LLM utilize\
    \ different types of feedback?* – It demonstrates higher success rates correcting\
    \ bugs with accurate human feedback. We analyze GPT-4 Turbo's effectiveness in\
    \ utilizing automatic and human feedback for bug correction. As illustrated in\
    \ Figure [4,](#page-7-3) while LLMs struggle with initial bug identification,\
    \ they demonstrate a strong capacity for correction when provided with accurate\
    \ bug information. With accurate automatic bug identification, LLMs successfully\
    \ resolve 75% of bugs. This increases to 87% with accurate human feedback. Based\
    \ on our analysis, when GPT-4 Turbo failed to correct errors despite receiving\
    \ accurate feedback, the errors are predominantly of Control Flow Errors and Suboptimal\
    \ Errors. This indicates that direct human code modification may be necessary\
    \ to resolve these error types.\n\n<span id=\"page-7-3\"></span>![](_page_7_Figure_10.jpeg)\n\
    \nFigure 4: Bug correction success rates via correct and incorrect automatic (*left*)\
    \ and human feedback (*right*) .\n\n<span id=\"page-7-1\"></span><sup>7</sup>Refer\
    \ to Appendix [C.4](#page-16-0) for results on coding state.\n\n## 5 Conclustion\n\
    \nWe study the effectiveness and characteristics of human-LLM competitive programming\
    \ by (1) introducing a novel taxonomy of human feedback for fine-grained evaluation;\
    \ (2) providing ELAB-ORATIONSET, a new dataset for human-LLM collaboration; and\
    \ (3) developing ELABORATION, a benchmark for evaluating off-the-shelf methods\
    \ and identifying their pros and cons. Thus, our work stands out as a valuable\
    \ resource to provide guidance for future improvement in this field.\n\n# Limitations\n\
    \nSensitive to Prompts. As with other LLM prompting studies[\\(Zhang et al.,](#page-11-7)\
    \ [2024b;](#page-11-7) [Huang et al.,](#page-9-19) [2024b;](#page-9-19) [Zhang\
    \ et al.,](#page-11-8) [2024a;](#page-11-8) [Chen et al.,](#page-9-20) [2024\\\
    )](#page-9-20), our results may be sensitive to prompt. While our prompts underwent\
    \ rigorous review and testing, and our main experiments report averages across\
    \ over 8,000 problems, optimizing prompts for this specific task remains a significant\
    \ challenge and area for future research.\n\nGeneralizability to Other Programming\
    \ Tasks. In accordance with scientific rigor, this study defines its scope as\
    \ Human-LLM collaboration within competitive programming, a domain chosen to examine\
    \ the capabilities and limitations of both LLMs and human performance. While acknowledging\
    \ the potential relevance to broader programming tasks, we limit our evaluations\
    \ and analyses to this specific context and defer extending the representativeness\
    \ of our results to general software development or other programming domains.\
    \ Despite this focus, elements of our work offer valuable insights applicable\
    \ to diverse programming scenarios. The problem-solving process shares fundamental\
    \ similarities across programming contexts, and our proposed human feedback taxonomy\
    \ and methods for improving problem comprehension in LLMs may readily translate.\
    \ Developers, for example, can leverage clear and detailed feedback on specifications,\
    \ as demonstrated in our benchmark, to guide LLMs towards a better understanding\
    \ of software requirements. We believe this highlights pathways for broader applicability\
    \ and welcome further discussion.\n\n## Ethics Statement\n\nThe proposed dataset\
    \ for this study is primarily sourced from publicly available, reputable com-\n\
    \npetitive programming websites. Our data collection process strictly avoids any\
    \ personally identifiable information, such as user IDs, avatars, or comments,\
    \ ensuring maximum transparency and accessibility. Furthermore, in our work, the\
    \ dataset is manually annotated, and human-LLM collaborative programming is employed.\
    \ During our experiments, we provide human participants with a full explanation\
    \ of data usage and publication; at no point are participants exposed to inappropriate\
    \ content. We ensure that the whole process adhered to all ethical guidelines\
    \ and ensured the responsible and transparent use of human participants' time\
    \ and effort, thereby promoting the advancement of research in the field. For\
    \ all human-subject studies, we strictly adhered to IRB approval. To control the\
    \ workload of the human annotators, we used a two-stage annotation process, starting\
    \ with three LLMs(O1-mini) performing automatic annotation, followed by human\
    \ annotators to control the quality. Specifically, human annotators would further\
    \ annotate issues where there was disagreement in the automatic annotation by\
    \ the LLMs. Two human annotators individually annotated 244 and 318 items, respectively.\
    \ Participants in the human-subject study were compensated \\$150 for their involvement,\
    \ while human annotators were each compensated \\$300 for their work.\n\n## Acknowledgements\n\
    \nThis work was supported in part by the National Natural Science Foundation of\
    \ China (No. 62272330 and No.U24A20328); in part by the Fundamental Research Funds\
    \ for the Central Universities (No. YJ202219); in part by the Science Fund for\
    \ Creative Research Groups of Sichuan Province Natural Science Foundation (No.\
    \ 2024NSFTD0035); in part by the National Major Scientific Instruments and Equipments\
    \ Development Project of Natural Science Foundation of China under Grant (No.\
    \ 62427820); in part by the Natural Science Foundation of Sichuan (No. 2024YFHZ0233)\n\
    \n## References\n\n- <span id=\"page-8-1\"></span>2024. Claude 3.5 sonnet. [https://www.anthropic.](https://www.anthropic.com/news/claude-3-5-sonnet)\
    \ [com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet).\n\
    - <span id=\"page-8-0\"></span>Brett A Becker, Paul Denny, James Finnie-Ansley,\
    \ Andrew Luxton-Reilly, James Prather, and Eddie Antonio Santos. 2023. Programming\
    \ is hard-or at least it used to be: Educational opportunities and challenges\n\
    \nof ai code generation. In *Proceedings of the 54th ACM Technical Symposium on\
    \ Computer Science Education V. 1*, pages 500–506.\n\n- <span id=\"page-9-17\"\
    ></span>Anya Belz, Shubham Agarwal, Yvette Graham, Ehud Reiter, and Anastasia\
    \ Shimorina, editors. 2021. *[Pro](https://aclanthology.org/2021.humeval-1.0)[ceedings\
    \ of the Workshop on Human Evaluation of](https://aclanthology.org/2021.humeval-1.0)\
    \ [NLP Systems \\(HumEval\\)](https://aclanthology.org/2021.humeval-1.0)*. Association\
    \ for Computational Linguistics, Online.\n- <span id=\"page-9-5\"></span>Hyungjoo\
    \ Chae, Taeyoon Kwon, Seungjun Moon, Yongho Song, Dongjin Kang, Kai Tzu-iunn Ong,\
    \ Beong-woo Kwak, Seonghyeon Bae, Seung-won Hwang, and Jinyoung Yeo. 2024. Coffee-gym:\
    \ An environment for evaluating and improving natural language feedback on erroneous\
    \ code. *arXiv preprint arXiv:2409.19715*.\n- <span id=\"page-9-18\"></span>Mark\
    \ Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto,\
    \ Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\
    \ al. 2021. Evaluating large language models trained on code. *arXiv preprint\
    \ arXiv:2107.03374*.\n- <span id=\"page-9-20\"></span>Yue Chen, Chen Huang, Yang\
    \ Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, and Tat-Seng Chua. 2024. Style: Improving\
    \ domain transferability of asking clarification questions in large language model\
    \ powered conversational agents. *arXiv preprint arXiv:2405.12059*.\n- <span id=\"\
    page-9-8\"></span>Yue Chen, Dingnan Jin, Chen Huang, Jia Liu, and Wenqiang Lei.\
    \ 2023. Travel: Tag-aware conversational faq retrieval via reinforcement learning.\
    \ In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language\
    \ Processing*, pages 3861–3872.\n- <span id=\"page-9-0\"></span>Thomas H Cormen,\
    \ Charles E Leiserson, Ronald L Rivest, and Clifford Stein. 2022. *Introduction\
    \ to algorithms*. MIT press.\n- <span id=\"page-9-2\"></span>Nell B Dale and Chip\
    \ Weems. 2014. *Programming and problem solving with C++*. Jones & Bartlett Publishers.\n\
    - <span id=\"page-9-21\"></span>Anubrata Das, Houjiang Liu, Venelin Kovatchev,\
    \ and Matthew Lease. 2023. The state of human-centered nlp technology for fact-checking.\
    \ *Information processing & management*, 60(2):103219.\n- <span id=\"page-9-15\"\
    ></span>Sally Fincher. 1999. What are we doing when we teach programming? In *FIE'99\
    \ Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing\
    \ the Future of Science and Engineering Education. Conference Proceedings (IEEE\
    \ Cat. No. 99CH37011*, volume 1, pages 12A4–1. IEEE.\n- <span id=\"page-9-11\"\
    ></span>Bill Gallmeister. 1995. *POSIX. 4 Programmers Guide: Programming for the\
    \ real world*. \" O'Reilly Media, Inc.\".\n- <span id=\"page-9-6\"></span>Jie\
    \ Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon\
    \ Tangi Perrault, and Thomas W Malone. 2024. A taxonomy for humanllm interaction\
    \ modes: An initial exploration. In *Extended Abstracts of the CHI Conference\
    \ on Human Factors in Computing Systems*, pages 1–11.\n- <span id=\"page-9-3\"\
    ></span>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,\
    \ Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the\
    \ large language model meets programming– the rise of code intelligence. *arXiv\
    \ preprint arXiv:2401.14196*.\n- <span id=\"page-9-9\"></span>Dan Hendrycks, Steven\
    \ Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\
    \ Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021a. Measuring\
    \ coding challenge competence with apps. *NeurIPS*.\n- <span id=\"page-9-13\"\
    ></span>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,\
    \ Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021b.\
    \ Measuring coding challenge competence with apps. *arXiv preprint arXiv:2105.09938*.\n\
    - <span id=\"page-9-12\"></span>Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng\
    \ Lv, Tat-Seng Chua, and Jimmy Xiangji Huang. 2025. [How to enable effective cooperation\
    \ between humans](http://arxiv.org/abs/2501.05714) [and nlp models: A survey of\
    \ principles, formaliza](http://arxiv.org/abs/2501.05714)[tions, and beyond.](http://arxiv.org/abs/2501.05714)\n\
    - <span id=\"page-9-7\"></span>Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei,\
    \ Jiancheng Lv, and Tat-Seng Chua. 2024a. Concept– an evaluation protocol on conversation\
    \ recommender systems with system-and user-centric factors. *arXiv e-prints*,\
    \ pages arXiv–2404.\n- <span id=\"page-9-14\"></span>Chen Huang, Peixin Qin, Wenqiang\
    \ Lei, and Jiancheng Lv. 2023a. [Reduce human labor on evaluating con](https://doi.org/10.18653/v1/2023.emnlp-main.670)[versational\
    \ information retrieval system: A human](https://doi.org/10.18653/v1/2023.emnlp-main.670)[machine\
    \ collaboration approach.](https://doi.org/10.18653/v1/2023.emnlp-main.670) In\
    \ *Proceedings of the 2023 Conference on Empirical Methods in Natural Language\
    \ Processing*, pages 10876–10891, Singapore. Association for Computational Linguistics.\n\
    - <span id=\"page-9-19\"></span>Chen Huang, Xinwei Yang, Yang Deng, Wenqiang Lei,\
    \ JianCheng Lv, and Tat-Seng Chua. 2024b. Comatching: Towards human-machine collaborative\
    \ legal case matching. *arXiv preprint arXiv:2405.10248*.\n- <span id=\"page-9-10\"\
    ></span>Tao Huang, Zhihong Sun, Zhi Jin, Ge Li, and Chen Lyu. 2024c. Knowledge-aware\
    \ code generation with large language models. In *Proceedings of the 32nd IEEE/ACM\
    \ International Conference on Program Comprehension*, pages 52–63.\n- <span id=\"\
    page-9-1\"></span>Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai Lu,\
    \ Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, et al. 2023b. Competition-level\
    \ problems are effective llm evaluators. *arXiv preprint arXiv:2312.02143*.\n\
    - <span id=\"page-9-16\"></span>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang,\
    \ Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al.\
    \ 2024. Qwen2. 5-coder technical report. *arXiv preprint arXiv:2409.12186*.\n\
    - <span id=\"page-9-4\"></span>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\
    \ Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.\
    \ 2024. Livecodebench: Holistic and contamination free evaluation of large language\
    \ models for code. *arXiv preprint arXiv:2403.07974*.\n- <span id=\"page-10-11\"\
    ></span>Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang,\
    \ Md Rizwan Parvez, and Shafiq Joty. 2023. xcodeeval: A large scale multilingual\
    \ multitask benchmark for code understanding, generation, translation and retrieval.\
    \ *arXiv preprint arXiv:2303.03004*.\n- <span id=\"page-10-18\"></span>Min Kyung\
    \ Lee. 2018. Understanding perception of algorithmic decisions: Fairness, trust,\
    \ and emotion in response to algorithmic management. *Big Data & Society*, 5(1):2053951718756684.\n\
    - <span id=\"page-10-1\"></span>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\
    \ Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia\
    \ Li, Jenny Chim, et al. 2023a. Starcoder: may the source be with you! *arXiv\
    \ preprint arXiv:2305.06161*.\n- <span id=\"page-10-6\"></span>Rongao Li, Jie\
    \ Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and\
    \ Ge Li. 2023b. Taco: Topics in algorithmic code generation dataset. *arXiv preprint\
    \ arXiv:2312.14852*.\n- <span id=\"page-10-10\"></span>Yujia Li, David Choi, Junyoung\
    \ Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling,\
    \ Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation\
    \ with alphacode. *Science*, 378(6624):1092–1097.\n- <span id=\"page-10-5\"></span>Zhaofeng\
    \ Liu, Jing Su, Jia Cai, Jingzhi Yang, and Chenfan Wu. 2024. Instruct-code-llama:\
    \ Improving capabilities of language model in competition level code generation\
    \ by online judge feedback. In *Advanced Intelligent Computing Technology and\
    \ Applications*, pages 127–137, Singapore. Springer Nature Singapore.\n- <span\
    \ id=\"page-10-4\"></span>Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico\
    \ Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,\
    \ Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation.\
    \ *arXiv preprint arXiv:2402.19173*.\n- <span id=\"page-10-13\"></span>Robert\
    \ Cecil Martin. 2003. *Agile software development: principles, patterns, and practices*.\
    \ Prentice Hall PTR.\n- <span id=\"page-10-8\"></span>Hussein Mozannar, Valerie\
    \ Chen, Dennis Wei, Prasanna Sattigeri, Manish Nagireddy, Subhro Das, Ameet Talwalkar,\
    \ and David Sontag. 2023. Simulating iterative human-ai interaction in programming\
    \ with llms. In *NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following*.\n\
    - <span id=\"page-10-0\"></span>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\
    \ Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An\
    \ open large language model for code with multi-turn program synthesis. *arXiv\
    \ preprint arXiv:2203.13474*.\n- <span id=\"page-10-20\"></span>OpenAI. 2024a.\
    \ [Hello gpt-4o.](https://openai.com/index/hello-gpt-4o/)\n- <span id=\"page-10-19\"\
    ></span>OpenAI. 2024b. [Openai o1-mini.](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/)\n\
    - <span id=\"page-10-21\"></span>R OpenAI. 2023. Gpt-4 technical report. arxiv\
    \ 2303.08774. *View in Article*, 2(5).\n- <span id=\"page-10-23\"></span>Paul\
    \ Owoicho, Ivan Sekulic, Mohammad Aliannejadi, Jeffrey Dalton, and Fabio Crestani.\
    \ 2023. Exploiting simulated user feedback for conversational search: Ranking,\
    \ rewriting, and beyond. In *Proceedings of the 46th International ACM SIGIR Conference\
    \ on Research and Development in Information Retrieval*, pages 632–642.\n- <span\
    \ id=\"page-10-12\"></span>Erick B Passos, Danilo B Medeiros, Pedro AS Neto, and\
    \ Esteban WG Clua. 2011. Turning real-world software development into a game.\
    \ In *2011 Brazilian Symposium on Games and Digital Entertainment*, pages 260–269.\
    \ IEEE.\n- <span id=\"page-10-14\"></span>Tung Phung, José Cambronero, Sumit Gulwani,\
    \ Tobias Kohn, Rupak Majumdar, Adish Singla, and Gustavo Soares. 2023. Generating\
    \ high-precision feedback for programming syntax errors using large language models.\
    \ *arXiv preprint arXiv:2302.04662*.\n- <span id=\"page-10-17\"></span>Vera Pless.\
    \ 2011. *Introduction to the theory of errorcorrecting codes*. John Wiley & Sons.\n\
    - <span id=\"page-10-3\"></span>Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\
    \ Code generation with alphacodium: From prompt engineering to flow engineering.\
    \ *arXiv preprint arXiv:2401.08500*.\n- <span id=\"page-10-16\"></span>Anthony\
    \ Robins, Janet Rountree, and Nathan Rountree. 2003. Learning and teaching programming:\
    \ A review and discussion. *Computer science education*, 13(2):137–172.\n- <span\
    \ id=\"page-10-24\"></span>Amir Rosenfeld, Markus D Solbach, and John K Tsotsos.\
    \ 2018. Totally looks like-how humans compare, compared to machines. In *Proceedings\
    \ of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*,\
    \ pages 1961–1964.\n- <span id=\"page-10-2\"></span>Baptiste Roziere, Jonas Gehring,\
    \ Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu\
    \ Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models\
    \ for code. *arXiv preprint arXiv:2308.12950*.\n- <span id=\"page-10-7\"></span>Quan\
    \ Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024. Can language models\
    \ solve olympiad programming? *arXiv preprint arXiv:2404.10952*.\n- <span id=\"\
    page-10-22\"></span>Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin\
    \ Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang,\
    \ et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions\
    \ of tokens of context. *arXiv preprint arXiv:2403.05530*.\n- <span id=\"page-10-15\"\
    ></span>Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu,\
    \ and Maosong Sun. 2024. Debugbench: Evaluating debugging capability of large\
    \ language models. *arXiv preprint arXiv:2401.04621*.\n- <span id=\"page-10-9\"\
    ></span>Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu, and Yi Wang. 2024. Rocks\
    \ coding, not development: A human-centric, experimental evaluation of llm-supported\
    \ se tasks. *Proceedings of the ACM on Software Engineering*, 1(FSE):699–721.\n\
    - <span id=\"page-11-6\"></span>Cheng-Kuang Wu, Zhi Rui Tam, Chao-Chung Wu, Chieh-Yen\
    \ Lin, Hung-yi Lee, and Yun-Nung Chen. 2024. I need help! evaluating llm's ability\
    \ to ask for users' support: A case study on text-to-sql generation. *arXiv preprint\
    \ arXiv:2407.14767*.\n- <span id=\"page-11-0\"></span>Weixiang Yan, Haitian Liu,\
    \ Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu,\
    \ Hari Sundaram, et al. 2023. Codescope: An execution-based multilingual multitask\
    \ multidimensional benchmark for evaluating llms on code understanding and generation.\
    \ *arXiv preprint arXiv:2311.08588*.\n- <span id=\"page-11-5\"></span>John Yang,\
    \ Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. 2024. Intercode: Standardizing\
    \ and benchmarking interactive coding with execution feedback. *Advances in Neural\
    \ Information Processing Systems*, 36.\n- <span id=\"page-11-8\"></span>Tong Zhang,\
    \ Chen Huang, Yang Deng, Hongru Liang, Jia Liu, Zujie Wen, Wenqiang Lei, and Tat-Seng\
    \ Chua. 2024a. Strength lies in differences! towards effective non-collaborative\
    \ dialogues via tailored strategy planning. *arXiv e-prints*, pages arXiv–2403.\n\
    - <span id=\"page-11-7\"></span>Tong Zhang, Peixin Qin, Yang Deng, Chen Huang,\
    \ Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, and Tat-Seng Chua. 2024b.\
    \ Clamber: A benchmark of identifying and clarifying ambiguous information needs\
    \ in large language models. *arXiv preprint arXiv:2405.12063*.\n- <span id=\"\
    page-11-3\"></span>Yifan Zhang, Chen Huang, Zachary Karas, Dung Thuy Nguyen, Kevin\
    \ Leach, and Yu Huang. 2025. Enhancing code llm training with programmer attention.\
    \ *arXiv preprint arXiv:2503.14936*.\n- <span id=\"page-11-1\"></span>Tianyu Zheng,\
    \ Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and\
    \ Xiang Yue. 2024. Opencodeinterpreter: Integrating code generation with execution\
    \ and refinement. *arXiv preprint arXiv:2402.14658*.\n- <span id=\"page-11-4\"\
    ></span>Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi\
    \ Ye, and Jiachi Chen. 2023. A survey of large language models for code: Evolution,\
    \ benchmarking, and future trends. *arXiv preprint arXiv:2311.10372*.\n\n# <span\
    \ id=\"page-11-2\"></span>A Details of Dataset Description and Construction\n\n\
    ## A.1 Dataset Description\n\nOur dataset include the following threefold information.\
    \ This dataset will be openly released soon. Static Dataset. As shown in Figure\
    \ [5,](#page-12-0) ELABO-RATIONSET is primarily composed of two parts: Problem\
    \ Information Collection and Annotations for Human Interaction. The former includes\
    \ problem statements, input/output formats, test cases, examples, dates, tags,\
    \ and difficulty levels. The\n\nlatter consists of carefully annotated problem\
    \ statement clarifications, as well as algorithm-specific knowledge summaries,\
    \ which include the required algorithms to solve each problem along with their\
    \ definitions and pseudocodes.\n\nHuman Simulator-LLM Competitive Programming\
    \ Dataset. Our dataset also includes multiturn interaction data between 13 LLMs\
    \ and two human simulators (emulated by O1-Mini), encompassing multi-turn feedback\
    \ from human simualtor and LLM-generated codes. This data facilitates future research\
    \ into LLM behavior.\n\nReal Human-LLM Competitive Programming Dataset. Additionally,\
    \ we also include multi-turn interaction data between GPT-4 Turbo and five real\
    \ humans. This covers 300 problems of varying difficulty. See Appendix [B.1](#page-13-1)\
    \ for construction details.\n\n<span id=\"page-11-9\"></span>\n\n| Dataset   \
    \ | Difficulty | Difficulty Level | Problem Number |\n|------------|------------|------------------|----------------|\n\
    | Codeforces | Easy       | (0,750]          | 2332           |\n|           \
    \ | Middle     | (750,1000]       | 907            |\n|            | Hard    \
    \   | (1000,1500]      | 1423           |\n| Atcoder    | Easy       | (0,350]\
    \          | 1310           |\n|            | Middle     | (350,550]        |\
    \ 1191           |\n|            | Hard       | (550,900]        | 1157      \
    \     |\n\nTable 8: ELABORATIONSET: Difficulty Level\n\n# A.2 Static Dataset Construction\n\
    \n## A.2.1 Problem Information Collection\n\nBasic Problem Collection. Following\
    \ [Jain et al.](#page-9-4) [\\(2024\\)](#page-9-4), we collected problem statements,\
    \ input/output formats, example test cases, publication dates, algorithmic tags,\
    \ and difficulty levels from publicly accessible sections of the AtCoder and Codeforces\
    \ websites, removing any duplicates. Notably, we focus on scraping only the publicly\
    \ accessible sections of these sites, steering clear of any data that may be behind\
    \ paywalls or require user login or interaction. We will release our code along\
    \ with our dataset.\n\nProblem Difficulty Level. Codeforces and At-Coder assign\
    \ difficulty scores to problems using points-based systems, with higher scores\
    \ indicating greater difficulty. Codeforces categorizes problems as Easy, Medium,\
    \ and Hard based on score ranges of (0, 750], (750, 1000], and (1000, 1500], respectively\
    \ (Table [8\\)](#page-11-9). AtCoder uses ranges of (0, 350], (350, 550], and\
    \ (550, 900]. We excluded the most hard problems, as these are currently beyond\
    \ the capabilities of LLMs.\n\n<span id=\"page-12-0\"></span>![](_page_12_Figure_0.jpeg)\n\
    \nFigure 5: Description of dataset\n\nGeneraing Test Case When Necessary. While\
    \ we collected test cases from both websites, we found that some problems lacked\
    \ them, specially for problems from Codeforces. In response, we used GPT-4o to\
    \ generate them, following the approach of [Li](#page-10-6) [et al.](#page-10-6)\
    \ [\\(2023b\\)](#page-10-6); [Jain et al.](#page-9-4) [\\(2024\\)](#page-9-4).\
    \ Using prompt [C.5.2,](#page-19-0) we generated 15 diverse test case inputs per\
    \ problem based on the corresponding problem statements, and then generated corresponding\
    \ outputs using ground truth Python code. Three separate ground truth codes and\
    \ human programmers validated the accuracy of these test cases.\n\nFair Use and\
    \ Academic Purpose. In accordance with [Hendrycks et al.](#page-9-13) [\\(2021b\\\
    )](#page-9-13); [Jain et al.](#page-9-4) [\\(2024\\)](#page-9-4), we adhere to\
    \ Fair Use §107, which states that \"the fair use of a copyrighted work, including\
    \ such use by ... scholarship or research, is not an infringement of copyright.\"\
    \ Fair use is assessed based on \"the purpose and character of the use, including\
    \ whether it is of a commercial nature or for nonprofit educational purposes,\"\
    \ \"the amount and substantiality\n\nof the portion used in relation to the copyrighted\
    \ work as a whole,\" and \"the effect of the use upon the potential market for\
    \ or value of the copyrighted work.\" We emphasize that the problems we have collected\
    \ are used solely for academic purposes, and we do not train on these collected\
    \ problems.\n\n## <span id=\"page-12-1\"></span>A.2.2 Annotations for Human Interaction\n\
    \nThe annotation process aims to guarantee the precision and clarity of problem\
    \ statements, algorithmspecific knowledge, and ground truth solutions. To accomplish\
    \ this, we employ a multi-step approach. Initially, O1-mini carries out automatic\
    \ annotation, followed by manual verification.\n\nProblem Statement Clarifications.\
    \ Initially, we utilize O1-Mini following the structured prompt [F](#page-39-0)\
    \ to generate clarifications of problem statements for each issue. This includes\
    \ refining ambiguous descriptions and outlining clear and concise requirements\
    \ and specifications to guide problem-solving. We used three LLMs O1-Mini for\
    \ annotation and conducted discussions. After the automatic generation step, we\
    \ involved two master's students in computer science to perform a thorough manual\
    \ evaluation of the generated content for those problems where there was no agreement\
    \ in the results. These annotators verify the alignment of the requirements and\
    \ specifications with the problem objectives and ensure that all critical aspects\
    \ are covered. Their expertise helps ensure the annotations are precise, comprehensive,\
    \ and aligned with academic standards.\n\nKnowledge Summaries Annotated. For the\
    \ algorithm-specific knowledge summaries, we compile detailed descriptions and\
    \ pseudocode for 33 distinct algorithms. Initially, three LLMs O1-Mini are employed\
    \ to generate drafts of these summaries, including algorithm definitions, purposes,\
    \ and stepby-step pseudocode. Subsequently, for the results where the LLMs could\
    \ not reach consensus, the generated outputs undergo meticulous manual review\
    \ to verify the correctness of both the descriptions and the pseudocode. This\
    \ verification process includes cross-referencing with standard algorithmic literature\
    \ to ensure consistency and accuracy. Examples of these algorithm summaries are\
    \ presented in Table [C.5.2,](#page-19-0) showcasing both the variety and depth\
    \ of the annotations.\n\nGround Truth Solution Filtering Regarding ground truth\
    \ solutions, we collect five correct code submissions for each problem from a\
    \ reliable online source. These submissions are subjected to a cleaning process\
    \ to eliminate any potentially contaminated or duplicate code. This ensures the\
    \ final ground truth solutions is robust and representative.\n\nOverall, the annotation\
    \ process emphasizes a balance between automation and expert evaluation. By combining\
    \ model-generated outputs with detailed human review, we aim to produce high-quality\
    \ annotations that serve as a solid foundation for subsequent analyses. The multi-step\
    \ approach not only ensures reliability but also promotes transparency in our\
    \ methodology.\n\n# <span id=\"page-13-0\"></span>B Implementation Details\n\n\
    We conduct all our experiments using a single Nvidia RTX A100 GPU for the 7B size\
    \ LLMs, two A100 GPUs for the 13B size LLMs, and four A100 GPUs for the 34B size\
    \ LLMs. For these opensource LLMs, we utilize the Xinference framework. For all\
    \ LLMs, we employ nucleus sampling with a temperature of 0.7 and a top-p value\
    \ of 0.95, allowing for a maximum of 10 iterations per stage with\n\nhuman programmers.\
    \ For the pass@k metrics, we calculate it using the macro average method.\n\n\
    ## <span id=\"page-13-1\"></span>B.1 Implementation of Real Human Experiments\n\
    \nWe involve five computer science graduate students collaboratively solving competitive\
    \ programming problems with LLMs. Each human participant receives detailed instructions\
    \ (cf. Appendix [F\\)](#page-39-0) before commencing the experiment, and is permitted\
    \ to utilize any resources, including our dataset and internet search, to cooperate\
    \ with one LLM, ensuring a realistic debugging process. Each participant in the\
    \ human-subject study received a compensation of \\$150 for their participation.\
    \ Following [Shi et al.](#page-10-7) [\\(2024\\)](#page-10-7); [Tian et al.](#page-10-15)\
    \ [\\(2024\\)](#page-10-15), human interaction is limited to textual feedback\
    \ identifying errors (syntactic and semantic, see Table [17\\)](#page-30-0) rather\
    \ than direct code editing. This debugging process ends until a correct solution\
    \ is produced or a maximum number of iterations 10 is reached. During this process,\
    \ the specific challenges LLM would encounter are not known in advance, potentially\
    \ leading to high human labor. Thus, to mitigate this labor, we focus human participation\
    \ on the debugging stage using a subset of 300 randomly selected problems of varying\
    \ difficulty from ELABORATIONSET (100 for easy, 100 for middle and 100 for hard\
    \ problems). For the LLM, we consider GPT-4-Turbo (due to its balance of strong\
    \ performance and reasonable cost). It is allows to refine its solutions based\
    \ on both compiler feedback and simulator feedback played by O1-Mini (We term\
    \ this process as Automatic Debug), which reduces the human workload for bug identification.\n\
    \nTo ensure a rigorous and comprehensive analysis of the generated code, we conducted\
    \ a detailed post-experiment review process. In this stage, all generated code\
    \ was meticulously analyzed, and any identified bugs were carefully annotated.\
    \ The error annotation task focused on code generated by GPT-4 Turbo, leveraging\
    \ the expertise of a team of five graduate students in computer science. All members\
    \ of the team possess substantial experience in Olympic-level competitive programming,\
    \ equipping them with the necessary skills to identify subtle and complex coding\
    \ issues. For each problem, at least two annotators independently reviewed the\
    \ generated code to ensure accuracy and consistency in error identification. The\
    \ annotation process was guided by a detailed set of instructions,\n\noutlined\
    \ in Table [E.2.](#page-31-0) These guidelines provided step-by-step instructions\
    \ on how to identify, categorize, and label errors. Specific categories of errors\
    \ included logical flaws, syntax issues, edge case failures, inefficiencies, and\
    \ implementation inconsistencies. The standardized approach ensured uniformity\
    \ across the annotations, reducing subjectivity and enhancing reliability. Once\
    \ the annotations were completed, any discrepancies between annotators were resolved\
    \ through consensus discussions, ensuring that the final error labels accurately\
    \ reflected the issues present in the code. Examples of annotated bugs, including\
    \ descriptions and their corresponding fixes, are provided in Table [E.3,](#page-35-0)\
    \ illustrating the depth and clarity of the annotation process. The annotated\
    \ error dataset forms a crucial part of this study and serves as a valuable resource\
    \ for understanding common pitfalls in LLM-generated code. To promote transparency\
    \ and support future research, this supplementary dataset will be publicly released\
    \ alongside the primary dataset. By sharing this resource, we aim to facilitate\
    \ further investigation into the strengths and limitations of code-generation\
    \ models while fostering advancements in the field of AI-assisted programming.\n\
    \n## B.2 LLM Implementation\n\nWe tested a total of 13 different large language\
    \ models. The details of the models considered in our study are described in Table\
    \ [9.](#page-15-0)\n\n## B.3 Implementation of Evaluation\n\nIn the comprehension\
    \ stage, after the LLM has developed an understanding of the problem statement,\
    \ O1-Mini is used for automatic fact-checking against the annotations of the clarifications\
    \ and requirements specified in the dataset, the detailed results are presented\
    \ in Table [10.](#page-15-1) During the planning stage, the LLM selects a suitable\
    \ algorithm, generates reasoning, and provides pseudocode. O1-Mini is then utilized\
    \ for automatic fact-checking against the annotated summaries of the algorithms\
    \ in the dataset. Since a single problem may correspond to multiple algorithm\
    \ options, it is sufficient for the LLM to accurately select one of the algorithms\
    \ (drawing on established fact-checking metrics [\\(Das](#page-9-21) [et al.,](#page-9-21)\
    \ [2023\\)](#page-9-21)), the detailed results are presented in Table [11.](#page-15-2)\
    \ In the coding stage, the LLM produces the complete code, which is subsequently\
    \ tested using the test case examples provided in the problem statement through\
    \ a compiler. In the debugging\n\nstage, all test cases are re-evaluated through\
    \ a compiler based on the modified code from the LLM.\n\n# <span id=\"page-14-0\"\
    ></span>C Additional Results\n\n# C.1 Results of Pass@K\n\nTables [15](#page-18-0)\
    \ and Table [16](#page-19-0) present additional results for human-LLM competitive\
    \ programming using Pass@3 and Pass@5 metrics, respectively. Key observations\
    \ we can draw from these tables are in line with ones in the main body of this\
    \ paper.\n\n# C.2 Case Studies\n\nFor better understanding, Appendix [D](#page-22-0)\
    \ provides a case study illustrating how human feedback from simulator improves\
    \ LLM performance in competitive programming. Additionally, Appendix [E](#page-30-1)\
    \ provides a case of LLM incorparating real human feedback to solve the coding\
    \ problem. More examples are available in our dataset.\n\n# <span id=\"page-14-1\"\
    ></span>C.3 Nuanced Understanding of Each Stage\n\nOur evaluation approach provides\
    \ a reliable and objective framework for assessing LLMs in competitive programming\
    \ by leveraging an expert-verified dataset to quantify understanding (accuracy\
    \ of problem comprehension), planning (correct algorithm selection), ensure consistency\
    \ (eliminating subjective human evaluation), and provide scalability (enabling\
    \ rapid evaluation across numerous problems). Beyond this, to enable a more nuanced\
    \ analysis of different LLMs at the comprehension and planning stages, we conducted\
    \ detailed manual evaluations with the goal of gaining a deeper understanding\
    \ of their performance. For each stage, we randomly selected 20 algorithmic problems(10\
    \ easy, 10 middle) for testing. Two computer science master's students participated\
    \ in this manual evaluation. All procedures were conducted under Institutional\
    \ Review Board (IRB) approval.\n\n# C.3.1 Analysis on Comprehension Stage\n\n\
    We design four dimensions for in-depth evaluation:\n\n- Understanding of Problem\
    \ Requirements: Measures whether the model accurately identifies and interprets\
    \ the core requirements of the problem, ensuring that the comprehension of the\
    \ task aligns with the given problem statements.\n- Correctness of Identified\
    \ Specifications: Assesses whether the model accurately identifies any constraints\
    \ or specifications inherent\n\n<span id=\"page-15-0\"></span>\n\n| Model Name\
    \                   | Cutoff Date | Link                         |\n|------------------------------|-------------|------------------------------|\n\
    | Deepseek-coder-6.7b-instruct | 09/2023     | deepseek-coder-6.7b-instruct |\n\
    | Deepseek-coder-33b-instruct  | 09/2023     | deepseek-coder-33b-instruct  |\n\
    | CodeLlama-7b-Instruct        | 01/2023     | CodeLlama-7b-Instruct        |\n\
    | CodeLlama-13b-Instruct       | 01/2023     | CodeLlama-13b-Instruct       |\n\
    | CodeLlama-34b-Instruct       | 01/2023     | CodeLlama-34b-Instruct       |\n\
    | Qwen2.5-Coder-7B             | 06/2024     | Qwen2.5-Coder-7B             |\n\
    | Qwen2.5-Coder-14B            | 06/2024     | Qwen2.5-Coder-14B            |\n\
    | Qwen2.5-Coder-32B            | 06/2024     | Qwen2.5-Coder-32B            |\n\
    | GPT-4-Turbo                  | 05/2023     | GPT-4-Turbo                  |\n\
    | GPT-4o                       | 11/2023     | GPT-4o                       |\n\
    | Claude-3.5-Sonnet            | 03/2024     | Claude-3.5-Sonnet            |\n\
    | Gemini-1.5-Pro               | 11/2023     | Gemini-1.5-Pro               |\n\
    | O1-Mini                      | 12/2023     | O1-Mini                      |\n\
    \nTable 9: LLMs Overview\n\n<span id=\"page-15-1\"></span>\n\n| Comprehension\
    \ Stage          | Easy | Middle | Hard |\n|------------------------------|------|--------|------|\n\
    | O1-Mini                      | 0.99 | 0.99   | 0.99 |\n| GPT-4o            \
    \           | 0.99 | 0.96   | 0.96 |\n| GPT-4-Turbo                  | 0.98 |\
    \ 0.93   | 0.91 |\n| Gemini-1.5-Pro               | 0.98 | 0.96   | 0.95 |\n|\
    \ Claude-3.5-Sonnet            | 0.99 | 0.96   | 0.95 |\n| Deepseek-coder-6.7b-instruct\
    \ | 0.95 | 0.89   | 0.84 |\n| CodeLlama-7b-Instruct        | 0.93 | 0.87   | 0.82\
    \ |\n| Qwen2.5-Coder-7B             | 0.94 | 0.89   | 0.85 |\n| CodeLlama-13b-Instruct\
    \       | 0.94 | 0.91   | 0.86 |\n| Qwen2.5-Coder-14B            | 0.95 | 0.93\
    \   | 0.87 |\n| Deepseek-coder-33b-instruct  | 0.96 | 0.94   | 0.90 |\n| CodeLlama-34b-Instruct\
    \       | 0.94 | 0.93   | 0.88 |\n| Qwen2.5-Coder-32B            | 0.97 | 0.95\
    \   | 0.92 |\n\nTable 10: Different LLMs' performance at comprehension stage\n\
    \nin the problem (e.g., time, space, or other domain-specific constraints).\n\n\
    • Clarity of Functionality: evaluates how clearly the model defines the purpose\
    \ and behavior of the proposed solution. It measures whether the model communicates\
    \ the solution's functionality in a concise and understandable way.\n\nAs Table\
    \ [13](#page-17-0) shown, the results from the manual evaluation provide a more\
    \ comprehensive display of different LLMs' performance at comprehension stage.\
    \ The experimental results reveal that while all models exhibit high accuracy\
    \ in requirements understanding and specification identification, small-scale\
    \ open-source models show notable deficiencies in problem breakdown and functionality\
    \ clarification compared to other models. These shortcomings are aspects that\
    \ cannot be captured\n\n<span id=\"page-15-2\"></span>\n\n| Planning Stage   \
    \            | Easy | Middle | Hard |\n|------------------------------|------|--------|------|\n\
    | O1-Mini                      | 0.97 | 0.93   | 0.90 |\n| GPT-4o            \
    \           | 0.91 | 0.77   | 0.66 |\n| GPT-4-Turbo                  | 0.82 |\
    \ 0.65   | 0.48 |\n| Gemini-1.5-Pro               | 0.91 | 0.78   | 0.64 |\n|\
    \ Claude-3.5-Sonnet            | 0.92 | 0.75   | 0.62 |\n| Deepseek-coder-6.7b-instruct\
    \ | 0.51 | 0.29   | 0.17 |\n| CodeLlama-7b-Instruct        | 0.48 | 0.25   | 0.14\
    \ |\n| Qwen2.5-Coder-7B             | 0.54 | 0.33   | 0.19 |\n| CodeLlama-13b-Instruct\
    \       | 0.53 | 0.31   | 0.18 |\n| Qwen2.5-Coder-14B            | 0.66 | 0.49\
    \   | 0.24 |\n| Deepseek-coder-33b-instruct  | 0.72 | 0.50   | 0.39 |\n| CodeLlama-34b-Instruct\
    \       | 0.64 | 0.37   | 0.28 |\n| Qwen2.5-Coder-32B            | 0.78 | 0.57\
    \   | 0.43 |\n\nTable 11: Different LLMs' performance at planning stage\n\nthrough\
    \ automatic evaluation.\n\n## C.3.2 Analysis on Planning Stage\n\nWe designed\
    \ four dimensions for in-depth evaluation:\n\n- Algorithm Selection Accuracy:\
    \ Whether the algorithm chosen by the model aligns with the problem's requirements.\n\
    - Pseudocode Generation Quality: The correctness and completeness of the generated\
    \ algorithm pseudocode.\n- Algorithm Justification Reasonableness: Whether the\
    \ rationale behind choosing the algorithm is sufficient and logical.\n\nAs Table\
    \ [14](#page-17-1) shown, the results from the manual evaluation provide a more\
    \ comprehensive display of different LLMs' performance at planning stage. They\
    \ further highlight that closed-source models (especially O1-mini) outperform\
    \ open-source models in the planning stage, particularly in complexity analysis\
    \ and algorithm justification —areas that cannot be assessed through automatic\
    \ evaluation.\n\n<span id=\"page-16-1\"></span>\n\n| Coding Stage | Original |\
    \ Automatic Feedback | Human Feedback |\n|--------------|----------|--------------------|----------------|\n\
    | Easy         | 13       | 15                 | 16             |\n| Middle  \
    \     | 5        | 6                  | 8              |\n| Hard         | 2 \
    \       | 4                  | 5              |\n| Total        | 20       | 25\
    \                 | 29             |\n\nTable 12: Performance in collaborating\
    \ with real humans on coding tasks\n\n## <span id=\"page-16-0\"></span>C.4 Collaborating\
    \ with Real Human on Coding Stage\n\nTo further investigate Human-LLM collaboration,\
    \ we conducted experiments incorporating real human participants during the coding\
    \ stage, utilizing the GPT-4-turbo model. Consistent with our human experiments\
    \ in the debugging stage, five computer science master's students participated,\
    \ and all procedures were conducted under Institutional Review Board (IRB) approval.\
    \ And We employ O1-mini as the teacher programmer to provide automatic feedback.\
    \ Due to time constraints, we employed a randomly selected set of 60 problems\
    \ from an unseen dataset, with 20 problems each from the easy, middle, and hard\
    \ categories. This dataset was used to evaluate the performance of the GPT-4-turbo\
    \ model. Furthermore, to facilitate Human-LLM collaboration, we augmented the\
    \ automatic feedback generated by our teacher-programmer simulator with additional\
    \ human feedback.\n\nExperimental Results. As shown in Table [12,](#page-16-1)\
    \ GPT-4-turbo initially solved 13 easy problems, 5 medium problems, and 2 hard\
    \ problems. After receiving automatic feedback, the model was able to solve 15\
    \ easy problems, 6 medium problems, and 4 hard problems. With the addition of\
    \ human feedback, it solved 16 easy problems, 8 medium problems, and 5 hard problems.\
    \ These results highlight the effectiveness of Human-LLM collaboration in solution\
    \ provision during the coding stage. However, compared to collaboration during\
    \ the debugging stage, the improvement is relatively modest. This can be attributed\
    \ to the teacher-programmer simulator, which, by already referencing the ground\
    \ truth solution, offers the correct approach. Consequently, adding human feedback\
    \ to an already accurate solution provides limited additional benefit.\n\n## C.5\
    \ Analysis of Feedback from Human Simulators and Real Humans\n\n## C.5.1 Analysis\
    \ on Human Simulators\n\nIn this paper, we include two participant groups representing\
    \ a range of programming expertise: student programmers and teacher programmers.\
    \ The differences between these groups are as follows:\n\nResource Utilization.\
    \ In the benchmark experiments, the O1-Mini model serves as the student programmer,\
    \ offering feedback based on its internal knowledge. In contrast, the teacher\
    \ programmer simulator leverages the full ELABORA-TIONSET dataset to guide the\
    \ O1-Mini model, ensuring expert-level performance. This difference in resource\
    \ utilization directly impacts the effectiveness of the feedback provided.\n\n\
    Feedback Quality. As shown in Table 5, the effectiveness of feedback is directly\
    \ correlated to its specificity, detail, and strategic insights. Teacher feedback,\
    \ by providing these elements, promotes a deeper and more effective learning experience\
    \ for the LLM. Student feedback, lacking these features, is often less impactful,\
    \ resulting in slower or less significant improvements in LLM performance. The\
    \ key is that more precise and insightful feedback leads to better understanding\
    \ and problem-solving capabilities in the LLM.\n\n## C.5.2 Analysis on Real Humans\n\
    \nAs shown in table [6](#page-7-2) and [7,](#page-7-0) human debugging demonstrates\
    \ superior effectiveness compared to automatic debugging. In this module, we will\
    \ discuss the difference between real human feedback and human simulator feedback\
    \ at debugging stage. We illustrate an example of human-LLM collaboration during\
    \ the debugging stage in Appendix [E.2.](#page-31-0) The effectiveness of feedback\
    \ is directly tied to the accuracy of error identification and the level of detail\
    \ in modification suggestions. Real human debugging excels in these aspects, providing\
    \ more precise bug identification and more detailed modification suggestions compared\
    \ to automatic debugging. In contrast, automatic feedback, lacking these critical\
    \ features, often proves less impactful, resulting in slower or less substantial\
    \ debugging improvements in LLM performance. Ultimately, the key to enhancing\
    \ the LLM's bug-fixing capabilities lies in the ability to deliver accurate bug\
    \ identification and detailed modification suggestions.\n\n<span id=\"page-17-0\"\
    ></span>\n\n|                              | Problem Requirements<br>Specifications\
    \ Identification<br>Problem Breakdown |             | Functionality | All    \
    \       |         |\n|------------------------------|----------------------------------------------------------------------------|-------------|---------------|---------------|---------|\n\
    |                              | Understanding                               \
    \                               | Correctness | Correctness   | Clarification\
    \ | Correct |\n| O1-Mini                      | 20                           \
    \                                              | 20          | 20            |\
    \ 20            | 20      |\n| GPT-4o                       | 20             \
    \                                                            | 20          | 19\
    \            | 19            | 19      |\n| GPT-4-Turbo                  | 20\
    \                                                                         | 19\
    \          | 18            | 18            | 18      |\n| Gemini-1.5-Pro     \
    \          | 20                                                              \
    \           | 20          | 19            | 19            | 19      |\n| Claude-3.5-Sonnet\
    \            | 20                                                            \
    \             | 20          | 19            | 19            | 19      |\n| Deepseek-coder-6.7b-instruct\
    \ | 19                                                                       \
    \  | 17          | 16            | 18            | 16      |\n| CodeLlama-7b-Instruct\
    \        | 17                                                                \
    \         | 17          | 15            | 17            | 15      |\n| Qwen2.5-Coder-7B\
    \             | 19                                                           \
    \              | 19          | 18            | 18            | 18      |\n| CodeLlama-13b-Instruct\
    \       | 19                                                                 \
    \        | 19          | 17            | 17            | 17      |\n| Qwen2.5-Coder-14B\
    \            | 20                                                            \
    \             | 19          | 18            | 19            | 18      |\n| Deepseek-coder-33b-instruct\
    \  | 20                                                                      \
    \   | 20          | 19            | 19            | 19      |\n| CodeLlama-34b-Instruct\
    \       | 20                                                                 \
    \        | 20          | 18            | 18            | 18      |\n| Qwen2.5-Coder-32B\
    \            | 20                                                            \
    \             | 20          | 20            | 20            | 20      |\n| Average\
    \ Performance          | 19.5                                                \
    \                       | 19.2        | 18.2          | 18.5          | 18.2 \
    \   |\n\nTable 13: Nuanced evaluation for different LLMs' performance at comprehension\
    \ stage\n\n<span id=\"page-17-1\"></span>\n\n|                              |\
    \ Algorithm Selection | Pseudocode Generation | Complexity Analysis | Algorithm\
    \ Justification | All     |\n|------------------------------|---------------------|-----------------------|---------------------|-------------------------|---------|\n\
    |                              | Accuracy            | Quality               |\
    \ Accuracy            | Reasonableness          | Correct |\n| O1-Mini       \
    \               | 20                  | 20                    | 19           \
    \       | 19                      | 19      |\n| GPT-4o                      \
    \ | 19                  | 18                    | 17                  | 18   \
    \                   | 17      |\n| GPT-4-Turbo                  | 18         \
    \         | 17                    | 16                  | 15                 \
    \     | 15      |\n| Gemini-1.5-Pro               | 19                  | 18 \
    \                   | 18                  | 17                      | 17     \
    \ |\n| Claude-3.5-Sonnet            | 19                  | 18               \
    \     | 17                  | 18                      | 17      |\n| Deepseek-coder-6.7b-instruct\
    \ | 10                  | 10                    | 8                   | 7    \
    \                   | 7       |\n| CodeLlama-7b-Instruct        | 7          \
    \         | 6                     | 5                   | 5                  \
    \     | 5       |\n| Qwen2.5-Coder-7B             | 10                  | 10 \
    \                   | 9                   | 8                       | 8      \
    \ |\n| CodeLlama-13b-Instruct       | 11                  | 9                \
    \     | 9                   | 8                       | 8       |\n| Qwen2.5-Coder-14B\
    \            | 13                  | 12                    | 12              \
    \    | 13                      | 12      |\n| Deepseek-coder-33b-instruct  | 16\
    \                  | 14                    | 13                  | 14        \
    \              | 13      |\n| CodeLlama-34b-Instruct       | 13              \
    \    | 11                    | 10                  | 11                      |\
    \ 10      |\n| Qwen2.5-Coder-32B            | 17                  | 16       \
    \             | 15                  | 14                      | 14      |\n| Average\
    \ Performance          | 14.8                | 13.8                  | 12.9  \
    \              | 12.8                    | 12.5    |\n\nTable 14: Nuanced evaluation\
    \ for different LLMs' performance at planning stage\n\n<span id=\"page-18-0\"\
    ></span>\n\n| Model (Cut-off Date Release Date)       | Contamination Evaluation\
    \ |              |              |              | Contamination-free Evaluation\
    \ |              |              |              |\n|-----------------------------------------|--------------------------|--------------|--------------|--------------|-------------------------------|--------------|--------------|--------------|\n\
    |                                         | Easy                     | Middle\
    \       | Hard         | Overall      | Easy                          | Middle\
    \       | Hard         | Overall      |\n| O1-Mini (2023-12   2024-09)       \
    \      | 93.5                     | 78.0         | 50.7         | 74.1       \
    \  | 86.2                          | 73.2         | 34.8         | 64.7      \
    \   |\n| GPT-4o (2023-11   2024-05)              | 84.4                     |\
    \ 54.5         | 23.2         | 54.0         | 79.0                          |\
    \ 34.8         | 11.5         | 41.8         |\n| + Student Programmer Feedback\
    \           | 87.7                     | 57.5         | 27.2         | 57.5  \
    \       | 82.3                          | 38.5         | 16.4         | 45.7 \
    \        |\n| + Teacher Programmer Feedback           | 92.8                 \
    \    | 72.1         | 42.8         | 69.2         | 86.5                     \
    \     | 47.5         | 26.1         | 53.4         |\n| GPT-4-Turbo (2023-05 \
    \  2023-11)         | 74.0                     | 44.1         | 9.7          |\
    \ 42.6         | 70.3                          | 30.2         | 6.5          |\
    \ 35.7         |\n| + Student Programmer Feedback           | 80.2           \
    \          | 50.6         | 13.5         | 48.1         | 76.4               \
    \           | 36.5         | 9.8          | 40.9         |\n| + Teacher Programmer\
    \ Feedback           | 89.9                     | 64.7         | 22.5        \
    \ | 59.0         | 81.3                          | 43.8         | 16.0       \
    \  | 47.0         |\n| Gemini-1.5-pro (2023-11   2024-02)      | 85.3        \
    \             | 52.2         | 24.6         | 54.0         | 79.1            \
    \              | 36.5         | 10.4         | 42.0         |\n| + Student Programmer\
    \ Feedback           | 88.7                     | 54.6         | 28.1        \
    \ | 57.1         | 81.6                          | 38.9         | 14.7       \
    \  | 45.1         |\n| + Teacher Programmer Feedback           | 96.0        \
    \             | 71.5         | 41.0         | 69.5         | 87.5            \
    \              | 44.5         | 27.1         | 53.0         |\n| Claude-3.5 (2024-03\
    \   2024-06)          | 82.1                     | 55.9         | 18.3       \
    \  | 52.1         | 80.5                          | 37.8         | 6.1       \
    \   | 41.5         |\n| + Student Programmer Feedback           | 86.8       \
    \              | 60.5         | 27.0         | 58.1         | 82.7           \
    \               | 40.8         | 8.8          | 44.1         |\n| + Teacher Programmer\
    \ Feedback           | 93.9                     | 73.3         | 37.4        \
    \ | 68.2         | 89.8                          | 48.6         | 18.5       \
    \  | 52.3         |\n| Avg.                                    | 81.5        \
    \             | 51.7         | 19.0         | 50.7         | 77.2            \
    \              | 34.8         | 8.6          | 40.3         |\n| + Student Programmer\
    \ Feedback           | 85.9 (+4.4)              | 55.8 (+4.1)  | 24.0 (+5.0) \
    \ | 55.2 (+4.5)  | 80.8 (+3.6)                   | 38.7 (+3.9)  | 12.4 (+3.8)\
    \  | 44.0 (+3.7)  |\n| + Teacher Programmer Feedback           | 93.2 (+11.7)\
    \             | 70.4 (+18.7) | 35.9 (+16.9) | 66.5 (+15.8) | 86.3 (+9.1)     \
    \              | 46.1 (+11.3) | 21.9 (+13.3) | 51.4 (+11.1) |\n|             \
    \                            |                          |              | ∼7B Scale\
    \    |              |                               |              |         \
    \     |              |\n| CodeLlama-7B (2023-01   2024-01)        | 32.8     \
    \                | 6.5          | 0.6          | 13.3         | 16.2         \
    \                 | 2.3          | 0.3          | 6.3          |\n| + Student\
    \ Programmer Feedback           | 39.6                     | 11.2         | 2.5\
    \          | 17.8         | 26.1                          | 3.4          | 1.6\
    \          | 10.4         |\n| + Teacher Programmer Feedback           | 52.4\
    \                     | 19.3         | 7.7          | 26.5         | 38.8    \
    \                      | 9.3          | 5.3          | 17.8         |\n| Deepseek-Coder-6.7B\
    \ (2023-09   2023-11) |                          | 16.9         | 2.0        \
    \  | 20.9         |                               | 7.7          | 0.8       \
    \   | 10.5         |\n| + Student Programmer Feedback           | 43.8<br>50.0\
    \             | 20.5         | 4.8          | 25.1         | 23.1<br>30.2    \
    \              | 12.3         | 2.3          | 14.9         |\n| + Teacher Programmer\
    \ Feedback           |                          | 30.6         | 9.1         \
    \ | 34.3         |                               | 26.4         | 6.9        \
    \  | 25.3         |\n| Qwen2.5-Coder-7B (2024-06   2024-11)    | 63.1        \
    \             | 24.6         | 5.4          | 31.9         | 42.6            \
    \              | 10.3         | 0.6          | 21.1         |\n|             \
    \                            | 65.7                     |              |     \
    \         |              | 52.5                          |              |    \
    \          |              |\n| + Student Programmer Feedback           | 75.7\
    \                     | 29.3         | 7.5          | 37.5         | 58.3    \
    \                      | 13.5         | 2.6          | 24.8         |\n| + Teacher\
    \ Programmer Feedback           | 82.4                     | 39.1         | 12.3\
    \         | 44.6         | 62.5                          | 23.8         | 6.6\
    \          | 31.0         |\n| Avg.                                    | 47.4\
    \                     | 16.0         | 2.7          | 22.0         | 30.6    \
    \                      | 6.8          | 0.6          | 12.6         |\n| + Student\
    \ Programmer Feedback           | 55.1 (+7.7)              | 20.3 (+4.3)  | 4.9\
    \ (+2.2)   | 26.8 (+4.8)  | 38.2 (+7.6)                   | 9.7 (+2.9)   | 2.2\
    \ (+1.6)   | 16.7 (+4.1)  |\n| + Teacher Programmer Feedback           | 66.0\
    \ (+18.6)             | 29.7 (+13.7) | 9.7 (+7.0)   | 35.1 (+13.1) | 48.0 (+17.4)\
    \                  | 19.8 (+13.0) | 6.3 (+5.7)   | 24.7 (+12.1) |\n| ∼13B Scale\
    \                              |                          |              |   \
    \           |              |                               |              |  \
    \            |              |\n| CodeLlama-13B (2023-01   2024-01)       | 38.5\
    \                     | 8.0          | 1.9          | 16.1         | 25.0    \
    \                      | 3.3          | 0.3          | 9.5          |\n| + Student\
    \ Programmer Feedback           | 44.3                     | 13.1         | 3.2\
    \          | 20.2         | 28.1                          | 10.7         | 1.6\
    \          | 13.5         |\n| + Teacher Programmer Feedback           | 47.7\
    \                     | 21.9         | 6.5          | 25.4         | 32.3    \
    \                      | 16.1         | 3.5          | 17.3         |\n| Qwen2.5-Coder-14B\
    \ (2024-06   2024-11)   | 75.4                     | 31.5         | 8.6      \
    \    | 38.5         | 62.3                          | 16.6         | 2.5     \
    \     | 27.1         |\n| + Student Programmer Feedback           | 81.0     \
    \                | 36.9         | 11.4         | 43.1         | 66.4         \
    \                 | 20.8         | 4.6          | 30.6         |\n| + Teacher\
    \ Programmer Feedback           | 86.5                     | 45.6         | 15.9\
    \         | 49.3         | 71.6                          | 26.7         | 7.6\
    \          | 35.3         |\n| Avg.                                    | 57.0\
    \                     | 19.8         | 5.3          | 27.3         | 43.7    \
    \                      | 10.0         | 1.4          | 18.3         |\n| + Student\
    \ Programmer Feedback           | 62.7 (+5.7)              | 25.0 (+5.2)  | 7.3\
    \ (+2.0)   | 31.7 (+4.4)  | 47.3 (+3.6)                   | 15.8 (+5.8)  | 3.1\
    \ (+1.7)   | 22.1 (+3.8)  |\n| + Teacher Programmer Feedback           | 67.1\
    \ (+10.1)             | 33.8 (+14.0) | 11.2 (+5.9)  | 37.4 (+10.1) | 52.0 (+8.3)\
    \                   | 21.4 (+11.4) | 5.6 (+4.2)   | 26.3 (+8.0)  |\n| ∼34B Scale\
    \                              |                          |              |   \
    \           |              |                               |              |  \
    \            |              |\n| CodeLlama-34B (2023-01   2024-01)       | 40.3\
    \                     | 8.6          | 3.5          | 17.5         | 26.7    \
    \                      | 5.6          | 1.1          | 11.1         |\n| + Student\
    \ Programmer Feedback           | 45.4                     | 13.5         | 4.5\
    \          | 21.1         | 28.2                          | 9.2          | 2.6\
    \          | 13.3         |\n| + Teacher Programmer Feedback           | 52.7\
    \                     | 20.7         | 6.9          | 26.8         | 34.7    \
    \                      | 14.2         | 5.0          | 18.0         |\n| Deepseek-Coder-33B\
    \ (2023-09   2023-11)  | 68.3                     | 25.8         | 4.8       \
    \   | 33.0         | 54.0                          | 11.4         | 1.4      \
    \    | 22.3         |\n| + Student Programmer Feedback           | 80.1      \
    \               | 31.6         | 7.8          | 39.8         | 59.9          \
    \                | 14.5         | 3.5          | 26.0         |\n| + Teacher Programmer\
    \ Feedback           | 85.2                     | 44.1         | 13.8        \
    \ | 47.7         | 73.4                          | 22.4         | 6.2        \
    \  | 34.0         |\n| Qwen2.5-Coder-32B (2024-06   2024-11)   | 82.0        \
    \             | 45.2         | 10.1         | 45.8         | 74.6            \
    \              | 22.3         | 3.6          | 33.5         |\n| + Student Programmer\
    \ Feedback           | 86.1                     | 49.3         | 12.3        \
    \ | 49.2         | 77.1                          | 25.4         | 4.5        \
    \  | 35.7         |\n| + Teacher Programmer Feedback           | 91.4        \
    \             | 58.7         | 17.7         | 55.9         | 82.0            \
    \              | 33.1         | 8.5          | 41.2         |\n| Avg.        \
    \                            | 63.5                     | 26.5         | 6.1 \
    \         | 32.1         | 51.8                          | 13.1         | 2.0\
    \          | 22.3         |\n| + Student Programmer Feedback           | 70.5\
    \ (+7.0)              | 31.5 (+5.0)  | 8.2 (+2.1)   | 36.7 (+4.6)  | 55.1 (+3.3)\
    \                   | 16.4 (+3.3)  | 3.5 (+1.5)   | 25.0 (+2.7)  |\n| + Teacher\
    \ Programmer Feedback           | 76.4 (+12.9)             | 41.2 (+14.7) | 12.8\
    \ (+6.7)  | 43.5 (+11.4) | 63.4 (+11.6)                  | 23.2 (+10.1) | 6.6\
    \ (+4.6)   | 31.1 (+8.8)  |\n|                                         |     \
    \                     |              |              |              |         \
    \                      |              |              |              |\n| Average\
    \ over All LLMs                   | 64.4                     | 31.2         |\
    \ 9.4          | 35.0         | 53.6                          | 18.2         |\
    \ 3.8          | 25.2         |\n| + Student Programmer Feedback           | 70.5\
    \ (+6.1)              | 35.7 (+4.5)  | 12.5 (+3.1)  | 39.6 (+4.6)  | 58.1 (+4.5)\
    \                   | 22.0 (+3.8)  | 6.1 (+2.3)   | 28.8 (+3.6)  |\n| + Teacher\
    \ Programmer Feedback           | 77.8 (+13.4)             | 46.8 (+15.6) | 19.5\
    \ (+10.1) | 48.0 (+13.0) | 65.3 (+11.7)                  | 29.7 (+11.5) | 11.4\
    \ (+7.6)  | 35.5 (+10.3) |\n\nTable 15: ELABORATION benchmark analysis of human-LLM\
    \ competitive programming across different LLM backbones and varying levels of\
    \ human feedback expertise. Because O1-Mini was recently released, our experiments\
    \ with this model have been deferred. Here, we report Pass@3 scores (%).\n\n<span\
    \ id=\"page-19-0\"></span>\n\n| Model (Cut-off Date Release Date)       | Contamination\
    \ Evaluation |              |              |              | Contamination-free\
    \ Evaluation |              |              |              |\n|-----------------------------------------|--------------------------|--------------|--------------|--------------|-------------------------------|--------------|--------------|--------------|\n\
    |                                         | Easy                     | Middle\
    \       | Hard         | Overall      | Easy                          | Middle\
    \       | Hard         | Overall      |\n| O1-Mini (2023-12   2024-09)       \
    \      | 95.3                     | 81.4         | 52.8         | 76.5       \
    \  | 89.0                          | 76.4         | 37.5         | 67.6      \
    \   |\n| GPT-4o (2023-11   2024-05)              | 86.7                     |\
    \ 57.8         | 24.9         | 56.5         | 82.2                          |\
    \ 36.8         | 12.8         | 43.9         |\n| + Student Programmer Feedback\
    \           | 89.0                     | 61.0         | 29.0         | 59.7  \
    \       | 85.6                          | 40.4         | 18.0         | 48.0 \
    \        |\n| + Teacher Programmer Feedback           | 95.0                 \
    \    | 76.3         | 45.1         | 72.1         | 89.1                     \
    \     | 50.3         | 28.1         | 55.8         |\n| GPT-4-Turbo (2023-05 \
    \  2023-11)         | 75.7                     | 46.8         | 10.6         |\
    \ 44.4         | 72.0                          | 32.4         | 7.2          |\
    \ 37.2         |\n| + Student Programmer Feedback           | 82.0           \
    \          | 53.5         | 14.6         | 50.0         | 78.5               \
    \           | 38.5         | 10.7         | 42.6         |\n| + Teacher Programmer\
    \ Feedback           | 92.1                     | 68.6         | 24.0        \
    \ | 61.6         | 84.2                          | 46.3         | 17.3       \
    \  | 49.3         |\n| Gemini-1.5-pro (2023-11   2024-02)      | 87.0        \
    \             | 55.5         | 26.1         | 56.2         | 82.2            \
    \              | 39.3         | 11.7         | 44.4         |\n| + Student Programmer\
    \ Feedback           | 91.0                     | 58.5         | 30.1        \
    \ | 59.9         | 85.2                          | 42.7         | 16.0       \
    \  | 48.0         |\n| + Teacher Programmer Feedback           | 99.8        \
    \             | 75.7         | 44.7         | 73.4         | 91.0            \
    \              | 47.8         | 29.3         | 56.0         |\n| Claude-3.5 (2024-03\
    \   2024-06)          | 84.0                     | 59.3         | 19.7       \
    \  | 54.3         | 83.7                          | 40.5         | 6.9       \
    \   | 43.7         |\n| + Student Programmer Feedback           | 89.3       \
    \              | 64.2         | 29.4         | 61.0         | 86.2           \
    \               | 43.8         | 9.6          | 46.5         |\n| + Teacher Programmer\
    \ Feedback           | 97.2                     | 77.7         | 40.3        \
    \ | 71.7         | 93.3                          | 51.5         | 20.2       \
    \  | 55.0         |\n| Avg.                                    | 83.4        \
    \             | 54.9         | 20.3         | 52.9         | 80.0            \
    \              | 37.3         | 9.7          | 42.3         |\n| + Student Programmer\
    \ Feedback           | 87.8 (+4.4)              | 59.3 (+4.4)  | 25.8 (+5.5) \
    \ | 57.7 (+4.8)  | 83.9 (+3.9)                   | 41.4 (+4.1)  | 13.6 (+3.9)\
    \  | 46.3 (+4.0)  |\n| + Teacher Programmer Feedback           | 96.0 (+12.6)\
    \             | 74.6 (+19.7) | 38.5 (+18.2) | 69.7 (+16.8) | 89.4 (+9.4)     \
    \              | 49.0 (+11.7) | 23.7 (+14.0) | 54.0 (+11.7) |\n|             \
    \                            |                          |              | ∼7B Scale\
    \    |              |                               |              |         \
    \     |              |\n| CodeLlama-7B (2023-01   2024-01)        | 33.9     \
    \                | 6.8          | 0.6          | 13.8         | 16.7         \
    \                 | 2.4          | 0.3          | 6.5          |\n| + Student\
    \ Programmer Feedback           | 40.9                     | 11.6         | 2.7\
    \          | 18.4         | 26.9                          | 3.6          | 1.7\
    \          | 10.7         |\n| + Teacher Programmer Feedback           | 54.2\
    \                     | 20.4         | 8.2          | 27.6         | 40.3    \
    \                      | 9.8          | 5.7          | 18.6         |\n| Deepseek-Coder-6.7B\
    \ (2023-09   2023-11) | 45.0                     | 17.7         | 2.1        \
    \  | 21.6         | 24.0                          | 8.1          | 0.8       \
    \   | 11.0         |\n| + Student Programmer Feedback           | 52.0       \
    \              | 21.5         | 5.1          | 26.2         | 31.3           \
    \               | 13.0         | 2.5          | 15.6         |\n| + Teacher Programmer\
    \ Feedback           | 65.5                     | 32.3         | 9.7         \
    \ | 35.8         | 44.0                          | 27.9         | 7.4        \
    \  | 26.4         |\n| Qwen2.5-Coder-7B (2024-06   2024-11)    | 67.9        \
    \             | 25.8         | 5.8          | 33.2         | 54.6            \
    \              | 10.9         | 0.7          | 22.1         |\n| + Student Programmer\
    \ Feedback           | 78.3                     | 30.8         | 8.1         \
    \ | 39.1         | 60.1                          | 14.3         | 2.8        \
    \  | 25.7         |\n| + Teacher Programmer Feedback           | 85.1        \
    \             | 41.4         | 13.4         | 46.6         | 64.9            \
    \              | 25.2         | 7.1          | 32.4         |\n| Avg.        \
    \                            | 48.9                     | 16.8         | 2.8 \
    \         | 22.9         | 31.8                          | 7.1          | 0.6\
    \          | 13.2         |\n| + Student Programmer Feedback           | 57.1\
    \ (+8.2)              | 21.3 (+4.5)  | 5.3 (+2.5)   | 27.9 (+5.0)  | 39.4 (+7.6)\
    \                   | 10.3 (+3.2)  | 2.3 (+1.7)   | 17.3 (+4.1)  |\n| + Teacher\
    \ Programmer Feedback           | 68.3 (+19.4)             | 31.4 (+14.6) | 10.4\
    \ (+7.6)  | 36.7 (+13.8) | 49.7 (+17.9)                  | 21.0 (+13.9) | 6.7\
    \ (+6.1)   | 25.8 (+12.6) |\n|                                         |     \
    \                     |              | ∼13B Scale   |              |         \
    \                      |              |              |              |\n| CodeLlama-13B\
    \ (2023-01   2024-01)       | 39.8                     | 8.5          | 2.0  \
    \        | 16.8         | 26.0                          | 3.5          | 0.3 \
    \         | 9.9          |\n| + Student Programmer Feedback           | 46.1 \
    \                    | 13.9         | 3.4          | 21.1         | 29.2     \
    \                     | 11.3         | 1.7          | 14.1         |\n| + Teacher\
    \ Programmer Feedback           | 49.6                     | 23.2         | 6.9\
    \          | 26.6         | 33.4                          | 17.0         | 3.8\
    \          | 18.1         |\n| Qwen2.5-Coder-14B (2024-06   2024-11)   | 78.0\
    \                     | 33.4         | 9.1          | 40.2         | 64.2    \
    \                      | 17.6         | 2.7          | 28.2         |\n| + Student\
    \ Programmer Feedback           | 83.5                     | 39.2         | 12.1\
    \         | 44.9         | 68.3                          | 22.0         | 4.9\
    \          | 31.7         |\n| + Teacher Programmer Feedback           | 89.1\
    \                     | 48.3         | 16.8         | 51.4         | 73.1    \
    \                      | 28.3         | 8.2          | 36.5         |\n| Avg.\
    \                                    | 58.9                     | 21.0       \
    \  | 5.6          | 28.5         | 45.1                          | 10.6      \
    \   | 1.5          | 19.1         |\n| + Student Programmer Feedback         \
    \  | 64.8 (+5.9)              | 26.6 (+5.6)  | 7.8 (+2.2)   | 33.0 (+4.5)  | 48.8\
    \ (+3.7)                   | 16.7 (+6.1)  | 3.3 (+1.8)   | 22.9 (+3.8)  |\n| +\
    \ Teacher Programmer Feedback           | 69.4 (+10.5)             | 35.8 (+14.8)\
    \ | 11.9 (+6.3)  | 39.0 (+10.5) | 53.3 (+8.2)                   | 22.7 (+12.1)\
    \ | 6.0 (+4.5)   | 27.3 (+8.2)  |\n| ∼34B Scale                              |\
    \                          |              |              |              |    \
    \                           |              |              |              |\n|\
    \ CodeLlama-34B (2023-01   2024-01)       | 41.5                     | 9.0   \
    \       | 3.8          | 18.1         | 27.6                          | 5.9  \
    \        | 1.2          | 11.6         |\n| + Student Programmer Feedback    \
    \       | 46.8                     | 14.2         | 4.9          | 22.0      \
    \   | 29.3                          | 9.7          | 2.8          | 13.9     \
    \    |\n| + Teacher Programmer Feedback           | 54.1                     |\
    \ 21.8         | 7.4          | 27.8         | 36.0                          |\
    \ 15.1         | 5.4          | 18.8         |\n| Deepseek-Coder-33B (2023-09\
    \   2023-11)  | 70.1                     | 27.2         | 5.2          | 34.2\
    \         | 55.9                          | 12.0         | 1.6          | 23.2\
    \         |\n| + Student Programmer Feedback           | 83.2                \
    \     | 33.4         | 8.4          | 41.7         | 62.1                    \
    \      | 15.4         | 3.8          | 27.1         |\n| + Teacher Programmer\
    \ Feedback           | 88.7                     | 46.7         | 15.0        \
    \ | 50.1         | 75.8                          | 23.7         | 6.8        \
    \  | 35.4         |\n| Qwen2.5-Coder-32B (2024-06   2024-11)   | 84.5        \
    \             | 47.1         | 11.0         | 47.5         | 76.9            \
    \              | 23.6         | 3.9          | 34.8         |\n| + Student Programmer\
    \ Feedback           | 89.0                     | 51.5         | 13.3        \
    \ | 51.3         | 79.2                          | 26.9         | 4.8        \
    \  | 37.0         |\n| + Teacher Programmer Feedback           | 94.3        \
    \             | 61.6         | 19.0         | 58.3         | 84.6            \
    \              | 35.1         | 9.2          | 43.0         |\n| Avg.        \
    \                            | 65.4                     | 27.8         | 6.7 \
    \         | 33.3         | 53.5                          | 13.8         | 2.2\
    \          | 23.2         |\n| + Student Programmer Feedback           | 73.0\
    \ (+7.6)              | 33.0 (+5.2)  | 8.9 (+2.2)   | 38.3 (+5.0)  | 56.9 (+3.4)\
    \                   | 17.3 (+3.5)  | 3.8 (+1.6)   | 26.0 (+2.8)  |\n| + Teacher\
    \ Programmer Feedback           | 79.0 (+13.6)             | 43.4 (+15.6) | 13.8\
    \ (+7.1)  | 45.4 (+12.1) | 65.5 (+12.0)                  | 24.6 (+10.8) | 7.1\
    \ (+4.9)   | 32.4 (+9.2)  |\n| Average over All LLMs                   | 66.2\
    \                     | 32.9         | 10.1         | 36.4         | 55.5    \
    \                      | 19.4         | 4.2          | 26.4         |\n| + Student\
    \ Programmer Feedback           | 72.6 (+6.4)              | 37.8 (+4.9)  | 13.4\
    \ (+3.3)  | 41.3 (+4.9)  | 60.2 (+4.7)                   | 23.5 (+4.1)  | 6.6\
    \ (+2.4)   | 30.1 (+3.7)  |\n| + Teacher Programmer Feedback           | 80.4\
    \ (+14.2)             | 49.5 (+16.6) | 20.9 (+10.8) | 50.3 (+13.9) | 67.5 (+12.0)\
    \                  | 31.5 (+12.1) | 12.4 (+8.2)  | 37.1 (+10.7) |\n\nTable 16:\
    \ ELABORATION benchmark analysis of human-LLM competitive programming across different\
    \ LLM backbones and varying levels of human feedback expertise. Because O1-Mini\
    \ was recently released, our experiments with this model have been deferred. Here,\
    \ we report Pass@5 scores (%)\n\n## Random Input Generator Prompt\n\nYou are an\
    \ expert programmer and your goal is to generate test case input based based on\
    \ the problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT},\
    \ output format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE},\
    \ the input number lower boundary input\\_min{INPUT\\_MIN}, the input number upper\\\
    _boundaryINPUT\\_MAX. You will create appropriate generators and ultimately develop\
    \ a test\\_case\\_inputs function that returns a list of varied inputs sampled\
    \ from the generator. Be sure to adhere closely to the instructions and constraints\
    \ outlined in the problem statement.\n\n```\nimport numpy as np\n```\n\n```\n\
    def random_input_generator(weight_min, weight_max, size_min, size_max):\n    weights_size\
    \ = np.random.randint(size_min, size_max + 1)\n    weights = np.random.randint(weight_min,\
    \ weight_max, size=weights_size).tolist()\n    k = np.random.randint(1, len(weights)\
    \ + 1)\n    return weights, k\ndef construct_inputs():\n    inputs_list = []\n\
    \    for _ in range(15):\n        inputs_list.append(random_input_generator(input\\\
    _min, input\\_max ,1))\n    return inputs_list\n```\nConstruct a random input\
    \ generator. Return a single function named test\\_case\\_input that creates diverse\
    \ inputs, following the format used in the example above.\n\n## Algorithm konwledge\
    \ summaries examples\n\n### Greedy:\n\nA greedy algorithm is an approach for solving\
    \ optimization problems by making a sequence of choices, each of which looks best\
    \ at the moment. The idea is to choose the option that seems the most advantageous\
    \ at each stage with the hope that these local optimal choices will lead to a\
    \ global optimal solution. Greedy algorithms are typically used in problems like\
    \ finding the minimum spanning tree, shortest paths in graphs, and various resource\
    \ allocation problems.\n\nThe key characteristics of greedy algorithms are:\n\n\
    1. Locally Optimal Choice: At each step, the algorithm chooses the best option\
    \ available without considering the global context.\n\n2. Feasibility: The choice\
    \ made must satisfy the problem's constraints.\n\n3. Irrevocability: Once a choice\
    \ is made, it cannot be undone.\n\nPseudocode for a Greedy Algorithm:\n\n```\n\
    Function GreedyFractionalKnapsack(capacity, items):\n    // items is a list of\
    \ tuples (value, weight)\n    // Calculate value-to-weight ratio for each item\n\
    \    for each item in items:\n        item.ratio = item.value / item.weight\n\
    \    // Sort items by their value-to-weight ratio in descending order\n    Sort\
    \ items by item.ratio in descending order\n    total_value = 0\n    for each item\
    \ in items:\n        if capacity == 0:\n            break\n        // Check if\
    \ the item can be added fully\n        if item.weight <= capacity:\n         \
    \   capacity -= item.weight\n            total_value += item.value\n        else:\n\
    \            // Add the fraction of the remaining item\n            total_value\
    \ += item.value * (capacity / item.weight)\n            capacity = 0 // Knapsack\
    \ is full\n```\nreturn total\\_value\n\n### Depth-First Search:\n\nDepth-First\
    \ Search (DFS) is an algorithm for traversing or searching tree or graph data\
    \ structures. The algorithm starts at the root (or an arbitrary node in a graph)\
    \ and explores as far as possible along each branch before backtracking. This\
    \ method is particularly useful for exploring all possible paths in a graph or\
    \ tree, finding connected components, and solving puzzles with a single solution.\n\
    \nKey Characteristics of DFS:\n\n1. Exploration Depth: DFS explores the deepest\
    \ nodes first before backtracking.\n\n2. Stack-Based: It can be implemented using\
    \ recursion or an explicit stack data structure.\n\n3. Complete: In finite graphs,\
    \ DFS will visit all reachable nodes. Space Complexity: The space complexity can\
    \ be high due to the depth of recursion or the size of the stack.\n\n```\nFunction\
    \ DFS(graph, start_node):\n    Initialize an empty set called visited\n    Call\
    \ DFS_Visit(graph, start_node, visited)\nFunction DFS_Visit(graph, node, visited):\n\
    \    // Mark the current node as visited\n    visited.add(node)\n    Process the\
    \ node (e.g., print or store the node value)\n    // Recursively visit each adjacent\
    \ node\n    for each neighbor in graph[node]:\n        if neighbor not in visited:\n\
    \            DFS_Visit(graph, neighbor, visited)\n```\n# <span id=\"page-22-0\"\
    ></span>D Human Feeback Case Study\n\nWe provided a detailed example showing how\
    \ teacher programmer help Deepseek-Coder-33B solve competitive programming problems.\n\
    \n## Codeforces Problem 1994 C\n\n### Problem Statement:\n\nYaroslav is playing\
    \ a computer game, and at one of the levels, he encountered n mushrooms arranged\
    \ in a row. Each mushroom has its own level of toxicity; the i-th mushroom from\
    \ the beginning has a toxicity level of ai. Yaroslav can choose two integers 1\
    \ ≤ l ≤ r ≤ n, and then his character will take turns from left to right to eat\
    \ mushrooms from this subsegment one by one, i.e., the mushrooms with numbers\
    \ l, l + 1, l + 2, . . . , r.\n\nThe character has a toxicity level g, initially\
    \ equal to 0. The computer game is defined by the number x — the maximum toxicity\
    \ level at any given time. When eating a mushroom with toxicity level k, the following\
    \ happens: - The toxicity level of the character is increased by k. - If g ≤ x,\
    \ the process continues; otherwise, g becomes zero and the process continues.\n\
    \nYaroslav became interested in how many ways there are to choose the values of\
    \ l and r such that the final value of g is not zero. Help Yaroslav find this\
    \ number!\n\n### Input Format:\n\nEach test consists of multiple test cases. The\
    \ first line contains an integer t (1 ≤ t ≤ 10<sup>4</sup> ) — the number of test\
    \ cases. Then follows the description of the test cases. - The first line of each\
    \ test case contains two integers n, x (1 ≤ n ≤ 2 · 10<sup>5</sup> , 1 ≤ x ≤ 10<sup>9</sup>\
    \ ) — the number of mushrooms and the maximum toxicity level. - The second line\
    \ of each test case contains n numbers a1, a2, . . . , a<sup>n</sup> (1 ≤ a<sup>i</sup>\
    \ ≤ 10<sup>9</sup> ). It is guaranteed that the sum of n over all test cases does\
    \ not exceed 2 · 10<sup>5</sup> .\n\n# Output Format:\n\nFor each test case, output\
    \ a single number — the number of subsegments such that the final value of g will\
    \ not be zero.\n\n```\nInput\n5\n4 2\n1 1 1 1\n3 2\n1 2 3\n1 6\n10\n6 3\n1 2 1\
    \ 4 3 8\n5 999999999\n999999999 999999998 1000000000 1000000000 500000000\nOutput\n\
    8\n2\n0\n10\n7\n```\n## Deepseek-Coder-33B generation at comprehension stage\n\
    \n#### \"Problem Requirements\":\n\n1. \"Input:\" - The first line contains an\
    \ integer t (number of test cases). - Each test case consists of: - A line containing\
    \ two integers n (the number of mushrooms) and x (the maximum toxicity level).\
    \ - A line containing n integers a1, a2, . . . , a<sup>n</sup> (the toxicity levels\
    \ of the mushrooms).\n\n2. \"Output:\" - For each test case, output a single integer\
    \ representing the number of valid subsegments for which the final toxicity level\
    \ g is not zero after consuming the mushrooms in that segment.\n\n3. \"Functionality:\"\
    \ - The program should efficiently handle multiple test cases with constraints\
    \ on the total number of mushrooms across all test cases.\n\n\"Problem Specification\"\
    : - Constraints: - 1 ≤ t ≤ 10<sup>4</sup> - 1 ≤ n ≤ 2 · 10<sup>5</sup> - 1 ≤ x\
    \ ≤ 10<sup>9</sup> - 1 ≤ a<sup>i</sup> ≤ 10<sup>9</sup> - The sum of n across\
    \ all test cases does not exceed 2 · 10<sup>5</sup> .\n\n- Each subsegment is\
    \ defined by two indices l and r (1-based indexing) such that 1 ≤ l ≤ r ≤ n.\n\
    \n## Teacher programmer feedback at comprehension stage\n\n#### Teacher Programmer\
    \ Generation:\n\n\"Unsatisfied\": \"You have not understood the problem statement,\
    \ missing the requirements and specifications. Let me clarify the problem statement\
    \ for you. Please modify your answer accordingly.\"\n\n#### \\*\\*Problem Requirements\\\
    *\\*:\n\n1. \\*\\*Input\\*\\*: - The first line contains an integer t (1 ≤ t ≤\
    \ 10<sup>4</sup> ) — the number of test cases. - For each test case: - The first\
    \ line contains two integers n and x (1 ≤ n ≤ 2 × 10<sup>5</sup> , 1 ≤ x ≤ 10<sup>9</sup>\
    \ ) — the number of mushrooms and the maximum toxicity level. - The second line\
    \ contains n integers a1, a2, . . . , a<sup>n</sup> (1 ≤ a<sup>i</sup> ≤ 10<sup>9</sup>\
    \ ) — the toxicity levels of the mushrooms.\n\n2. \\*\\*Output\\*\\*: - For each\
    \ test case, output a single integer — the number of subsegments [l, r] (1 ≤ l\
    \ ≤ r ≤ n) such that after processing the subsegment, the final value of toxicity\
    \ level g is not zero.\n\n3. \\*\\*Functionality\\*\\*: - For each subsegment\
    \ [l, r], simulate the process where the character starts with toxicity level\
    \ g = 0. - The character eats mushrooms from left to right within the subsegment:\
    \ - Upon eating the i-th mushroom (l ≤ i ≤ r): - Increase g by ai. - If g ≤ x,\
    \ continue to the next mushroom. - If g > x, reset g to zero and continue. - After\
    \ eating all mushrooms in the subsegment, check if g is not zero. - Count all\
    \ such subsegments where g ̸= 0 after processing.\n\n\\*\\*Problem Specification\\\
    *\\*:\n\n- \\*\\*Constraints\\*\\*: - 1 ≤ t ≤ 10<sup>4</sup> - 1 ≤ n ≤ 2 × 10<sup>5</sup>\
    \ - 1 ≤ x ≤ 10<sup>9</sup> - 1 ≤ a<sup>i</sup> ≤ 10<sup>9</sup> for all i from\
    \ 1 to n. - The total sum of n across all test cases does not exceed 2 × 10<sup>5</sup>\
    \ .\n\n- \\*\\*Additional Notes\\*\\*: - A subsegment is defined by two indices\
    \ l and r such that 1 ≤ l ≤ r ≤ n. - The goal is to efficiently compute the number\
    \ of valid subsegments for each test case, adhering to the given constraints.\
    \ - Optimal algorithms may be required to handle the large input size within reasonable\
    \ time limits.\n\n#### Deepseek-Coder-33B:\n\n#### \"Problem Requirements\":\n\
    \n1. \"Input Format\": Each test consists of multiple test cases. The first line\
    \ contains an integer t (1 ≤ t ≤ 10<sup>4</sup> ) — the number of test cases.\
    \ Then follows the description of the test cases. - The first line of each test\
    \ case contains two integers n, x (1 ≤ n ≤ 2 · 10<sup>5</sup> , 1 ≤ x ≤ 10<sup>9</sup>\
    \ ) — the number of mushrooms and the maximum toxicity level. - The second line\
    \ of each test case contains n numbers a1, a2, . . . , a<sup>n</sup> (1 ≤ a<sup>i</sup>\
    \ ≤ 10<sup>9</sup> ).\n\nIt is guaranteed that the sum of n over all test cases\
    \ does not exceed 2 · 10<sup>5</sup> .\n\n2. \"Output Format\": For each test\
    \ case, output a single number — the number of subsegments such that the final\
    \ value of g will not be zero.\n\n3. \"Functionality\": Yaroslav is playing a\
    \ computer game where he encounters n mushrooms with varying toxicity levels arranged\
    \ in a row. The task is to find the number of subsegments such that the character's\
    \ final toxicity level is not zero after consuming mushrooms within that subsegment.\
    \ The character's toxicity level increases as mushrooms are consumed, but if it\
    \ exceeds the maximum toxicity level, it resets to zero.\n\n\"Problem Specification\"\
    : Yaroslav faces a challenge in determining the count of subsegments where the\
    \ character's toxicity level remains non-zero. The input includes the number of\
    \ test cases, the number of mushrooms, the maximum toxicity level, and the toxicity\
    \ levels of each mushroom. The output should provide the count of such subsegments\
    \ for each test case. The algorithm should consider the sequential consumption\
    \ of mushrooms and the toxicity level constraints to calculate the desired outcome\
    \ accurately.\n\n### Teacher Programmer:\n\nYou have understood the problem statement,\
    \ including the requirements and specifications.\n\n## Deepseek-Coder-33B generation\
    \ at planning stage\n\n\"Algorithm\": Sliding Window Technique.\n\n### \"Reasons\
    \ for the algorithm choice\":\n\nThe Sliding Window Technique is suitable for\
    \ this problem as it involves maintaining a window of variable size while iterating\
    \ through the mushrooms. This method allows us to efficiently calculate the toxicity\
    \ level and keep track of subsegments where the final toxicity level is non-zero.\
    \ By adjusting the window boundaries based on toxicity levels and constraints,\
    \ we can accurately count the desired subsegments within the given limits.\n\n\
    ### \"Algorithm Pseudocode\":\n\n1. Initialize variables count = 0, g = 0, start\
    \ = 0.\n\n- 2. Iterate through the mushrooms from 0 to n-1 using a loop:\n- Increment\
    \ g by the toxicity level of the current mushroom.\n- While g is greater than\
    \ x, reduce g by the toxicity level of the mushroom at the start index and increment\
    \ start.\n- If g is not equal to 0, update count by adding the size of the current\
    \ window.\n- 3. Output count as the number of subsegments where the final value\
    \ of g is not zero.\n\n## Teacher programmer feedback at planning stage\n\n###\
    \ Teacher Programmer :\n\n\"Unsatisfied\": \"I think you have choosen the wrong\
    \ algorithm. Please modify your answer accordingly.\"\n\n\\*\\*\"Algorithm\":\\\
    *\\* Two-Pointer Sliding Window\n\n\\*\\*\"Reasons for the algorithm choice\"\
    :\\*\\*\n\n1. \\*\\*Efficiency\\*\\*: The two-pointer sliding window technique\
    \ operates in linear time O(n), which is optimal given the constraints (n ≤ 2\
    \ × 10<sup>5</sup> per test case and t ≤ 10<sup>4</sup> ).\n\n2. \\*\\*Simplicity\\\
    *\\*: This method is straightforward to implement and understand, reducing the\
    \ likelihood of errors during coding.\n\n3. \\*\\*Applicability\\*\\*: The problem\
    \ essentially reduces to counting the number of subarrays with a sum ≤ x. The\
    \ sliding window approach is well-suited for such range-based sum queries, especially\
    \ when dealing with only positive integers (as toxicity levels a<sup>i</sup> are\
    \ ≥ 1).\n\n4. \\*\\*Optimal for Constraints\\*\\*: Given that the sum of n across\
    \ all test cases does not exceed 2 × 10<sup>5</sup> , an O(n) per test case algorithm\
    \ ensures that the solution will run efficiently within the time limits.\n\n###\
    \ Deepseek-Coder-33B:\n\n\"Algorithm\": Two-Pointer Sliding Window\n\n### \"Reasons\
    \ for the algorithm choice\":\n\nThe decision to utilize the sliding window algorithm\
    \ was based on a thorough analysis of various factors. Firstly, the algorithm's\
    \ linear time complexity of O(n) aligns well with the constraints of the problem,\
    \ ensuring optimal efficiency given the input size limitations (n ≤ 2 × 10<sup>5</sup>\
    \ per test case and t ≤ 10<sup>4</sup> ). Secondly, its straightforward implementation\
    \ and intuitive nature simplify the coding process, reducing the chances of errors.\
    \ Additionally, the problem's nature of counting subarrays with a sum less than\
    \ or equal to a given threshold x makes the sliding window technique particularly\
    \ well-suited, especially when dealing with non-negative integers (as indicated\
    \ by the toxicity levels a<sup>i</sup> ≥ 1). Lastly, considering the cumulative\
    \ sum of n across all test cases does not exceed 2 × 10<sup>5</sup> , an O(n)\
    \ per test case approach ensures that the solution remains efficient and within\
    \ the specified time limits.\n\n## \"Algorithm Pseudocode\":\n\n```\nfunction\
    \ slidingWindow(arr, target):\n    left = 0\n    result = some initial value\n\
    \    for right from 0 to length(arr) - 1:\n        windowValue = arr[right]\n\
    \        while some condition on the window (e.g., sum > target, size of window\
    \ exceeds limit):\n            windowValue += arr[left]\n            left -= 1\
    \ # Move the left pointer to the right\n        result = updateResult(result,\
    \ windowValue)\n    return result\n```\n## Teacher programmer feedback at planning\
    \ stage\n\n### Teacher Programmer Generation:\n\n\"Unsatisfied\": \"I think the\
    \ your pseudocode has some errors, please modify it accordingly\"\n\n### Pseudocode:\n\
    \n```\nfor each test case:\n    # Step 1: Read inputs\n    read n and x # n is\
    \ the size of the array, x is the sum constraint\n    read array a of size n #\
    \ Array of size n\n    # Step 2: Initialize variables\n    left = 0 # Left pointer\
    \ of the sliding window\n    current_sum = 0 # The sum of elements in the current\
    \ window\n    total_subsegments = 0 # To store the count of subarrays with sum\
    \ <= x\n    # Step 3: Traverse the array using the right pointer\n    for right\
    \ from 0 to n-1:\n        # Add the current element to the window\n        current_sum\
    \ += a[right]\n        # Step 4: Shrink the window if the sum exceeds x\n    \
    \    while current_sum > x and left <= right:\n            # Remove the leftmost\
    \ element from the window\n            current_sum -= a[left]\n            # Move\
    \ the left pointer to the right\n            left += 1\n        # Step 5: Count\
    \ all valid subarrays ending at 'right'\n        # All subarrays starting from\
    \ 'left' to 'right' have sum <= x\n        total_subsegments += (right - left\
    \ + 1)\n    # Step 6: Output the result for this test case\n```\n# Deepseek-Coder-33B:\n\
    \nOK, here is my new answer:\n\noutput total\\_subsegments\n\n```\nfor each test\
    \ case:\n    read n and x # n is the size of the array, x is the sum constraint\n\
    \    read array a of size n\n    left = 0 # left pointer of the sliding window\n\
    \    current_sum = 0 # the sum of elements in the current window\n    total_subsegments\
    \ = 0 # count of valid subarrays with sum <= x\n    for right from 0 to n-1:\n\
    \        # Add the current element to the current sum\n        current_sum +=\
    \ a[right]\n        while current_sum > x and left <= right:\n            # Remove\
    \ the leftmost element from the current sum\n            current_sum -= a[left]\n\
    \            # Move the left pointer to the right\n            left += 1\n   \
    \     # All subarrays starting from left to right have sum <= x\n        total_subsegments\
    \ += (right - left + 1)\n    output total_subsegments\n```\n## Teacher Programmer:\n\
    \n\"Satisfied\": You have chosen the correct algorithm and generated the corresponding\
    \ accurate pseudocode.\"\n\n```\nDeepseek-Coder-33B generation at coding stage\n\
    # Function to solve the problem for each test case\ndef solve_test_case(n, x,\
    \ a):\n    left = 0\n    current_sum = 0\n    total_subsegments = 0\n    for right\
    \ in range(n):\n        current_sum += a[right]\n        while current_sum > x\
    \ and left <= right:\n            current_sum -= a[left]\n            left +=\
    \ 1\n        total_subsegments += (right - left + 1)\n    return total_subsegments\n\
    # Main function to read input and process test cases\ndef main():\n    t = int(input())\n\
    \    for _ in range(t):\n        n, x = map(int, input().split())\n        a =\
    \ list(map(int, input().split()))\n        result = solve_test_case(n, x, a)\n\
    \        print(result)\n# Run the main function\nif __name__ == \"__main__\":\n\
    \    main()\n```\n## Deepseek-Coder-33B generation at coding stage\n\n#### Teacher\
    \ Programmer:\n\n\"Unsatisfied\" : \"The solution provided has some errors and\
    \ misunderstandings that need to be addressed for it to correctly solve the problem.\
    \ Let's break down the issues and provide suggestions for modifications.\" Suggestions\
    \ for the Solution\n\n1. \\*\\*Understand the reset condition\\*\\*: - For each\
    \ subsegment '[l, r]', we need to track whether the cumulative sum exceeds 'x'.\
    \ If it does, we reset 'g' to zero and stop counting subsegments starting from\
    \ that index. - We need to count the number of valid subsegments where the cumulative\
    \ sum never exceeds 'x'.\n\n2. \\*\\*Sliding window for subsegments\\*\\*: - Instead\
    \ of just moving the left pointer whenever the sum exceeds 'x', we need to adjust\
    \ the window and ensure that the sum never exceeds 'x' at any point during the\
    \ traversal of the subsegment. - Specifically, if the sum goes above 'x', the\
    \ subsegment is invalid, and we should break out of the loop.\n\n3. \\*\\*Efficiently\
    \ count valid subsegments\\*\\*: - For each test case, we should compute all possible\
    \ subsegments efficiently, taking into account the maximum possible sum 'x'. For\
    \ a subsegment to be valid, the cumulative sum should not exceed 'x' at any time.\n\
    \n4. \\*\\*Optimization with prefix sums\\*\\*: - One approach would be to maintain\
    \ a prefix sum and calculate the sum for each subsegment '[l, r]'. This allows\
    \ for fast sum calculation without needing to sum over each range repeatedly.\
    \ - The sliding window can be adjusted dynamically by checking whether the sum\
    \ exceeds 'x' as we move through the array. General Steps for the New Solution:\n\
    \n1. \\*\\*Iterate through each test case\\*\\*. 2. For each test case: - Initialize\
    \ 'left' pointer and set 'current\\_sum' to 0. - Use the 'right' pointer to iterate\
    \ through the array. - For each 'right', add 'a[right]' to 'current\\_sum'. -\
    \ While 'current\\_sum' exceeds 'x', move the 'left' pointer right to shrink the\
    \ window. - For each valid window (where 'current\\_sum <= x'), count all subarrays\
    \ that end at 'right' and start from any index between 'left' and 'right'. 3.\
    \ Print the result for each test case.\n\nThis approach ensures that the problem\
    \ is solved in an efficient manner, using sliding window techniques, and counting\
    \ the valid subsegments in a way that satisfies the problem's constraints.\n\n\
    Complexity Considerations: - The sliding window approach ensures that both the\
    \ 'left' and 'right' pointers move from 0 to 'n-1', making the solution efficient\
    \ with a time complexity of 'O(n)' for each test case. - Given that the total\
    \ sum of 'n' across all test cases is constrained to 2 × 10<sup>5</sup> , the\
    \ approach is efficient enough for the input limits.\n\nDeepseek-Coder-33B: Here\
    \ is my revision code.\n\n```\ndef solve_test_case(n, x, a):\n    left = 0\n \
    \   current_sum = 0\n    total_subsegments = 0\n    # Use sliding window to count\
    \ valid subsegments\n    for right in range(n):\n        current_sum += a[right]\
    \ # Expand the window by adding the current element\n        # Shrink the window\
    \ if the current_sum exceeds x\n        while current_sum > x:\n            current_sum\
    \ -= a[left]\n            left += 1\n        # Count all valid subsegments ending\
    \ at 'right'\n        total_subsegments += (right - left + 1)\n    return total_subsegments\n\
    def main():\n    # Read the number of test cases\n    t = int(input())\n    results\
    \ = []\n    for _ in range(t):\n        # Read n and x\n        n, x = map(int,\
    \ input().split())\n        # Read array a\n        a = list(map(int, input().split()))\n\
    \        # Solve for the current test case and store the result\n        result\
    \ = solve_test_case(n, x, a)\n        results.append(result)\n    # Print all\
    \ results\n    print(\"\\n\".join(map(str, results)))\nif __name__ == \"__main__\"\
    :\n    main()\n```\n\n```\n\"Satisfied:\" \"You have chosen the correct solution.\"\
    \n```\n## Deepseek-Coder-33B generation at debugging stage\n\n```\n\"Based on\
    \ the compiler output, I modify my code.\"\n\"Bug Identification\"\nReset condition\
    \ handling: If the cumulative sum exceeds x, not only should the left pointer\
    \ adjust, but we also need to\nensure the sum resets properly so invalid subsegments\
    \ are not included.\nValid subsegment count: When toxicity resets (g becomes zero),\
    \ all subsegments involving the invalid segment should\nbe discarded.\n\"problem\
    \ Resolution:\"\n# Function to solve the problem for each test case\ndef solve_test_case(n,\
    \ x, a):\n    left = 0\n    current_sum = 0\n    total_subsegments = 0\n    for\
    \ right in range(n):\n        current_sum += a[right]\n        # If toxicity exceeds\
    \ x, reset and adjust the left pointer\n        while current_sum > x and left\
    \ <= right:\n             current_sum -= a[left]\n             left += 1\n   \
    \     # Only count valid subsegments where the current_sum never exceeds x\n \
    \       total_subsegments += (right - left + 1)\n    return total_subsegments\n\
    def main():\n    # Read number of test cases\n    t = int(input())\n    results\
    \ = []\n    for _ in range(t):\n        n, x = map(int, input().split())\n   \
    \     a = list(map(int, input().split()))\n        # Solve and collect results\n\
    \        results.append(solve_test_case(n, x, a))\n    # Output all results\n\
    \    print(\"\\n\".join(map(str, results)))\nif __name__ == \"__main__\":\n  \
    \  main()\n```\n## Teacher programmer feedback at debugging stage\n\nTeacher Programmer:\
    \ \\*\\*\"unsatisfied\":\\*\\* \"There are some syntactic(semantic) errors in\
    \ your code. Here are my suggestions for modifications:\n\n- \\*\\*Syntactic errors\\\
    *\\*:\n\n1. \\*\\*Function related errors\\*\\*: Your code has function-related\
    \ errors in the 'solve\\_test\\_case' function, where it incorrectly calculates\
    \ the total number of subsegments. This results in a wrong value being returned\
    \ when reset conditions are not handled correctly.\n\n2. \\*\\*Operation errors\\\
    *\\*: Your code uses the '+' operation in a context where the cumulative sum logic\
    \ does not account for invalid subsegments, leading to an overcount of subsegments\
    \ in certain cases.\n\n- \\*\\*Semantic errors\\*\\*:\n\n1. \\*\\*Control flow\
    \ errors\\*\\*: Your code has control flow errors in the while-loop condition\
    \ within 'solve\\_test\\_case', which fails to efficiently handle the reset condition.\
    \ As a result, the toxicity reset logic does not properly skip over invalid ranges.\n\
    \n2. \\*\\*Logical direction errors\\*\\*: Your code counts all subsegments ending\
    \ at 'right' even after a reset occurs, which is incorrect. Valid subsegments\
    \ should only include those where the cumulative sum is within the limit. Please\
    \ rewrite your code accordingly.\"\n\n#### Deepseek-Coder-33B:\n\n```\ndef solve(n,\
    \ x, a):\n    nextlex = [0] * n\n    i = 0\n    j = 1\n    while i < n - 1:\n\
    \        if j == n - 1:\n            nextlex[i] = j\n            i += 1\n    \
    \    elif j == i:\n            j += 1\n        else:\n            if a[j] <= x:\n\
    \                 nextlex[i] = j\n                 i += 1\n            else:\n\
    \                 j += 1\n    pointto = {}\n    count = [0] * n\n    i, j, sumi\
    \ = 0, 0, 0\n    while i < n:\n        if j == n:\n            count[i] = j -\
    \ i\n            sumi -= a[i]\n            i += 1\n        elif sumi + a[j] <=\
    \ x:\n            sumi += a[j]\n            j += 1\n        else:\n          \
    \  count[i] = j - i\n            if j < n - 1:\n                 pointto[i] =\
    \ nextlex[j]\n            sumi -= a[i]\n            i += 1\n    for i in range(n\
    \ - 1, -1, -1):\n        if i in pointto:\n            count[i] += count[pointto[i]]\n\
    \    return sum(count)\ndef main():\n    t = int(input())\n    results = []\n\
    \    for _ in range(t):\n        n, x = map(int, input().split())\n        a =\
    \ list(map(int, input().split()))\n        results.append(solve(n, x, a))\n  \
    \  print(\"\\n\".join(map(str, results)))\nif __name__ == \"__main__\":\n    main()\n\
    ```\nTeache Programmer: \"Satisfied\": \"You've fixed all the bugs in your code.\"\
    \n\n# <span id=\"page-30-1\"></span>E Error Analysis\n\n# E.1 Error Classification\n\
    \n<span id=\"page-30-0\"></span>\n\n| Error Category   | Error Type          \
    \    | Error Explanation                                                     \
    \                                                                            \
    \                        |  |  |  |  |\n|------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|--|--|\n\
    | Syntactic Errors | Function Related Errors | 1. Return Error: Returns a wrong\
    \ value in an unexpected format.<br>2. Function Call Error: Incorrect function\
    \ name, wrong argu<br>ments, or incorrect method call target. |  |  |  |  |\n\
    |                  | Operation Errors        | Operation is applied to an inappropriate\
    \ data type.                                                                 \
    \                                                      |  |  |  |  |\n|      \
    \            |                         | 1. Code Block Error: Incorrectly generated\
    \ or omitted state<br>ments, leading to task failure.                        \
    \                                                    |  |  |  |  |\n|        \
    \          | Structure Errors        | 2. Punctuation Error: Errors in punctuation,\
    \ such as missing<br>semicolons, commas, or brackets.                        \
    \                                                  |  |  |  |  |\n|          \
    \        | Declaration Errors      | Declaration Error: Incorrect or duplicate\
    \ declaration of variables<br>or functions.                                  \
    \                                                     |  |  |  |  |\n|       \
    \           | Import Errors           | Library/Module Import Error: Failure to\
    \ import external libraries<br>or nonexistent library/module.                \
    \                                                       |  |  |  |  |\n| Semantic\
    \ Errors  | Control Flow Errors     | 1. Condition Error: Logical error in a conditional\
    \ statement<br>causing unexpected execution. (if, else)                      \
    \                                            |  |  |  |  |\n|                \
    \  |                         | 2. Loop Error: Error leading to infinite loops\
    \ or incorrect loop<br>exits. (while, for)                                   \
    \                                                |  |  |  |  |\n|            \
    \      | Reference Errors        | 1. Wrong Function: Calling the wrong function.\
    \                                                                            \
    \                                                |  |  |  |  |\n|            \
    \      |                         | 2. Wrong Variable: Calling the wrong variable.\
    \                                                                            \
    \                                                |  |  |  |  |\n|            \
    \      | Calculation Errors      | 1. Incorrect Arithmetic Operation.        \
    \                                                                            \
    \                                                    |  |  |  |  |\n|        \
    \          |                         | 2. Incorrect Comparison Operation.    \
    \                                                                            \
    \                                                        |  |  |  |  |\n|    \
    \              | Incomplete Errors       | Missing essential logical steps: The\
    \ code generation is incom<br>plete.                                         \
    \                                                          |  |  |  |  |\n|  \
    \                | Logical Direction Error | The code significantly deviates from\
    \ intended logic and expected<br>outcomes.                                   \
    \                                                          |  |  |  |  |\n|  \
    \                | Suboptimal Errors       | Suboptimal solutions lead to exceeding\
    \ time or memory limits.                                                     \
    \                                                        |  |  |  |  |\n\nTable\
    \ 17: Taxonomy of syntactic/semantic characteristics of code errors made by LLMs\n\
    \n## <span id=\"page-31-0\"></span>E.2 Real Human Debug\n\nGuidelines for human\
    \ programmers to identify code errors and provide suggestions for problem resolution.\n\
    \nYou need to help identify code errors and provide suggestions based on the problem\
    \ statement {PROB-LEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output format\
    \ {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE}, the python code\
    \ {CODE} LLM programmer generated, the compiler output {COM-PILER\\_OUTPUT}, all\
    \ the test cases TEST\\_CASE}.\n\nYour feedback needs to focus on the following\
    \ content:\n\n\\*\\*Code Syntactic Correctness\\*\\*: You should concentrate on\
    \ errors such as those related to functions, operations, structure, declarations,\
    \ and external dependencies. Here are the detailed classification:\n\n1. Function\
    \ Related Errors: Return Error: Returns a wrong value in an unexpected format.\
    \ Function Call Error: Incorrect function name, wrong arguments, or incorrect\
    \ method call target.\n\n2. Operation Errors: Operation is applied to an inappropriate\
    \ data type.\n\n3. Structure Errors: Code Block Error: Incorrectly generated or\
    \ omitted statements, leading to task failure. Punctuation Error: Errors in punctuation,\
    \ such as missing semicolons, commas, or brackets.\n\n4. Declaration Errors: Declaration\
    \ Error: Incorrect or duplicate declaration of variables or functions.\n\n5. Import\
    \ Errors: Library/Module Import Error: Failure to import external libraries or\
    \ nonexistent library/module.\n\n\\*\\*Code Semantic Correctness\\*\\*: You should\
    \ concentrate on issues related to control flow, references, calculations, incompleteness,\
    \ logical direction and suboptimal errors.\n\n1. Control Flow Errors: Condition\
    \ Error: Logical error in a conditional statement causing unexpected execution.\
    \ Loop Error: Error leading to infinite loops or incorrect loop exits. (while,\
    \ for).\n\n2. Wrong Function: Calling the wrong function. Wrong Variable: Calling\
    \ the wrong variable.\n\n3. Calculation Errors: Incorrect Arithmetic Operation.\
    \ Incorrect Comparison Operation.\n\n4. Incomplete Errors: Missing essential logical\
    \ steps: The code generation is incomplete.\n\n5. Logical Direction Error: The\
    \ code significantly deviates from intended logic and expected outcomes.\n\n6.\
    \ Suboptimal Errors: Suboptimal solutions lead to exceeding time or memory limits.\n\
    \nPlease provide your feedback in the following format.\n\n\"Code Error Identifications:\"\
    \ \"Syntactic Errors:\" ... \"Semantic Errors:\" ...\n\n\"Resolution Suggestions:\"\
    \ ...\n\n## GPT-4-Turbo generated code:\n\n```\nMOD = 998244353\ndef solve(N,\
    \ M, conditions):\n    dp = [[0] * (N + 1) for _ in range(N + 1)]\n    dp[0][0]\
    \ = 1\n    for L, R, X in conditions:\n        for i in range(N, 0, -1):\n   \
    \         for j in range(N, -1, -1):\n                if j < L - 1 or j > R:\n\
    \                    dp[i][j] = (dp[i][j] + dp[i-1][j]) % MOD\n              \
    \  elif j < X - 1 or j > R:\n                    dp[i][j] = (dp[i][j] + dp[i-1][j-1])\
    \ % MOD\n                elif X - 1 <= j <= R:\n                    dp[i][j] =\
    \ (dp[i][j] + dp[i-1][R]) % MOD\n    return sum(dp[N]) % MOD\nN, M = map(int,\
    \ input().split())\nconditions = []\nfor _ in range(M):\n    L, R, X = map(int,\
    \ input().split())\n    conditions.append((L - 1, R - 1, X - 1))\nresult = solve(N,\
    \ M, conditions)\nprint(result)\n```\n### Automatic Debug:\n\n### Error Type:\
    \ Logical Direction Errors Resolution Suggestions:\n\nOff-by-One Error: The input\
    \ uses 1-based indexing, but the code uses 0-based indexing. The conditions (L,\
    \ R, X) should be adjusted properly.\n\nIncorrect Dynamic Programming Transitions:\
    \ The transitions in the dp table are not correctly defined based on the ranges\
    \ (L, R, X), leading to incorrect updates.\n\nInefficient Looping: The dp table\
    \ is updated from the wrong direction, which may overwrite needed values during\
    \ the same iteration.\n\nAdjust Indexing: Convert the input conditions (L, R,\
    \ X) to 0-based indexing by subtracting 1 when reading input. Correct Transitions\
    \ in dp Table: Update dp[i][j] based on the range conditions, correctly handling:\
    \ Values outside [L, R] inherit directly from dp[i-1][j]. Values within [L, X-1]\
    \ inherit from dp[i-1][j-1]. Values within [X, R] inherit from dp[i-1][R].\n\n\
    Use Backward Looping: Process the dp table backwards (from N to 1 and j from N\
    \ to 0) to avoid overwriting values that are still needed in the same iteration.\n\
    \n## GPT-4-Turbo Correction:\n\n```\nMOD = 998244353\ndef solve(N, M, conditions):\n\
    \    dp = [[0] * (N + 1) for _ in range(N + 1)]\n    dp[0][0] = 1 # Base case\n\
    \    for L, R, X in conditions:\n        for i in range(N, 0, -1):\n         \
    \   for j in range(N, -1, -1):\n                if j < L or j > R:\n         \
    \           dp[i][j] = (dp[i][j] + dp[i-1][j]) % MOD\n                elif L <=\
    \ j < X:\n                    dp[i][j] = (dp[i][j] + dp[i-1][j-1]) % MOD\n   \
    \             elif X <= j <= R:\n                    dp[i][j] = (dp[i][j] + dp[i-1][R])\
    \ % MOD\n    return sum(dp[N]) % MOD\n# Reading input\nN, M = map(int, input().split())\n\
    conditions = [tuple(map(lambda x: int(x)-1, input().split())) for _ in range(M)]\n\
    # Output the result\nprint(solve(N, M, conditions))\n```\nHuman Debug Semantic\
    \ Errors: Control Flow Errors:\n\nYour code still remain some control flow errors.\n\
    \nfor length from 1 to n: Traverse the subsequence lengths, from 1 to n.\n\nfor\
    \ i from 1 to n - length + 1: Traverse the starting positions i of the subsequences,\
    \ ensuring the subsequence length does not exceed n.\n\nj = i + length - 1: Calculate\
    \ the ending position j of the subsequence based on the starting position i and\
    \ the subsequence length.\n\nfor k from i to j: Traverse all possible split points\
    \ k, dividing the subsequence [i, j] into two subranges: [i, k-1] and [k+1, j].\n\
    \nif g[i][j][k] is true then continue: If there is a constraint at position k\
    \ within the range [i, j], skip that split point.\n\ndp[i][j] = dp[i][j] + dp[i][k\
    \ - 1] \\* dp[k + 1][j] \\* C[j - i][k - i]: Calculate the number of valid ways\
    \ for the current subsequence [i, j], using the valid counts of the left subrange\
    \ [i, k-1] and the right subrange [k+1, j], along with the combination count C[j\
    \ - i][k - i].\n\ndp[i][j] = dp[i][j] % MOD: Take the result modulo MOD to prevent\
    \ overflow.\n\n### GPT-4-Turbo Correction\n\n```\nMOD = 998244353\nN = 510\nn,\
    \ m = map(int, input().split())\ndp = [[0] * N for _ in range(N)]\nfor i in range(1,\
    \ n + 2):\n    dp[i][i - 1] = 1 # Initialize dp[i][i-1] as 1 (empty subsequence)\n\
    for length in range(1, n + 1):\n    for i in range(1, n - length + 2):\n     \
    \   j = i + length - 1\n        for k in range(i, j + 1):\n            if g[i][j][k]:\n\
    \                continue\n            dp[i][j] = (dp[i][j] + dp[i][k - 1] * dp[k\
    \ + 1][j] * C[j - i][k - i]) % MOD\nprint(dp[1][n])\n```\n## Human Debug\n\n##\
    \ Incomplete Errors\n\nYour code has some incomplete errors, you need to complete\
    \ the array g iteration.\n\nIterate over subinterval lengths: It loops through\
    \ all possible subinterval lengths, starting from 2 up to n. The length represents\
    \ the current length of the subinterval.\n\nIterate over starting positions: For\
    \ each subinterval length, it loops through all valid starting positions i such\
    \ that the subinterval does not exceed the array bounds. The loop runs from 1\
    \ to n - length + 1.\n\nCalculate ending position: The ending position j of the\
    \ current subinterval is calculated using the formula j = i + length - 1.\n\n\
    Iterate over possible split points: For each subinterval [i, j], it loops through\
    \ all possible positions k (from i to j) within the subinterval.\n\nUpdate constraint\
    \ information: For each position k, it updates the value of g[i][j][k]. This represents\
    \ whether position k in the subinterval [i, j] is affected by constraints. Specifically,\
    \ the value is updated by checking if the position k is constrained in either\
    \ the subinterval [i+1, j] or [i, j-1]. If either of these neighboring subintervals\
    \ has a constraint at position k, the current subinterval [i, j] will also have\
    \ a constraint at position k.\n\n## GPT-4-Turbo Correction\n\n```\nMOD = 998244353\n\
    N = 510\nn, m = map(int, input().split())\nC = [[0] * N for _ in range(N)]\nfor\
    \ i in range(N):\n    C[i][0] = C[i][i] = 1\n    for j in range(1, i):\n     \
    \   C[i][j] = (C[i - 1][j] + C[i - 1][j - 1]) % MOD\ng = [[[False] * N for _ in\
    \ range(N)] for _ in range(N)]\nfor _ in range(m):\n    l, r, x = map(int, input().split())\n\
    \    g[l][r][x] = True\nfor length in range(2, n + 1):\n    for i in range(1,\
    \ n - length + 2):\n        j = i + length - 1\n        for k in range(i, j +\
    \ 1):\n            g[i][j][k] = g[i][j][k] or g[i + 1][j][k] or g[i][j - 1][k]\n\
    dp = [[0] * N for _ in range(N)]\nfor i in range(1, n + 2):\n    dp[i][i - 1]\
    \ = 1\nfor length in range(1, n + 1):\n    for i in range(1, n - length + 2):\n\
    \        j = i + length - 1\n        for k in range(i, j + 1):\n            if\
    \ g[i][j][k]:\n                continue\n            dp[i][j] = (dp[i][j] + dp[i][k\
    \ - 1] * dp[k + 1][j] * C[j - i][k - i]) % MOD\nprint(dp[1][n])\n```\n## <span\
    \ id=\"page-35-0\"></span>E.3 Code Bug Annotation Example\n\n```\nAtCoder Regular\
    \ Contest 183 Problem C\nGPT-4-Turbo generated code:\nMOD = 998244353\ndef solve(N,\
    \ M, conditions):\n    dp = [[0] * (N + 1) for _ in range(N + 1)]\n    dp[0][0]\
    \ = 1\n    for L, R, X in conditions:\n        for i in range(N, 0, -1):\n   \
    \         for j in range(N, -1, -1):\n                if j < L - 1 or j > R:\n\
    \                    dp[i][j] = (dp[i][j] + dp[i-1][j]) % MOD\n              \
    \  elif j < X - 1 or j > R:\n                    dp[i][j] = (dp[i][j] + dp[i-1][j-1])\
    \ % MOD\n                elif X - 1 <= j <= R:\n                    dp[i][j] =\
    \ (dp[i][j] + dp[i-1][R]) % MOD\n    return sum(dp[N]) % MOD\nN, M = map(int,\
    \ input().split())\nconditions = []\nfor _ in range(M):\n    L, R, X = map(int,\
    \ input().split())\n    conditions.append((L - 1, R - 1, X - 1))\nresult = solve(N,\
    \ M, conditions)\nprint(result)\n```\n\n```\nHuman Programmer Annotation:\nSemantic\
    \ erros:\nControl Flow Errors and Incomplete Errors:\nThis code has control flow\
    \ errors and incomplete errors in the following section:\n[language=Python]\n\
    if j < L - 1 or j > R:\n    dp[i][j] = (dp[i][j] + dp[i-1][j]) % MOD\nelif j <\
    \ X - 1 or j > R:\n    dp[i][j] = (dp[i][j] + dp[i-1][j-1]) % MOD\nelif X - 1\
    \ <= j <= R:\n    dp[i][j] = (dp[i][j] + dp[i-1][R]) % MOD\nCorrection Suggestions:\n\
    [language=Python]\n# Loop through each possible subsequence length w\nfor w in\
    \ range(2, n + 1):\n    # Initialize an array y to keep track of constraints for\
    \ length w\n    y = [0] * n\n    # Preprocess constraints that end before the\
    \ current range\n    for r in range(w - 1):\n        for ll, x in pr[r]:\n   \
    \         # Increment the constraint count for y[x]\n            y[x] += 1\n \
    \   # Iterate over all valid starting indices l for a subsequence of length w\n\
    \    for l in range(n - w + 1):\n        r = l + w - 1 # Calculate the end index\
    \ of the current subsequence\n        # Consider constraints ending at r\n   \
    \     for ll, x in pr[r]:\n            if l <= ll:\n                y[x] += 1\n\
    \        # Calculate number of valid sequences for the current subsequence l to\
    \ r\n        for x in range(l, r + 1):\n            if y[x] == 0: # Check if there\
    \ are no active constraints at x\n                if x == l:\n               \
    \      q[l][r] += q[x + 1][r]\n                elif x == r:\n                \
    \     q[l][r] += q[l][x - 1]\n                else:\n                     # Calculate\
    \ the number of ways to split the sequence at x\n                   q[l][r] +=\
    \ (q[l][x - 1] * q[x + 1][r] * fa[r - l] * fb[x - l] * fb[r - x]) % M\n      \
    \          # Take modulo to prevent overflow\n                q[l][r] %= M\n \
    \       # Adjust constraints as we move the starting index l\n        for rr,\
    \ x in pl[l]:\n            if rr <= r:\n                y[x] -= 1\n```\n\n```\n\
    AtCoder Regular Contest 180 Problem D\n```\n\n```\nGPT-4-Turbo Generated Code:\n\
    ```\n\n```\n[language=Python]\ndef min_partition_sum(A, L, R):\n    if R - L +\
    \ 1 < 3:\n        return float('inf')\n    max_left = max(A[L:L + 2])\n    max_right\
    \ = max(A[R-2:R+1])\n    max_middle = float('-inf')\n    min_sum = float('inf')\n\
    \    for i in range(L + 2, R - 1):\n        max_middle = max(max_middle, A[i])\n\
    \        current_sum = max_left + max_middle + max_right\n        min_sum = min(min_sum,\
    \ current_sum)\n        if A[i - 1] == max_left:\n            max_left = max(A[L\
    \ + 2:i])\n        else:\n            max_left = max(max_left, A[L + 2:i - 1])\n\
    \    max_middle = max(max_middle, A[R - 1])\n    current_sum = max_left + max_middle\
    \ + max_right\n    min_sum = min(min_sum, current_sum)\n    max_middle = max(max(A[L\
    \ + 1:R - 1]), A[R - 1])\n    current_sum = max_left + max_middle + max(A[R -\
    \ 2:R])\n    min_sum = min(min_sum, current_sum)\n    return min_sum\ndef main():\n\
    \    N, Q = map(int, input().split())\n    A = list(map(int, input().split()))\n\
    \    results = []\n    for _ in range(Q):\n        L, R = map(int, input().split())\n\
    \        results.append(min_partition_sum(A, L - 1, R - 1))\n    for result in\
    \ results:\n        print(result)\nif __name__ == \"__main__\":\n    main()\n\
    ```\n### Human Annotation:\n\n# Syntactic Errors:\n\nOperation Errors: This code\
    \ has control flow errors in the following section: max\\_left = max(max\\_left,\
    \ A[L + 2:i - 1]), '>' not supported between instances of 'list' and 'int' .\n\
    \n### Correction Suggestions:\n\n[language=Python]\n\n```\n# Find the minimum\
    \ sum of max values by trying different partitions\n    for i in range(L + 2,\
    \ R): # Ensure we don't go out of bounds\n        max_middle = max(max_middle,\
    \ A[i])\n        current_sum = max_left + max_middle + max_right\n        min_sum\
    \ = min(min_sum, current_sum)\n        # Update max_left based on the current\
    \ position\n        if A[i - 1] == max_left:\n            # Only recalculate if\
    \ max_left needs updating\n            if i - 2 >= L + 2: # Ensure we don't go\
    \ out of bounds\n                max_left = max(A[L + 2:i - 1])\n        else:\n\
    \            max_left = max(max_left, A[L + 2:i - 1])\nEnsure max_left is Always\
    \ an Integer:\n```\nWhen updating the position of max\\_left, ensure that the\
    \ parameters used in the max function are valid numeric values and avoid passing\
    \ a list to the max function.\n\nBoundary Condition Checks:\n\nWhen calculating\
    \ max\\_left, add boundary condition checks to ensure that accessing the list\
    \ does not go out of bounds. Clarification of Logic:\n\nIn the logic for updating\
    \ max\\_left, clarify the conditions and the updating method to ensure that max\\\
    _left consistently receives the correct value.\n\n# <span id=\"page-39-0\"></span>F\
    \ Prompt\n\nSystem prompt for LLMs at comprehension stage\n\nYou are a competitive\
    \ programming programmer who gives the problem comprehension based on the problem\
    \ statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output format\
    \ {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE}.\n\nYour comprehension\
    \ needs to focus on the following content:\n\n1. \\*\\*Problem Requirements\\\
    *\\*: Problem Requirements refer to the necessary input and output formats that\
    \ must be followed, as well as the specific functionalities that the program is\
    \ required to implement.\n\n2. \\*\\*Problem Specifications\\*\\*: Problem Specifications\
    \ refer to the specific conditions and constraints that must be met for a solution\
    \ to a problem to be considered valid and effective.\n\nYour output must follow\
    \ the format below.\n\n\"Problem Requirements\": ...\n\n1. \"Input:\" ...\n\n\
    2. \"Output\" ...\n\n3. \"Functionality...\"\n\n\"Problem Specification\": ...\n\
    \n## System prompt for LLMs at planning stage\n\nYou are a competitive programming\
    \ programmer who chooses algorithm to solve this problem based on the problem\
    \ statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output format\
    \ {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE}, and problem requirements\
    \ {PROBLEM\\_REQUIREMENTS}, problem specification {PROBLEM\\_SPECIFICATION} you\
    \ generated in comprehension stage.\n\nThe algorithm you choose needs to focus\
    \ on the following content:\n\n1. \\*\\*Algorithm Correctness\\*\\*: Algorithm\
    \ you choose should be effective for this problem, your goal is to pass all the\
    \ test cases.\n\nYour output must follow the format below.\n\n\"Algorithm\": ...\
    \ \"Reasons for the algorithm choice\": ... \"Algorithm psedocode:\"...\n\n##\
    \ System prompt for LLMs at coding stage\n\nYou are a competitive programming\
    \ programmer who generates python code to solve this problem based on the problem\
    \ statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output format\
    \ {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE}, problem requirements\
    \ {PROBLEM\\_REQUIREMENTS}, problem specification {PROBLEM\\_SPECIFICATION} you\
    \ generated in comprehension stage, algorithm {ALGORITHM} you choose in the planning\
    \ stage.\n\nnote you must not return anything except for the python code.\n\n\
    ## System prompt for LLMs at debugging stage\n\nYou are a competitive programming\
    \ programmer who corrects python code errors based on the problem statement {PROBLEM\\\
    _STATEMENT}, input format {INPUT\\_FORMAT}, output format {OUTPUT\\_FORMAT}, test\
    \ case example {TEST\\_CASE\\_EXAMPLE}, problem requirements {PROBLEM\\_REQUIREMENTS},\
    \ problem specification {PROBLEM\\_SPECIFICATION} you generated in comprehension\
    \ stage, algorithm {ALGORITHM} you chose in the planning stage, python code {CODE}\
    \ you generated in coding stage and the compiler output {COMPILER\\_OUTPUT}.\n\
    \nYou need to follow these steps to debug.\n\n1. \\*\\*Bug Identification\\*\\\
    *: Bug identification involves locating and diagnosing errors within your Python\
    \ code. 2. \\*\\*Problem Resolution\\*\\*: Problem Resolution refers to making\
    \ modifications to the code based on the identified bugs.\n\nYour output must\
    \ follow the format below. You must not return anything except for the python\
    \ code for problem resolution.\n\n\"Bug Identification\": ... \"Problem Resolution\"\
    : ...\n\n## System prompt for student programmer gives feedback at comprehension\
    \ stage\n\nYou are a competitive programming programmer who gives feedback to\
    \ clarify the problem statement for the LLM programmer based on the problem statement\
    \ {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output format {OUTPUT\\\
    _FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE} , the problem requirements\
    \ {PROBLEM\\_REQUIREMENTS} and problem specifications {PROBLEM\\_SPECIFICATION}\
    \ the LLM programmer generated.\n\nYour feedback needs to focus on the following\
    \ content:\n\n1. \\*\\*Problem Requirements\\*\\*: Problem Requirements refer\
    \ to the necessary input and output formats that must be followed, as well as\
    \ the specific functionalities that the program is required to implement.\n\n\
    2. \\*\\*Problem Specifications\\*\\*: Problem Specifications refer to the specific\
    \ conditions and constraints that must be met for a solution to a problem to be\
    \ considered valid and effective.\n\nYour feedback must follow the one of the\
    \ format below.\n\n\"satisfied\": \"You have understood the problem statement,\
    \ including the requirements and specifications.\",\n\n\"unsatisfied\": \"You\
    \ have not understood the problem statement, missing the requirements and specifications.\
    \ Let me clarify the problem statement for you ..., please modify your answer\"\
    \n\n## System prompt for student programmer gives feedback at planning stage\n\
    \nYou are a competitive programming programmer who gives feedback to provide effective\
    \ algorithms for the LLM programmer based on the problem statement {PROBLEM\\\
    _STATEMENT}, input format {INPUT\\_FORMAT}, output format {OUTPUT\\_FORMAT}, test\
    \ case example {TEST\\_CASE\\_EXAMPLE}, the algorithm {ALGORITHM} LLM chose ,the\
    \ reason why LLM choose this algorithm {REASONS}, the algorithm pseducode {PSEDOCODE}\
    \ LLM programmer generated.\n\nYour feedback needs to focus on the following content:\n\
    \n1. \\*\\*Algorithm Correctness\\*\\*: Algorithm Correctness refers to whether\
    \ the LLM programmer has chosen an appropriate algorithm for the current problem,\
    \ including whether the rationale behind the LLM's choice of algorithm is reasonable.\
    \ 2. \\*\\*Pseudocode Correctness\\*\\*: Pseudocode Correctness refers to whether\
    \ the LLM programmer can generate accurate pseudocode code for the chosen algorithm,\
    \ testing whether the LLM truly understands the selected algorithm. Your feedback\
    \ must follow the one of the format below. Note: You can provide the relevant\
    \ algorithm and details to the LLM. You cannot directly provide pseudocode; you\
    \ can only point out the incorrect parts of the pseudocode. \"satisfied\": \"\
    You have chosen the correct algorithm and generated the corresponding accurate\
    \ pseudocode.\",\n\n\"unsatisfied\": \"I think you have choosen the wrong algorithm...\
    \ or the reason why you choose this algorithm is not correct... or the pseudocode\
    \ you generated has some errors..., please modify your answer\"\n\n## System prompt\
    \ for student programmer gives feedback at coding stage\n\nYou are a competitive\
    \ programming programmer who gives feedback to provide problem solutions for the\
    \ LLM programmer based on the problem statement {PROBLEM\\_STATEMENT}, input format\
    \ {INPUT\\_FORMAT}, output format {OUTPUT\\_FORMAT}, test case example {TEST\\\
    _CASE\\_EXAMPLE}, the python code {CODE} LLM programmer generated.\n\nYour feedback\
    \ needs to focus on the following content:\n\n1. \\*\\*Solution Correctness\\\
    *\\*: Evaluate the correctness of the code logic generated by the LLM programmer\
    \ to determine if it follows the correct approach.\n\nYour feedback must follow\
    \ the one of the format below. Note: You cannot directly provide or modify code;\
    \ you can only offer some guidance.\n\n\"satisfied\": \"You have chosen the correct\
    \ solution\",\n\n\"unsatisfied\": \"There are some errors in your solution. Here\
    \ are my suggestions for modifications... Please rewrite your code accordingly.\"\
    \n\n## System prompt for student programmer gives feedback at debug stage\n\n\
    You are a competitive programming programmer who gives feedback to help debug\
    \ the code errors based on the problem statement {PROBLEM\\_STATEMENT}, input\
    \ format {INPUT\\_FORMAT}, output format {OUTPUT\\_FORMAT}, test case example\
    \ {TEST\\_CASE\\_EXAMPLE}, the python code {CODE} LLM programmer generated, the\
    \ compiler output {COMPILER\\_OUTPUT}\n\nYour feedback needs to focus on the following\
    \ content:\n\n\\*\\*Code Syntactic Correctness\\*\\*: You should concentrate on\
    \ errors such as those related to functions, operations, structure, declarations,\
    \ and external dependencies. Here are the detailed classification:\n\n1. Function\
    \ Related Errors: Return Error: Returns a wrong value in an unexpected format.\
    \ Function Call Error: Incorrect function name, wrong arguments, or incorrect\
    \ method call target.\n\n2. Operation Errors: Operation is applied to an inappropriate\
    \ data type.\n\n3. Structure Errors: Code Block Error: Incorrectly generated or\
    \ omitted statements, leading to task failure. Punctuation Error: Errors in punctuation,\
    \ such as missing semicolons, commas, or brackets.\n\n4. Declaration Errors: Declaration\
    \ Error: Incorrect or duplicate declaration of variables or functions.\n\n5. Import\
    \ Errors: Library/Module Import Error: Failure to import external libraries or\
    \ nonexistent library/module.\n\n\\*\\*Code Semantic Correctness\\*\\*: You should\
    \ concentrate on issues related to control flow, references, calculations, incompleteness,\
    \ logical direction and suboptimal errors.\n\n1. Control Flow Errors: Condition\
    \ Error: Logical error in a conditional statement causing unexpected execution.\
    \ Loop Error: Error leading to infinite loops or incorrect loop exits. (while,\
    \ for).\n\n2. Wrong Function: Calling the wrong function. Wrong Variable: Calling\
    \ the wrong variable.\n\n3. Calculation Errors: Incorrect Arithmetic Operation.\
    \ Incorrect Comparison Operation.\n\n4. Incomplete Errors: Missing essential logical\
    \ steps: The code generation is incomplete.\n\n5. Logical Direction Error: The\
    \ code significantly deviates from intended logic and expected outcomes.\n\n6.\
    \ Suboptimal Errors: Suboptimal solutions lead to exceeding time or memory limits.\n\
    \nNote: You cannot directly provide or modify code; you can only offer some guidance.\
    \ You need to classify the errors you found into one of the categories mentioned\
    \ above. Your feedback must follow the one of the format below. \"satisfied\"\
    : \"You've fixed all the bugs in your code. \",\n\n\"unsatisfied\": \"There are\
    \ some syntactic(semantic) errors in your code. Here are my suggestions for modifications...\
    \ syntactic errors: 1. function related errors: your code has function related\
    \ errors in ..., which return a wrong value ... 2. operation errors: your code\
    \ has function related errors in ... ..., which \"+\" operation can not add string\
    \ type and int type. semantic errors: 1. control flow errors: your code has control\
    \ flow errors in..., which leads to incorrect if condition... Please rewrite your\
    \ code accordingly.\"\n\n## System prompt for teacher programmer gives feedback\
    \ at comprehension stage\n\nYou are a competitive programming programmer who gives\
    \ feedback to clarify the problem statement for the LLM programmer based on the\
    \ problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT}, output\
    \ format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE} , the problem\
    \ requirements {PROBLEM\\_REQUIREMENTS} and problem specifications {PROBLEM\\\
    _SPECIFICATION} the LLM programmer generated, the correct requirements and specifications\
    \ in ELABORATIONSETAnnotated\\_Statement.\n\nYour feedback needs to focus on the\
    \ following content:\n\n1. \\*\\*Problem Requirements\\*\\*: Problem Requirements\
    \ refer to the necessary input and output formats that must be followed, as well\
    \ as the specific functionalities that the program is required to implement.\n\
    \n2. \\*\\*Problem Specifications\\*\\*: Problem Specifications refer to the specific\
    \ conditions and constraints that must be met for a solution to a problem to be\
    \ considered valid and effective.\n\nYour feedback must follow the one of the\
    \ format below.\n\n\"satisfied\": \"You have understood the problem statement,\
    \ including the requirements and specifications.\",\n\n\"unsatisfied\": \"You\
    \ have not understood the problem statement, missing the requirements and specifications.\
    \ Let me clarify the problem statement for you ...\"\n\n## System prompt for teacher\
    \ programmer gives feedback at planning stage\n\nYou are a competitive programming\
    \ programmer who gives feedback to provide effective algorithms for the LLM programmer\
    \ based on the problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\\
    _FORMAT}, output format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE},\
    \ the algorithm {ALGORITHM} LLM chose ,the reason why LLM choose this algorithm\
    \ {REASONS}, the algorithm pseducode {PSEDOCODE} LLM programmer generated, the\
    \ correct algorithm {ANNOTATED\\_ALGORITHM} and psedocode {ANNO-TATED\\_PSEDOCODE}\
    \ in ELABORATIONSET.\n\nYour feedback needs to focus on the following content:\n\
    \n1. \\*\\*Algorithm Correctness\\*\\*: Algorithm Correctness refers to whether\
    \ the LLM programmer has chosen an appropriate algorithm for the current problem,\
    \ including whether the rationale behind the LLM's choice of algorithm is reasonable.\
    \ 2. \\*\\*Pseudocode Correctness\\*\\*: Pseudocode Correctness refers to whether\
    \ the LLM programmer can generate accurate pseudocode code for the chosen algorithm,\
    \ testing whether the LLM truly understands the selected algorithm. Your feedback\
    \ must follow the one of the format below. Note: You can provide the relevant\
    \ algorithm and details to the LLM. You cannot directly provide pseudocode; you\
    \ can only point out the incorrect parts of the pseudocode. \"satisfied\": \"\
    You have chosen the correct algorithm and generated the corresponding accurate\
    \ pseudocode.\",\n\n\"unsatisfied\": \"I think you have choosen the wrong algorithm...\
    \ or the reason why you choose this algorithm is not correct... or the pseudocode\
    \ you generated has some errors...\"\n\n## System prompt for teacher programmer\
    \ gives feedback at coding stage\n\nYou are a competitive programming programmer\
    \ who gives feedback to provide problem solutions for the LLM programmer based\
    \ on the problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT},\
    \ output format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE},\
    \ the python code {CODE} LLM programmer generated, the ground\\_truth solutions\
    \ {GROUND\\_TRUTH\\_SOLUTION} in ELABORATIONSET.\n\nYour feedback needs to focus\
    \ on the following content:\n\n1. \\*\\*Solution Correctness\\*\\*: Evaluate the\
    \ correctness of the code logic generated by the LLM programmer to determine if\
    \ it follows the correct approach.\n\nYour feedback must follow the one of the\
    \ format below. Note: You cannot directly provide or modify code; you can only\
    \ offer some guidance.\n\n\"satisfied\": \"You have chosen the correct solution\"\
    ,\n\n\"unsatisfied\": \"There are some errors in your solution. Here are my suggestions\
    \ for modifications... Please rewrite your code accordingly.\"\n\n## System prompt\
    \ for teacher programmer gives feedback at debugging stage\n\nYou are a competitive\
    \ programming programmer who gives feedback to help debug the code errors based\
    \ on the problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT},\
    \ output format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE},\
    \ the python code {CODE} LLM programmer generated, the compiler output {COMPILER\\\
    _OUTPUT}, all the test cases {TEST\\_CASE}.\n\nYour feedback needs to focus on\
    \ the following content:\n\n\\*\\*Code Syntactic Correctness\\*\\*: You should\
    \ concentrate on errors such as those related to functions, operations, structure,\
    \ declarations, and external dependencies. Here are the detailed classification:\n\
    \n1. Function Related Errors: Return Error: Returns a wrong value in an unexpected\
    \ format. Function Call Error: Incorrect function name, wrong arguments, or incorrect\
    \ method call target.\n\n2. Operation Errors: Operation is applied to an inappropriate\
    \ data type.\n\n3. Structure Errors: Code Block Error: Incorrectly generated or\
    \ omitted statements, leading to task failure. Punctuation Error: Errors in punctuation,\
    \ such as missing semicolons, commas, or brackets.\n\n4. Declaration Errors: Declaration\
    \ Error: Incorrect or duplicate declaration of variables or functions.\n\n5. Import\
    \ Errors: Library/Module Import Error: Failure to import external libraries or\
    \ nonexistent library/module.\n\n\\*\\*Code Semantic Correctness\\*\\*: You should\
    \ concentrate on issues related to control flow, references, calculations, incompleteness,\
    \ logical direction and suboptimal errors.\n\n1. Control Flow Errors: Condition\
    \ Error: Logical error in a conditional statement causing unexpected execution.\
    \ Loop Error: Error leading to infinite loops or incorrect loop exits. (while,\
    \ for).\n\n2. Wrong Function: Calling the wrong function. Wrong Variable: Calling\
    \ the wrong variable.\n\n3. Calculation Errors: Incorrect Arithmetic Operation.\
    \ Incorrect Comparison Operation.\n\n4. Incomplete Errors: Missing essential logical\
    \ steps: The code generation is incomplete.\n\n5. Logical Direction Error: The\
    \ code significantly deviates from intended logic and expected outcomes.\n\n6.\
    \ Suboptimal Errors: Suboptimal solutions lead to exceeding time or memory limits.\n\
    \nNote: You cannot directly provide or modify code; you can only offer some guidance.\
    \ You need to classify the errors you found into one of the categories mentioned\
    \ above. Your feedback must follow the one of the format below. \"satisfied\"\
    : \"You've fixed all the bugs in your code. \",\n\n\"unsatisfied\": \"There are\
    \ some syntactic(semantic) errors in your code. Here are my suggestions for modifications...\
    \ syntactic errors: 1. function related errors: your code has function related\
    \ errors in ..., which return a wrong value ... 2. operation errors: your code\
    \ has function related errors in ... ..., which \"+\" operation can not add string\
    \ type and int type. semantic errors: 1. control flow errors: your code has control\
    \ flow errors in..., which leads to incorrect if condition... Please rewrite your\
    \ code accordingly.\"\n\n## Instructions for human programmer gives feedback to\
    \ help debug\n\nYou need to give feedback to help debug the code errors based\
    \ on the problem statement {PROBLEM\\_STATEMENT}, input format {INPUT\\_FORMAT},\
    \ output format {OUTPUT\\_FORMAT}, test case example {TEST\\_CASE\\_EXAMPLE},\
    \ the python code {CODE} LLM programmer generated, the compiler output {COMPILER\\\
    _OUTPUT}, all the test cases {TEST\\_CASE}. In addition to these, you can also\
    \ access the content of ELABORATIONSET and conduct web searches.\n\nYour feedback\
    \ needs to focus on the following content:\n\n\\*\\*Code Syntactic Correctness\\\
    *\\*: You should concentrate on errors such as those related to functions, operations,\
    \ structure, declarations, and external dependencies. Here are the detailed classification:\n\
    \n1. Function Related Errors: Return Error: Returns a wrong value in an unexpected\
    \ format. Function Call Error: Incorrect function name, wrong arguments, or incorrect\
    \ method call target.\n\n2. Operation Errors: Operation is applied to an inappropriate\
    \ data type.\n\n3. Structure Errors: Code Block Error: Incorrectly generated or\
    \ omitted statements, leading to task failure. Punctuation Error: Errors in punctuation,\
    \ such as missing semicolons, commas, or brackets.\n\n4. Declaration Errors: Declaration\
    \ Error: Incorrect or duplicate declaration of variables or functions.\n\n5. Import\
    \ Errors: Library/Module Import Error: Failure to import external libraries or\
    \ nonexistent library/module.\n\n\\*\\*Code Semantic Correctness\\*\\*: You should\
    \ concentrate on issues related to control flow, references, calculations, incompleteness,\
    \ logical direction and suboptimal errors.\n\n1. Control Flow Errors: Condition\
    \ Error: Logical error in a conditional statement causing unexpected execution.\
    \ Loop Error: Error leading to infinite loops or incorrect loop exits. (while,\
    \ for).\n\n2. Wrong Function: Calling the wrong function. Wrong Variable: Calling\
    \ the wrong variable.\n\n3. Calculation Errors: Incorrect Arithmetic Operation.\
    \ Incorrect Comparison Operation.\n\n4. Incomplete Errors: Missing essential logical\
    \ steps: The code generation is incomplete.\n\n5. Logical Direction Error: The\
    \ code significantly deviates from intended logic and expected outcomes.\n\n6.\
    \ Suboptimal Errors: Suboptimal solutions lead to exceeding time or memory limits.\n\
    \nNote: You cannot directly provide or modify code; you can only offer some guidance.\
    \ You need to classify the errors you found into one of the categories mentioned\
    \ above. Your feedback must follow the one of the format below. \"satisfied\"\
    : \"You've fixed all the bugs in your code. \",\n\n\"unsatisfied\": \"There are\
    \ some syntactic(semantic) errors in your code. Here are my suggestions for modifications...\
    \ syntactic errors: 1. function related errors: your code has function related\
    \ errors in ..., which return a wrong value ... 2. operation errors: your code\
    \ has function related errors in ... ..., which \"+\" operation can not add string\
    \ type and int type. semantic errors: 1. control flow errors: your code has control\
    \ flow errors in..., which leads to incorrect if condition... Please rewrite your\
    \ code accordingly.\""
