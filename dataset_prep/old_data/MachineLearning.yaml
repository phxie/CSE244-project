papers:
- abstract: Adversarial attacks in Natural Language Processing apply perturbations
    in the character or token levels. Token-level attacks, gaining prominence for
    their use of gradient-based methods, are susceptible to altering sentence semantics,
    leading to invalid adversarial examples. While character-level attacks easily
    maintain semantics, they have received less attention as they cannot easily adopt
    popular gradient-based methods, and are thought to be easy to defend. Challenging
    these beliefs, we introduce Charmer, an efficient query-based adversarial attack
    capable of achieving high attack success rate (ASR) while generating highly similar
    adversarial examples. Our method successfully targets both small (BERT) and large
    (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in
    4.84% points and the USE similarity in 8% points with respect to the previous
    art. Our implementation is available in github.com/LIONS-EPFL/Charmer.
  keywords: Computing methodologies, Artificial intelligence, Natural language processing,
    Information extraction, Natural language generation, Machine learning, Learning
    paradigms, Reinforcement learning, Adversarial learning, Machine learning approaches,
    Neural networks, Information systems, Information retrieval, Retrieval models
    and ranking, Learning to rank, Similarity measures
  references: "Alzantot, M., Sharma, Y., Elgohary, A., Ho, B.-J., Srivastava, M.,\
    \ and Chang, K.-W. Generating natural language adversarial examples. In Conference\
    \ on Empirical Methods in Natural Language Processing (EMNLP), 2018. Belinkov,\
    \ Y. and Bisk, Y. Synthetic and natural noise both break neural machine translation.\
    \ In International Conference on Learning Representations, 2018. Bengio, Y., Ducharme,\
    \ R., and Vincent, P. A neural probabilistic language model. Advances in neural\
    \ information processing systems (NeurIPS), 2000. Bojanowski, P., Grave, E., Joulin,\
    \ A., and Mikolov, T. Enriching word vectors with subword information. Transactions\
    \ of the Association for Computational Linguistics, 2017. Brown, T., Mann, B.,\
    \ Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\
    \ P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan,\
    \ T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen,\
    \ M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,\
    \ S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot\
    \ learners. In Advances in neural information processing systems (NeurIPS), 2020.\
    \ Carlini, N. and Wagner, D. Towards evaluating the robustness of neural networks.\
    \ In 2017 IEEE symposium on security and privacy (sp), 2017. Carlini, N., Nasr,\
    \ M., Choquette-Choo, C. A., Jagielski, M., Gao, I., Koh, P. W., Ippolito, D.,\
    \ Tram\xE8r, F., and Schmidt, L. Are aligned neural networks adversarially aligned?\
    \ In Advances in neural information processing systems (NeurIPS), 2023. Cer, D.,\
    \ Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N., St. John, R., Constant, N., Guajardo-Cespedes,\
    \ M., Yuan, S., Tar, C., Strope, B., and Kurzweil, R. Universal sentence encoder\
    \ for English. In Proceedings of the 2018 Conference on Empirical Methods in Natural\
    \ Language Processing: System Demonstrations, 2018. Chen, M., Tworek, J., Jun,\
    \ H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y.,\
    \ Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,\
    \ H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power,\
    \ A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings,\
    \ D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol,\
    \ A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders,\
    \ W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E.,\
    \ Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P.,\
    \ McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating\
    \ large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\
    \ Cheng, M., Yi, J., Chen, P.-Y., Zhang, H., and Hsieh, C.-J. Seq2sick: Evaluating\
    \ the robustness of sequence-to-sequence models with adversarial examples. AAAI\
    \ Conference on Artificial Intelligence, 34, 2020. Chiang, W.-L., Li, Z., Lin,\
    \ Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez,\
    \ J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt\
    \ quality. See https://vicuna.lmsys.org (accessed 14 April 2023), 2023. Dagan,\
    \ I., Glickman, O., and Magnini, B. The pascal recognising textual entailment\
    \ challenge. In Qui\xF1onero-Candela, J., Dagan, I., Magnini, B., and d'Alch\xE9\
    \ Buc, F. (eds.), Machine Learning Challenges. Evaluating Predictive Uncertainty,\
    \ Visual Object Classification, and Recognising Tectual Entailment. Springer Berlin\
    \ Heidelberg, 2006. Davis, M. Psycholinguistic evidence on scrambled letters in\
    \ reading, 2003. URL https://www.mrc-cbu.cam.ac.uk/people/matt.davis/cmabridge/.\
    \ Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita-Rotaru,\
    \ C., and Roli, F. Why do adversarial attacks transfer? explaining transferability\
    \ of evasion and poisoning attacks. In 28th USENIX security symposium (USENIX\
    \ security 19), pp. 321-338, 2019. Devlin, J., Chang, M.-W., Lee, K., and Toutanova,\
    \ K. BERT: Pre-training of deep bidirectional transformers for language understanding.\
    \ In Proceedings of the 2019 Conference of the North American Chapter of the Association\
    \ for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\
    \ Short Papers), 2019. Dyrmishi, S., Ghamizi, S., and Cordy, M. How do humans\
    \ perceive adversarial text? a reality check on the validity and naturalness of\
    \ word-based adversarial attacks. In Proceedings of the 61st Annual Meeting of\
    \ the Association for Computational Linguistics (Volume 1: Long Papers), 2023.\
    \ Ebrahimi, J., Rao, A., Lowd, D., and Dou, D. HotFlip: White-box adversarial\
    \ examples for text classification. In Proceedings of the 56th Annual Meeting\
    \ of the Association for Computational Linguistics (Volume 2: Short Papers), 2018.\
    \ Gao, J., Lanchantin, J., Soffa, M. L., and Qi, Y. Blackbox generation of adversarial\
    \ text sequences to evade deep learning classifiers. In IEEE Security and Privacy\
    \ Workshops (SPW), 2018. Garg, S. and Ramakrishnan, G. BAE: BERT-based adversarial\
    \ examples for text classification. In Proceedings of the 2020 Conference on Empirical\
    \ Methods in Natural Language Processing (EMNLP), 2020. Goodfellow, I. J., Shlens,\
    \ J., and Szegedy, C. Explaining and harnessing adversarial examples. In Bengio,\
    \ Y. and LeCun, Y. (eds.), International Conference on Learning Representations\
    \ (ICLR), 2015. Gulli, A. Ag's corpus of news articles, 2005. URL http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html.\
    \ Guo, C., Sablayrolles, A., Jegou, H., and Kiela, D. Gradient-based adversarial\
    \ attacks against text transformers. In Proceedings of the 2021 Conference on\
    \ Empirical Methods in Natural Language Processing, 2021. Held, M., Wolfe, P.,\
    \ and Crowder, H. P. Validation of subgradient optimization. Mathematical programming,\
    \ 6: 62-88, 1974. Hou, B., Jia, J., Zhang, Y., Zhang, G., Zhang, Y., Liu, S.,\
    \ and Chang, S. Textgrad: Advancing robustness evaluation in NLP by gradient-driven\
    \ optimization. In International Conference on Learning Representations (ICLR),\
    \ 2023. Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. Is bert really robust?\
    \ a strong baseline for natural language attack on text classification and entailment.\
    \ AAAI Conference on Artificial Intelligence, 2020. Jones, E., Jia, R., Raghunathan,\
    \ A., and Liang, P. Robust encodings: A framework for combating adversarial typos.\
    \ In Proceedings of the 58th Annual Meeting of the Association for Computational\
    \ Linguistics, 2020. Kudo, T. and Richardson, J. SentencePiece: A simple and language\
    \ independent subword tokenizer and detokenizer for neural text processing. In\
    \ Proceedings of the 2018 Conference on Empirical Methods in Natural Language\
    \ Processing: System Demonstrations. Association for Computational Linguistics,\
    \ 2018. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.\
    \ Albert: A lite bert for self-supervised learning of language representations.\
    \ In International Conference on Learning Representations, 2020. Lee, D., Moon,\
    \ S., Lee, J., and Song, H. O. Query-efficient and scalable black-box adversarial\
    \ attacks on discrete sequential data via bayesian optimization. In International\
    \ Conference on Machine Learning, pp. 12478-12497. PMLR, 2022. Lei, Q., Wu, L.,\
    \ Chen, P.-Y., Dimakis, A., Dhillon, I. S., and Witbrock, M. J. Discrete adversarial\
    \ attacks and submodular optimization with applications to text classification.\
    \ Proceedings of Machine Learning and Systems, 1:146-165, 2019. Levenshtein, V.\
    \ I. et al. Binary codes capable of correcting deletions, insertions, and reversals.\
    \ In Soviet physics doklady, volume 10, pp. 707-710. Soviet Union, 1966. Li, J.,\
    \ Ji, S., Du, T., Li, B., and Wang, T. Textbugger: Generating adversarial text\
    \ against real-world applications. Network and Distributed Systems Security (NDSS)\
    \ Symposium, 2019. Li, L., Ma, R., Guo, Q., Xue, X., and Qiu, X. BERT-ATTACK:\
    \ Adversarial attack against BERT using BERT. In Proceedings of the 2020 Conference\
    \ on Empirical Methods in Natural Language Processing (EMNLP), 2020. Liu, A.,\
    \ Yu, H., Hu, X., Li, S., Lin, L., Ma, F., Yang, Y., and Wen, L. Character-level\
    \ white-box adversarial attacks against transformers via attachable subwords substitution.\
    \ In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\
    \ Processing, 2022. Liu, X., Xu, N., Chen, M., and Xiao, C. Autodan: Generating\
    \ stealthy jailbreak prompts on aligned large language models. arXiv preprint\
    \ arXiv:2310.04451, 2023. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\
    \ D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly\
    \ optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\
    \ Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep\
    \ learning models resistant to adversarial attacks. In International Conference\
    \ on Learning Representations (ICLR), 2018. Mihov, S. and Schulz, K. U. Fast approximate\
    \ search in large dictionaries. Computational Linguistics, 30(4):451-477, 2004.\
    \ URL https://aclanthology.org/J04-4003. Mikolov, T., Sutskever, I., Chen, K.,\
    \ Corrado, G. S., and Dean, J. Distributed representations of words and phrases\
    \ and their compositionality. In Advances in neural information processing systems\
    \ (NeurIPS), 2013. Mitankin, P. N. Universal levenshtein automata. building and\
    \ properties. Sofia University St. Kliment Ohridski, 2005. Morris, J., Lifland,\
    \ E., Lanchantin, J., Ji, Y., and Qi, Y. Reevaluating adversarial examples in\
    \ natural language. In Findings of the Association for Computational Linguistics:\
    \ EMNLP 2020, 2020a. Morris, J., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D.,\
    \ and Qi, Y. Textattack: A framework for adversarial attacks, data augmentation,\
    \ and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical\
    \ Methods in Natural Language Processing: System Demonstrations, 2020b. Nasr,\
    \ M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo,\
    \ C. A., Wallace, E., Tramer, F., and Lee, K. Scalable extraction of training\
    \ data from (production) language models, 2023. OpenAI. Gpt-4 technical report.\
    \ arXiv preprint arXiv:2303.08774, 2023. Palmer, D. D. Tokenisation and sentence\
    \ segmentation. Handbook of natural language processing, 2000. Pennington, J.,\
    \ Socher, R., and Manning, C. D. Glove: Global vectors for word representation.\
    \ In Proceedings of the 2014 conference on empirical methods in natural language\
    \ processing (EMNLP), pp. 1532-1543, 2014. Peters, M. E., Neumann, M., Iyyer,\
    \ M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized\
    \ word representations. In Proceedings of the 2018 Conference of the North American\
    \ Chapter of the Association for Computational Linguistics: Human Language Technologies,\
    \ Volume 1 (Long Papers), 2018. Pruthi, D., Dhingra, B., and Lipton, Z. C. Combating\
    \ adversarial misspellings with robust word recognition. In Proceedings of the\
    \ 57th Annual Meeting of the Association for Computational Linguistics, 2019.\
    \ Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.\
    \ Language models are unsupervised multitask learners. OpenAI blog, 2019. Rajpurkar,\
    \ P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine\
    \ comprehension of text. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings\
    \ of the 2016 Conference on Empirical Methods in Natural Language Processing,\
    \ pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics.\
    \ URL https://aclanthology.org/D16-1264. Rawlinson, G. The Significance of Letter\
    \ Position in Word Recognition. Phd thesis, Nottingham University, 1976. Ren,\
    \ S., Deng, Y., He, K., and Che, W. Generating natural language adversarial examples\
    \ through probability weighted word saliency. In Proceedings of the 57th Annual\
    \ Meeting of the Association for Computational Linguistics, 2019. Sadrizadeh,\
    \ S., Aghdam, A. D., Dolamic, L., and Frossard, P. Targeted adversarial attacks\
    \ against neural machine translation. In ICASSP 2023-2023 IEEE International Conference\
    \ on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023a. Sadrizadeh,\
    \ S., Dolamic, L., and Frossard, P. Transfool: An adversarial attack against neural\
    \ machine translation models. Transactions on Machine Learning Research, 2023b.\
    \ ISSN 2835-8856. URL https://openreview.net/forum?id=sFk3aBNb81. Sennrich, R.,\
    \ Haddow, B., and Birch, A. Neural machine translation of rare words with subword\
    \ units. arXiv preprint arXiv:1508.07909, 2015. Socher, R., Perelygin, A., Wu,\
    \ J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models\
    \ for semantic compositionality over a sentiment treebank. In Proceedings of the\
    \ 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),\
    \ 2013. Song, X., Salcianu, A., Song, Y., Dopson, D., and Zhou, D. Fast WordPiece\
    \ tokenization. In Proceedings of the 2021 Conference on Empirical Methods in\
    \ Natural Language Processing. Association for Computational Linguistics, 2021.\
    \ Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with\
    \ neural networks. Advances in neural information processing systems (NeurIPS),\
    \ 27, 2014. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow,\
    \ I., and Fergus, R. Intriguing properties of neural networks. In International\
    \ Conference on Learning Representations (ICLR), 2014. Touvron, H., Martin, L.,\
    \ Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\
    \ Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat\
    \ models. arXiv preprint arXiv:2307.09288, 2023. Touzet, H. On the levenshtein\
    \ automaton and the size of the neighbourhood of a word. In Language and Automata\
    \ Theory and Applications, pp. 207-218. Springer, 2016. Wallace, E., Stern, M.,\
    \ and Song, D. Imitation attacks and defenses for black-box machine translation\
    \ systems. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of\
    \ the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\
    \ Online, 2020. Association for Computational Linguistics. Wang, A., Singh, A.,\
    \ Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark\
    \ and analysis platform for natural language understanding. In International Conference\
    \ on Learning Representations (ICLR), 2019. Webster, J. J. and Kit, C. Tokenization\
    \ as the initial phase in NLP. In COLING 1992 Volume 4: The 14th International\
    \ Conference on Computational Linguistics, 1992. Williams, A., Nangia, N., and\
    \ Bowman, S. A broad-coverage challenge corpus for sentence understanding through\
    \ inference. In Proceedings of the 2018 Conference of the North American Chapter\
    \ of the Association for Computational Linguistics: Human Language Technologies,\
    \ Volume 1 (Long Papers). Association for Computational Linguistics, 2018. Yang,\
    \ P., Chen, J., Hsieh, C.-J., Wang, J.-L., and Jordan, M. I. Greedy attack and\
    \ gumbel attack: Generating adversarial examples for discrete data. Journal of\
    \ Machine Learning Research, 21(43):1-36, 2020. URL http://jmlr.org/papers/v21/19-569.html.\
    \ Zhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., and Jordan, M. Theoretically\
    \ principled trade-off between robustness and accuracy. In International Conference\
    \ on Machine Learning (ICML), 2019. Zhang, X., Zhao, J., and LeCun, Y. Character-level\
    \ convolutional networks for text classification. In Cortes, C., Lawrence, N.,\
    \ Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information\
    \ Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.\
    \ Zhu, S., Zhang, R., An, B., Wu, G., Barrow, J., Wang, Z., Huang, F., Nenkova,\
    \ A., and Sun, T. Autodan: Automatic and interpretable adversarial attacks on\
    \ large language models. arXiv preprint arXiv:2310.15140, 2023. Zou, A., Wang,\
    \ Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial\
    \ attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
  title: Revisiting character-level adversarial attacks for language models
  url: https://dl.acm.org/doi/10.5555/3692070.3692071
- abstract: This paper proposes a payoff perturbation technique for the Mirror Descent
    (MD) algorithm in games where the gradient of the payoff functions is monotone
    in the strategy profile space, potentially containing additive noise. The optimistic
    family of learning algorithms, exemplified by optimistic MD, successfully achieves
    last-iterate convergence in scenarios devoid of noise, leading the dynamics to
    a Nash equilibrium. A recent re-emerging trend underscores the promise of the
    perturbation approach, where payoff functions are perturbed based on the distance
    from an anchoring, or slingshot, strategy. In response, we propose Adaptively
    Perturbed MD (APMD), which adjusts the magnitude of the perturbation by repeatedly
    updating the slingshot strategy at a predefined interval. This innovation empowers
    us to find a Nash equilibrium of the underlying game with guaranteed rates. Empirical
    demonstrations affirm that our algorithm exhibits significantly accelerated convergence.
  keywords: Computing methodologies, Machine learning, Mathematics of computing, Mathematical
    analysis, Mathematical optimization, Theory of computation, Design and analysis
    of algorithms, Algorithm design techniques, Mathematical optimization, Theory
    and algorithms for application domains, Algorithmic game theory and mechanism
    design
  references: "Abe, K., Sakamoto, M., and Iwasaki, A. Mutation-driven follow the regularized\
    \ leader for last-iterate convergence in zero-sum games. In UAI, pp. 1-10, 2022.\
    \ Abe, K., Ariu, K., Sakamoto, M., Toyoshima, K., and Iwasaki, A. Last-iterate\
    \ convergence with full and noisy feedback in two-player zero-sum games. In AISTATS,\
    \ pp. 7999-8028, 2023. Anagnostides, I. and Panageas, I. Frequency-domain representation\
    \ of first-order methods: A simple and robust framework of analysis. In SOSA,\
    \ pp. 131-160, 2022. Azizian, W., Iutzeler, F., Malick, J., and Mertikopoulos,\
    \ P. The last-iterate convergence rate of optimistic mirror descent in stochastic\
    \ variational inequalities. In COLT, pp. 326-358, 2021. Bailey, J. P. and Piliouras,\
    \ G. Multiplicative weights update in zero-sum games. In Economics and Computation,\
    \ pp. 321-338, 2018. Beck, A. and Teboulle, M. Mirror descent and nonlinear projected\
    \ subgradient methods for convex optimization. Operations Research Letters, 31(3):167-175,\
    \ 2003. Bloembergen, D., Tuyls, K., Hennes, D., and Kaisers, M. Evolutionary dynamics\
    \ of multi-agent learning: A survey. Journal of Artificial Intelligence Research,\
    \ 53:659-697, 2015. Bravo, M., Leslie, D., and Mertikopoulos, P. Bandit learning\
    \ in concave N-person games. In NeurIPS, pp. 5666-5676, 2018. Cai, Y. and Daskalakis,\
    \ C. On minmax theorems for multiplayer games. In SODA, pp. 217-234, 2011. Cai,\
    \ Y. and Zheng, W. Doubly optimal no-regret learning in monotone games. arXiv\
    \ preprint arXiv:2301.13120, 2023. Cai, Y., Candogan, O., Daskalakis, C., and\
    \ Papadimitriou, C. Zero-sum polymatrix games: A generalization of minmax. Mathematics\
    \ of Operations Research, 41(2): 648-655, 2016. Cai, Y., Oikonomou, A., and Zheng,\
    \ W. Finite-time last-iterate convergence for learning in multi-player games.\
    \ In NeurIPS, pp. 33904-33919, 2022a. Cai, Y., Oikonomou, A., and Zheng, W. Tight\
    \ last-iterate convergence of the extragradient method for constrained monotone\
    \ variational inequalities. arXiv preprint arXiv:2204.09228, 2022b. Cai, Y., Luo,\
    \ H., Wei, C.-Y., and Zheng, W. Uncoupled and convergent learning in two-player\
    \ zero-sum Markov games. arXiv preprint arXiv:2303.02738, 2023. Cen, S., Wei,\
    \ Y., and Chi, Y. Fast policy extragradient methods for competitive games with\
    \ entropy regularization. In NeurIPS, pp. 27952-27964, 2021. Cen, S., Chi, Y.,\
    \ Du, S. S., and Xiao, L. Faster last-iterate convergence of policy optimization\
    \ in zero-sum Markov games. In ICLR, 2023. Cevher, V., Piliouras, G., Sim, R.,\
    \ and Skoulakis, S. Min-max optimization made simple: Approximating the proximal\
    \ point method via contraction maps. In Symposium on Simplicity in Algorithms\
    \ (SOSA), pp. 192-206, 2023. Cohen, J., H\xE9liou, A., and Mertikopoulos, P. Learning\
    \ with bandit feedback in potential games. In NeurIPS, pp. 6372-6381, 2017. Coucheney,\
    \ P., Gaujal, B., and Mertikopoulos, P. Penalty-regulated dynamics and robust\
    \ learning procedures in games. Mathematics of Operations Research, 40(3):611-633,\
    \ 2015. Daskalakis, C. and Panageas, I. The limit points of (optimistic) gradient\
    \ descent in min-max optimization. In NeurIPS, pp. 9256-9266, 2018. Daskalakis,\
    \ C. and Panageas, I. Last-iterate convergence: Zero-sum games and constrained\
    \ min-max optimization. In ITCS, pp. 27:1-27:18, 2019. Daskalakis, C., Ilyas,\
    \ A., Syrgkanis, V., and Zeng, H. Training GANs with optimism. In ICLR, 2018.\
    \ de Montbrun, \xC9. and Renault, J. Convergence of optimistic gradient descent\
    \ ascent in bilinear games. arXiv preprint arXiv:2208.03085, 2022. Debreu, G.\
    \ A social equilibrium existence theorem. Proceedings of the National Academy\
    \ of Sciences, 38(10): 886-893, 1952. Drusvyatskiy, D., Fazel, M., and Ratliff,\
    \ L. J. Improved rates for derivative free gradient play in strongly monotone\
    \ games. In CDC, pp. 3403-3408. IEEE, 2022. Facchinei, F. and Pang, J.-S. Finite-dimensional\
    \ variational inequalities and complementarity problems. Springer, 2003. Giannou,\
    \ A., Vlatakis-Gkaragkounis, E. V., and Mertikopoulos, P. Survival of the strictest:\
    \ Stable and unstable equilibria under regularized learning with partial information.\
    \ In COLT, pp. 2147-2148, 2021a. Giannou, A., Vlatakis-Gkaragkounis, E.-V., and\
    \ Mertikopoulos, P. On the rate of convergence of regularized learning in games:\
    \ From bandits and uncertainty to optimism and beyond. In NeurIPS, pp. 22655-22666,\
    \ 2021b. Golowich, N., Pattathil, S., and Daskalakis, C. Tight last-iterate convergence\
    \ rates for no-regret learning in multiplayer games. In NeurIPS, pp. 20766-20778,\
    \ 2020a. Golowich, N., Pattathil, S., Daskalakis, C., and Ozdaglar, A. Last iterate\
    \ is slower than averaged iterate in smooth convex-concave saddle point problems.\
    \ In COLT, pp. 1758-1784, 2020b. Gorbunov, E., Taylor, A., and Gidel, G. Last-iterate\
    \ convergence of optimistic gradient method for monotone variational inequalities.\
    \ In NeurIPS, pp. 21858-21870, 2022. Halpern, B. Fixed points of nonexpanding\
    \ maps. Bulletin of the American Mathematical Society, 73(6):957 - 961, 1967.\
    \ Hart, S. and Mas-Colell, A. A simple adaptive procedure leading to correlated\
    \ equilibrium. Econometrica, 68(5): 1127-1150, 2000. Hsieh, Y.-G., Iutzeler, F.,\
    \ Malick, J., and Mertikopoulos, P. On the convergence of single-call stochastic\
    \ extra-gradient methods. In NeurIPS, pp. 6938-6948, 2019. Hsieh, Y.-G., Iutzeler,\
    \ F., Malick, J., and Mertikopoulos, P. Explore aggressively, update conservatively:\
    \ Stochastic extragradient methods with variable stepsize scaling. Advances in\
    \ Neural Information Processing Systems, 33: 16223-16234, 2020. Hsieh, Y.-G.,\
    \ Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games:\
    \ Optimal regret bounds and convergence to Nash equilibrium. In COLT, pp. 2388-2422,\
    \ 2021. Hsieh, Y.-G., Antonakopoulos, K., Cevher, V., and Mertikopoulos, P. No-regret\
    \ learning in games with noisy feedback: Faster rates and adaptivity via learning\
    \ rate separation. In NeurIPS, pp. 6544-6556, 2022. Hussain, A. A., Belardinelli,\
    \ F., and Piliouras, G. Asymptotic convergence and performance of multi-agent\
    \ Q-learning dynamics. arXiv preprint arXiv:2301.09619, 2023. Kannan, A. and Shanbhag,\
    \ U. V. Optimal stochastic extragradient schemes for pseudomonotone stochastic\
    \ variational inequality problems and their variants. Computational Optimization\
    \ and Applications, 74(3):779-820, 2019. Koshal, J., Nedi\u0107, A., and Shanbhag,\
    \ U. V. Single timescale regularized stochastic approximation schemes for monotone\
    \ nash games under uncertainty. In CDC, pp. 231-236. IEEE, 2010. Koshal, J., Nedic,\
    \ A., and Shanbhag, U. V. Regularized iterative stochastic approximation methods\
    \ for stochastic variational inequality problems. IEEE Transactions on Automatic\
    \ Control, 58(3):594-609, 2013. Lattimore, T. and Szepesv\xE1ri, C. Bandit algorithms.\
    \ Cambridge University Press, 2020. Lee, S. and Kim, D. Fast extra gradient methods\
    \ for smooth structured nonconvex-nonconcave minimax problems. In NeurIPS, pp.\
    \ 22588-22600, 2021. Lei, Q., Nagarajan, S. G., Panageas, I., et al. Last iterate\
    \ convergence in no-regret learning: constrained min-max optimization for convex-concave\
    \ landscapes. In AISTATS, pp. 1441-1449, 2021. Leslie, D. S. and Collins, E. J.\
    \ Individual q-learning in normal form games. SIAM Journal on Control and Optimization,\
    \ 44(2):495-514, 2005. Liang, T. and Stokes, J. Interaction matters: A note on\
    \ nonasymptotic local convergence of generative adversarial networks. In AISTATS,\
    \ pp. 907-915, 2019. Lin, T., Zhou, Z., Mertikopoulos, P., and Jordan, M. Finite-time\
    \ last-iterate convergence for multi-agent learning in games. In ICML, pp. 6161-6171,\
    \ 2020. Liu, M., Ozdaglar, A., Yu, T., and Zhang, K. The power of regularization\
    \ in solving extensive-form games. In ICLR, 2023. McKelvey, R. D. and Palfrey,\
    \ T. R. Quantal response equilibria for normal form games. Games and economic\
    \ behavior, 10(1):6-38, 1995. McKelvey, R. D. and Palfrey, T. R. Quantal response\
    \ equilibria for extensive form games. Experimental economics, 1:9-41, 1998. Mertikopoulos,\
    \ P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff\
    \ functions. Mathematical Programming, 173(1):465-507, 2019. Mertikopoulos, P.,\
    \ Papadimitriou, C., and Piliouras, G. Cycles in adversarial regularized learning.\
    \ In SODA, pp. 2703-2717, 2018. Mertikopoulos, P., Lecouat, B., Zenati, H., Foo,\
    \ C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point\
    \ problems: Going the extra (gradient) mile. In ICLR, 2019. Mertikopoulos, P.,\
    \ Hsieh, Y.-P., and Cevher, V. Learning in games from a stochastic approximation\
    \ viewpoint. arXiv preprint arXiv:2206.03922, 2022. Nash, J. Non-cooperative games.\
    \ Annals of mathematics, pp. 286-295, 1951. Nedi\u0107, A. and Lee, S. On stochastic\
    \ subgradient mirror-descent algorithm with weighted averaging. SIAM Journal on\
    \ Optimization, 24(1):84-107, 2014. Nemirovski, A., Juditsky, A., Lan, G., and\
    \ Shapiro, A. Robust stochastic approximation approach to stochastic programming.\
    \ SIAM Journal on Optimization, 19(4):1574-1609, 2009. Nemirovskij, A. S. and\
    \ Yudin, D. B. Problem complexity and method efficiency in optimization. Wiley,\
    \ 1983. Pattathil, S., Zhang, K., and Ozdaglar, A. Symmetric (optimistic) natural\
    \ policy gradient for multi-agent learning with parameter convergence. In AISTATS,\
    \ pp. 5641-5685, 2023. Perolat, J., Munos, R., Lespiau, J.-B., Omidshafiei, S.,\
    \ Rowland, M., Ortega, P., Burch, N., Anthony, T., Balduzzi, D., De Vylder, B.,\
    \ et al. From Poincar\xE9 recurrence to convergence in imperfect information games:\
    \ Finding equilibrium via regularization. In ICML, pp. 8525-8535, 2021. Rakhlin,\
    \ A. and Sridharan, K. Online learning with predictable sequences. In COLT, pp.\
    \ 993-1019, 2013a. Rakhlin, S. and Sridharan, K. Optimization, learning, and games\
    \ with predictable sequences. In NeurIPS, pp. 3066-3074, 2013b. Rockafellar, R.\
    \ T. Convex analysis, volume 11. Princeton university press, 1997. Shalev-Shwartz,\
    \ S. and Singer, Y. Convex repeated games and fenchel duality. Advances in neural\
    \ information processing systems, 19, 2006. Sokota, S., D'Orazio, R., Kolter,\
    \ J. Z., Loizou, N., Lanctot, M., Mitliagkas, I., Brown, N., and Kroer, C. A unified\
    \ approach to reinforcement learning, quantal response equilibria, and two-player\
    \ zero-sum games. In ICLR, 2023. Tammelin, O. Solving large imperfect information\
    \ games using CFR+. arXiv preprint arXiv:1407.5042, 2014. Tatarenko, T. and Kamgarpour,\
    \ M. Learning Nash equilibria in monotone games. In CDC, pp. 3104-3109. IEEE,\
    \ 2019. Tuyls, K., Hoen, P. J., and Vanschoenwinkel, B. An evolutionary dynamical\
    \ analysis of multi-agent learning in iterated games. Autonomous Agents and Multi-Agent\
    \ Systems, 12(1):115-153, 2006. Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H.\
    \ Linear last-iterate convergence in Constrained saddle-point-optimaization. In\
    \ ICLR,2021. Yoon, T. and Ryu, E. K. Accelerated algorithms for smooth convex-concave\
    \ minimax problems with O(1/k2) rate on squared gradient norm. In ICML, pp. 12098-12109,\
    \ 2021. Yousefian, F., Nedi\u0107, A., and Shanbhag, U. V. On smoothing, regularization,\
    \ and averaging in stochastic approximation methods for stochastic variational\
    \ inequality problems. Mathematical Programming, 165:391-431, 2017. Zhou, Z.,\
    \ Mertikopoulos, P., Moustakas, A. L., Bambos, N., and Glynn, P. Mirror descent\
    \ learning in continuous games. In CDC, pp. 5776-5783. IEEE, 2017."
  title: Adaptively perturbed mirror descent for learning in games
  url: https://dl.acm.org/doi/10.5555/3692070.3692072
- abstract: "Large language models are increasingly integrated with external environments,\
    \ tools, and agents like ChatGPT plugins to extend their capability beyond language-centric\
    \ tasks. However, today's LLM inference systems are designed for standalone LLMs.\
    \ They treat each external interaction as the end of LLM generation and form a\
    \ new request when the interaction finishes, causing unnecessary recomputation\
    \ of already computed contexts, which accounts for 37-40% of total model forwarding\
    \ time. This paper presents INFERCEPT, the first LLM inference framework targeting\
    \ augmented LLMs and supporting the efficient interception of LLM generation.\
    \ INFERCEPT minimizes the GPU resource waste caused by LLM interceptions and dedicates\
    \ saved memory for serving more requests. INFERCEPT improves the overall serving\
    \ throughput by 1.6\xD7-2\xD7 and completes 2\xD7 more requests per second compared\
    \ to the state-of-the-art LLM inference systems."
  keywords: Computing methodologies, Artificial intelligence, Knowledge representation
    and reasoning, Logic programming and answer set programming, Software and its
    engineering, Software notations and tools, General programming languages, Language
    features, Frameworks, Language types, Theory of computation, Theory and algorithms
    for application domains, Machine learning theory, Inductive inference
  references: "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,\
    \ F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4\
    \ technical report. arXiv preprint arXiv:2303.08774, 2023. Agrawal, A., Panwar,\
    \ A., Mohan, J., Kwatra, N., Gulavani, B. S., and Ramjee, R. Sarathi: Efficient\
    \ llm inference by piggybacking decodes with chunked prefills. arXiv preprint\
    \ arXiv:2308.16369, August 2023. AI, S. Bark: Text-to-speech model. https://github.com/suno-ai/bark,\
    \ 2023. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F.,\
    \ and Sanghai, S. GQA: Training generalized multi-query transformer models from\
    \ multi-head check-points. In Proceedings of the 2023 Conference on Empirical\
    \ Methods in Natural Language Processing (EMNLP 2023), Singapore, December 2023.\
    \ Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase,\
    \ O., Smith, S., Zhang, M., Rasley, J., et al. Deepspeed-inference: enabling efficient\
    \ inference of transformer models at unprecedented scale. In SC22: International\
    \ Conference for High Performance Computing, Networking, Storage and Analysis,\
    \ Dallas, Texas, November 2022. IEEE. Baeza-Yates, R., Ribeiro-Neto, B., et al.\
    \ Modern Information Retrieval, volume 463. ACM Press, New York, 1999. Betker,\
    \ J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J.,\
    \ Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., and Ramesh,\
    \ A. Improving image generation with better captions. https://cdn.openai.com/papers/dall-e-3.pdf,\
    \ 2024. Brockman, G., Eleti, A., Georges, E., Jang, J., Kilpatrick, L., Lim, R.,\
    \ Miller, L., and Pokrass, M. Introducing chatgpt and whisper apis. https://openai.com/blog/introducing-chatgpt-and-whisper-apis,\
    \ March 1 2023. Brysbaert, M. How many words do we read per minute? a review and\
    \ meta-analysis of reading rate. Journal of memory and language, 109:104047, 2019.\
    \ Chase, H. LangChain. https://github.com/langchain-ai/langchain, October 2022.\
    \ Chen, L., Chen, Z., Tan, B., Long, S., Gasic, M., and Yu, K. Agentgraph: Towards\
    \ universal dialogue management with structured deep reinforcement learning. arXiv\
    \ preprint arXiv:1905.11259, May 2019. Costa-juss\xE0, M. R., Cross, J., \xC7\
    \ elebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J.,\
    \ Licht, D., Maillard, J., Sun, A., Wang, S., Wenzek, G., Young-blood, A., Akula,\
    \ B., Barrault, L., Gonzalez, G. M., Hansanti, P., Hoffman, J., Jarrett, S., Sadagopan,\
    \ K. R., Rowe, D., Spruit, S., Tran, C., Andrews, P., Ayan, N. F., Bhosale, S.,\
    \ Edunov, S., Fan, A., Gao, C., Goswami, V., Guzm\xE1n, F., Koehn, P., Mourachko,\
    \ A., Ropers, C., Saleem, S., Schwenk, H., and Wang, J. No language left behind:\
    \ Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672,\
    \ July 2022. Greyling, C. When using the chatgpt api, users will have to manage\
    \ the context. https://cobusgreyling.medium.com/when-using-the-chatgpt-api-users-will-have-to-manage-the-context-ba5869238913,\
    \ March 6 2023. Gu, J., Stefani, E., Wu, Q., Thomason, J., and Wang, X. Vision-and-language\
    \ navigation: A survey of tasks, methods, and future directions. In Proceedings\
    \ of the 60th Annual Meeting of the Association for Computational Linguistics\
    \ (Volume 1: Long Papers), Dublin, Ireland, 2022. Association for Computational\
    \ Linguistics. Hao, S., Liu, T., Wang, Z., and Hu, Z. Toolkengpt: Augmenting frozen\
    \ language models with massive tools via tool embeddings. In Advances in Neural\
    \ Information Processing Systems 36, New Orleans, Louisiana, December 2023. Hu,\
    \ C., Huang, H., Xu, L., Chen, X., Xu, J., Chen, S., Feng, H., Wang, C., Wang,\
    \ S., Bao, Y., Sun, N., and Shan, Y. Inference without interference: Disaggregate\
    \ llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181,\
    \ January 2024. Hu, Y., Lin, F., Zhang, T., Yi, L., and Gao, Y. Look before you\
    \ leap: Unveiling the power of gpt- 4v in robotic vision-language planning. arXiv\
    \ preprint arXiv:2311.17842, November 2023. Huang, W., Abbeel, P., Pathak, D.,\
    \ and Mordatch, I. Language models as zero-shot planners: Extracting actionable\
    \ knowledge for embodied agents. In Proceedings of 39th International Conference\
    \ on Machine Learning, Honolulu, Hawai'i, 2022. Izacard, G., Lewis, P., Lomeli,\
    \ M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel,\
    \ S., and Grave, E. Atlas: Few-shot learning with retrieval augmented language\
    \ models, 2022. Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam,\
    \ K., A, S. V., Haq, S., Sharma, A., Joshi, T. T., Moazam, H., Miller, H., Zaharia,\
    \ M., and Potts, C. DSPy: Compiling declarative language model calls into state-of-the-art\
    \ pipelines. In The Twelfth International Conference on Learning Representations\
    \ (ICLR '24), Vienna, Austria, 2024. URL https://openreview.net/forum?id=sY5N0zY5Od.\
    \ Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J.,\
    \ Zhang, H., and Stoica, I. Efficient memory management for large language model\
    \ serving with pagedattention. In Proceedings of the 29th Symposium on Operating\
    \ Systems Principles, Koblenz, Germany, October 2023. Li, Z., Zheng, L., Zhong,\
    \ Y., Liu, V., Sheng, Y., Jin, X., Huang, Y., Chen, Z., Zhang, H., Gonzalez, J.\
    \ E., and Stoica, I. AlpaServe: Statistical multiplexing with model parallelism\
    \ for deep learning serving. In 17th USENIX Symposium on Operating Systems Design\
    \ and Implementation (OSDI 23), Boston, MA, July 2023. Lu, P., Peng, B., Cheng,\
    \ H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C., and Gao, J. Chameleon:\
    \ Plug-and-play compositional reasoning with large language models. In Proceedings\
    \ of the 37th International Conference on Neural Information Processing Systems\
    \ (NeurIPS '23), New Orleans, Louisiana, December 2023. Ma, Z., Edge, D., Findlater,\
    \ L., and Tan, H. Z. Haptic keyclick feedback improves typing speed and reduces\
    \ typing errors on a flat keyboard. In 2015 IEEE World Haptics Conference (WHC),\
    \ Evanston, Illinois, 2015. IEEE. Meta. Meta llama 3. https://llama.meta.com/llama3/,\
    \ 2024. Mialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu,\
    \ R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun,\
    \ Y., and Scialom, T. Augmented language models: a survey. Transactions on Machine\
    \ Learning Research (TMLR), 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=jh7wH2AzKK.\
    \ Survey Certification. Ng, A. The batch weekly issues 241. https://www.deeplearning.ai/the-batch/issue-241/,\
    \ March 2024. NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer,\
    \ 2023. OpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins, March\
    \ 2023. OpenTable. New: Chatgpt restaurant recs, powered by opentable. https://www.opentable.com/blog/chatgpt/,\
    \ March 23 2023. Parisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented language\
    \ models. arXiv preprint arXiv:2205.12255, May 2022. Patel, P., Choukse, E., Zhang,\
    \ C., Shah, A., Goiri, \xCD., Maleki, S., and Bianchini, R. Splitwise: Efficient\
    \ generative llm inference using phase splitting. In The 53th International Symposium\
    \ on Computer Architecture (ISCA 2024), Buenos Aires, Argentina, June 2024. Patil,\
    \ S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model\
    \ connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. Qian, C.,\
    \ Han, C., Fung, Y., Qin, Y., Liu, Z., and Ji, H. CREATOR: Tool creation for disentangling\
    \ abstract and concrete reasoning of large language models. In Bouamor, H., Pino,\
    \ J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics:\
    \ (EMNLP '23), Singapore, December 2023. URL https://aclanthology.org/2023.findings-emnlp.462.\
    \ Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y.,\
    \ Xiao, C., Han, C., Fung, Y. R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K.,\
    \ Liang, S., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu,\
    \ Y., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X.,\
    \ Sun, X., Li, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., and Sun, M. Tool\
    \ learning with foundation models. arXiv preprint arXiv:2304.08354, June 2023.\
    \ Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution\
    \ image synthesis with latent diffusion models. arXiv preprint arXiv:2112.10752,\
    \ 2021. Schick, T., Dwivedi-Yu, J., Dess\xEC, R., Raileanu, R., Lomeli, M., Hambro,\
    \ E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models\
    \ can teach themselves to use tools. 37th Conference on Neural Information Processing\
    \ Systems, 2023. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. HuggingGPT:\
    \ Solving AI tasks with chatGPT and its friends in hugging face. In Proceedings\
    \ of the 37th International Conference on Neural Information Processing Systems\
    \ (NeurIPS '23), New Orleans, Louisiana, December 2023. Shoeybi, M., Patwary,\
    \ M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training\
    \ multibillion parameter language models using model parallelism. arXiv preprint\
    \ arXiv:1909.08053, 2019. Shridhar, M., Yuan, X., C\xF4t\xE9, M.-A., Bisk, Y.,\
    \ Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments\
    \ for interactive learning. In Proceedings of the International Conference on\
    \ Learning Representations (ICLR), Virtual, 2021. Srivatsa, V., He, Z., Abhyankar,\
    \ R., Li, D., and Zhang, Y. Preble: Efficient distributed prompt scheduling for\
    \ llm serving. UCSD CSE Technical Reports, May 2024. URL https://escholarship.org/uc/item/1bm0k1w0.\
    \ Suris, D., Menon, S., and Vondrick, C. Vipergpt: Visual inference via python\
    \ execution for reasoning. In 2023 IEEE/CVF International Conference on Computer\
    \ Vision (ICCV '23), pp. 11854-11864, Los Alamitos, CA, USA, October 2023. Touvron,\
    \ H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\xE8\
    re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation\
    \ language models. arXiv preprint arXiv:2302.13971, 2023. Vaidya, N., Comly, N.,\
    \ DeLaere, J., Patel, A., and Oh, F. Nvidia tensorrt-llm supercharges large language\
    \ model inference on nvidia h100 gpus. https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/,\
    \ 2023. Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion Parameter Autoregressive\
    \ Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\
    \ Wang, L., Ma, C., Feng, X., Zhang, Z., ran Yang, H., Zhang, J., Chen, Z.-Y.,\
    \ Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., and rong Wen, J. A survey\
    \ on large language model based autonomous agents. arXiv preprint arXiv:2308.11432,\
    \ August 2023. URL https://api.semanticscholar.org/CorpusID:261064713. Wei, J.,\
    \ Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q.\
    \ V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language\
    \ models. In Proceedings of the 36th International Conference on Neural Information\
    \ Processing Systems, New Orleans, Louisiana, 2023. Wolfram, S. Chatgpt gets its\
    \ 'wolfram superpowers'! https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/,\
    \ March 2023. Wu, B., Zhong, Y., Zhang, Z., Huang, G., Liu, X., and Jin, X. Fast\
    \ distributed inference serving for large language models. arXiv preprint arXiv:2305.05920,\
    \ May 2023. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,\
    \ R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop\
    \ question answering. arXiv preprint arXiv:1809.09600, 2018. Yao, S., Zhao, J.,\
    \ Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing\
    \ reasoning and acting in language models. In The Eleventh International Conference\
    \ on Learning Representations, Kigali, Rwanda, May 2023. Yu, G.-I., Jeong, J.\
    \ S., Kim, G.-W., Kim, S., and Chun, B.- G. Orca: A Distributed Serving System\
    \ for Transformer-Based Generative Models. In 16th USENIX Symposium on Operating\
    \ Systems Design and Implementation (OSDI '22), Carlsbad, CA, July 2022. Zhang,\
    \ Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in\
    \ large language models. In The Eleventh International Conference on Learning\
    \ Representations, Kigali, Rwanda, May 2023. Zheng, L., Chiang, W.-L., Sheng,\
    \ Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang,\
    \ H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and\
    \ chatbot arena. In Thirtyseventh Conference on Neural Information Processing\
    \ Systems Datasets and Benchmarks Track, New Orleans, Louisiana, December 2023a.\
    \ Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H., Cao, S., Kozyrakis,\
    \ C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Efficiently programming\
    \ large language models using sglang. arXiv preprint arXiv:2312.07104, December\
    \ 2023b. Zhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin, X., and\
    \ Zhang, H. Distllm: Disaggregating prefill and decoding for goodput-optimized\
    \ large language model serving. In Proceedings of the 18th USENIX Symposium on\
    \ Operating Systems Design and Implementation (OSDI '24), Santa Clara, CA, July\
    \ 2024."
  title: 'INFERCEPT: efficient intercept support for augmented large language model
    inference'
  url: https://dl.acm.org/doi/10.5555/3692070.3692073
- abstract: Subset or core-set selection offers a data-efficient way for training
    deep learning models. One-shot subset selection poses additional challenges as
    subset selection is only performed once and full set data become unavailable after
    the selection. However, most existing methods tend to choose either diverse or
    difficult data samples, which fail to faithfully represent the joint data distribution
    that is comprised of both feature and label information. The selection is also
    performed independently from the subset size, which plays an essential role in
    choosing what types of samples. To address this critical gap, we propose to conduct
    Feature similarity and Label variability Balanced One-shot Subset Selection (BOSS),
    aiming to construct an optimal size-aware subset for data-efficient deep learning.
    We show that a novel balanced core-set loss bound theoretically justifies the
    need to simultaneously consider both diversity and difficulty to form an optimal
    subset. It also reveals how the subset size influences the bound. We further connect
    the inaccessible bound to a practical surrogate target which is tailored to subset
    sizes and varying levels of overall difficulty. We design a novel Beta-scoring
    importance function to delicately control the optimal balance of diversity and
    difficulty. Comprehensive experiments conducted on both synthetic and real data
    justify the important theoretical properties and demonstrate the superior performance
    of BOSS as compared with the competitive baselines.
  keywords: Computing methodologies, Machine learning, Learning paradigms, Supervised
    learning, Supervised learning by regression, Machine learning algorithms, Regularization,
    Machine learning approaches, Theory of computation, Theory and algorithms for
    application domains, Machine learning theory, Active learning, Bayesian analysis
  references: "Agarwal, S., Arora, H., Anand, S., and Arora, C. Contextual diversity\
    \ for active learning. In Computer Vision-ECCV 2020: 16th European Conference,\
    \ Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16, pp. 137-153. Springer,\
    \ 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\
    \ P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models\
    \ are few-shot learners. Advances in neural information processing systems, 33:\
    \ 1877-1901, 2020. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton,\
    \ G. E. Big self-supervised models are strong semi-supervised learners. Advances\
    \ in neural information processing systems, 33:22243-22255, 2020. Dosovitskiy,\
    \ A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\
    \ M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An\
    \ image is worth 16\xD716 words: Transformers for image recognition at scale.\
    \ In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\
    \ Farahani, R. Z. and Hekmatfar, M. Facility location: concepts, models, algorithms\
    \ and case studies. Springer Science & Business Media, 2009. Feldman, D. Introduction\
    \ to core-sets: an updated survey. arXiv preprint arXiv:2011.09384, 2020. Feldman,\
    \ V. and Zhang, C. What neural networks memorize and why: Discovering the long\
    \ tail via influence estimation. Advances in Neural Information Processing Systems,\
    \ 33:2881-2891, 2020. Guo, C., Zhao, B., and Bai, Y. Deepcore: A comprehensive\
    \ library for coreset selection in deep learning. In Database and Expert Systems\
    \ Applications: 33rd International Conference, DEXA 2022, Vienna, Austria, August\
    \ 22-24, 2022, Proceedings, Part I, pp. 181-195. Springer, 2022. Har-Peled, S.\
    \ and Kushal, A. Smaller coresets for k-median and k-means clustering. In Proceedings\
    \ of the twentyfirst annual symposium on Computational geometry, pp. 126-134,\
    \ 2005. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image\
    \ recognition. In Proceedings of the IEEE conference on computer vision and pattern\
    \ recognition, pp. 770-778, 2016. He, Y., Xiao, L., and Zhou, J. T. You only condense\
    \ once: Two rules for pruning condensed datasets. Advances in Neural Information\
    \ Processing Systems, 36, 2023. Kaushal, V., Ramakrishnan, G., and Iyer, R. Submodlib:\
    \ A submodular optimization library. arXiv preprint arXiv:2202.10680, 2022. Killamsetty,\
    \ K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. Grad-match: Gradient matching\
    \ based data subset selection for efficient deep model training. In International\
    \ Conference on Machine Learning, pp. 5464-5474. PMLR, 2021a. Killamsetty, K.,\
    \ Sivasubramanian, D., Ramakrishnan, G., and Iyer, R. Glister: Generalization\
    \ based data subset selection for efficient and robust learning. In Proceedings\
    \ of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8110-8118,\
    \ 2021b. Killamsetty, K., Zhao, X., Chen, F., and Iyer, R. Retrieve: Coreset selection\
    \ for efficient and robust semi-supervised learning. Advances in Neural Information\
    \ Processing Systems, 34:14488-14501, 2021c. Liu, Y., Ott, M., Goyal, N., Du,\
    \ J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\
    \ V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,\
    \ 2019. Madigan, D., Raghavan, N., DuMouchel, W., Nason, M., Posse, C., and Ridgeway,\
    \ G. Likelihood-based data squashing: A modeling approach to instance construction.\
    \ Data Mining and Knowledge Discovery, 6:173-190, 2002. Mirzasoleiman, B., Bilmes,\
    \ J., and Leskovec, J. Coresets for data-efficient training of machine learning\
    \ models. In International Conference on Machine Learning, pp. 6950-6960. PMLR,\
    \ 2020. Nguyen, C. V., Li, Y., Bui, T. D., and Turner, R. E. Variational continual\
    \ learning. In International Conference on Learning Representations, 2018. URL\
    \ https://openreview.net/forum?id=BkQqq0gRb. Paul, M., Ganguli, S., and Dziugaite,\
    \ G. K. Deep learning on a data diet: Finding important examples early in training.\
    \ Advances in Neural Information Processing Systems, 34: 20596-20607, 2021. Pearlmutter,\
    \ B. A. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,\
    \ 1994. Pleiss, G., Zhang, T., Elenberg, E., and Weinberger, K. Q. Identifying\
    \ mislabeled data using the area under the margin ranking. Advances in Neural\
    \ Information Processing Systems, 33:17044-17056, 2020. Pooladzandi, O., Davini,\
    \ D., and Mirzasoleiman, B. Adaptive second order coresets for data-efficient\
    \ machine learning. In International Conference on Machine Learning, pp. 17848-17869.\
    \ PMLR, 2022. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A.,\
    \ Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In International\
    \ Conference on Machine Learning, pp. 8821-8831. PMLR, 2021. Schwartz, R., Dodge,\
    \ J., Smith, N. A., and Etzioni, O. Green ai. Communications of the ACM, 63(12):54-63,\
    \ 2020. Sener, O. and Savarese, S. Active learning for convolutional neural networks:\
    \ A core-set approach. In International Conference on Learning Representations,\
    \ 2018. URL https://openreview.net/forum?id=H1aIuk-RW. Shin, S., Bae, H., Shin,\
    \ D., Joo, W., and Moon, I.-C. Loss-curvature matching for dataset selection and\
    \ condensation. In International Conference on Artificial Intelligence and Statistics,\
    \ pp. 8606-8628. PMLR, 2023. Sorscher, B., Geirhos, R., Shekhar, S., Ganguli,\
    \ S., and Morcos, A. Beyond neural scaling laws: beating power law scaling via\
    \ data pruning. Advances in Neural Information Processing Systems, 35:19523-19536,\
    \ 2022. Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations\
    \ for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. Tan, M. and\
    \ Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks.\
    \ In International conference on machine learning, pp. 6105-6114. PMLR, 2019.\
    \ Toneva, M., Sordoni, A., des Combes, R. T., Trischler, A., Bengio, Y., and Gordon,\
    \ G. J. An empirical study of example forgetting during deep neural network learning.\
    \ In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=BJlxm30cKm.\
    \ Tsang, I. W., Kwok, J. T., Cheung, P.-M., and Cristianini, N. Core vector machines:\
    \ Fast svm training on very large data sets. Journal of Machine Learning Research,\
    \ 6(4), 2005. Wan, Z., Wang, Z., Chung, C., and Wang, Z. A survey of data optimization\
    \ for problems in computer vision datasets. arXiv preprint arXiv:2210.11717, 2022.\
    \ Welling, M. Herding dynamical weights to learn. In Proceedings of the 26th Annual\
    \ International Conference on Machine Learning, pp. 1121-1128, 2009. Xia, X.,\
    \ Liu, J., Yu, J., Shen, X., Han, B., and Liu, T. Moderate coreset: A universal\
    \ method of data selection for real-world data-efficient deep learning. In The\
    \ Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=7D5EECbOaf9.\
    \ Zheng, H., Liu, R., Lai, F., and Prakash, A. Coverage-centric coreset selection\
    \ for high pruning rates. In The Eleventh International Conference on Learning\
    \ Representations, 2023. URL https://openreview.net/forum?id=QwKvL6wC8Yi."
  title: Balancing feature similarity and label variability for optimal size-aware
    one-shot subset selection
  url: https://dl.acm.org/doi/10.5555/3692070.3692074
- abstract: As machine learning becomes more prominent there is a growing demand to
    perform several inference tasks in parallel. Multi-task learning (MTL) addresses
    this challenge by learning a single model that solves several tasks simultaneously
    and efficiently. Often optimizing MTL models entails first computing the gradient
    of the loss for each task, and then aggregating all the gradients to obtain a
    combined update direction. However, common methods following this approach do
    not consider an important aspect, the sensitivity in the dimensions of the gradients.
    Some dimensions may be more lenient for changes while others may be more restrictive.
    Here, we introduce a novel gradient aggregation procedure using Bayesian inference.
    We place a probability distribution over the task-specific parameters, which in
    turn induce a distribution over the gradients of the tasks. This valuable information
    allows us to quantify the uncertainty associated with each of the gradients' dimensions
    which is factored in when aggregating them. We empirically demonstrate the benefits
    of our approach in a variety of datasets, achieving state-of-the-art performance.
  keywords: Computing methodologies, Artificial intelligence, Knowledge representation
    and reasoning, Probabilistic reasoning, Vagueness and fuzzy logic, Machine learning,
    Learning paradigms, Machine learning approaches, Learning in probabilistic graphical
    models, Theory of computation, Theory and algorithms for application domains,
    Machine learning theory, Inductive inference
  references: "Achituve, I., Maron, H., and Chechik, G. Self-supervised learning for\
    \ domain adaptation on point clouds. In Proceedings of the IEEE/CVF winter conference\
    \ on applications of computer vision, pp. 123-133, 2021a. Achituve, I., Navon,\
    \ A., Yemini, Y., Chechik, G., and Fetaya, E. GP-Tree: A Gaussian process classifier\
    \ for few-shot incremental learning. In International Conference on Machine Learning,\
    \ pp. 54-65. PMLR, 2021b. Achituve, I., Shamsian, A., Navon, A., Chechik, G.,\
    \ and Fetaya, E. Personalized federated learning with Gaussian processes. Advances\
    \ in Neural Information Processing Systems, 34:8392-8406, 2021c. Achituve, I.,\
    \ Chechik, G., and Fetaya, E. Guided deep kernel learning. In Uncertainty in Artificial\
    \ Intelligence. PMLR, 2023. Baxter, J. A model of inductive bias learning. Journal\
    \ of artificial intelligence research, 12:149-198, 2000. Bishop, C. Pattern recognition\
    \ and machine learning. Springer google schola, 2:531-537, 2006. Brier, G. W.\
    \ Verification of forecasts expressed in terms of probability. Monthly weather\
    \ review, 78(1):1-3, 1950. Brookes, M. The matrix reference manual. http://www.ee.imperial.ac.uk/hp/staff/dmb/matrix/intro.html,\
    \ 2020. Calandra, R., Peters, J., Rasmussen, C. E., and Deisenroth, M. P. Manifold\
    \ Gaussian processes for regression. In 2016 International Joint Conference on\
    \ Neural Networks (IJCNN), pp. 3338-3345. IEEE, 2016. Caruana, R. Multitask learning.\
    \ Machine learning, 28: 41-75, 1997. Chen, Z., Badrinarayanan, V., Lee, C.-Y.,\
    \ and Rabinovich, A. GradNorm: Gradient normalization for adaptive loss balancing\
    \ in deep multitask networks. In Dy, J. and Krause, A. (eds.), Proceedings of\
    \ the 35th International Conference on Machine Learning, volume 80 of Proceedings\
    \ of Machine Learning Research, pp. 794-803. PMLR, 10-15 Jul 2018. Chen, Z., Ngiam,\
    \ J., Huang, Y., Luong, T., Kretzschmar, H., Chai, Y., and Anguelov, D. Just pick\
    \ a sign: Optimizing deep multitask models with gradient sign dropout. In Larochelle,\
    \ H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural\
    \ Information Processing Systems, volume 33, pp. 2039-2050. Curran Associates,\
    \ Inc., 2020. Chennupati, S., Sistu, G., Yogamani, S., and Rawashdeh, S. Multinet++:\
    \ Multi-stream feature aggregation and geometric loss strategy for multi-task\
    \ learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition (CVPR) Workshops, June 2019. Daheim, N., M\xF6llenhoff, T., Ponti,\
    \ E., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gradient\
    \ matching. In The Twelfth International Conference on Learning Representations,\
    \ 2023. Dai, Y., Fei, N., and Lu, Z. Improvable gap balancing for multi-task learning.\
    \ In Uncertainty in Artificial Intelligence, pp. 496-506. PMLR, 2023. D'Angelo,\
    \ F. and Fortuin, V. Repulsive deep ensembles are Bayesian. Advances in Neural\
    \ Information Processing Systems, 34:3451-3465, 2021. Davies, R. B. Numerical\
    \ inversion of a characteristic function. Biometrika, 60(2):415-417, 1973. Daxberger,\
    \ E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace\
    \ redux-effortless bayesian deep learning. Advances in Neural Information Processing\
    \ Systems, 34:20089-20103, 2021. D\xE9sid\xE9ri, J.-A. Multiple-gradient descent\
    \ algorithm (MGDA) for multiobjective optimization. Comptes Rendus Mathematique,\
    \ 350(5-6):313-318, 2012. Devin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine,\
    \ S. Learning modular neural network policies for multitask and multi-robot transfer.\
    \ In 2017 IEEE international conference on robotics and automation (ICRA), pp.\
    \ 2169-2176. IEEE, 2017. Dimitriadis, N., Frossard, P., and Fleuret, F. Pareto\
    \ manifold learning: Tackling multiple tasks via ensembles of single-task models.\
    \ In International Conference on Machine Learning, pp. 8015-8052. PMLR, 2023.\
    \ Elich, C., Kirchdorfer, L., K\xF6hler, J. M., and Schott, L. Challenging common\
    \ assumptions in multi-task learning. arXiv preprint arXiv:2311.04698, 2023. Fernando,\
    \ H., Shen, H., Liu, M., Chaudhury, S., Murugesan, K., and Chen, T. Mitigating\
    \ gradient bias in multi-objective learning: A provably convergent stochastic\
    \ approach. In International Conference on Learning Representations, 2023. Fey,\
    \ M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric.\
    \ In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. Fortuin,\
    \ V., Garriga-Alonso, A., Ober, S. W., Wenzel, F., Ratsch, G., Turner, R. E.,\
    \ van der Wilk, M., and Aitchison, L. Bayesian neural network priors revisited.\
    \ In International Conference on Learning Representations, 2021. Gilmer, J., Schoenholz,\
    \ S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for\
    \ quantum chemistry. In International conference on machine learning, pp. 1263-1272.\
    \ PMLR, 2017. Guo, M., Haque, A., Huang, D.-A., Yeung, S., and Fei-Fei, L. Dynamic\
    \ task prioritization for multitask learning. In Proceedings of the European Conference\
    \ on Computer Vision (ECCV), September 2018. Immr, A., Bauer, M., Fortuin, V.,\
    \ R\xE4tsch, G., and Emtiyaz, K. M. Scalable marginal likelihood estimation for\
    \ model selection in deep learning. In International Conference on Machine Learning,\
    \ pp. 4563-4573. PMLR, 2021. Javaloy, A. and Valera, I. Rotograd: Gradient homogenization\
    \ in multitask learning. In International Conference on Learning Representations,\
    \ 2022. Kendall, A., Gal, Y., and Cipolla, R. Multi-task learning using uncertainty\
    \ to weigh losses for scene geometry and semantics. In Proceedings of the IEEE\
    \ conference on computer vision and pattern recognition, pp. 7482-7491, 2018.\
    \ Kingma, D. P. and Ba, J. ADAM: A method for stochastic optimization. In International\
    \ Conference on Learning Representations, 2014. Kingma, D. P. and Ba, J. Adam:\
    \ A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd\
    \ International Conference on Learning Representations, 2015. Kristiadi, A., Hein,\
    \ M., and Hennig, P. Being Bayesian, even just a bit, fixes overconfidence in\
    \ Relu networks. In International conference on machine learning, pp. 5436-5446.\
    \ PMLR, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features\
    \ from tiny images. Technical report, University of Toronto, 2009. Kurin, V.,\
    \ De Palma, A., Kostrikov, I., Whiteson, S., and Mudigonda, P. K. In defense of\
    \ the unitary scalarization for deep multi-task learning. Advances in Neural Information\
    \ Processing Systems, 35:12169-12183, 2022. Lakshminarayanan, B., Pritzel, A.,\
    \ and Blundell, C. Simple and scalable predictive uncertainty estimation using\
    \ deep ensembles. Advances in neural information processing systems, 30, 2017.\
    \ Lin, B., Ye, F., Zhang, Y., and Tsang, I.W. Reasonable effectiveness of random\
    \ weighting: A litmus test for multitask learning. Transactions on Machine Learning\
    \ Research, 2022. Liu, B., Liu, X., Jin, X., Stone, P., and Liu, Q. Conflict-averse\
    \ gradient descent for multi-task learning. Advances in Neural Information Processing\
    \ Systems, 34:18878-18890, 2021. Liu, B., Feng, Y., Stone, P., and Liu, Q. Famo:\
    \ Fast adaptive multitask optimization, 2023. Liu, L., Li, Y., Kuang, Z., Xue,\
    \ J.-H., Chen, Y., Yang, W., Liao, Q., and Zhang, W. Towards impartial multitask\
    \ learning. In International Conference on Learning Representations, 2020. Liu,\
    \ S., Johns, E., and Davison, A. J. End-to-end multitask learning with attention.\
    \ In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\
    \ pp. 1871-1880, 2019a. Liu, X., He, P., Chen, W., and Gao, J. Multi-task deep\
    \ neural networks for natural language understanding. In Proceedings of the 57th\
    \ Annual Meeting of the Association for Computational Linguistics, pp. 4487-4496,\
    \ 2019b. MacKay, D. J. Bayesian interpolation. Neural computation, 4(3):415-447,\
    \ 1992. Maninis, K.-K., Radosavovic, I., and Kokkinos, I. Attentive single-tasking\
    \ of multiple tasks. In Proceedings of the IEEE/CVF conference on computer vision\
    \ and pattern recognition, pp. 1851-1860, 2019. Martens, J. and Sutskever, I.\
    \ Learning recurrent neural networks with hessian-free optimization. In Proceedings\
    \ of the 28th international conference on machine learning (ICML-11), pp. 1033-1040,\
    \ 2011. Matena, M. S. and Raffel, C. A. Merging models with fisher-weighted averaging.\
    \ Advances in Neural Information Processing Systems, 35:17703-17716, 2022. Michelsanti,\
    \ D., Tan, Z.-H., Zhang, S.-X., Xu, Y., Yu, M., Yu, D., and Jensen, J. An overview\
    \ of deep-learning-based audio-visual speech enhancement and separation. IEEE/ACM\
    \ Transactions on Audio, Speech, and Language Processing, 29:1368-1396, 2021.\
    \ Minka, T. P. Expectation propagation for approximate bayesian inference. In\
    \ Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,\
    \ pp. 362-369, 2001. Misra, I., Shrivastava, A., Gupta, A., and Hebert, M. Cross-stitch\
    \ networks for multi-task learning. In Proceedings of the IEEE/CVF Conference\
    \ on Computer Vision and Pattern Recognition (CVPR), pp. 3994-4003, 06 2016. Naeini,\
    \ M. P., Cooper, G. F., and Hauskrecht, M. Obtaining well calibrated probabilities\
    \ using Bayesian binning. In Proceedings of the Twenty-Ninth AAAI Conference on\
    \ Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, pp. 2901-2907.\
    \ AAAI Press, 2015. Navon, A., Shamsian, A., Achituve, I., Maron, H., Kawaguchi,\
    \ K., Chechik, G., and Fetaya, E. Multitask learning as a bargaining game. In\
    \ International Conference on Machine Learning, pp. 16428-16446. PMLR, 2022. Neal,\
    \ R. M. and Hinton, G. E. A view of the EM algorithm that justifies incremental,\
    \ sparse, and other variants. In Learning in graphical models, pp. 355-368. Springer,\
    \ 1998. Ramakrishnan, R., Dral, P. O., Rupp, M., and Von Lilienfeld, O. A. Quantum\
    \ chemistry structures and properties of 134 kilo molecules. Scientific data,\
    \ 1(1):1-7, 2014. Rosenbaum, C., Klinger, T., and Riemer, M. Routing networks:\
    \ Adaptive selection of non-linear functions for multi-task learning. In International\
    \ Conference on Learning Representations, 2018. Ruder, S. An overview of multi-task\
    \ learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. Russakovsky,\
    \ O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,\
    \ A., Khosla, A., Bernstein, M., et al. ImageNet large scale visual recognition\
    \ challenge. International journal of computer vision, 115: 211-252, 2015. Sagawa,\
    \ S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural\
    \ networks. In International Conference on Learning Representations, 2019. S\xE4\
    rkk\xE4, S. Bayesian Filtering and Smoothing, volume 3 of Institute of Mathematical\
    \ Statistics textbooks. Cambridge University Press, 2013. Saul, L. K., Jaakkola,\
    \ T., and Jordan, M. I. Mean field theory for Sigmoid Belief Networks. Journal\
    \ of artificial intelligence research, 4:61-76, 1996. Schaul, T., Borsa, D., Modayil,\
    \ J., and Pascanu, R. Ray interference: a source of plateaus in deep reinforcement\
    \ learning, 2019. Schraudolph, N. N. Fast curvature matrix-vector products for\
    \ second-order gradient descent. Neural computation, 14(7):1723-1738, 2002. Sener,\
    \ O. and Koltun, V. Multi-task learning as multi-objective optimization. Advances\
    \ in neural information processing systems, 31, 2018. Senushkin, D., Patakin,\
    \ N., Kuznetsov, A., and Konushin, A. Independent component alignment for multi-task\
    \ learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition, pp. 20083-20093, 2023. Shamshad, F., Khan, S., Zamir, S. W., Khan,\
    \ M. H., Hayat, M., Khan, F. S., and Fu, H. Transformers in medical imaging: A\
    \ survey. Medical Image Analysis, pp. 102802, 2023. Shamsian, A., Navon, A., Glazer,\
    \ N., Kawaguchi, K., Chechik, G., and Fetaya, E. Auxiliary learning as an asymmetric\
    \ bargaining game. arXiv preprint arXiv:2301.13501, 2023. Shi, H., Ren, S., Zhang,\
    \ T., and Pan, S. J. Deep multitask learning with progressive parameter sharing.\
    \ In Proceedings of the IEEE/CVF International Conference on Computer Vision,\
    \ pp. 19924-19935, 2023. Shu, T., Xiong, C., and Socher, R. Hierarchical and interpretable\
    \ skill acquisition in multi-task reinforcement learning. In International Conference\
    \ on Learning Representations, 2018. Snoek, J., Rippel, O., Swersky, K., Kiros,\
    \ R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., and Adams, R. Scalable\
    \ Bayesian optimization using deep neural networks. In International conference\
    \ on machine learning, pp. 2171-2180. PMLR, 2015. Standley, T., Zamir, A., Chen,\
    \ D., Guibas, L., Malik, J., and Savarese, S. Which tasks should be learned together\
    \ in multi-task learning? In International Conference on Machine Learning, pp.\
    \ 9120-9132. PMLR, 2020. Taslimi, S., Taslimi, S., Fathi, N., Salehi, M., and\
    \ Rohban, M. H. SwincheX: Multi-label classification on chest X-ray images with\
    \ transformers. arXiv preprint arXiv:2206.04246, 2022. Vinyals, O., Bengio, S.,\
    \ and Kudlur, M. Order matters: Sequence to sequence for sets. In Bengio, Y. and\
    \ Le-Cun, Y. (eds.), 4th International Conference on Learning Representations,\
    \ ICLR, 2016. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and Summers, R.\
    \ M. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised\
    \ classification and localization of common thorax diseases. In Proceedings of\
    \ the IEEE conference on computer vision and pattern recognition, pp. 2097-2106,\
    \ 2017. Wang, Z., Tsvetkov, Y., Firat, O., and Cao, Y. Gradient vaccine: Investigating\
    \ and improving multi-task optimization in massively multilingual models. In International\
    \ Conference on Learning Representations, 2020. Wightman, R. Pytorch image models.\
    \ https://github.com/rwightman/pytorch-image-models, 2019. Wild, V. D., Ghalebikesabi,\
    \ S., Sejdinovic, D., and Knoblauch, J. A rigorous link between deep ensembles\
    \ and (variational) Bayesian methods. Advances in Neural Information Processing\
    \ Systems, 36, 2024. Wilson, A. G. and Izmailov, P. Bayesian deep learning and\
    \ a probabilistic perspective of generalization. Advances in neural information\
    \ processing systems, 33:4697-4708, 2020. Wilson, A. G., Hu, Z., Salakhutdinov,\
    \ R., and Xing, E. P. Deep kernel learning. In Artificial intelligence and statistics,\
    \ pp. 370-378. PMLR, 2016a. Wilson, A. G., Hu, Z., Salakhutdinov, R. R., and Xing,\
    \ E. P. Stochastic variational deep kernel learning. Advances in neural information\
    \ processing systems, 29, 2016b. Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes,\
    \ J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. MoleculeNet: a benchmark\
    \ for molecular machine learning. Chemical science, 9(2):513-530, 2018. Xin, D.,\
    \ Ghorbani, B., Gilmer, J., Garg, A., and Firat, O. Do current multi-task optimization\
    \ methods in deep learning even help? Advances in Neural Information Processing\
    \ Systems, 35:13597-13609, 2022. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman,\
    \ K., and Finn, C. Gradient surgery for multi-task learning. Advances in Neural\
    \ Information Processing Systems, 33: 5824-5836, 2020. Yun, H. and Cho, H. Achievement-based\
    \ training progress balancing for multi-task learning. In Proceedings of the IEEE/CVF\
    \ International Conference on Computer Vision (ICCV), pp. 16935-16944, October\
    \ 2023. Zhang, Y. and Yang, Q. A survey on multi-task learning. IEEE Transactions\
    \ on Knowledge and Data Engineering, 34(12):5586-5609, 2021. Zhang, Z., Song,\
    \ Y., and Qi, H. Age progression/regression by conditional adversarial autoencoder.\
    \ In Proceedings of the IEEE conference on computer vision and pattern recognition,\
    \ pp. 5810-5818, 2017. Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J.,\
    \ Kehtarnavaz, N., and Shah, M. Deep learning-based human pose estimation: A survey.\
    \ ACM Computing Surveys, 56(1):1-37, 2023. Zhou, C., Li, Q., Li, C., Yu, J., Liu,\
    \ Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., et al. A comprehensive survey\
    \ on pretrained foundation models: A history from BERT to ChatGPT. arXiv preprint\
    \ arXiv:2302.09419, 2023."
  title: Bayesian uncertainty for gradient aggregation in multi-task learning
  url: https://dl.acm.org/doi/10.5555/3692070.3692075
- abstract: "Large Language Models are prone to biased predictions and hallucinations,\
    \ underlining the paramount importance of understanding their model-internal reasoning\
    \ process. However, achieving faithful attributions for the entirety of a black-box\
    \ transformer model and maintaining computational efficiency is an unsolved challenge.\
    \ By extending the Layer-wise Relevance Propagation attribution method to handle\
    \ attention layers, we address these challenges effectively. While partial solutions\
    \ exist, our method is the first to faithfully and holistically attribute not\
    \ only input but also latent representations of transformer models with the computational\
    \ efficiency similar to a single backward pass. Through extensive evaluations\
    \ against existing methods on LLaMa 2, Mixtral 8\xD77b, Flan-T5 and vision transformer\
    \ architectures, we demonstrate that our proposed approach surpasses alternative\
    \ methods in terms of faithfulness and enables the understanding of latent representations,\
    \ opening up the door for concept-based explanations. We provide an LRP library\
    \ at https://github.com/rachtibat/LRP-eXplains-Transformers."
  keywords: Computing methodologies, Artificial intelligence, Knowledge representation
    and reasoning, Reasoning about belief and knowledge, Natural language processing,
    Information extraction, Lexical semantics, Natural language generation, Machine
    learning, Machine learning approaches, Learning latent representations, Information
    systems, Information retrieval, Retrieval models and ranking, Similarity measures,
    Retrieval tasks and goals, Sentiment analysis, Specialized information retrieval
  references: "Abnar, S. and Zuidema, W. H. (2020). Quantifying attention flow in\
    \ transformers. In Proceedings of the 58th Annual Meeting of the Association for\
    \ Computational Linguistics, pages 4190-4197. Achanta, R., Shaji, A., Smith, K.,\
    \ Lucchi, A., Fua, P., and S\xFCsstrunk, S. (2012). Slic superpixels compared\
    \ to state-of-the-art superpixel methods. IEEE transactions on pattern analysis\
    \ and machine intelligence, 34(11):2274-2282. Achtibat, R., Dreyer, M., Eisenbraun,\
    \ I., Bosse, S., Wiegand, T., Samek, W., and Lapuschkin, S. (2023). From attribution\
    \ maps to human-understandable explanations through concept relevance propagation.\
    \ Nature Machine Intelligence, 5(9):1006-1019. Ali, A., Schnake, T., Eberle, O.,\
    \ Montavon, G., M\xFCller, K.- R., and Wolf, L. (2022). Xai for transformers:\
    \ Better explanations through conservative propagation. In International Conference\
    \ on Machine Learning, pages 435-451. PMLR. Anders, C. J., Neumann, D., Samek,\
    \ W., M\xFCller, K.-R., and Lapuschkin, S. (2021). Software for dataset-wide xai:\
    \ from local explanations to global insights with zennit, corelay, and virelay.\
    \ arXiv preprint arXiv:2106.13200. Arras, L., Osman, A., and Samek, W. (2022).\
    \ Clevrxai: A benchmark dataset for the ground truth evaluation of neural network\
    \ explanations. Information Fusion, 81:14-40. Ba, J. L., Kiros, J. R., and Hinton,\
    \ G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450. Bach, S.,\
    \ Binder, A., Montavon, G., Klauschen, F., M\xFCller, K.-R., and Samek, W. (2015).\
    \ On pixel-wise explanations for non-linear classifier decisions by layer-wise\
    \ relevance propagation. PLoS ONE, 10(7):e0130140. Balduzzi, D., Frean, M., Leary,\
    \ L., Lewis, J., Ma, K. W.- D., and McWilliams, B. (2017). The shattered gradients\
    \ problem: If resnets are the answer, then what is the question? In International\
    \ Conference on Machine Learning, pages 342-350. PMLR. Binder, A., Montavon, G.,\
    \ Lapuschkin, S., M\xFCller, K.- R., and Samek, W. (2016). Layer-wise relevance\
    \ propagation for neural networks with local renormalization layers. In Artificial\
    \ Neural Networks and Machine Learning-ICANN 2016: 25th International Conference\
    \ on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings,\
    \ Part II 25, pages 63-71. Springer. Bl\xFCcher, S., Vielhaben, J., and Strodthoff,\
    \ N. (2024). Decoupling pixel flipping and occlusion strategy for consistent xai\
    \ benchmarks. arXiv preprint arXiv:2401.06654. Brocki, L. and Chung, N. C. (2023).\
    \ Feature perturbation augmentation for reliable evaluation of importance estimators\
    \ in neural networks. Pattern Recognition Letters, 176:131-139. Caron, M., Touvron,\
    \ H., Misra, I., J\xE9gou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021).\
    \ Emerging properties in self-supervised vision transformers. In Proceedings of\
    \ the IEEE/CVF International Conference on Computer Vision, pages 9650-9660. Chang,\
    \ C.-H., Creager, E., Goldenberg, A., and Duvenaud, D. (2018). Explaining image\
    \ classifiers by counterfactual generation. arXiv preprint arXiv:1807.08024. Chefer,\
    \ H., Gur, S., and Wolf, L. (2021a). Generic attention-model explainability for\
    \ interpreting bimodal and encoder-decoder transformers. In Proceedings of the\
    \ IEEE/CVF International Conference on Computer Vision, pages 397-406. Chefer,\
    \ H., Gur, S., and Wolf, L. (2021b). Transformer interpretability beyond attention\
    \ visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision\
    \ and Pattern Recognition, pages 782-791. Chen, T., Xu, B., Zhang, C., and Guestrin,\
    \ C. (2016). Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\
    \ Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang,\
    \ X., Dehghani, M., Brahma, S., et al. (2022). Scaling instruction-finetuned language\
    \ models. arXiv preprint arXiv:2210.11416. Clark, K., Khandelwal, U., Levy, O.,\
    \ and Manning, C. D. (2019). What does bert look at? an analysis of bert's attention.\
    \ In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\
    \ Neural Networks for NLP, pages 276-286. Dai, D., Dong, L., Hao, Y., Sui, Z.,\
    \ Chang, B., and Wei, F. (2022). Knowledge neurons in pretrained transformers.\
    \ In Proceedings of the 60th Annual Meeting of the Association for Computational\
    \ Linguistics (Volume 1: Long Papers), pages 8493-8502. Dao, T., Fu, D., Ermon,\
    \ S., Rudra, A., and R\xE9, C. (2022). Flashattention: Fast and memory-efficient\
    \ exact attention with io-awareness. Advances in Neural Information Processing\
    \ Systems, 35:16344-16359. Deb, M., Deiseroth, B., Weinbach, S., Schramowski,\
    \ P., and Kersting, K. (2023). Atman: Understanding transformer predictions through\
    \ memory efficient attention manipulation. arXiv preprint arXiv:2301.08110. Deng,\
    \ J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet:\
    \ A large-scale hierarchical image database. In 2009 IEEE conference on computer\
    \ vision and pattern recognition, pages 248-255. Ieee. Dettmers, T., Pagnoni,\
    \ A., Holtzman, A., and Zettlemoyer, L. (2024). Qlora: Efficient finetuning of\
    \ quantized llms. Advances in Neural Information Processing Systems, 36. Ding,\
    \ Y., Liu, Y., Luan, H., and Sun, M. (2017). Visualizing and understanding neural\
    \ machine translation. In Proceedings of the 55th Annual Meeting of the Association\
    \ for Computational Linguistics (Volume 1: Long Papers), pages 1150-1159. Dombrowski,\
    \ A.-K., Anders, C. J., M\xFCller, K.-R., and Kessel, P. (2022). Towards robust\
    \ explanations for deep neural networks. Pattern Recognition, 121:108194. Dosovitskiy,\
    \ A., Beyer, L., Kolesnikov, A., Weissenborn, D., et al. (2021). An image is worth\
    \ 16\xD716 words: Transformers for image recognition at scale. In 9th International\
    \ Conference on Learning Representations, ICLR. Fatima, S. S., Wooldridge, M.,\
    \ and Jennings, N. R. (2008). A linear approximation method for the shapley value.\
    \ Artificial Intelligence, 172(14):1673-1699. Fedus, W., Dean, J., and Zoph, B.\
    \ (2022). A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667.\
    \ Fong, R. C. and Vedaldi, A. (2017). Interpretable explanations of black boxes\
    \ by meaningful perturbation. In IEEE International Conference on Computer Vision\
    \ (ICCV), pages 3449-3457. Fryer, D., Str\xFCmke, I., and Nguyen, H. (2021). Shapley\
    \ values for feature selection: The good, the bad, and the axioms. IEEE Access,\
    \ 9:144352-144360. Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. (2022).\
    \ Transformer feed-forward layers build predictions by promoting concepts in the\
    \ vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods\
    \ in Natural Language Processing, pages 30-45. Geva, M., Schuster, R., Berant,\
    \ J., and Levy, O. (2021). Transformer feed-forward layers are key-value memories.\
    \ In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\
    \ Processing, pages 5484-5495. Gildenblat, J. (2020. Accessed on Dec 01, 2023).\
    \ Exploring explainability for vision transformers. https://jacobgil.github.io/deeplearning/vision-transformer-explainability.\
    \ Guidotti, R., Monreale, A., Ruggieri, S., Pedreschi, D., Turini, F., and Giannotti,\
    \ F. (2018). Local rule-based explanations of black box decision systems. arXiv\
    \ preprint arXiv:1805.10820. Hedstr\xF6m, A., Weber, L., Krakowczyk, D., Bareeva,\
    \ D., Motzkus, F., Samek, W., Lapuschkin, S., and H\xF6hne, M. M. M. (2023). Quantus:\
    \ An explainable ai toolkit for responsible evaluation of neural network explanations\
    \ and beyond. Journal of Machine Learning Research, 24(34):1-11. Huang, L., Yu,\
    \ W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin,\
    \ B., et al. (2023). A survey on hallucination in large language models: Principles,\
    \ taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232. Jiang,\
    \ A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot,\
    \ D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. (2024). Mixtral of\
    \ experts. arXiv preprint arXiv:2401.04088. Kokhlikyan, N., Miglani, V., Martin,\
    \ M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., Kliushkina, N., Araya,\
    \ C., Yan, S., et al. (2020). Captum: A unified and generic model interpretability\
    \ library for pytorch. arXiv preprint arXiv:2009.07896. Li, Y., Bubeck, S., Eldan,\
    \ R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. (2023). Textbooks are all\
    \ you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463. Lundberg,\
    \ S. M. and Lee, S. (2017). A unified approach to interpreting model predictions.\
    \ In Advances in Neural Information Processing Systems 30, pages 4765-4774. Maas,\
    \ A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning\
    \ word vectors for sentiment analysis. In Proceedings of the 49th annual meeting\
    \ of the association for computational linguistics: Human language technologies,\
    \ pages 142-150. Mao, C., Jiang, L., Dehghani, M., Vondrick, C., Sukthankar, R.,\
    \ and Essa, I. (2021). Discrete representations strengthen vision transformer\
    \ robustness. In International Conference on Learning Representations. Miglani,\
    \ V., Yang, A., Markosyan, A., Garcia-Olano, D., and Kokhlikyan, N. (2023). Using\
    \ captum to explain generative language models. In Proceedings of the 3rd Workshop\
    \ for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 165-173.\
    \ Montavon, G., Binder, A., Lapuschkin, S., Samek, W., and M\xFCller, K.-R. (2019).\
    \ Layer-wise relevance propagation: an overview. Explainable AI: interpreting,\
    \ explaining and visualizing deep learning, pages 193-209. Montavon, G., Lapuschkin,\
    \ S., Binder, A., Samek, W., and M\xFCller, K.-R. (2017). Explaining nonlinear\
    \ classification decisions with deep taylor decomposition. Pattern recognition,\
    \ 65:211-222. Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and Clune,\
    \ J. (2016). Synthesizing the preferred inputs for neurons in neural networks\
    \ via deep generator networks. Advances in neural information processing systems,\
    \ 29. Pahde, F., Yolcu, G. \xDC., Binder, A., Samek, W., and Lapuschkin, S. (2023).\
    \ Optimizing explanations by network canonization and hyperparameter search. In\
    \ Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\
    \ pages 3818-3827. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\
    \ Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019).\
    \ Pytorch: An imperative style, high-performance deep learning library. Advances\
    \ in Neural Information Processing Systems, 32. Rajpurkar, P., Jia, R., and Liang,\
    \ P. (2018). Know what you don't know: Unanswerable questions for squad. In Proceedings\
    \ of the 56th Annual Meeting of the Association for Computational Linguistics\
    \ (Volume 2: Short Papers), pages 784-789. Ribeiro, M. T., Singh, S., and Guestrin,\
    \ C. (2016). \"why should I trust you?\": Explaining the predictions of any classifier.\
    \ In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge\
    \ Discovery and Data Mining, pages 1135-1144. ACM. Samek, W., Binder, A., Montavon,\
    \ G., Lapuschkin, S., and M\xFCller, K.-R. (2017). Evaluating the visualization\
    \ of what a deep neural network has learned. IEEE Transactions on Neural Networks\
    \ and Learning Systems, 28(11):2660-2673. Scheepers, T. (2017). Improving the\
    \ compositionality of word embeddings. Master's thesis, Universiteit van Amsterdam,\
    \ Science Park 904, Amsterdam, Netherlands. Selvaraju, R. R., Cogswell, M., Das,\
    \ A., Vedantam, R., Parikh, D., and Batra, D. (2017). Grad-cam: Visual explanations\
    \ from deep networks via gradient-based localization. In Proceedings of the IEEE\
    \ International Conference on Computer Vision (ICCV), pages 618-626. Shaham, U.,\
    \ Ivgi, M., Efrat, A., Berant, J., and Levy, O. (2023). Zeroscrolls: A zero-shot\
    \ benchmark for long text understanding. arXiv preprint arXiv:2305.14196. Shrikumar,\
    \ A., Greenside, P., and Kundaje, A. (2017). Learning important features through\
    \ propagating activation differences. In International Conference on Machine Learning,\
    \ pages 3145-3153. PMLR. Simonyan, K., Vedaldi, A., and Zisserman, A. (2014).\
    \ Deep inside convolutional networks: visualising image classification models\
    \ and saliency maps. In Proceedings of the International Conference on Learning\
    \ Representations (ICLR). ICLR. Smilkov, D., Thorat, N., Kim, B., Vi\xE9gas, F.,\
    \ and Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv\
    \ preprint arXiv:1706.03825. Sundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic\
    \ attribution for deep networks. In International Conference on Machine Learning,\
    \ pages 3319-3328. PMLR. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\
    \ A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.\
    \ (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\
    \ arXiv:2307.09288. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\
    \ L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017). Attention is all\
    \ you need. Advances in Neural Information Processing Systems, 30. Voita, E.,\
    \ Ferrando, J., and Nalmpantis, C. (2023). Neurons in large language models: Dead,\
    \ n-gram, positional. arXiv preprint arXiv:2309.04827. Voita, E., Sennrich, R.,\
    \ and Titov, I. (2021). Analyzing the source and target contributions to predictions\
    \ in neural machine translation. In 59th Annual Meeting of the Association for\
    \ Computational Linguistics and the 11th International Joint Conference on Natural\
    \ Language Processing, ACL/IJCNLP, pages 1126-1140. Wiegreffe, S. and Pinter,\
    \ Y. (2019). Attention is not not explanation. In Proceedings of the 2019 Conference\
    \ on Empirical Methods in Natural Language Processing and the 9th International\
    \ Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 11-20.\
    \ Wikimedia Foundation (2023. Accessed on Dec 01, 2023). Wikimedia downloads.\
    \ https://dumps.wikimedia.org. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,\
    \ C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. (2019).\
    \ Huggingface's transformers: State-of-the-art natural language processing. arXiv\
    \ preprint arXiv:1910.03771. Zeiler, M. D. and Fergus, R. (2014). Visualizing\
    \ and understanding convolutional networks. In European Conference Computer Vision\
    \ - ECCV 2014, pages 818-833. Zhang, B. and Sennrich, R. (2019). Root mean square\
    \ layer normalization. Advances in Neural Information Processing Systems, 32."
  title: 'AttnLRP: attention-aware layer-wise relevance propagation for transformers'
  url: https://dl.acm.org/doi/10.5555/3692070.3692076
- abstract: This work considers the fundamental problem of learning an unknown object
    from training data using a given model class. We introduce a framework that allows
    for objects in arbitrary Hilbert spaces, general types of (random) linear measurements
    as training data and general types of nonlinear model classes. We establish a
    series of learning guarantees for this framework, which provide explicit relations
    between the amount of training data and the model class to ensure near-best generalization
    bounds. In doing so, we introduce the key notion of the variation of a model class
    with respect to a distribution of sampling operators. We show that this framework
    can accommodate many different types of well-known problems of interest, such
    as matrix sketching by random sampling, compressed sensing with isotropic vectors,
    active learning in regression and compressed sensing with generative models. In
    all cases, known results become straightforward corollaries of our general theory.
    Hence, this work provides a powerful framework for studying and analyzing many
    different types of learning problems.
  keywords: Computing methodologies, Machine learning, Machine learning approaches,
    Mathematics of computing, Information theory, Mathematical analysis, Numerical
    analysis, Probability and statistics, Theory of computation, Design and analysis
    of algorithms
  references: "Adcock, B. and Brugiapaglia, S. Is Monte Carlo a bad sampling strategy\
    \ for learning smooth functions in high dimensions? arXiv:2208.09045, 2022. Adcock,\
    \ B. and Dexter, N. The gap between theory and practice in function approximation\
    \ with deep neural networks. SIAM J. Math. Data Sci., 3(2):624-655, 2021. Adcock,\
    \ B. and Hansen, A. C. Compressive Imaging: Structure, Sampling, Learning. Cambridge\
    \ University Press, Cambridge, UK, 2021. Adcock, B., Hansen, A. C., Poon, C.,\
    \ and Roman, B. Breaking the coherence barrier: a new theory for compressed sensing.\
    \ Forum Math. Sigma, 5:e4, 2017. Adcock, B., Brugiapaglia, S., Dexter, N., and\
    \ Moraga, S. Deep neural networks are effective at learning high-dimensional Hilbert-valued\
    \ functions from limited data. In Bruna, J., Hesthaven, J. S., and Zdeborov\xE1\
    , L. (eds.), Proceedings of The Second Annual Conference on Mathematical and Scientific\
    \ Machine Learning, volume 145 of Proc. Mach. Learn. Res. (PMLR), pp. 1-36. PMLR,\
    \ 2021. Adcock, B., Brugiapaglia, S., Dexter, N., and Moraga, S. On efficient\
    \ algorithms for computing near-best polynomial approximations to high-dimensional,\
    \ Hilbert-valued functions from limited samples. arXiv:2203.13908, 2022a. Adcock,\
    \ B., Brugiapaglia, S., Dexter, N., and Moraga, S. Near-optimal learning of Banach-valued,\
    \ high-dimensional functions via deep neural networks. arXiv:2211.12633, 2022b.\
    \ Adcock, B., Brugiapaglia, S., and Webster, C. G. Sparse Polynomial Approximation\
    \ of High-Dimensional Functions. Comput. Sci. Eng. Society for Industrial and\
    \ Applied Mathematics, Philadelphia, PA, 2022c. Adcock, B., Cardenas, J. M., Dexter,\
    \ N., and Moraga, S. Towards optimal sampling for learning sparse approximation\
    \ in high dimensions, chapter 2, pp. 9-77. Springer Optimization and Its Applications.\
    \ Springer, 2022d. Adcock, B., Cardenas, J. M., and Dexter, N. CS4ML: A general\
    \ framework for active learning with arbitrary data based on Christoffel functions.\
    \ arXiv:2306.00945, 2023. Alekseev, A. K., Navon, I. M., and Zelentsov, M. E.\
    \ The estimation of functional uncertainty using polynomial chaos and adjoint\
    \ equations. Internat. J. Numer. Methods Fluids, 67(3):328-341, 2011. Avron, H.,\
    \ Kapralov, M., Musco, C., Musco, C., Velingker, A., and Zandieh, A. Random Fourier\
    \ features for kernel ridge regression: approximation bounds and statistical guarantees.\
    \ In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference\
    \ on Machine Learning, volume 70 of Proceedings of Machine Learning Research,\
    \ pp. 253-262. PMLR, 2017. Avron, H., Kapralov, M., Musco, C., Musco, C., Velingker,\
    \ A., and Zandieh, A. A universal sampling method for reconstructing signals with\
    \ simple Fourier transforms. In Proceedings of the 51st Annual ACM SIGACT Symposium\
    \ on Theory of Computing, STOC 2019, pp. 1051-1063, New York, NY, USA, 2019. Association\
    \ for Computing Machinery. Baraniuk, R. G., Cevher, V., Duarte, M. F., and Hedge,\
    \ C. Model-based compressive sensing. IEEE Trans. Inform. Theory, 56(4):1982-2001,\
    \ 2010. Berk, A., Brugiapaglia, S., Joshi, B., Plan, Y., Scott, M., and Yilmaz,\
    \ O. A coherence parameter characterizing generative compressed sensing with Fourier\
    \ measurements. IEEE J. Sel. Areas Inf. Theory, 3(3):502-512, 2023a. Berk, A.,\
    \ Brugiapaglia, S., Plan, Y., Scott, M., Sheng, X., and Yilmaz, O. Model-adapted\
    \ Fourier sampling for generative compressed sensing. arXiv:2310.04984, 2023b.\
    \ Bigot, J., Boyer, C., and Weiss, P. An analysis of block sampling strategies\
    \ in compressed sensing. IEEE Trans. Inform. Theory, 62(4):2125-2139, 2016. Bora,\
    \ A., Jalal, A., Price, E., and Dimakis, A. G. Compressed sensing using generative\
    \ models. In International Conference on Machine Learning, pp. 537-546, 2017.\
    \ Bourrier, A., Davies, M. E., Peleg, T., P\xE9rez, P., and Gribonval, R. Fundamental\
    \ performance limits for ideal decoders in high-dimensional linear inverse problems.\
    \ IEEE Trans. Inform. Theory, 60(12):7928-7946, 2014. Boyer, C., Bigot, J., and\
    \ Weiss, P. Compressed sensing with structured sparsity and structured acquisition.\
    \ Appl. Comput. Harmon. Anal., 46(2):312-350, 2019. Brugiapaglia, S., Dirksen,\
    \ S., Jung, H. C., and Rauhut, H. Sparse recovery in bounded Riesz systems with\
    \ applications to numerical methods for PDEs. Appl. Comput. Harmon. Anal., 53:231-269,\
    \ 2021. Cand\xE8s, E. J. and Plan, Y. A probabilistic and RIPless theory of compressed\
    \ sensing. IEEE Trans. Inform. Theory, 57 (11):7235-7254, 2011. Chen, J., Scarlett,\
    \ J., Ng, M., and Liu, Z. A unified framework for uniform signal recovery in nonlinear\
    \ generative compressed sensing. In Thirty-seventh Conference on Neural Information\
    \ Processing Systems, 2023. Chen, S., Varma, R., Singh, A., and Kova\u010Dcevi\u0107\
    , J. A statistical perspective of sampling scores for linear regression. In 2016\
    \ IEEE International Symposium on Information Theory (ISIT), pp. 1556-1560, 2016.\
    \ Chen, X. and Price, E. Active regression via linear-sample sparsification. In\
    \ Beygelzimer, A. and Hsu, D. (eds.), Proceedings of the Thirty-Second Conference\
    \ on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pp.\
    \ 663-695. PMLR, 2019. Chun, I.-Y. and Adcock, B. Compressed sensing and parallel\
    \ acquisition. IEEE Trans. Inform. Theory, 63(8):4860-4882, 2017. Cohen, A. and\
    \ DeVore, R. A. Approximation of high-dimensional parametric PDEs. Acta Numer.,\
    \ 24:1-159, 2015. Cohen, A. and Migliorati, G. Optimal weighted least-squares\
    \ methods. SMAI J. Comput. Math., 3:181-203, 2017. Czarnecki, W. M., Osindero,\
    \ S., Jaderberg, M., Swirszcz, G., and Pascanu, R. Sobolev training for neural\
    \ networks. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R.,\
    \ Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing\
    \ Systems, volume 30. Curran Associates, Inc., 2017. Davenport, M. A. and Romberg,\
    \ J. An overview of low-rank matrix recovery from incomplete observations. IEEE\
    \ J. Sel. Topics Signal Process., 10(4):602-622, 2016. Davenport, M. A., Duarte,\
    \ M. F., Eldar, Y. C., and Kutyniok, G. Introduction to compressed sensing. In\
    \ Eldar, Y. C. and Kutyniok, G. (eds.), Compressed Sensing: Theory and Applications,\
    \ pp. 1-64. Cambridge University Press, Cambridge, UK, 2012. Derezinski, M., Warmuth,\
    \ M. K. K., and Hsu, D. J. Leveraged volume sampling for linear regression. In\
    \ Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and\
    \ Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume\
    \ 31. Curran Associates, Inc., 2018. Dexter, N., Tran, H., and Webster, C. A mixed\
    \ \u21131 regularization approach for sparse simultaneous approximation of parameterized\
    \ PDEs. ESAIM Math. Model. Numer. Anal., 53:2025-2045, 2019. Dirksen, S. Dimensionality\
    \ reduction with subgaussian matrices: a unified theory. Found. Comput. Math.,\
    \ 16: 1367-1396, 2016. Dolbeault, M. and Cohen, A. Optimal sampling and christoffel\
    \ functions on general domains. Constructive Approximation, 56(1):121-163, 2022.\
    \ Donoho, D. L. and Tanner, J. Observed universality of phase transitions in high-dimensional\
    \ geometry, with implications for modern data analysis and signal processing.\
    \ Philos. Trans. Roy. Soc. A, 367(1906):4273-4293, 2009. Duarte, M. F. and Eldar,\
    \ Y. C. Structured compressed sensing: from theory to applications. IEEE Trans.\
    \ Signal Process., 59(9):4053-4085, 2011. Eigel, M., Schneider, R., and Trunschke,\
    \ P. Convergence bounds for empirical nonlinear least-squares. ESAIM Math. Model.\
    \ Numer. Anal., 56(1):79-104, 2022. Erdelyi, T., Musco, C., and Musco, C. Fourier\
    \ sparse leverage scores and approximate kernel learning. In Larochelle, H., Ranzato,\
    \ M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information\
    \ Processing Systems, volume 33, pp. 109-122. Curran Associates, Inc., 2020. Feng,\
    \ X. and Zeng, L. Gradient-enhanced deep neural network approximations. J. Mach.\
    \ Learn. Model. Comput., 3(4):73-91, 2022. Foucart, S. and Rauhut, H. A Mathematical\
    \ Introduction to Compressive Sensing. Appl. Numer. Harmon. Anal. Birkh\xE4user,\
    \ New York, NY, 2013. Gajjar, A., Musco, C., and Hegde, C. Active learning for\
    \ single neuron models with Lipschitz non-linearities. In Ruiz, F., Dy, J., and\
    \ van de Meent, J.-W. (eds.), Proceedings of The 26th International Conference\
    \ on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine\
    \ Learning Research, pp. 4101-4113. PMLR, 2023. Genzel, M., Kutyniok, G., and\
    \ M\xE4rz, M. \u21131-analysis minimization and generalized (co-)sparsity: When\
    \ does recovery succeed? Appl. Comput. Harmon. Anal., 52:82-140, 2021. ISSN 1063-5203.\
    \ Han, J., Jentzen, A., and E, W. Solving high-dimensional partial differential\
    \ equations using deep learning. Proc. Natl. Acad. Sci. U.S.A., 115(34):8505-8510,\
    \ 2018. Hashemi, A., Schaeffer, H., Shi, R., Topcu, U., Tran, G., and Ward, R.\
    \ Generalization bounds for sparse random feature expansions. Appl. Comput. Harmon.\
    \ Anal., 62: 310-330, 2023. Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis,\
    \ A., and Tamir, J. Robust compressed sensing MR imaging with deep generative\
    \ priors. In NeurIPS 2021 Workshop on Deep Learning and Inverse Problems, 2021.\
    \ Kabanava, M. and Rauhut, H. Cosparsity in compressed sensing. In Boche, H.,\
    \ Calderbank, R., Kutyniok, G., and Vyb\xEDral, J. (eds.), Compressed Sensing\
    \ and its Applications: MATHEON Workshop 2013, Applied and Numerical Harmonic\
    \ Analysis, pp. 315-339. Birkh\xE4user, Cham, 2015. Krahmer, F. and Ward, R. Stable\
    \ and robust sampling strategies for compressive imaging. IEEE Trans. Image Process.,\
    \ 23(2):612-622, 2013. Krahmer, F., Needell, D., and Ward, R. Compressive sensing\
    \ with redundant dictionaries and structured measurements. SIAM J. Math. Anal.,\
    \ 47(6):4606-4629, 2015. Ma, P., Mahoney, M.W., and Yu, B. A statistical perspective\
    \ on algorithmic leveraging. J. Mach. Learn. Res., 16:861-911, 2015. Malik, O.,\
    \ Xu, Y., Cheng, N., Becker, S., Doostan, A., and Narayan, A. Fast algorithms\
    \ for monotone lower subsets of Kronecker least squares problems. arXiv:2209.05662,\
    \ 2022. McRobbie, D. W., Moore, E. A., Graves, M. J., and Prince, M. R. MRI: From\
    \ Picture to Proton. Cambridge University Press, Cambridge, 2nd edition, 2006.\
    \ Monajemi, H., Jafarpour, S., Gavish, M., Donoho, D. L., Ambikasaran, S., Bacallado,\
    \ S., Bharadia, D., Chen, Y., Choi, Y., Chowdhury, M., et al. Deterministic matrices\
    \ matching the compressed sensing phase transitions of Gaussian random matrices.\
    \ Proc. Natl. A. Sci. USA, 110 (4):1181-1186, Jan. 2013. Nam, S., Davies, M. E.,\
    \ Elad, M., and Gribonval, R. The cosparse analysis model and algorithms. Appl.\
    \ Comput. Harmon. Anal., 34(1):30-56, 2013. O'Leary-Roseberry, T., Villa, U.,\
    \ Chen, P., and Ghattas, O. Derivative-informed projected neural networks for\
    \ high-dimensional parametric maps governed by PDEs. Comput. Methods Appl. Mech.\
    \ Engrg., 388(1):114199, 2022. O'Leary-Roseberry, T., Chen, P., Villa, U., and\
    \ Ghattas, O. Derivative-Informed Neural Operator: An efficient framework for\
    \ high-dimensional parametric derivative learning. Journal of Computational Physics,\
    \ 496:112555, 2024. ISSN 0021-9991. Peng, J., Hampton, J., and Doostan, A. On\
    \ polynomial chaos expansion via gradient-enhanced l1-minimization. J. Comput.\
    \ Phys., 310:440-458, 2016. Raissi, M., Perdikaris, P., and Karniadakis, G. E.\
    \ Physics-informed neural networks: a deep learning framework for solving forward\
    \ and inverse problems involving nonlinear partial differential equations. J.\
    \ Comput. Phys., 378:686-707, 2019. Rauhut, H. and Ward, R. Interpolation via\
    \ weighted \u21131 minimization. Appl. Comput. Harmon. Anal., 40(2):321-351, 2016.\
    \ Tillmann, A. M. and Pfetsch, M. E. The computational complexity of the restricted\
    \ isometry property, the nullspace property, and related concepts in compressed\
    \ sensing. IEEE Trans. Inform. Theory, 60(2):1248-1259, 2014. Traonmilin, Y. and\
    \ Gribonval, R. Stable recovery of low-dimensional cones in Hilbert spaces: one\
    \ RIP to rule them all. Appl. Comput. Harmon. Anal., 45(1):170-205, 2018. Traonmilin,\
    \ Y., Puy, G., Gribonval, R., and Davies, M. E. Compressed sensing in Hilbert\
    \ spaces. In Boche, H., Caire, G., Calderbank, R., M\xE4rz, M., Kutyniok, G.,\
    \ and Mathar, R. (eds.), Compressed Sensing and its Applications: Second International\
    \ MATHEON Conference 2015, Applied and Numerical Harmonic Analysis, pp. 359-384.\
    \ Birkh\xE4user, Cham, 2017. Trunschke, P. Convergence bounds for local least\
    \ squares approximation. arXiv:2208.10954, 2023. van den Berg, E. and Friedlander,\
    \ M. P. Probing the pareto frontier for basis pursuit solutions. SIAM J. Sci.\
    \ Comput., 31(2):890-912, 2008. Vershynin, R. High-Dimensional Probability: An\
    \ Introduction with Applications in Data Science. Cambridge University Press,\
    \ Cambridge, UK, 2018. Woodruff, D. P. Sketching as a tool for numerical linear\
    \ algebra. Foundations and Trends in Theoretical Computer Science, 10(1-2):1-157,\
    \ 2014. Yu, J., Lu, L., Meng, X., and Karniadakis, G. E. Gradient-enhanced physics-informed\
    \ neural networks for forward and inverse PDE problems. Comput. Methods Appl.\
    \ Mech. Engrg., 393:114823, 2022."
  title: A unified framework for learning with nonlinear model classes from arbitrary
    linear samples
  url: https://dl.acm.org/doi/10.5555/3692070.3692077
- abstract: Transformers are the backbone of powerful foundation models for many Vision
    and Natural Language Processing tasks. But their compute and memory/storage footprint
    is large, and so, serving such models is expensive often requiring high-end hardware.
    To mitigate this difficulty, Post-Training Quantization seeks to modify a pretrained
    model and quantize it to eight bits or lower, significantly boosting compute/memory/latency
    efficiency. Such models have been successfully quantized to four bits with some
    performance loss. In this work, we outline a simple scheme to quantize Transformer-based
    models to just two bits (plus some overhead) with only a small drop in accuracy.
    Key to our formulation is a concept borrowed from Harmonic analysis called Fusion
    Frames. Our main finding is that the quantization must take place not in the original
    weight space, but instead in the Fusion Frame representations. If quantization
    is interpreted as the addition of noise, our casting of the problem allows invoking
    an extensive body of known consistent recovery and noise robustness guarantees.
    Further, if desired, de-noising filters are known in closed form. We show empirically,
    via a variety of experiments, that (almost) two-bit quantization for Transformer
    models promises sizable efficiency gains. The code is available at https://github.com/vsingh-group/FrameQuant
  keywords: Computer systems organization, Architectures, Other architectures, Computing
    methodologies, Artificial intelligence, Computer vision, Computer vision representations,
    Image representations, Computer vision tasks, Activity recognition and understanding,
    Machine learning, Learning paradigms, Supervised learning, Supervised learning
    by regression, Theory of computation, Theory and algorithms for application domains,
    Machine learning theory, Kernel methods
  references: "Banner, R., Nahshan, Y., and Soudry, D. Post training 4-bit quantization\
    \ of convolutional networks for rapid-deployment. Advances in Neural Information\
    \ Processing Systems, 32, 2019. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.\
    \ Piqa: Reasoning about physical commonsense in natural language. In Proceedings\
    \ of the AAAI conference on artificial intelligence, 2020. Boufounos, P., Kutyniok,\
    \ G., and Rauhut, H. Compressed sensing for fusion frames. Proceedings of SPIE\
    \ - The International Society for Optical Engineering, 10 2009. Casazza, P. G.\
    \ and Kova\u010Devi\u0107, J. Equal-norm tight frames with erasures. Advances\
    \ in Computational Mathematics, 18, 2003. Casazza, P. G. and Kutyniok, G. Finite\
    \ frames, theory and applications, 2012. URL https://link.springer.com/book/10.1007/978-0-8176-8373-3.\
    \ Casazza, P. G., Kutyniok, G., and Li, S. Fusion frames and distributed processing.\
    \ Applied and computational harmonic analysis, 25(1), 2008. Casazza, P. G., Fickus,\
    \ M., Mixon, D. G., Wang, Y., and Zhou, Z. Constructing tight fusion frames. Applied\
    \ and Computational Harmonic Analysis, 30(2), 2011. ISSN 1063-5203. URL https://www.sciencedirect.com/science/article/pii/S1063520310000850.\
    \ Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked\
    \ generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer\
    \ Vision and Pattern Recognition (CVPR), June 2022. Chee, J., Cai, Y., Kuleshov,\
    \ V., and Sa, C. D. QuIP: 2-bit quantization of large language models with guarantees.\
    \ In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\
    \ URL https://openreview.net/forum?id=xrk9g5vcXR. Chen, S. and Zhao, Q. Shallowing\
    \ deep networks: Layer-wise pruning based on feature representations. IEEE Transactions\
    \ on Pattern Analysis and Machine Intelligence, 41(12):3048-3056, 2019. 2018.2874634.\
    \ Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., and Girdhar, R. Masked-attention\
    \ mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF\
    \ conference on computer vision and pattern recognition, 2022. Christensen, O.\
    \ An introduction to frames and riesz bases, 2018. URL https://link.springer.com/book/10.1007/978-3-319-25613-9.\
    \ Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,\
    \ K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv\
    \ preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T.,\
    \ Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question\
    \ answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457,\
    \ 2018. Dao, T. fast-hadamard-transform, 2023. URL https://github.com/Dao-AILab/fast-hadamard-transform.\
    \ Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet:\
    \ A large-scale hierarchical image database. In 2009 IEEE conference on computer\
    \ vision and pattern recognition, 2009. Ding, Y., Qin, H., Yan, Q., Chai, Z.,\
    \ Liu, J., Wei, X., and Liu, X. Towards accurate post-training quantization for\
    \ vision transformer. In Proceedings of the 30th ACM International Conference\
    \ on Multimedia, MM '22, New York, NY, USA, 2022. ISBN 9781450392037. Donoho,\
    \ D., Vetterli, M., DeVore, R., and Daubechies, I. Data compression and harmonic\
    \ analysis. IEEE Transactions on Information Theory, 44(6), 1998. Dosovitskiy,\
    \ A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\
    \ M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An\
    \ image is worth 16\xD716 words: Transformers for image recognition at scale.\
    \ In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\
    \ Eldar, Y. C. and Michaeli, T. Beyond bandlimited sampling: Nonlinearities, smoothness\
    \ and sparsity. ArXiv, abs/0812.3066, 2008. URL https://api.semanticscholar.org/CorpusID:8702589.\
    \ Fickus, M., Iverson, J. W., Jasper, J., and Mixon, D. G. Harmonic grassmannian\
    \ codes. Applied and Computational Harmonic Analysis, 65, 2023. ISSN 1063-5203.\
    \ URL https://www.sciencedirect.com/science/article/pii/S1063520323000106. Frantar,\
    \ E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training\
    \ quantization and pruning. Advances in Neural Information Processing Systems,\
    \ 35, 2022. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate\
    \ quantization for generative pretrained transformers. In The Eleventh International\
    \ Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.\
    \ Gao, L., Tow, J., Abbasi, B., et al. A framework for few-shot language model\
    \ evaluation, 2023. URL https://zenodo.org/records/10256836. Gerganov, G. llama.cpp,\
    \ 2023. URL https://github.com/ggerganov/llama.cpp. Gholami, A., Kim, S., Dong,\
    \ Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods\
    \ for efficient neural network inference. In Low-Power Computer Vision. Chapman\
    \ and Hall/CRC, 2022. Goyal, V., Vetterli, M., and Thao, N. Quantized overcomplete\
    \ expansions in Rn: analysis, synthesis, and algorithms. IEEE Transactions on\
    \ Information Theory, 44(1), 1998. Goyal, V. K., Kova\u010Devi\u0107, J., and\
    \ Kelner, J. A. Quantized frame expansions with erasures. Applied and Computational\
    \ Harmonic Analysis, 10(3), 2001. ISSN 1063-5203. URL https://www.sciencedirect.com/science/article/pii/S1063520300903403.\
    \ Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural\
    \ network with pruning, trained quantization and huffman coding. In Bengio, Y.\
    \ and Le-Cun, Y. (eds.), 4th International Conference on Learning Representations,\
    \ ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings,\
    \ 2016. URLhttp://arxiv.org/abs/1510.00149. Hassibi, B., Stork, D. G., and Wolff,\
    \ G. J. Optimal brain surgeon and general network pruning. In IEEE international\
    \ conference on neural networks. IEEE, 1993. Hinton, G., Vinyals, O., and Dean,\
    \ J. Distilling the knowledge in a neural network, 2015. Hudson, D. A. and Zitnick,\
    \ L. Generative adversarial transformers. In Meila, M. and Zhang, T. (eds.), Proceedings\
    \ of the 38th International Conference on Machine Learning, volume 139 of Proceedings\
    \ of Machine Learning Research. PMLR, 18-24 Jul 2021. Kaplan, J., McCandlish,\
    \ S., Henighan, T., et al. Scaling laws for neural language models, 2020. Kutyniok,\
    \ G., Pezeshki, A., Calderbank, R., and Liu, T. Robust dimension reduction, fusion\
    \ frames, and grassmannian packings. Applied and Computational Harmonic Analysis,\
    \ 26(1):64-76, 2009. ISSN 1063-5203. URL https://www.sciencedirect.com/science/article/pii/S1063520308000249.\
    \ Le, Q., Sarl\xF3s, T., Smola, A., et al. Fastfood-approximating kernel expansions\
    \ in loglinear time. In Proceedings of the international conference on machine\
    \ learning, 2013. Lecun, Y., Denker, J., and Solla, S. Optimal brain damage. In\
    \ Advances in Neural Information Processing Systems, volume 2, 01 1989. Li, Z.,\
    \ Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale reparameterization for post-training\
    \ quantization of vision transformers. In Proceedings of the IEEE/CVF International\
    \ Conference on Computer Vision, 2023. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei,\
    \ Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer\
    \ using shifted windows. In Proceedings of the IEEE/CVF international conference\
    \ on computer vision, 2021a. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., and\
    \ Gao, W. Post-training quantization for vision transformer. Advances in Neural\
    \ Information Processing Systems, 34, 2021b. Merity, S., Xiong, C., Bradbury,\
    \ J., and Socher, R. Pointer sentinel mixture models. In International Conference\
    \ on Learning Representations, 2017. URL https://openreview.net/forum?id=Byj72udxe.\
    \ Nagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization\
    \ through weight equalization and bias correction. In Proceedings of the IEEE/CVF\
    \ International Conference on Computer Vision, 2019. Nagel, M., Amjad, R. A.,\
    \ Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding\
    \ for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings\
    \ of the 37th International Conference on Machine Learning, volume 119 of Proceedings\
    \ of Machine Learning Research. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/nagel20a.html.\
    \ Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y., van Baalen, M., and\
    \ Blankevoort, T. A white paper on neural network quantization. ArXiv, abs/2106.08295,\
    \ 2021. URL https://api.semanticscholar.org/CorpusID:235435934. Namburi, S. S.\
    \ S., Sreedhar, M., Srinivasan, S., et al. The cost of compression: Investigating\
    \ the impact of compression on parametric knowledge in language models. In Findings\
    \ of the Association for Computational Linguistics: EMNLP 2023. Association for\
    \ Computational Linguistics, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee,\
    \ K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits\
    \ of transfer learning with a unified text-to-text transformer. Journal of Machine\
    \ Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\
    \ Ranftl, R., Bochkovskiy, A., and Koltun, V. Vision transformers for dense prediction.\
    \ In Proceedings of the IEEE/CVF international conference on computer vision,\
    \ 2021. Rokh, B., Azarpeyvand, A., and Khanteymoori, A. A comprehensive survey\
    \ on model quantization for deep neural networks in image classification. ACM\
    \ Transactions on Intelligent Systems and Technology, 14(6):1-50, November 2023.\
    \ ISSN 2157-6912. Rozell, C. and Johnson, D. Analysis of noise reduction in redundant\
    \ expansions under distributed processing requirements. In Proceedings. (ICASSP\
    \ '05). IEEE International Conference on Acoustics, Speech, and Signal Processing,\
    \ 2005., volume 4, 04 2005. ISBN 0-7803-8874-7. Sakaguchi, K., Bras, R. L., Bhagavatula,\
    \ C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale.\
    \ Communications of the ACM, 64(9):99-106, 2021. Scao, T. L., Fan, A., et al.\
    \ Bloom: A 176b-parameter open-access multilingual language model, 2023. Strohmer,\
    \ T. and Heath Jr, R. W. Grassmannian frames with applications to coding and communication.\
    \ Applied and computational harmonic analysis, 14(3), 2003. Touvron, H., Cord,\
    \ M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient\
    \ image transformers; distillation through attention. In International Conference\
    \ on Machine Learning, volume 139, July 2021. Touvron, H., Cord, M., and J\xE9\
    gou, H. Deit iii: Revenge of the vit. In European Conference on Computer Vision.\
    \ Springer, 2022. Touvron, H., Martin, L., Stone, K., et al. Llama 2: Open foundation\
    \ and fine-tuned chat models, 2023. Waldron, S. F. D. An introduction to finite\
    \ tight frames, 2019. URL https://link.springer.com/book/10.1007/978-0-8176-4815-2.\
    \ Wightman, R. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\
    \ 2019. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant:\
    \ Accurate and efficient post-training quantization for large language models.\
    \ In Proceedings of the 40th International Conference on Machine Learning, 2023.\
    \ Yao, Z., Yazdani Aminabadi, R., Zhang, M., et al. Zero-quant: Efficient and\
    \ affordable post-training quantization for large-scale transformers. In Advances\
    \ in Neural Information Processing Systems, 2022. Yu, D., Seide, F., Li, G., and\
    \ Deng, L. Exploiting sparseness in deep neural networks for large vocabulary\
    \ speech recognition. In 2012 IEEE International Conference on Acoustics, Speech\
    \ and Signal Processing (ICASSP), pp. 4409-4412, 2012. Yuan, Z., Xue, C., Chen,\
    \ Y., Wu, Q., and Sun, G. Ptq4vit: Post-training quantization for vision transformers\
    \ with twin uniform quantization. In European Conference on Computer Vision, 2022.\
    \ Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar,\
    \ S. O (n) connections are expressive enough: Universal approximability of sparse\
    \ transformers. Advances in Neural Information Processing Systems, 33:13783-13794,\
    \ 2020. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag:\
    \ Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,\
    \ 2019. Zeng, Z., Davies, M., Pulijala, P., et al. Lookupffn: making transformers\
    \ compute-lite for cpu inference. In Proceedings of the 40th International Conference\
    \ on Machine Learning, ICML'23. JMLR.org, 2023. Zhai, X., Kolesnikov, A., Houlsby,\
    \ N., and Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF\
    \ Conference on Computer Vision and Pattern Recognition, 2022. Zhang, B., Haddow,\
    \ B., and Birch, A. Prompting large language model for machine translation: A\
    \ case study. In Proceedings of the 40th International Conference on Machine Learning,\
    \ ICML'23, 2023. Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.\
    \ M., and Shum, H.-Y. Dino: Detr with improved denoising anchor boxes for end-to-end\
    \ object detection, 2022a. Zhang, S., Roller, S., Goyal, N., et al. Opt: Open\
    \ pretrained transformer language models, 2022b. Zhu, Z., Hong, J., and Zhou,\
    \ J. Data-free knowledge distillation for heterogeneous federated learning. In\
    \ International conference on machine learning, pp. 12878-12889. PMLR, 2021."
  title: 'FrameQuant: flexible low-bit quantization for transformers'
  url: https://dl.acm.org/doi/10.5555/3692070.3692078
- abstract: Training reinforcement learning (RL) agents directly from high-dimensional
    image observations continues to be a challenging problem. Recent line of work
    on behavioral distances proposes to learn representations that encode behavioral
    similarities quantified by the bisimulation metric. By learning an isometric mapping
    to a lower dimensional space that preserves this metric, such methods attempt
    to learn representations that group together functionally similar states. However,
    such an isometric mapping may not exist, making the learning objective ill-defined.
    We propose an alternative objective that allows distortions in long-range distances,
    while preserving local metric structure - inducing representations that highlight
    natural clusters in the state space. This leads to new representations, which
    we term Behavioral Eigenmaps (BeigeMaps), corresponding to the eigenfunctions
    of similarity kernels induced by behavioral distances. We empirically demonstrate
    that when added as a drop-in modification, BeigeMaps improve the policy performance
    of prior behavioral distance based RL algorithms.
  keywords: Computing methodologies, Artificial intelligence, Computer vision, Computer
    vision representations, Image representations, Machine learning, Learning paradigms,
    Unsupervised learning, Cluster analysis, Machine learning approaches, Neural networks
  references: "Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare,\
    \ M. Deep reinforcement learning at the edge of the statistical precipice. Advances\
    \ in neural information processing systems, 34:29304-29320, 2021. Badia, A. P.,\
    \ Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, Z. D., and Blundell,\
    \ C. Agent57: Outperforming the atari human benchmark. In International conference\
    \ on machine learning, pp. 507-517. PMLR, 2020. Baker, B., Akkaya, I., Zhokov,\
    \ P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune,\
    \ J. Video pretraining (vpt): Learning to act by watching unlabeled online videos.\
    \ Advances in Neural Information Processing Systems, 35:24639-24654, 2022. Belkin,\
    \ M. and Niyogi, P. Laplacian eigenmaps and spectral techniques for embedding\
    \ and clustering. Advances in neural information processing systems, 14, 2001.\
    \ Bourgain, J. On lipschitz embedding of finite metric spaces in hilbert space.\
    \ Israel Journal of Mathematics, 52:46-52, 1985. Castro, P. S. Scalable methods\
    \ for computing state similarity in deterministic markov decision processes. In\
    \ Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.\
    \ 10069-10076, 2020. Castro, P. S., Kastner, T., Panangaden, P., and Rowland,\
    \ M. Mico: Improved representations via sampling-based state similarity for markov\
    \ decision processes. In Neural Information Processing Systems, 2021. Castro,\
    \ P. S., Kastner, T., Panangaden, P., and Rowland, M. A kernel perspective on\
    \ behavioural metrics for markov decision processes. Transactions on Machine Learning\
    \ Research, 2023. Chen, J. and Pan, S. Learning representations via a robust behavioral\
    \ metric for deep reinforcement learning. Advances in Neural Information Processing\
    \ Systems, 35: 36654-36666, 2022. Comanici, G. and Precup, D. Basis function discovery\
    \ using spectral clustering and bisimulation metrics. In InternationalWorkshop\
    \ on Adaptive and Learning Agents, pp. 85-99. Springer, 2011. Dayan, P. Improving\
    \ generalization for temporal difference learning: The successor representation.\
    \ Neural computation, 5(4):613-624, 1993. Deng, Z. and Luo, Y. Learning neural\
    \ eigenfunctions for unsupervised semantic segmentation. Proceedings of the IEEE/CVF\
    \ International Conference on Computer Vision, pp. 551-561, 2023. Deng, Z., Shi,\
    \ J., Zhang, H., Cui, P., Lu, C., and Zhu, J. Neural eigenfunctions are structured\
    \ representation learners. ArXiv, abs/2210.12637, 2022a. Deng, Z., Shi, J., and\
    \ Zhu, J. Neuralef: Deconstructing kernels by deep neural networks. In International\
    \ Conference on Machine Learning, pp. 4976-4992. PMLR, 2022b. Duan, Y., Ke, T.,\
    \ and Wang, M. State aggregation learning from markov transition data. Advances\
    \ in Neural Information Processing Systems, 32, 2019. Efroni, Y., Misra, D., Krishnamurthy,\
    \ A., Agarwal, A., and Langford, J. Provably filtering exogenous distractors using\
    \ multistep inverse dynamics. In International Conference on Learning Representations,\
    \ 2021. Ferns, N. and Precup, D. Bisimulation metrics are optimal value functions.\
    \ In UAI Conference on Uncertainty in Artificial Intelligence, pp. 210-219, 2014.\
    \ Ferns, N., Panangaden, P., and Precup, D. Metrics for finite markov decision\
    \ processes. In AAAI Conference on Artificial Intelligence, 2004. Ferns, N., Panangaden,\
    \ P., and Precup, D. Bisimulation metrics for continuous markov decision processes.\
    \ SIAM J. Comput., 40:1662-1714, 2011. Fu, X., Yang, G., Agrawal, P., and Jaakkola,\
    \ T. Learning task informed abstractions. In International Conference on Machine\
    \ Learning, pp. 3480-3491. PMLR, 2021. Garreau, D., Jitkrittum, W., and Kanagawa,\
    \ M. Large sample analysis of the median heuristic. arXiv: Statistics Theory,\
    \ 2017. Ghosh, D. and Bellemare, M. G. Representations for stable off-policy reinforcement\
    \ learning. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International\
    \ Conference on Machine Learning, volume 119 of Proceedings of Machine Learning\
    \ Research, pp. 3556-3565. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/ghosh20b.html.\
    \ Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy\
    \ maximum entropy deep reinforcement learning with a stochastic actor. In International\
    \ conference on machine learning, pp. 1861-1870. PMLR, 2018. Hafner, D., Lillicrap,\
    \ T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination.\
    \ In International Conference on Learning Representations, 2019a. Hafner, D.,\
    \ Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J.\
    \ Learning latent dynamics for planning from pixels. In International conference\
    \ on machine learning, pp. 2555-2565. PMLR, 2019b. Jaderberg, M., Mnih, V., Czarnecki,\
    \ W. M., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforcement\
    \ learning with unsupervised auxiliary tasks. In International Conference on Learning\
    \ Representations, 2016. Kemertas, M. and Aumentado-Armstrong, T. Towards robust\
    \ bisimulation metric learning. Advances in Neural Information Processing Systems,\
    \ 34:4764-4777, 2021. Kemertas, M. and Jepson, A. Approximate policy iteration\
    \ with bisimulation metrics. Transactions on Machine Learning Research, 2022.\
    \ Lamb, A., Islam, R., Efroni, Y., Didolkar, A. R., Misra, D., Foster, D. J.,\
    \ Molu, L. P., Chari, R., Krishnamurthy, A., and Langford, J. Guaranteed discovery\
    \ of control-endogenous latent states with multi-step inverse models. Transactions\
    \ on Machine Learning Research, 2022. Lange, S. and Riedmiller, M. Deep auto-encoder\
    \ neural networks in reinforcement learning. In The 2010 international joint conference\
    \ on neural networks (IJCNN), pp. 1-8. IEEE, 2010. Lange, S., Riedmiller, M.,\
    \ and Voigtl\xE4nder, A. Autonomous reinforcement learning on raw visual input\
    \ data in a real world application. In The 2012 international joint conference\
    \ on neural networks (IJCNN), pp. 1-8. IEEE, 2012. Laskin, M., Lee, K., Stooke,\
    \ A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented\
    \ data. Advances in neural information processing systems, 33: 19884-19895, 2020a.\
    \ Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations\
    \ for reinforcement learning. In International Conference on Machine Learning,\
    \ pp. 5639-5650. PMLR, 2020b. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine,\
    \ S. Stochastic latent actor-critic: Deep reinforcement learning with a latent\
    \ variable model. Advances in Neural Information Processing Systems, 33:741-752,\
    \ 2020. Linial, N., London, E., and Rabinovich, Y. The geometry of graphs and\
    \ some of its algorithmic applications. Combinatorica, 15:215-245, 1995. Liu,\
    \ Q., Zhou, Q., Yang, R., and Wang, J. Robust representation learning by clustering\
    \ with bisimulation metrics for visual reinforcement learning with distractions.\
    \ In AAAI Conference on Artificial Intelligence, 2023. Machado, M. C., Bellemare,\
    \ M. G., and Bowling, M. A laplacian framework for option discovery in reinforcement\
    \ learning. In International Conference on Machine Learning, pp. 2295-2304. PMLR,\
    \ 2017a. Machado, M. C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., and Campbell,\
    \ M. Eigenoption discovery through the deep successor representation. In ICLR\
    \ International Conference on Learning Representations, 2017b. Mahadevan, S. and\
    \ Maggioni, M. Proto-value functions: A laplacian framework for learning representation\
    \ and control in markov decision processes. Journal of Machine Learning Research,\
    \ 8(10), 2007. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\
    \ Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G.,\
    \ et al. Human-level control through deep reinforcement learning.nature,518(7540):\
    \ 529-533,2015. Ng, A., Jordan, M., and Weiss, Y. On spectral clustering: Analysis\
    \ and an algorithm. Advances in neural information processing systems, 14, 2001.\
    \ Nystr\xF6m, E. J. \xDC ber die praktische aufl\xF6sung von integralgleichungen\
    \ mit anwendungen auf randwertaufgaben. 1930. Oord, A. v. d., Li, Y., and Vinyals,\
    \ O. Representation learning with contrastive predictive coding. arXiv preprint\
    \ arXiv:1807.03748, 2018. Pathak, D., Agrawal, P., Efros, A. A., and Darrell,\
    \ T. Curiosity-driven exploration by self-supervised prediction. In International\
    \ conference on machine learning, pp. 2778-2787. PMLR, 2017. Petrik, M. An analysis\
    \ of laplacian methods for value function approximation in mdps. In IJCAI, pp.\
    \ 2574-2579, 2007. Peyr\xE9, G. and Cuturi, M. Computational optimal transport.\
    \ Found. Trends Mach. Learn., 11:355-607, 2018. Pfau, D., Petersen, S., Agarwal,\
    \ A., Barrett, D. G. T., and Stachenfeld, K. L. Spectral inference networks: Unifying\
    \ deep and spectral learning. In International Conference on Learning Representations,\
    \ 2018. Rahimi, A. and Recht, B. Random features for large-scale kernel machines.\
    \ In Neural Information Processing Systems, 2007. Ren, T., Zhang, T., Lee, L.,\
    \ Gonzalez, J. E., Schuurmans, D., and Dai, B. Spectral decomposition representation\
    \ for reinforcement learning. In ICLR International Conference on Learning Representations,\
    \ 2023. Shaham, U., Stanton, K. P., Li, H., Nadler, B., Basri, R., and Kluger,\
    \ Y. Spectralnet: Spectral clustering using deep neural networks. In ICLR International\
    \ Conference on Learning Representations, 2018. Shi, J. and Malik, J. Normalized\
    \ cuts and image segmentation. Proceedings of IEEE Computer Society Conference\
    \ on Computer Vision and Pattern Recognition, pp. 731-737, 1997. Smola, A. J.\
    \ and Kondor, R. Kernels and regularization on graphs. In Learning Theory and\
    \ Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop,\
    \ COLT/Kernel 2003, pp. 144-158. Springer, 2003. Stachenfeld, K. L., Botvinick,\
    \ M., and Gershman, S. J. Design principles of the hippocampal cognitive map.\
    \ Advances in neural information processing systems, 27, 2014. Stooke, A., Lee,\
    \ K., Abbeel, P., and Laskin, M. Decoupling representation learning from reinforcement\
    \ learning. In International Conference on Machine Learning, pp. 9870-9879. PMLR,\
    \ 2021. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D.,\
    \ Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T. P., and\
    \ Riedmiller, M. A. Deepmind control suite. ArXiv, abs/1801.00690, 2018. Tenenbaum,\
    \ J. B., Silva, V. d., and Langford, J. C. A global geometric framework for nonlinear\
    \ dimensionality reduction. Science, 290(5500):2319-2323, 2000. Van Hasselt, H.,\
    \ Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement\
    \ learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018. Varadhan,\
    \ S. R. S. On the behavior of the fundamental solution of the heat equation with\
    \ variable coefficients. Communications on Pure and Applied Mathematics, 20 (2):431-455,\
    \ 1967. Von Luxburg, U. A tutorial on spectral clustering. Statistics and computing,\
    \ 17:395-416, 2007. Wang, K., Zhou, K., Zhang, Q., Shao, J., Hooi, B., and Feng,\
    \ J. Towards better laplacian representation in reinforcement learning with generalized\
    \ graph drawing. In International Conference on Machine Learning, pp. 11003-11012.\
    \ PMLR, 2021. Wang, T., Du, S., Torralba, A., Isola, P., Zhang, A., and Tian,\
    \ Y. Denoised mdps: Learning world models better than the world itself. In International\
    \ Conference on Machine Learning, pp. 22591-22612. PMLR, 2022. Williams, C. K.\
    \ I. and Seeger, M. W. Using the nystr\xF6m method to speed up kernel machines.\
    \ In Neural Information Processing Systems, 2000. Wu, Y., Tucker, G., and Nachum,\
    \ O. The laplacian in rl: Learning representations with efficient approximations.\
    \ In ICLR International Conference on Learning Representations, 2019. Yarats,\
    \ D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving\
    \ sample efficiency in model-free reinforcement learning from images. In AAAI\
    \ Conference on Artificial Intelligence, 2019. Yarats, D., Fergus, R., and Kostrikov,\
    \ I. Image augmentation is all you need: Regularizing deep reinforcement learning\
    \ from pixels. In 9th International Conference on Learning Representations, ICLR\
    \ 2021, 2021a. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Reinforcement\
    \ learning with prototypical representations. In International Conference on Machine\
    \ Learning, pp. 11920-11931. PMLR, 2021b. Yen-Chen, L., Zeng, A., Song, S., Isola,\
    \ P., and Lin, T.-Y. Learning to see before learning to act: Visual pretraining\
    \ for manipulation. In 2020 IEEE International Conference on Robotics and Automation\
    \ (ICRA), pp. 7286-7293. IEEE, 2020. Zhang, A., McAllister, R. T., Calandra, R.,\
    \ Gal, Y., and Levine, S. Learning invariant representations for reinforcement\
    \ learning without reconstruction. International Conference on Learning Representations,\
    \ 2020. Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D., and Dai, B.\
    \ Making linear mdps practical via contrastive representation learning. In International\
    \ Conference on Machine Learning, pp. 26447-26466. PMLR, 2022."
  title: 'BeigeMaps: behavioral eigenmaps for reinforcement learning from images'
  url: https://dl.acm.org/doi/10.5555/3692070.3692079
- abstract: The question-answering (QA) capabilities of foundation models are highly
    sensitive to prompt variations, rendering their performance susceptible to superficial,
    non-meaning-altering changes. This vulnerability often stems from the model's
    preference or bias towards specific input characteristics, such as option position
    or superficial image features in multi-modal settings. We propose to rectify this
    bias directly in the model's internal representation. Our approach, STEERFAIR,
    finds the bias direction in the model's representation space and steers activation
    values away from it during inference. Specifically, we exploit the observation
    that bias often adheres to simple association rules, such as the spurious association
    between the first option and correctness likelihood. Next, we construct demonstrations
    of these rules from unlabeled samples and use them to identify the bias directions.
    We empirically show that STEERFAIR significantly reduces instruction-tuned model
    performance variance across prompt modifications on three benchmark tasks. Remarkably,
    our approach surpasses a supervised baseline with 100 labels by an average of
    10.86% accuracy points and 12.95 score points and matches the performance with
    500 labels.
  keywords: Computing methodologies, Artificial intelligence, Natural language processing,
    Discourse, dialogue and pragmatics, Natural language generation, Machine learning,
    Learning settings, Information systems, Information retrieval, Retrieval tasks
    and goals, Question answering
  references: "Adila, D., Shin, C., Cai, L., and Sala, F. Zero-shot robustification\
    \ of zero-shot models with foundation models. arXiv preprint arXiv:2309.04344,\
    \ 2023. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,\
    \ Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual\
    \ language model for few-shot learning. Advances in Neural Information Processing\
    \ Systems, 35:23716-23736, 2022. Burns, C., Ye, H., Klein, D., and Steinhardt,\
    \ J. Discovering latent knowledge in language models without supervision. arXiv\
    \ preprint arXiv:2212.03827, 2022. Caliskan, A., Bryson, J. J., and Narayanan,\
    \ A. Semantics derived automatically from language corpora contain human-like\
    \ biases. Science, 356(6334): 183-186, 2017. Chowdhery, A., Narang, S., Devlin,\
    \ J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\
    \ Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal\
    \ of Machine Learning Research, 24(240):1-113, 2023. Dai, W., Li, J., Li, D.,\
    \ Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\
    \ general-purpose vision-language models with instruction tuning. arxiv 2023.\
    \ arXiv preprint arXiv:2305.06500. Elhage, N., Nanda, N., Olsson, C., Henighan,\
    \ T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma,\
    \ N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion,\
    \ J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish,\
    \ S., and Olah, C. A mathematical framework for transformer circuits. Transformer\
    \ Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.\
    \ Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X.,\
    \ Li, K., Sun, X., et al. Mme: A comprehensive evaluation benchmark for multimodal\
    \ large language models. arXiv preprint arXiv:2306.13394, 2023. Gurnee, W., Nanda,\
    \ N., Pauly, M., et al. Finding neurons in a haystack: Case studies with sparse\
    \ probing, may 2023. URL http://arxiv. org/abs/2305.01610.\u2192p, 9. Khosla,\
    \ A., Zhou, T., Malisiewicz, T., Efros, A. A., and Torralba, A. Undoing the damage\
    \ of dataset bias. In Computer Vision-ECCV 2012: 12th European Conference on Computer\
    \ Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12, pp. 158-171.\
    \ Springer, 2012. Lauren\xE7on, H., Saulnier, L., Tronchon, L., Bekman, S., Singh,\
    \ A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A. M., Kiela, D., et al. Obelisc:\
    \ An open web-scale filtered dataset of interleaved image-text documents. arXiv\
    \ preprint arXiv:2306.16527, 2023. Li, K., Hopkins, A. K., Bau, D., Vi\xE9gas,\
    \ F., Pfister, H., and Wattenberg, M. Emergent world representations: Exploring\
    \ a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382,\
    \ 2022. Li, K., Patel, O., Vi\xE9gas, F., Pfister, H., and Wattenberg, M. Inference-time\
    \ intervention: Eliciting truthful answers from a language model, 2023. Liang,\
    \ W., Tadesse, G. A., Ho, D., Fei-Fei, L., Zaharia, M., Zhang, C., and Zou, J.\
    \ Advances, challenges and opportunities in creating data for trustworthy ai.\
    \ Nature Machine Intelligence, 4(8):669-677, 2022. Lin, T., Maire, M., Belongie,\
    \ S. J., Bourdev, L. D., Girshick, R. B., Hays, J., Perona, P., Ramanan, D., Doll'a\
    \ r, P., and Zitnick, C. L. Microsoft COCO: common objects in context. CoRR, abs/1405.0312,\
    \ 2014. URL http://arxiv.org/abs/1405.0312. Liu, H., Li, C., Li, Y., and Lee,\
    \ Y. J. Improved baselines with visual instruction tuning, 2023a. Liu, H., Li,\
    \ C., Wu, Q., and Lee, Y. J. Visual instruction tuning, 2023b. Liu, S., Xing,\
    \ L., and Zou, J. In-context vectors: Making in context learning more effective\
    \ and controllable through latent space steering. arXiv preprint arXiv:2311.06668,\
    \ 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord,\
    \ O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought\
    \ chains for science question answering. In The 36th Conference on Neural Information\
    \ Processing Systems (NeurIPS), 2022. Moschella, L., Maiorca, V., Fumero, M.,\
    \ Norelli, A., Locatello, F., and Rodola, E. Relative representations enable zero-shot\
    \ latent space communication. arXiv preprint arXiv:2209.15430, 2022. Pezeshkpour,\
    \ P. and Hruschka, E. Large language models sensitivity to the order of options\
    \ in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023. Shi, B.,\
    \ Gai, S., Darrell, T., and Wang, X. Refocusing is key to transfer learning. arXiv\
    \ preprint arXiv:2305.15542, 2023. Subramani, N., Suresh, N., and Peters, M. E.\
    \ Extracting latent steering vectors from pretrained language models. arXiv preprint\
    \ arXiv:2205.05124, 2022. Torralba, A. and Efros, A. A. Unbiased look at dataset\
    \ bias. In CVPR 2011, pp. 1521-1528, 2011. Vaswani, A., Shazeer, N., Parmar, N.,\
    \ Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I.\
    \ Attention is all you need. Advances in neural information processing systems,\
    \ 30, 2017. Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu,\
    \ T., and Sui, Z. Large language models are not fair evaluators. arXiv preprint\
    \ arXiv:2305.17926, 2023. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,\
    \ C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,\
    \ S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,\
    \ S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural\
    \ language processing. In Liu, Q. and Schlangen, D. (eds.), Proceedings of the\
    \ 2020 Conference on Empirical Methods in Natural Language Processing: System\
    \ Demonstrations, pp. 38-45, Online, October 2020. Association for Computational\
    \ Linguistics. Yang, Y., Nushi, B., Palangi, H., and Mirzasoleiman, B. Mitigating\
    \ spurious correlations in multi-modal models during fine-tuning. arXiv preprint\
    \ arXiv:2304.03916, 2023. Zhang, M. and R\xE9, C. Contrastive adapters for foundation\
    \ model group robustness. Advances in Neural Information Processing Systems, 35:21682-21697,\
    \ 2022. Zhang, Q., Singh, C., Liu, L., Liu, X., Yu, B., Gao, J., and Zhao, T.\
    \ Tell your model where to attend: Post-hoc attention steering for llms. arXiv\
    \ preprint arXiv:2311.02262, 2023. Zhao, Z., Wallace, E., Feng, S., Klein, D.,\
    \ and Singh, S. Calibrate before use: Improving few-shot performance of language\
    \ models. In International Conference on Machine Learning, pp. 12697-12706. PMLR,\
    \ 2021. Zheng, C., Zhou, H., Meng, F., Zhou, J., and Huang, M. Large language\
    \ models are not robust multiple choice selectors. arXiv e-prints, pp. arXiv-2309,\
    \ 2023a. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y.,\
    \ Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench\
    \ and chatbot arena. arXiv preprint arXiv:2306.05685, 2023b."
  title: 'Discovering bias in latent space: an unsupervised debiasing approach'
  url: https://dl.acm.org/doi/10.5555/3692070.3692080
- abstract: "We investigate coresets for approximating the cost with respect to median\
    \ queries. In this problem, we are given a set of points P \u2282 \u211Dd and\
    \ median queries are \u03A3p\u2208P ||p - c|| for any point c \u2208 \u211Dd.\
    \ Our goal is to compute a small weighted summary S \u2282 P such that the cost\
    \ of any median query is approximated within a multiplicative (1 \xB1 \u03B5)\
    \ factor. We provide matching upper and lower bounds on the number of points contained\
    \ in S of the order $\\tilde{\\Theta}\\left(\\varepsilon^{-d/(d+1)}\\right)$."
  keywords: Information systems, Data management systems, Database management system
    engines, Database query processing, Mathematics of computing, Mathematical analysis,
    Functional analysis, Approximation, Theory of computation, Design and analysis
    of algorithms, Approximation algorithms analysis, Data structures design and analysis,
    Randomness, geometry and discrete structures, Computational geometry, Theory and
    algorithms for application domains, Database theory
  references: "Agarwal, P. K., Har-Peled, S., and Varadarajan, K. R. Geometric approximation\
    \ via coresets. In Combinatorial and computational geometry, MSRI, pp. 1-30. University\
    \ Press, 2005. Alexander, R. Geometric methods in the study of irregularities\
    \ of distribution. Combinatorica, 10:115-136, 1990. Badoiu, M. and Clarkson, K.\
    \ L. Optimal core-sets for balls. Comput. Geom., 40(1):14-22, 2008. Baker, D.\
    \ N., Braverman, V., Huang, L., Jiang, S. H., Krauthgamer, R., and Wu, X. Coresets\
    \ for clustering in graphs of bounded treewidth. In International Conference on\
    \ Machine Learning, ICML, pp. 569-579, 2020. URL http://proceedings.mlr.press/v119/baker20a.html.\
    \ Becchetti, L., Bury, M., Cohen-Addad, V., Grandoni, F., and Schwiegelshohn,\
    \ C. Oblivious dimension reduction for kmeans: beyond subspaces and the johnson-lindenstrauss\
    \ lemma. In Symposium on Theory of Computing, STOC, pp. 1039-1050, 2019. Bentley,\
    \ J. L. and Saxe, J. B. Decomposable searching problems i: Static-to-dynamic transformation.\
    \ J. Algorithms, 1(4):301-358, 1980. Braverman, V., Cohen-Addad, V., Jiang, S.\
    \ H., Krauthgamer, R., Schwiegelshohn, C., Toftrup, M. B., and Wu, X. The power\
    \ of uniform sampling for coresets. In Symposium on Foundations of Computer Science,\
    \ FOCS, pp. 462-473, 2022. Chen, K. On coresets for k-median and k-means clustering\
    \ in metric and Euclidean spaces and their applications. SIAM J. Comput., 39(3):923-947,\
    \ 2009. Cohen, M. B., Lee, Y. T., Miller, G. L., Pachocki, J., and Sidford, A.\
    \ Geometric median in nearly linear time. In Proceedings of the 48th Annual ACM\
    \ SIGACT Symposium on Theory of Computing, STOC, 2016. Cohen-Addad, V., Saulpic,\
    \ D., and Schwiegelshohn, C. Improved coresets and sublinear algorithms for power\
    \ means in euclidean spaces. In Neural Information Processing Systems, NeurIPS,\
    \ pp. 21085-21098, 2021. Cohen-Addad, V., Larsen, K. G., Saulpic, D., and Schwiegelshohn,\
    \ C. Towards optimal lower bounds for k-median and k-means coresets. In Symposium\
    \ on Theory of Computing, STOC, pp. 1038-1051, 2022a. Cohen-Addad, V., Larsen,\
    \ K. G., Saulpic, D., Schwiegelshohn, C., and Sheikh-Omar, O. A. Improved coresets\
    \ for euclidean k-means. In NeurIPS, 2022b. Feldman, D. and Langberg, M. A unified\
    \ framework for approximating and clustering data. In Symposium on Theory of Computing,\
    \ STOC, pp. 569-578, 2011. Feldman, D., Schmidt, M., and Sohler, C. Turning big\
    \ data into tiny data: Constant-size coresets for k-means, PCA and projective\
    \ clustering. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on\
    \ Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013,\
    \ pp. 1434-1453, 2013. Feng, Z., Kacham, P., and Woodruff, D. P. Dimensionality\
    \ reduction for the sum-of-distances metric. In International Conference on Machine\
    \ Learning, ICML, pp. 3220-3229, 2021. URL http://proceedings.mlr.press/v139/feng21a.html.\
    \ Har-Peled, S. and Kushal, A. Smaller coresets for k-median and k-means clustering.\
    \ Discrete & Computational Geometry, 37(1):3-19, 2007. Har-Peled, S. and Mazumdar,\
    \ S. On coresets for k-means and k-median clustering. In Symposium on Theory of\
    \ Computing, STOC, pp. 291-300, 2004. Huang, L. and Vishnoi, N. K. Coresets for\
    \ clustering in euclidean spaces: importance sampling is nearly optimal. In Symposium\
    \ on Theory of Computing, STOC 2020, 2020. Huang, L., Li, J., and Wu, X. Towards\
    \ optimal coreset construction for (k, z)-clustering: Breaking the quadratic dependency\
    \ on k. CoRR, abs/2211.11923, 2022. Huang, L., Huang, R., Huang, Z., and Wu, X.\
    \ On coresets for clustering in small dimensional euclidean spaces. In Krause,\
    \ A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),\
    \ International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,\
    \ Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 13891-13915.\
    \ PMLR, 2023. URL https://proceedings.mlr.press/v202/huang23h.html. Langberg,\
    \ M. and Schulman, L. J. Universal \u03B5-approximators for integrals. In Symposium\
    \ on Discrete Algorithms, SODA, pp. 598-607, 2010. Munteanu, A. and Schwiegelshohn,\
    \ C. Coresets-methods and history: A theoreticians design pattern for approximation\
    \ and streaming algorithms. K\xFCnstliche Intell., 32 (1):37-53, 2018. Sohler,\
    \ C. and Woodruff, D. P. Strong coresets for k-median and subspace approximation:\
    \ Goodbye dimension. In Symposium on Foundations of Computer Science, FOCS, pp.\
    \ 802-813, 2018."
  title: Optimal coresets for low-dimensional geometric median
  url: https://dl.acm.org/doi/10.5555/3692070.3692081
- abstract: EEG-based seizure detection models face challenges in terms of inference
    speed and memory efficiency, limiting their real-time implementation in clinical
    devices. This paper introduces a novel graph-based residual state update mechanism
    (REST) for real-time EEG signal analysis in applications such as epileptic seizure
    detection. By leveraging a combination of graph neural networks and recurrent
    structures, REST efficiently captures both non-Euclidean geometry and temporal
    dependencies within EEG data. Our model demonstrates high accuracy in both seizure
    detection and classification tasks. Notably, REST achieves a remarkable 9-fold
    acceleration in inference speed compared to state-of-the-art models, while simultaneously
    demanding substantially less memory than the smallest model employed for this
    task. These attributes position REST as a promising candidate for real-time implementation
    in clinical devices, such as Responsive Neurostimulation or seizure alert systems.
  keywords: Applied computing, Life and medical sciences, Health care information
    systems, Health informatics, Computer systems organization, Architectures, Other
    architectures, Computing methodologies, Machine learning, Machine learning approaches,
    Neural networks, Hardware, Emerging technologies
  references: "Acharya, J. N., Hani, A. J., Thirumala, P., and Tsuchida, T. N. American\
    \ clinical neurophysiology society guideline 3: a proposal for standard montages\
    \ to be used in clinical eeg. The Neurodiagnostic Journal, 56(4):253-260, 2016.\
    \ Ahmedt-Aristizabal, D., Fernando, T., Denman, S., Petersson, L., Aburn, M. J.,\
    \ and Fookes, C. Neural memory networks for seizure type classification. In 2020\
    \ 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology\
    \ Society (EMBC), pp. 569-575. IEEE, 2020. Asif, U., Roy, S., Tang, J., and Harrer,\
    \ S. Seizurenet: Multi-spectral deep feature learning for seizure type classification.\
    \ In Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-oncology:\
    \ Third International Workshop, MLCN 2020, and Second International Workshop,\
    \ RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4-8,\
    \ 2020, Proceedings 3, pp. 77-87. Springer, 2020. Beghi, E., Giussani, G., Nichols,\
    \ E., Abd-Allah, F., Abdela, J., Abdelalim, A., Abraha, H. N., Adib, M. G., Agrawal,\
    \ S., Alahdab, F., et al. Global, regional, and national burden of epilepsy, 1990-2016:\
    \ a systematic analysis for the global burden of disease study 2016. The Lancet\
    \ Neurology, 18(4):357-375, 2019. Cho, K., Van Merri\xEBnboer, B., Gulcehre, C.,\
    \ Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations\
    \ using rnn encoder-decoder for statistical machine translation. arXiv preprint\
    \ arXiv:1406.1078, 2014. Christou, V., Miltiadous, A., Tsoulos, I., Karvounis,\
    \ E., Tzimourta, K. D., Tsipouras, M. G., Anastasopoulos, N., Tzallas, A. T.,\
    \ and Giannakeas, N. Evaluating the window size's role in automatic eeg epilepsy\
    \ detection. Sensors, 22(23):9233, 2022. Covert, I. C., Krishnan, B., Najm, I.,\
    \ Zhan, J., Shore, M., Hixson, J., and Po, M. J. Temporal graph convolutional\
    \ networks for automatic seizure detection. In Machine Learning for Healthcare\
    \ Conference, pp. 160-180. PMLR, 2019. Fisher, R. S. and Velasco, A. L. Electrical\
    \ brain stimulation for epilepsy. Nature Reviews Neurology, 10(5):261-270, 2014a.\
    \ Fisher, R. S. and Velasco, A. L. Electrical brain stimulation for epilepsy.\
    \ Nature Reviews Neurology, 10(5):261-270, 2014b. Goldberger, A. L., Amaral, L.\
    \ A., Glass, L., Hausdorff, J. M., Ivanov, P. C., Mark, R. G., Mietus, J. E.,\
    \ Moody, G. B., Peng, C.-K., and Stanley, H. E. Physiobank, physiotoolkit, and\
    \ physionet: components of a new research resource for complex physiologic signals.\
    \ circulation, 101(23): e215-e220, 2000. Gotman, J. Automatic seizure detection:\
    \ improvements and evaluation. Electroencephalography and clinical Neurophysiology,\
    \ 76(4):317-324, 1990. Harrer, S., Shah, P., Antony, B., and Hu, J. Artificial\
    \ intelligence for clinical trial design. Trends in pharmacological sciences,\
    \ 40(8):577-591, 2019. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning\
    \ for image recognition. In Proceedings of the IEEE conference on computer vision\
    \ and pattern recognition, pp. 770-778, 2016. Ho, T. K. K. and Armanfard, N. Self-supervised\
    \ learning for anomalous channel detection in eeg graphs: application to seizure\
    \ analysis. In Proceedings of the AAAI Conference on Artificial Intelligence,\
    \ volume 37, pp. 7866-7874, 2023. Hochreiter, S. and Schmidhuber, J. Long short-term\
    \ memory. Neural computation, 9(8):1735-1780, 1997. Ie\u0161mantas, T. and Alzbutas,\
    \ R. Convolutional neural network for detection and classification of seizures\
    \ in clinical data. Medical & Biological Engineering & Computing, 58:1919-1932,\
    \ 2020. Jasper, H. H. Ten-twenty electrode system of the international federation.\
    \ Electroencephalogr Clin Neurophysiol, 10:371-375, 1958. Kingma, D. P. and Ba,\
    \ J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\
    \ 2014. Lee, K., Jeong, H., Kim, S., Yang, D., Kang, H.-C., and Choi, E. Real-time\
    \ seizure detection using eeg: a comprehensive comparison of recent approaches\
    \ under a realistic setting. arXiv preprint arXiv:2201.08780, 2022. Li, Y., Yu,\
    \ R., Shahabi, C., and Liu, Y. Graph convolutional recurrent neural network: Data-driven\
    \ traffic forecasting. arXiv preprint arXiv:1707.01926, 7(8), 2017. Li, Z., Hwang,\
    \ K., Li, K., Wu, J., and Ji, T. Graph-generative neural network for eeg-based\
    \ epileptic seizure detection via discovery of dynamic brain functional connectivity.\
    \ Scientific Reports, 12(1):18998, 2022. Loshchilov, I. and Hutter, F. Sgdr: Stochastic\
    \ gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\
    \ Mordvintsev, A., Randazzo, E., Niklasson, E., and Levin, M. Growing neural cellular\
    \ automata. Distill, 5(2):e23, 2020. Morris, C., Ritzert, M., Fey, M., Hamilton,\
    \ W. L., Lenssen, J. E., Rattan, G., and Grohe, M. Weisfeiler and leman go neural:\
    \ Higher-order graph neural networks. In Proceedings of the AAAI conference on\
    \ artificial intelligence, volume 33, pp. 4602-4609, 2019. Nakkiran, P., Kaplun,\
    \ G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. Deep double descent:\
    \ Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory\
    \ and Experiment, 2021(12):124003, 2021. Obeid, I. and Picone, J. The temple university\
    \ hospital eeg data corpus. Frontiers in neuroscience, 10:196, 2016. O'Shea, A.,\
    \ Lightbody, G., Boylan, G., and Temko, A. Neonatal seizure detection from raw\
    \ multi-channel eeg using a fully convolutional architecture. Neural Networks,\
    \ 123:12-25, 2020. Pajouheshgar, E., Xu, Y., Zhang, T., and S\xFCsstrunk, S. Dynca:\
    \ Real-time dynamic texture synthesis using neural cellular automata. In Proceedings\
    \ of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20742-20751,\
    \ 2023. Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training\
    \ recurrent neural networks. In International conference on machine learning,\
    \ pp. 1310-1318. Pmlr, 2013. Randazzo, E., Mordvintsev, A., Niklasson, E., Levin,\
    \ M., and Greydanus, S. Self-classifying mnist digits. Distill, 5(8):e00027-002,\
    \ 2020. Saab, K., Dunnmon, J., R\xE9, C., Rubin, D., and Lee-Messer, C. Weak supervision\
    \ as an efficient approach for automated seizure detection in electroencephalography.\
    \ NPJ digital medicine, 3(1):59, 2020. Shaeri, M. A., Shin, U., Yadav, A., Caramellino,\
    \ R., Rainer, G., and Shoaran, M. 33.3 mibmi: A 192/512-channel 2.46 mm2 miniaturized\
    \ brain-machine interface chipset enabling 31-class brain-to-text conversion through\
    \ distinctive neural codes. In 2024 IEEE International Solid-State Circuits Conference\
    \ (ISSCC), volume 67, pp. 546-548. IEEE, 2024. Shah, V., Von Weltin, E., Lopez,\
    \ S., McHugh, J. R., Veloso, L., Golmohammadi, M., Obeid, I., and Picone, J. The\
    \ temple university hospital seizure detection corpus. Frontiers in neuroinformatics,\
    \ 12:83, 2018. Shin, U., Ding, C., Zhu, B., Vyza, Y., Trouillet, A., Revol, E.\
    \ C., Lacour, S. P., and Shoaran, M. Neuraltree: A 256-channel 0.227-\xB5j/class\
    \ versatile neural activity classification and closed-loop neuromodulation soc.\
    \ IEEE Journal of Solid-State Circuits, 57(11):3243-3257, 2022. Shoaran, M., Shahshahani,\
    \ M., Farivar, M., Almajano, J., Shahshahani, A., Schmid, A., Bragin, A., Leblebici,\
    \ Y., and Emami, A. A 16-channel 1.1 mm 2 implantable seizure control soc with\
    \ sub-\xB5w/channel consumption and closed-loop stimulation in 0.18 \xB5m cmos.\
    \ In 2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits), pp. 1-2. Ieee, 2016.\
    \ Shoaran, M., Haghi, B. A., Taghavi, M., Farivar, M., and Emami-Neyestanak, A.\
    \ Energy-efficient classification for resource-constrained biomedical applications.\
    \ IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 8(4):693-707,\
    \ 2018. Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., and Vandergheynst,\
    \ P. The emerging field of signal processing on graphs: Extending high-dimensional\
    \ data analysis to networks and other irregular domains. IEEE signal processing\
    \ magazine, 30(3):83-98, 2013. Siddiqui, M. K., Morales-Menendez, R., Huang, X.,\
    \ and Hussain, N. A review of epileptic seizure detection using machine learning\
    \ classifiers. Brain informatics, 7(1): 1-18, 2020. Srivastava, N., Hinton, G.,\
    \ Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to\
    \ prevent neural networks from overfitting. The journal of machine learning research,\
    \ 15(1):1929-1958, 2014. Strein, M., Holton-Burke, J. P., Smith, L. R., and Brophy,\
    \ G. M. Prevention, treatment, and monitoring of seizures in the intensive care\
    \ unit. Journal of Clinical Medicine, 8 (8):1177, 2019. Sun, F. T. and Morrell,\
    \ M. J. The rns system: responsive cortical stimulation for the treatment of refractory\
    \ partial epilepsy. Expert review of medical devices, 11(6):563-572, 2014. Tang,\
    \ S., Dunnmon, J. A., Saab, K., Zhang, X., Huang, Q., Dubost, F., Rubin, D. L.,\
    \ and Lee-Messer, C. Self-supervised graph neural networks for improved electroencephalographic\
    \ seizure analysis. arXiv preprint arXiv:2104.08336, 2021. Thodoroff, P., Pineau,\
    \ J., and Lim, A. Learning robust features using deep learning for automatic seizure\
    \ detection. In Machine learning for healthcare conference, pp. 178-190. PMLR,\
    \ 2016. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\
    \ A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances\
    \ in neural information processing systems, 30, 2017. Voulodimos, A., Doulamis,\
    \ N., Doulamis, A., Protopapadakis, E., et al. Deep learning for computer vision:\
    \ A brief review. Computational intelligence and neuroscience, 2018, 2018. Yan,\
    \ J., Li, J., Xu, H., Yu, Y., and Xu, T. Seizure prediction based on transformer\
    \ using scalp electroencephalogram. Applied Sciences, 12(9):4158, 2022a. Yan,\
    \ J., Li, J., Xu, H., Yu, Y., and Xu, T. Seizure prediction based on transformer\
    \ using scalp electroencephalogram. Applied Sciences, 12(9):4158, 2022b. Zhu,\
    \ B., Farivar, M., and Shoaran, M. Resot: Resource-efficient oblique trees for\
    \ neural signal classification. IEEE Transactions on Biomedical Circuits and Systems,\
    \ 14(4):692-704, 2020. Zhu, B., Shin, U., and Shoaran, M. Closed-loop neural prostheses\
    \ with on-chip intelligence: A review and a low-latency machine learning model\
    \ for brain state detection. IEEE transactions on biomedical circuits and systems,\
    \ 15 (5):877-897, 2021."
  title: 'REST: efficient and accelerated EEG seizure analysis through residual state
    updates'
  url: https://dl.acm.org/doi/10.5555/3692070.3692082
- abstract: "Large Language Models (LLMs) with hundreds of billions of parameters\
    \ have transformed the field of machine learning. However, serving these models\
    \ at inference time is both compute and memory intensive, where a single request\
    \ can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention\
    \ is one of the key components of LLMs, which can account for over 50% of LLMs\
    \ memory and compute requirement. We observe that there is a high amount of redundancy\
    \ across heads on which tokens they pay attention to. Based on this insight, we\
    \ propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount\
    \ of correlation for self-attention at runtime, thus reducing both memory and\
    \ compute. In our experiments, we show that CHAI is able to reduce the memory\
    \ requirements for storing K,V cache by up to 21.4% and inference time latency\
    \ by up to 1.73\xD7 without any fine-tuning required. CHAI achieves this with\
    \ a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B,\
    \ LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets."
  keywords: Computer systems organization, Architectures, Other architectures, Neural
    networks, Computing methodologies, Machine learning, Learning paradigms, Machine
    learning algorithms, Dynamic programming for Markov decision processes, Machine
    learning approaches, Neural networks, General and reference, Cross-computing tools
    and techniques, Estimation, Theory of computation, Theory and algorithms for application
    domains, Machine learning theory
  references: "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time.\
    \ https://github.com/FMInference/DejaVu, 2024. Ainslie, J., Lee-Thorp, J., de\
    \ Jong, M., Zemlyanskiy, Y., Lebr\xF3n, F., and Sanghai, S. Gqa: Training generalized\
    \ multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245,\
    \ 2023. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about\
    \ physical commonsense in natural language. In Proceedings of the AAAI conference\
    \ on artificial intelligence, volume 34, pp. 7432-7439, 2020. Brown, T., Mann,\
    \ B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\
    \ P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances\
    \ in neural information processing systems, 33: 1877-1901, 2020. Chen, T., Frankle,\
    \ J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M. The lottery ticket\
    \ hypothesis for pretrained bert networks. Advances in neural information processing\
    \ systems, 33:15834-15846, 2020a. Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang,\
    \ Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets.\
    \ arXiv preprint arXiv:2101.00063, 2020b. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski,\
    \ T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty\
    \ of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P.,\
    \ Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord,\
    \ O. Think you have solved question answering? try arc, the ai2 reasoning challenge.\
    \ arXiv preprint arXiv:1803.05457, 2018. Dai, S., Genc, H., Venkatesan, R., and\
    \ Khailany, B. Efficient transformer inference with statically structured sparse\
    \ attention. In 2023 60th ACM/IEEE Design Automation Conference (DAC), pp. 1-6.\
    \ IEEE, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism\
    \ and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D.,\
    \ Ermon, S., Rudra, A., and R\xE9, C. Flashattention: Fast and memory-efficient\
    \ exact attention with io-awareness. Advances in Neural Information Processing\
    \ Systems, 35:16344-16359, 2022. Dettmers, T. 8-bit approximations for parallelism\
    \ in deep learning. arXiv preprint arXiv:1511.04561, 2015. Dettmers, T. and Zettlemoyer,\
    \ L. The case for 4-bit precision: k-bit inference scaling laws. In International\
    \ Conference on Machine Learning, pp. 7750-7774. PMLR, 2023. Dettmers, T., Lewis,\
    \ M., Belkada, Y., and Zettlemoyer, L. Llm. int8 (): 8-bit matrix multiplication\
    \ for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. facebookresearch.\
    \ xformers - toolbox to accelerate research on transformers. https://github.com/facebookresearch/xformers,\
    \ 2023. Accessed: December 12, 2023. Fan, A., Grave, E., and Joulin, A. Reducing\
    \ transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556,\
    \ 2019. Fang, C., Zhou, A., and Wang, Z. An algorithm-hardware co-optimized framework\
    \ for accelerating n: M sparse transformers. IEEE Transactions on Very Large Scale\
    \ Integration (VLSI) Systems, 30(11):1573-1586, 2022. Frankle, J. and Carbin,\
    \ M. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\
    \ arXiv preprint arXiv:1803.03635, 2018. Frantar, E., Ashkboos, S., Hoefler, T.,\
    \ and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained\
    \ transformers. arXiv preprint arXiv:2210.17323, 2022. Ham, T. J., Jung, S. J.,\
    \ Kim, S., Oh, Y. H., Park, Y., Song, Y., Park, J.-H., Lee, S., Park, K., Lee,\
    \ J. W., et al. A\u2227 3: Accelerating attention mechanisms in neural networks\
    \ with approximation. In 2020 IEEE International Symposium on High Performance\
    \ Computer Architecture (HPCA), pp. 328-341. IEEE, 2020. Ham, T. J., Lee, Y.,\
    \ Seo, S. H., Kim, S., Choi, H., Jung, S. J., and Lee, J. W. Elsa: Hardware-software\
    \ co-design for efficient, lightweight self-attention mechanism in neural networks.\
    \ In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\
    \ (ISCA), pp. 692-705. IEEE, 2021. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,\
    \ E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,\
    \ A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556,\
    \ 2022. Hooker, S. The hardware lottery. Communications of the ACM, 64(12):58-65,\
    \ 2021. Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A.,\
    \ Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step! outperforming\
    \ larger language models with less training data and smaller model sizes. arXiv\
    \ preprint arXiv:2305.02301, 2023. Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen,\
    \ X., Li, L., Wang, F., and Liu, Q. Tinybert: Distilling bert for natural language\
    \ understanding. arXiv preprint arXiv:1909.10351, 2019. Kaplan, J., McCandlish,\
    \ S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\
    \ Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint\
    \ arXiv:2001.08361, 2020. Kim, S., Gholami, A., Yao, Z., Mahoney, M. W., and Keutzer,\
    \ K. I-bert: Integer-only bert quantization. In International conference on machine\
    \ learning, pp. 5506-5518. PMLR, 2021. Kitaev, N., Kaiser, \u0141., and Levskaya,\
    \ A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\
    \ Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B., Goin,\
    \ M., and Alistarh, D. The optimal bert surgeon: Scalable and accurate second-order\
    \ pruning for large language models. arXiv preprint arXiv:2203.07259, 2022. Lagunas,\
    \ F., Charlaix, E., Sanh, V., and Rush, A. M. Block pruning for faster transformers.\
    \ arXiv preprint arXiv:2109.04838, 2021. Leviathan, Y., Kalman, M., and Matias,\
    \ Y. Fast inference from transformers via speculative decoding. In International\
    \ Conference on Machine Learning, pp. 19274-19286. PMLR, 2023. Liu, Z., Desai,\
    \ A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A.\
    \ Scissorhands: Exploiting the persistence of importance hypothesis for llm kv\
    \ cache compression at test time. arXiv preprint arXiv:2305.17118, 2023a. Liu,\
    \ Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang,\
    \ C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms\
    \ at inference time. In International Conference on Machine Learning, pp. 22137-22176.\
    \ PMLR, 2023b. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better\
    \ than one? Advances in neural information processing systems, 32, 2019. Prasanna,\
    \ S., Rogers, A., and Rumshisky, A. When bert plays the lottery, all tickets are\
    \ winning. arXiv preprint arXiv:2005.00561, 2020. Qin, Y., Wang, Y., Deng, D.,\
    \ Zhao, Z., Yang, X., Liu, L., Wei, S., Hu, Y., and Yin, S. Fact: Ffn-attention\
    \ cooptimized transformer architecture with eager correlation prediction. In Proceedings\
    \ of the 50th Annual International Symposium on Computer Architecture, pp. 1-14,\
    \ 2023. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et\
    \ al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,\
    \ 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,\
    \ Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with\
    \ a unified text-to-text transformer. The Journal of Machine Learning Research,\
    \ 21(1):5485-5551, 2020. Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On\
    \ the effect of dropping layers of pre-trained transformer models. Computer Speech\
    \ & Language, 77:101429, 2023. Sanh, V., Debut, L., Chaumond, J., and Wolf, T.\
    \ Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.\
    \ arXiv preprint arXiv:1910.01108, 2019. Shazeer, N. Fast transformer decoding:\
    \ One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Shen,\
    \ S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer,\
    \ K. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings\
    \ of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815-8821,\
    \ 2020. Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.- Y., Donato, M.,\
    \ Sanh, V., Whatmough, P., Rush, A. M., Brooks, D., et al. Edgebert: Sentence-level\
    \ energy optimizations for latency-aware multi-task nlp inference. In MICRO-54:\
    \ 54th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 830-844,\
    \ 2021. TheBloke. Llama-7b gptq. https://huggingface.co/TheBloke/LLaMa-7B-GPTQ,\
    \ 2023. Accessed: December 12, 2023. Thorndike, R. L. Who belongs in the family?\
    \ Psychometrika, 18(4):267-276, 1953. Touvron, H., Lavril, T., Izacard, G., Martinet,\
    \ X., Lachaux, M.-A., Lacroix, T., Rozi\xE8re, B., Goyal, N., Hambro, E., Azhar,\
    \ F., et al. Llama: Open and efficient foundation language models. arXiv preprint\
    \ arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\
    \ A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.\
    \ Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\
    \ 2023b. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\
    \ A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances\
    \ in neural information processing systems, 30, 2017. Voita, E., Talbot, D., Moiseev,\
    \ F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized\
    \ heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418,\
    \ 2019. Waleffe, R. and Rekatsinas, T. Principal component networks: Parameter\
    \ reduction early in training. arXiv preprint arXiv:2006.13347, 2020. Wang, H.,\
    \ Agarwal, S., and Papailiopoulos, D. Pufferfish: Communication-efficient models\
    \ at no extra cost. Proceedings of Machine Learning and Systems, 3:365-386, 2021a.\
    \ Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture\
    \ with cascade token and head pruning. In 2021 IEEE International Symposium on\
    \ High-Performance Computer Architecture (HPCA), pp. 97-110. IEEE, 2021b. Wang,\
    \ H., Agarwal, S., Tanaka, Y., Xing, E., Papailiopoulos, D., et al. Cuttlefish:\
    \ Low-rank model training without all the tuning. Proceedings of Machine Learning\
    \ and Systems, 5, 2023. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.\
    \ Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768,\
    \ 2020. Wang, Z., Wohlwend, J., and Lei, T. Structured pruning of large language\
    \ models. arXiv preprint arXiv:1910.04732, 2019. Xia, H., Ge, T., Wang, P., Chen,\
    \ S.-Q., Wei, F., and Sui, Z. Speculative decoding: Exploiting speculative execution\
    \ for accelerating seq2seq generation. In Findings of the Association for Computational\
    \ Linguistics: EMNLP 2023, pp. 3909-3925, 2023. Xiao, G., Lin, J., Seznec, M.,\
    \ Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training\
    \ quantization for large language models. In International Conference on Machine\
    \ Learning, pp. 38087-38099. PMLR, 2023. Yang, S., Lee, G., Cho, J., Papailiopoulos,\
    \ D., and Lee, K. Predictive pipelined decoding: A compute-latency trade-off for\
    \ exact llm decoding. arXiv preprint arXiv:2307.05908, 2023. You, H., Li, C.,\
    \ Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y. Drawing\
    \ early-bird tickets: Towards more efficient training of deep networks. arXiv\
    \ preprint arXiv:1909.11957, 2019. Zellers, R., Holtzman, A., Bisk, Y., Farhadi,\
    \ A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? arXiv\
    \ preprint arXiv:1905.07830, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe,\
    \ M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt:\
    \ Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,\
    \ 2022. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z.,\
    \ Tian, Y., R\xE9, C., Barrett, C., et al. H 2 o: Heavy-hitter oracle for efficient\
    \ generative inference of large language models. arXiv preprint arXiv:2306.14048,\
    \ 2023. Zhou, Y., Du, N., Huang, Y., Peng, D., Lan, C., Huang, D., Shakeri, S.,\
    \ So, D., Dai, A. M., Lu, Y., et al. Brainformers: Trading simplicity for efficiency.\
    \ In International Conference on Machine Learning, pp. 42531-42542. PMLR, 2023."
  title: 'CHAI: clustered head attention for efficient LLM inference'
  url: https://dl.acm.org/doi/10.5555/3692070.3692083
- abstract: Model-based reinforcement learning agents utilizing transformers have
    shown improved sample efficiency due to their ability to model extended context,
    resulting in more accurate world models. However, for complex reasoning and planning
    tasks, these methods primarily rely on continuous representations. This complicates
    modeling of discrete properties of the real world such as disjoint object classes
    between which interpolation is not plausible. In this work, we introduce discrete
    abstract representations for transformer-based learning (DART), a sample-efficient
    method utilizing discrete representations for modeling both the world and learning
    behavior. We incorporate a transformer-decoder for auto-regressive world modeling
    and a transformer-encoder for learning behavior by attending to task-relevant
    cues in the discrete representation of the world model. For handling partial observability,
    we aggregate information from past time steps as memory tokens. DART outperforms
    previous state-of-the-art methods that do not use look-ahead search on the Atari
    100k sample efficiency benchmark with a median human-normalized score of 0.790
    and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/.
  keywords: Computing methodologies, Computer graphics, Animation, Procedural animation,
    Machine learning, Learning paradigms, Reinforcement learning, Machine learning
    algorithms, Dynamic programming for Markov decision processes, Q-learning, Machine
    learning approaches, Instance-based learning, Neural networks, Theory of computation,
    Logic, Abstraction, Theory and algorithms for application domains, Algorithmic
    game theory and mechanism design
  references: "Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare,\
    \ M. Deep reinforcement learning at the edge of the statistical precipice. Advances\
    \ in neural information processing systems, 34:29304-29320, 2021. Andersen, P.-A.,\
    \ Goodwin, M., and Granmo, O.-C. The dreaming variational autoencoder for reinforcement\
    \ learning environments. In Artificial Intelligence XXXV: 38th SGAI International\
    \ Conference on Artificial Intelligence, AI 2018, Cambridge, UK, December 11-13,\
    \ 2018, Proceedings 38, pp. 143-155. Springer, 2018. Atkeson, C. G. and Santamaria,\
    \ J. C. A comparison of direct and model-based reinforcement learning. In Proceedings\
    \ of international conference on robotics and automation, volume 4, pp. 3557-3564.\
    \ IEEE, 1997. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade\
    \ learning environment: An evaluation platform for general agents. Journal of\
    \ Artificial Intelligence Research, 47:253-279, 2013. Buckman, J., Hafner, D.,\
    \ Tucker, G., Brevdo, E., and Lee, H. Sample-efficient reinforcement learning\
    \ with stochastic ensemble value expansion. Advances in neural information processing\
    \ systems, 31, 2018. Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory\
    \ transformer. Advances in Neural Information Processing Systems, 35:11079-11091,\
    \ 2022. Cartuyvels, R., Spinks, G., and Moens, M.-F. Discrete and continuous representations\
    \ and processing in deep learning: Looking forward. AI Open, 2:143-159, 2021.\
    \ Chen, C., Wu, Y.-F., Yoon, J., and Ahn, S. Transdreamer: Reinforcement learning\
    \ with transformer world models. arXiv preprint arXiv:2202.09481, 2022. Corneil,\
    \ D., Gerstner, W., and Brea, J. Efficient model-based deep reinforcement learning\
    \ with variational state tabulation. In International Conference on Machine Learning,\
    \ pp. 1049-1058. PMLR, 2018. Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le,\
    \ Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond\
    \ a fixed-length context. In Annual Meeting of the Association for Computational\
    \ Linguistics, 2019. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. Vision\
    \ transformers need registers. In The Twelfth International Conference on Learning\
    \ Representations, 2024. URL https://openreview.net/forum?id=2dnO3LLiJ1. Deng,\
    \ F., Park, J., and Ahn, S. Facing off world model backbones: RNNs, transformers,\
    \ and s4. In Thirty-seventh Conference on Neural Information Processing Systems,\
    \ 2023. URL https://openreview.net/forum?id=GDYuzX0rwj. Didolkar, A., Gupta, K.,\
    \ Goyal, A., Gundavarapu, N. B., Lamb, A. M., Ke, N. R., and Bengio, Y. Temporal\
    \ latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence\
    \ learning. Advances in Neural Information Processing Systems, 35:10505-10520,\
    \ 2022. Doerr, A., Daniel, C., Schiegg, M., Duy, N.-T., Schaal, S., Toussaint,\
    \ M., and Sebastian, T. Probabilistic recurrent state-space models. In International\
    \ conference on machine learning, pp. 1280-1289. PMLR, 2018. Dosovitskiy, A.,\
    \ Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\
    \ M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words:\
    \ Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929,\
    \ 2020. Du, S. S., Kakade, S. M., Wang, R., and Yang, L. F. Is a good representation\
    \ sufficient for sample efficient reinforcement learning? In International Conference\
    \ on Learning Representations, 2020. URL https://openreview.net/forum?id=r1genAVKPB.\
    \ Dunion, M., McInroe, T., Luck, K. S., Hanna, J. P., and Albrecht, S. V. Temporal\
    \ disentanglement of representations for improved generalisation in reinforcement\
    \ learning. In The Eleventh International Conference on Learning Representations,\
    \ 2023. URL https://openreview.net/forum?id=sPgP6aISLTD. Esser, P., Rombach, R.,\
    \ and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings\
    \ of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883,\
    \ 2021. Guss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C. R., Veloso,\
    \ M. M., and Salakhutdinov, R. Minerl: A large-scale dataset of minecraft demonstrations.\
    \ In International Joint Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID:199000710.\
    \ Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2018.\
    \ Hafner, D. Benchmarking the spectrum of agent capabilities. In International\
    \ Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=1W0z96MFEoH.\
    \ Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning\
    \ behaviors by latent imagination. In International Conference on Learning Representations,\
    \ 2020. URL https://openreview.net/forum?id=S1lOTC4tDS. Hafner, D., Lillicrap,\
    \ T. P., Norouzi, M., and Ba, J. Mastering atari with discrete world models. In\
    \ International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu.\
    \ Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains\
    \ through world models. arXiv preprint arXiv:2301.04104, 2023. Hamrick, J. B.,\
    \ Friesen, A. L., Behbahani, F., Guez, A., Viola, F., Witherspoon, S., Anthony,\
    \ T., Buesing, L., Veli\u010Dkovi\u0107, P., and Weber, T. On the role of planning\
    \ in model-based deep reinforcement learning. arXiv preprint arXiv:2011.04021,\
    \ 2020. Islam, R., Zang, H., Goyal, A., Lamb, A. M., Kawaguchi, K., Li, X., Laroche,\
    \ R., Bengio, Y., and Tachet des Combes, R. Discrete compositional representations\
    \ as an abstraction for goal conditioned reinforcement learning. Advances in Neural\
    \ Information Processing Systems, 35: 3885-3899, 2022. James, S., Ma, Z., Arrojo,\
    \ D. R., and Davison, A. J. Rlbench: The robot learning benchmark & learning environment.\
    \ IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020. Ji, T., Luo, Y.,\
    \ Sun, F., Jing, M., He, F., and Huang, W. When to update your model: Constrained\
    \ model-based reinforcement learning. Advances in Neural Information Processing\
    \ Systems, 35:23150-23163, 2022. Kaelbling, L. P., Littman, M. L., and Cassandra,\
    \ A. R. Planning and acting in partially observable stochastic domains. Artificial\
    \ intelligence, 101(1-2):99-134, 1998. Ke, N. R., Singh, A., Touati, A., Goyal,\
    \ A., Bengio, Y., Parikh, D., and Batra, D. Modeling the long term future in model-based\
    \ reinforcement learning. In International Conference on Learning Representations,\
    \ 2018. Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah,\
    \ M. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1-41,\
    \ 2022. Kingma, D. P., Welling, M., et al. An introduction to variational autoencoders.\
    \ Foundations and Trends\xAE in Machine Learning, 12(4):307-392, 2019. Kurutach,\
    \ T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. Model-ensemble trust-region\
    \ policy optimization. In International Conference on Learning Representations,\
    \ 2018. URL https://openreview.net/forum?id=SJJinbWRZ. Laskin, M., Srinivas, A.,\
    \ and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement\
    \ learning. In International Conference on Machine Learning, pp. 5639-5650. PMLR,\
    \ 2020. Lee, L., Eysenbach, B., Salakhutdinov, R. R., Gu, S. S., and Finn, C.\
    \ Weakly-supervised reinforcement learning for controllable behavior. Advances\
    \ in Neural Information Processing Systems, 33:2661-2673, 2020. Li, L. and Qiu,\
    \ X. Token-aware virtual adversarial training in natural language understanding.\
    \ Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):8410-8418,\
    \ 2021. Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. AI Open,\
    \ 2022. Mai, V., Mani, K., and Paull, L. Sample efficient deep reinforcement learning\
    \ via uncertainty estimation. In International Conference on Learning Representations,\
    \ 2022. URL https://openreview.net/forum?id=vrW3tvDfOJQ. Mao, C., Jiang, L., Dehghani,\
    \ M., Vondrick, C., Sukthankar, R., and Essa, I. Discrete representations strengthen\
    \ vision transformer robustness. In International Conference on Learning Representations,\
    \ 2022. URL https://openreview.net/forum?id=8hWs60AZcWk. McInroe, T., Sch\xE4\
    fer, L., and Albrecht, S. V. Learning temporally-consistent representations for\
    \ data-efficient reinforcement learning. arXiv preprint arXiv:2110.04935, 2021.\
    \ Micheli, V., Alonso, E., and Fleuret, F. Transformers are sample-efficient world\
    \ models. In The Eleventh International Conference on Learning Representations,\
    \ 2023. URL https://openreview.net/forum?id=vhFu1Acb0xb. Moerland, T. M., Broekens,\
    \ J., Plaat, A., Jonker, C. M., et al. Model-based reinforcement learning: A survey.\
    \ Foundations and Trends\xAE in Machine Learning, 16(1): 1-118, 2023a. Moerland,\
    \ T. M., Broekens, J., Plaat, A., Jonker, C. M., et al. Model-based reinforcement\
    \ learning: A survey. Foundations and Trends\xAE in Machine Learning, 16(1): 1-118,\
    \ 2023b. Mu, Y., Zhuang, Y., Wang, B., Zhu, G., Liu, W., Chen, J., Luo, P., Li,\
    \ S., Zhang, C., and Hao, J. Model-based reinforcement learning via imagination\
    \ with derived memory. Advances in Neural Information Processing Systems, 34:\
    \ 9493-9505, 2021. Ni, T., Ma, M., Eysenbach, B., and Bacon, P.-L. When do transformers\
    \ shine in RL? decoupling memory from credit assignment. Advances in Neural Information\
    \ Processing Systems, 36, 2024. Okada, M. and Taniguchi, T. Dreaming: Model-based\
    \ reinforcement learning by latent imagination without reconstruction. In 2021\
    \ ieee international conference on robotics and automation (icra), pp. 4209-4215.\
    \ IEEE, 2021. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar,\
    \ S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., et al. Stabilizing\
    \ transformers for reinforcement learning. In International conference on machine\
    \ learning, pp. 7487-7498. PMLR, 2020. Pascanu, R., Mikolov, T., and Bengio, Y.\
    \ On the difficulty of training recurrent neural networks. In International conference\
    \ on machine learning, pp. 1310-1318. Pmlr, 2013. Plaat, A., Kosters, W., and\
    \ Preuss, M. High-accuracy model-based reinforcement learning, a survey. Artificial\
    \ Intelligence Review, pp. 1-33, 2023. Polydoros, A. S. and Nalpantidis, L. Survey\
    \ of model-based reinforcement learning: Applications on robotics. Journal of\
    \ Intelligent & Robotic Systems, 86(2):153-173, 2017. Qi, M., Huang, Y., Yao,\
    \ Y., Wang, M., Gu, B., and Sundaresan, N. Is next token prediction sufficient\
    \ for gpt? exploration on code logic comprehension. arXiv preprint arXiv:2404.08885,\
    \ 2024. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et\
    \ al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,\
    \ 2019. Robine, J., H\xF6ftmann, M., Uelwer, T., and Harmeling, S. Transformer-based\
    \ world models are happy with 100k interactions. In The Eleventh International\
    \ Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=TdBaDGCpjly.\
    \ Robine, J., Uelwer, T., and Harmeling, S. Smaller world models for reinforcement\
    \ learning. Neural Processing Letters, pp. 1-31, 2023b. Schwarzer, M., Anand,\
    \ A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. Data-efficient reinforcement\
    \ learning with self-predictive representations. In International Conference on\
    \ Learning Representations, 2021. URL https://openreview.net/forum?id=uCQfPZwRaUu.\
    \ Seo, Y., Hafner, D., Liu, H., Liu, F., James, S., Lee, K., and Abbeel, P. Masked\
    \ world models for visual control. In Conference on Robot Learning, pp. 1332-1344.\
    \ PMLR, 2023. Sutton, R. S. Dyna, an integrated architecture for learning, planning,\
    \ and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991. Svidchenko, O. and Shpilman,\
    \ A. Maximum entropy model-based reinforcement learning. In Deep RL Workshop NeurIPS\
    \ 2021, 2021. URL https://openreview.net/forum?id=uBDG3yX-2sQ. Van Den Oord, A.,\
    \ Vinyals, O., et al. Neural discrete representation learning. Advances in neural\
    \ information processing systems, 30, 2017. Vaswani, A., Shazeer, N., Parmar,\
    \ N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin,\
    \ I. Attention is all you need. Advances in neural information processing systems,\
    \ 30, 2017. Wang, C., Yang, T., Hao, J., Zheng, Y., Tang, H., Barez, F., Liu,\
    \ J., Peng, J., Piao, H., and Sun, Z. Ed2: An environment dynamics decomposition\
    \ framework for world model construction. arXiv preprint arXiv:2112.02817, 2021a.\
    \ Wang, J., Li, W., Jiang, H., Zhu, G., Li, S., and Zhang, C. Offline reinforcement\
    \ learning with reverse model-based imagination. Advances in Neural Information\
    \ Processing Systems, 34:29420-29432, 2021b. Wolf, T., Debut, L., Sanh, V., Chaumond,\
    \ J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et\
    \ al. Transformers: State-of-the-art natural language processing. In Proceedings\
    \ of the 2020 conference on empirical methods in natural language processing:\
    \ system demonstrations, pp. 38-45, 2020. Xiao, C., Wu, Y., Ma, C., Schuurmans,\
    \ D., and M\xFCller, M. Learning to combat compounding-error in model-based reinforcement\
    \ learning. arXiv preprint arXiv:1912.11206, 2019. Xiong, R., Yang, Y., He, D.,\
    \ Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On\
    \ layer normalization in the transformer architecture. In International Conference\
    \ on Machine Learning, pp. 10524-10533. PMLR, 2020. Xu, R., Yang, K., Liu, K.,\
    \ and He, F. e(2)-equivariant vision transformer. In Uncertainty in Artificial\
    \ Intelligence, pp. 2356-2366. PMLR, 2023. Yarats, D., Kostrikov, I., and Fergus,\
    \ R. Image augmentation is all you need: Regularizing deep reinforcement learning\
    \ from pixels. In International Conference on Learning Representations, 2021.\
    \ URL https://openreview.net/forum?id=GY6-6sTvGaf. Ye, W., Liu, S., Kurutach,\
    \ T., Abbeel, P., and Gao, Y. Mastering atari games with limited data. Advances\
    \ in neural information processing systems, 34:25476-25488, 2021. Yin, Z.-H.,\
    \ Ye, W., Chen, Q., and Gao, Y. Planning for sample efficient imitation learning.\
    \ Advances in Neural Information Processing Systems, 35:2577-2589, 2022. Yoon,\
    \ J., Wu, Y.-F., Bae, H., and Ahn, S. An investigation into pre-training object-centric\
    \ representations for reinforcement learning. In International Conference on Machine\
    \ Learning, 2023. URL https://api.semanticscholar.org/CorpusID:256697459. Yu,\
    \ T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world:\
    \ A benchmark and evaluation for multi-task and meta reinforcement learning. In\
    \ Conference on robot learning, pp. 1094-1100. PMLR, 2020. Yu, Y. Towards sample\
    \ efficient reinforcement learning. In IJCAI, pp. 5739-5743, 2018. Yun, C., Bhojanapalli,\
    \ S., Rawat, A. S., Reddi, S., and Kumar, S. Are transformers universal approximators\
    \ of sequence-to-sequence functions? In International Conference on Learning Representations,\
    \ 2020. URL https://openreview.net/forum?id=ByxRM0Ntvr. Zhang, L., Lieffers, J.,\
    \ and Pyarelal, A. Deep reinforcement learning with vector quantized encoding.\
    \ arXiv preprint arXiv:2211.06733, 2022. Zhang, W., Wang, G., Sun, J., Yuan, Y.,\
    \ and Huang, G. STORM: Efficient stochastic transformer based world models for\
    \ reinforcement learning. In Thirty-seventh Conference on Neural Information Processing\
    \ Systems, 2023. URL https://openreview.net/forum?id=WxnrX42rnS. Zhu, G., Zhang,\
    \ M., Lee, H., and Zhang, C. Bridging imagination and reality for model-based\
    \ deep reinforcement learning. Advances in Neural Information Processing Systems,\
    \ 33:8993-9006, 2020. \u0141ukasz Kaiser, Babaeizadeh, M., Mi\u0141os, P., Osi&n#324;ski,\
    \ B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine,\
    \ S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. Model based\
    \ reinforcement learning for atari. In International Conference on Learning Representations,\
    \ 2020. URL https://openreview.net/forum?id=S1xCPJHtDB."
  title: Learning to play atari in a world of tokens
  url: https://dl.acm.org/doi/10.5555/3692070.3692084
- abstract: 'Zhang et al. (ICML 2021, PLMR 139, pp. 12447-12457) introduced probabilistic
    generating circuits (PGCs) as a probabilistic model to unify probabilistic circuits
    (PCs) and determinantal point processes (DPPs). At a first glance, PGCs store
    a distribution in a very different way, they compute the probability generating
    polynomial instead of the probability mass function and it seems that this is
    the main reason why PGCs are more powerful than PCs or DPPs. However, PGCs also
    allow for negative weights, whereas classical PCs assume that all weights are
    nonnegative. One main insight of this work is that the negative weights are the
    cause for the power of PGCs and not the different representation. PGCs are PCs
    in disguise: we show how to transform any PGC on binary variables into a PC with
    negative weights with only polynomial blowup. PGCs were defined by Zhang et al.
    only for binary random variables. As our second main result, we show that there
    is a good reason for this: we prove that PGCs for categorical variables with larger
    image size do not support tractable marginalization unless NP = P. On the other
    hand, we show that we can model categorical variables with larger image size as
    PC with negative weights computing set-multilinear polynomials. These allow for
    tractable marginalization. In this sense, PCs with negative weights strictly subsume
    PGCs.'
  keywords: Mathematics of computing, Probability and statistics, Probabilistic algorithms,
    Probabilistic reasoning algorithms, Markov-chain Monte Carlo methods, Random number
    generation, Sequential Monte Carlo methods, Theory of computation, Models of computation,
    Probabilistic computation
  references: "Arora, S. and Barak, B. Computational complexity: a modern approach.\
    \ Cambridge University Press, 2009. Bl\xE4ser, M. Not all strongly rayleigh distributions\
    \ have small probabilistic generating circuits. In Krause, A., Brunskill, E.,\
    \ Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International\
    \ Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\
    \ USA, volume 202 of Proceedings of Machine Learning Research, pp. 2592-2602.\
    \ PMLR, 2023. URL https://proceedings.mlr.press/v202/blaser23a.html. Bl\xE4ser,\
    \ M. and Curticapean, R. The complexity of the cover polynomials for planar graphs\
    \ of bounded degree. In Murlak, F. and Sankowski, P. (eds.), Mathematical Foundations\
    \ of Computer Science 2011, pp. 96-107, Berlin, Heidelberg, 2011. Springer Berlin\
    \ Heidelberg. ISBN 978-3-642-22993-0. Borodin, A. and Rains, E. M. Eynard-Mehta\
    \ theorem, Schur process, and their Pfaffian analogs. Journal of Statistical Physics,\
    \ 121:291-317, 2005. Broadrick, O., Zhang, H., and den Broeck, G. V. Polynomial\
    \ semantics of tractable probabilistic circuits. arXiv 2402.09085, 2024. B\xFC\
    rgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory,\
    \ volume 315 of Grundlehren der mathematischen Wissenschaften. Springer, 1997.\
    \ ISBN 3-540-60582-7. Cantor, D. G. and Kaltofen, E. L. On fast multiplication\
    \ of polynomials over arbitrary algebras. Acta Informatica, 28(7):693-701, 1991.\
    \ Choi, Y., Vergari, A., and Van den Broeck, G. Probabilistic circuits: A unifying\
    \ framework for tractable probabilistic models. Technical report, UCLA, Oct 2020.\
    \ URL http://starai.cs.ucla.edu/papers/ProbCirc20.pdf. Dagum, P. and Luby, M.\
    \ Approximating the permanent of graphs with large factors. Theoretical Computer\
    \ Science, 102(2):283-305, 1992. ISSN 0304-3975. URL https://www.sciencedirect.com/science/article/pii/0304397592902347.\
    \ Darwiche, A. Modeling and reasoning with Bayesian networks. Cambridge University\
    \ Press, 2009. Harviainen, J., Ramaswamy, V. P., and Koivisto, M. On inference\
    \ and learning with probabilistic generating circuits. In Evans, R. J. and Shpitser,\
    \ I. (eds.), Uncertainty in Artificial Intelligence, UAI 2023, July 31 - 4 August\
    \ 2023, Pittsburgh, PA, USA, volume 216 of Proceedings of Machine Learning Research,\
    \ pp. 829-838. PMLR, 2023. URL https://proceedings.mlr.press/v216/harviainen23b.html.\
    \ Kisa, D., Van den Broeck, G., Choi, A., and Darwiche, A. Probabilistic sentential\
    \ decision diagrams. In Baral, C., Giacomo, G. D., and Eiter, T. (eds.), Principles\
    \ of Knowledge Representation and Reasoning: Proceedings of the Fourteenth International\
    \ Conference, KR 2014, Vienna, Austria, July 20-24, 2014. AAAI Press, 2014. URL\
    \ http://www.aaai.org/ocs/index.php/KR/KR14/paper/view/8005. Koller, D. and Friedman,\
    \ N. Probabilistic Graphical Models Principles and Techniques. MIT Press, 2009.\
    \ Kulesza, A. and Taskar, B. Determinantal point processes for machine learning.\
    \ Foundations and Trends\xAE in Machine Learning, 5(2-3):123-286, 2012. ISSN 1935-8237.\
    \ Martens, J. and Medabalimi, V. On the expressive efficiency of sum product networks.\
    \ CoRR, abs/1411.7717, 2014. URL http://arxiv.org/abs/1411.7717. Meila, M. and\
    \ Jordan, M. I. Learning with mixtures of trees. J. Mach. Learn. Res., 1:1-48,\
    \ 2000. URL http://jmlr.org/papers/v1/meila00a.html. Mhammedi, Z., Gr\xFCnwald,\
    \ P., and Guedj, B. Pac-bayes un-expected bernstein inequality. In Wallach, H.\
    \ M., Larochelle, H., Beygelzimer, A., d'Alch\xE9-Buc, F., Fox, E. B., and Garnett,\
    \ R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference\
    \ on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,\
    \ 2019, Vancouver, BC, Canada, pp. 12180-12191, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3dea6b598a16b334a53145e78701fa87-Abstract.html.\
    \ Papadimitriou, C. Computational Complexity. Addison Welsey, 1994. Poon, H. and\
    \ Domingos, P. M. Sum-product networks: A new deep architecture. CoRR, abs/1202.3732,\
    \ 2012. URL http://arxiv.org/abs/1202.3732. Roth, D. On the hardness of approximate\
    \ reasoning. Artificial Intelligence, 82(1):273-302, 1996. ISSN 0004-3702. URL\
    \ https://www.sciencedirect.com/science/article/pii/0004370294000921. Saptharishi,\
    \ R. et al. A selection of lower bounds in arithmetic circuit complexity, 2021.\
    \ URL https://github.com/dasarpmar/lowerbounds-survey/releases/tag/v9.0.3. Version\
    \ 2021-07-27. Shih, A., Van den Broeck, G., Beame, P., and Amarilli, A. Smoothing\
    \ structured decomposable circuits. In Wallach, H. M., Larochelle, H., Beygelzimer,\
    \ A., d'Alch\xE9-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural\
    \ Information Processing Systems 32: Annual Conference on Neural Information Processing\
    \ Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.\
    \ 11412-11422, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html.\
    \ Strassen, V. Vermeidung von Divisionen. Journal f\xFCr die reine und angewandte\
    \ Mathematik, 264:184-202, 1973. URL http://eudml.org/doc/151394. Thulasidasan,\
    \ S., Bhattacharya, T., Bilmes, J. A., Chennupati, G., and Mohd-Yusof, J. Combating\
    \ label noise in deep learning using abstention. In Chaudhuri, K. and Salakhutdinov,\
    \ R. (eds.), Proceedings of the 36th International Conference on Machine Learning,\
    \ ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings\
    \ of Machine Learning Research, pp. 6234-6243. PMLR, 2019. URL http://proceedings.mlr.press/v97/thulasidasan19a.html.\
    \ Valiant, L. G. The complexity of enumeration and reliability problems. SIAM\
    \ Journal on Computing, 8(3):410-421, 1979. Vergari, A., Choi, Y., Peharz, R.,\
    \ and Van den Broeck, G. Probabilistic circuits: Representations, inference, learning\
    \ and applications. AAAI Tutorial, 2020. Wu, Y. and Seldin, Y. Split-kl and pac-bayes-split-kl\
    \ inequalities for ternary random variables. In Koyejo, S., Mohamed, S., Agarwal,\
    \ A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information\
    \ Processing Systems 35: Annual Conference on Neural Information Processing Systems\
    \ 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.\
    \ URL http://papers.nips.cc/paper files/paper/2022/hash/49ffa271264808cf500ea528ed8ec9b3-Abstract-Conference.html.\
    \ Zhang, H., Holtzen, S., and Van den Broeck, G. On the relationship between probabilistic\
    \ circuits and determinantal point processes. In Adams, R. P. and Gogate, V. (eds.),\
    \ Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence,\
    \ UAI 2020, virtual online, August 3-6, 2020, volume 124 of Proceedings of Machine\
    \ Learning Research, pp. 1188-1197. AUAI Press, 2020. URL http://proceedings.mlr.press/v124/zhang20c.html.\
    \ Zhang, H., Juba, B., and Van Den Broeck, G. Probabilistic generating circuits.\
    \ In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference\
    \ on Machine Learning, volume 139 of Proceedings of Machine Learning Research,\
    \ pp. 12447-12457. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/zhang21i.html."
  title: "Probabilistic generating circuits\u2014demystified"
  url: https://dl.acm.org/doi/10.5555/3692070.3692085
- abstract: We design differentially private regret-minimizing algorithms in the online
    convex optimization (OCO) framework. Unlike recent results, our algorithms and
    analyses do not require smoothness, thus yielding the first private regret bounds
    with an optimal leading-order term for non-smooth loss functions. Additionally,
    even for smooth losses, the resulting regret guarantees improve upon previous
    results in terms their dependence of dimension. Our results provide the best known
    rates for DP-OCO in all practical regimes of the privacy parameter, barring when
    it is exceptionally small. The principal innovation in our algorithm design is
    the use of sampling from strongly log-concave densities which satisfy the Log-Sobolev
    Inequality. The resulting concentration of measure allows us to obtain a better
    trade-off for the dimension factors than prior work, leading to improved results.
    Following previous works on DP-OCO, the proposed algorithm explicitly limits the
    number of switches via rejection sampling. Thus, independently of privacy constraints,
    the algorithm also provides improved results for online convex optimization with
    a switching budget.
  keywords: Mathematics of computing, Mathematical analysis, Mathematical optimization,
    Continuous optimization, Convex optimization, Theory of computation, Design and
    analysis of algorithms, Mathematical optimization, Continuous optimization, Convex
    optimization, Models of computation, Interactive computation, Theory and algorithms
    for application domains, Machine learning theory, Online learning theory
  references: "Agarwal, N. and Singh, K. The price of differential privacy for online\
    \ learning. In ICML, volume 70 of Proceedings of Machine Learning Research, pp.\
    \ 32-40. PMLR, 2017. URL http://proceedings.mlr.press/v70/agarwal17a.html. Agarwal,\
    \ N., Kale, S., Singh, K., and Thakurta, A. Differentially private and lazy online\
    \ convex optimization. In Neu, G. and Rosasco, L. (eds.), Proceedings of Thirty\
    \ Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning\
    \ Research, pp. 4599-4632. PMLR, 12-15 Jul 2023. URL https://proceedings.mlr.press/v195/agarwal23d.html.\
    \ Altschuler, J. M. and Talwar, K. Online learning over a finite action set with\
    \ limited switching. Math. Oper. Res., 46(1):179-203, 2021. Anava, O., Hazan,\
    \ E., and Mannor, S. Online learning for adversaries with memory: price of past\
    \ mistakes. In Advances in Neural Information Processing Systems, pp. 784-792,\
    \ 2015. Asi, H., Feldman, V., Koren, T., and Talwar, K. Private online prediction\
    \ from experts: Separations and faster rates. In The Thirty Sixth Annual Conference\
    \ on Learning Theory, pp. 674-699. PMLR, 2023. Bakry, D. and \xC9mery, M. Diffusions\
    \ hypercontractives. In S\xE9minaire de Probabilit\xE9s XIX 1983/84: Proceedings,\
    \ pp. 177-206. Springer, 2006. Chen, L., Yu, Q., Lawrence, H., and Karbasi, A.\
    \ Minimax regret of switching-constrained online convex optimization: No phase\
    \ transition. Advances in Neural Information Processing Systems, 33:3477-3486,\
    \ 2020. Donsker, M. D. and Varadhan, S. S. Asymptotic evaluation of certain markov\
    \ process expectations for large time, i. Communications on Pure and Applied Mathematics,\
    \ 28 (1):1-47, 1975. Dwork, C. and Roth, A. The algorithmic foundations of differential\
    \ privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211-407,\
    \ 2014. Ganesh, A., Thakurta, A., and Upadhyay, J. Universality of langevin diffusion\
    \ for private optimization, with applications to sampling from rashomon sets.\
    \ In Neu, G. and Rosasco, L. (eds.), Proceedings of Thirty Sixth Conference on\
    \ Learning Theory, volume 195 of Proceedings of Machine Learning Research, pp.\
    \ 1730-1773. PMLR, 12-15 Jul 2023. URL https://proceedings.mlr.press/v195/ganesh23a.html.\
    \ Geulen, S., V\xF6cking, B., and Winkler, M. Regret minimization for online buffering\
    \ problems using the weighted majority algorithm. In Kalai, A. T. and Mohri, M.\
    \ (eds.), COLT, pp. 132-143. Omnipress, 2010. URL http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=140.\
    \ Gopi, S., Lee, Y. T., and Liu, D. Private convex optimization via exponential\
    \ mechanism. In Loh, P.-L. and Raginsky, M. (eds.), Proceedings of Thirty Fifth\
    \ Conference on Learning Theory, volume 178 of Proceedings of Machine Learning\
    \ Research, pp. 1948-1989. PMLR, 02-05 Jul 2022. URL https://proceedings.mlr.press/v178/gopi22a.html.\
    \ Hazan, E. Introduction to online convex optimization. Foundations and Trends\
    \ in Optimization, 2(3-4):157-325, 2016. Jain, P., Kothari, P., and Thakurta,\
    \ A. Differentially private online learning. In Proc. of the 25th Annual Conf.\
    \ on Learning Theory (COLT), volume 23, pp. 24.1-24.34, June 2012. Kairouz, P.,\
    \ McMahan, B., Song, S., Thakkar, O., Thakurta, A., and Xu, Z. Practical and private\
    \ (deep) learning without sampling or shuffling. In ICML, 2021. Kalai, A. and\
    \ Vempala, S. Efficient algorithms for online decision problems. Journal of Computer\
    \ and System Sciences, 71(3):291-307, 2005. Ledoux, M. Concentration of measure\
    \ and logarithmic sobolev inequalities. In Seminaire de probabilites XXXIII, pp.\
    \ 120-216. Springer, 1999. Levin, D. A. and Peres, Y. Markov chains and mixing\
    \ times, volume 107. American Mathematical Soc., 2017. Merhav, N., Ordentlich,\
    \ E., Seroussi, G., and Weinberger, M. J. On sequential strategies for loss functions\
    \ with memory. IEEE Trans. Inf. Theory, 48(7):1947-1958, 2002. Sherman, U. and\
    \ Koren, T. Lazy oco: Online convex optimization on a switching budget. In Conference\
    \ on Learning Theory, pp. 3972-3988. PMLR, 2021. Sherman, U. and Koren, T. Lazy\
    \ oco: Online convex optimization on a switching budget. arXiv preprint arXiv:2102.03803\
    \ version 7 (see also version 5), 2023. URL https://arxiv.org/abs/2102.03803.\
    \ Smith, A. and Thakurta, A. (nearly) optimal algorithms for private online learning\
    \ in full-information and bandit settings. In Advances in Neural Information Processing\
    \ Systems, pp. 2733-2741, 2013. Whitehouse, J., Ramdas, A., Rogers, R., and Wu,\
    \ Z. S. Fully adaptive composition in differential privacy. arXiv preprint arXiv:2203.05481,\
    \ 2022."
  title: 'Improved differentially private and lazy online convex optimization: lower
    regret without smoothness requirements'
  url: https://dl.acm.org/doi/10.5555/3692070.3692086
- abstract: We propose a generalization of the classical G-optimal design concept
    to non-linear function classes. The criterion, termed F-design, coincides with
    G-design in the linear case. We compute the value of the optimal design, termed
    the F-condition number, for several non-linear function classes. We further provide
    algorithms to construct designs with a bounded F-condition number. Finally, we
    employ the F-design in a variety of interactive machine learning tasks, where
    the design is naturally useful for data collection or exploration. We show that
    in four diverse settings of confidence band construction, contextual bandits,
    model-free reinforcement learning, and active learning, F-design can be combined
    with existing approaches in a black-box manner to yield state-of-the-art results
    in known problem settings as well as to generalize to novel ones.
  keywords: Computing methodologies, Machine learning, Machine learning approaches,
    Mathematics of computing, Mathematical analysis, Mathematical optimization, Theory
    of computation, Design and analysis of algorithms, Mathematical optimization,
    Models of computation, Interactive computation
  references: "Agarwal, A. and Zhang, T. Non-linear reinforcement learning in large\
    \ action spaces: Structural conditions and sample-efficiency of posterior sampling.\
    \ arXiv preprint arXiv:2203.08248, 2022. Agarwal, A., Dud\xEDk, M., Kale, S.,\
    \ Langford, J., and Schapire, R. Contextual bandit learning with predictable rewards.\
    \ In Artificial Intelligence and Statistics, pp. 19-26. PMLR, 2012. Audibert,\
    \ J.-Y. and Bubeck, S. Best arm identification in multi-armed bandits. In COLT-23th\
    \ Conference on Learning Theory-2010, pp. 13-p, 2010. Awerbuch, B. and Kleinberg,\
    \ R. Online linear optimization and adaptive routing. Journal of Computer and\
    \ System Sciences, 74(1):97-114, 2008. Balcan, M.-F. and Long, P. Active and passive\
    \ learning of linear separators under log-concave distributions. In Conference\
    \ on Learning Theory, pp. 288-316, 2013. Balcan, M.-F., Broder, A., and Zhang,\
    \ T. Margin based active learning. In International Conference on Computational\
    \ Learning Theory, pp. 35-50. Springer, 2007. Ball, K. An elementary introduction\
    \ to modern convex geometry. Flavors of geometry, 31:1-58, 1997. Bickel, P. J.\
    \ and Rosenblatt, M. On some global measures of the deviations of density function\
    \ estimates. The Annals of Statistics, pp. 1071-1095, 1973. Bubeck, S., Munos,\
    \ R., Stoltz, G., and Szepesv\xE1ri, C. Xarmed bandits. Journal of Machine Learning\
    \ Research, 12(5), 2011. Bubeck, S., Cesa-Bianchi, N., and Kakade, S. M. Towards\
    \ minimax policies for online linear optimization with bandit feedback. In Conference\
    \ on Learning Theory, pp. 41-1. JMLR Workshop and Conference Proceedings, 2012a.\
    \ Bubeck, S., Cesa-Bianchi, N., et al. Regret analysis of stochastic and nonstochastic\
    \ multi-armed bandit problems. Foundations and Trends\xAE in Machine Learning,\
    \ 5 (1):1-122, 2012b. Bubeck, S., Dekel, O., Koren, T., and Peres, Y. Bandit convex\
    \ optimization: \u221AT regret in one dimension. In Conference on Learning Theory,\
    \ pp. 266-278, 2015. Cai, L., Liu, R., Wang, S., and Yang, L. Simultaneous confidence\
    \ bands for mean and variance functions based on deterministic design. Statistica\
    \ Sinica, 29(1):505-525, 2019. Cai, T. T., Low, M., and Ma, Z. Adaptive confidence\
    \ bands for nonparametric regression functions. Journal of the American Statistical\
    \ Association, 109(507):1054-1070, 2014. Casella, G., Fienberg, S., and Olkin,\
    \ I. Statistical design. Springer, 2008. Cesa-Bianchi, N. and Lugosi, G. Prediction,\
    \ learning, and games. Cambridge university press, 2006. Chen, F., Mei, S., and\
    \ Bai, Y. Unified algorithms for rl with decision-estimation coefficients: No-regret,\
    \ pac, and reward-free learning. arXiv preprint arXiv:2209.11745, 2022. Cochran,\
    \ W. G. and Cox, G. M. Experimental designs. Technical report, North Carolina\
    \ State University. Dept. of Statistics, 1948. Dani, V., Hayes, T. P., and Kakade,\
    \ S. M. Stochastic linear optimization under bandit feedback. In Conference on\
    \ Learning Theory (COLT), 2008. Dayan, P. and Watkins, C. Q-learning. Machine\
    \ learning, 8 (3):279-292, 1992. Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan,\
    \ G., Sun, W., and Wang, R. Bilinear classes: A structural framework for provable\
    \ generalization in rl. In International Conference on Machine Learning, pp. 2826-2836.\
    \ PMLR, 2021. Foster, D. J. and Rakhlin, A. Beyond UCB: Optimal and efficient\
    \ contextual bandits with regression oracles. International Conference on Machine\
    \ Learning (ICML), 2020. Foster, D. J., Rakhlin, A., Simchi-Levi, D., and Xu,\
    \ Y. Instance-dependent complexity of contextual bandits and reinforcement learning:\
    \ A disagreement-based perspective. arXiv preprint arXiv:2010.03104, 2020. Foster,\
    \ D. J., Kakade, S. M., Qian, J., and Rakhlin, A. The statistical complexity of\
    \ interactive decision making. arXiv preprint arXiv:2112.13487, 2021. Foster,\
    \ D. J., Golowich, N., and Han, Y. Tight guarantees for interactive decision making\
    \ with the decision-estimation coefficient. arXiv preprint arXiv:2301.08215, 2023a.\
    \ Foster, D. J., Golowich, N., Qian, J., Rakhlin, A., and Sekhari, A. Model-free\
    \ reinforcement learning with the decision-estimation coefficient. In Thirty-seventh\
    \ Conference on Neural Information Processing Systems, 2023b. Gentile, C., Wang,\
    \ Z., and Zhang, T. Fast rates in pool-based batch active learning. arXiv preprint\
    \ arXiv:2202.05448, 2022. H\xE4rdle, W. Asymptotic maximal deviation of m-smoothers.\
    \ Journal of Multivariate Analysis, 29(2):163-179, 1989. Hazan, E. and Karnin,\
    \ Z. Hard-margin active linear regression. In International Conference on Machine\
    \ Learning, pp. 883-891. PMLR, 2014. Hazan, E. and Karnin, Z. Volumetric spanners:\
    \ an efficient exploration basis for learning. The Journal of Machine Learning\
    \ Research, 17(1):4062-4095, 2016. Jackson, M. and Cox, D. R. The principles of\
    \ experimental design and their application in sociology. Annual Review of Sociology,\
    \ 39:27-49, 2013. Jamieson, K. and Nowak, R. Best-arm identification algorithms\
    \ for multi-armed bandits in the fixed confidence setting. In 2014 48th Annual\
    \ Conference on Information Sciences and Systems (CISS), pp. 1-6. IEEE, 2014.\
    \ Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E.\
    \ Contextual decision processes with low bellman rank are pac-learnable. In International\
    \ Conference on Machine Learning, pp. 1704-1713. PMLR, 2017. Jin, C., Yang, Z.,\
    \ Wang, Z., and Jordan, M. I. Provably efficient reinforcement learning with linear\
    \ function approximation. In Conference on Learning Theory, pp. 2137-2143, 2020.\
    \ Jin, C., Liu, Q., and Miryoosefi, S. Bellman eluder dimension: New rich classes\
    \ of rl problems, and sample-efficient algorithms. Advances in neural information\
    \ processing systems, 34:13406-13418, 2021. John, F. Extremum problems with inequalities\
    \ as subsidiary conditions. Traces and emergence of nonlinear programming, pp.\
    \ 197-215, 2014. Johnston, G. J. Probabilities of maximal deviations for nonparametric\
    \ regression function estimates. Journal of Multivariate Analysis, 12(3):402-414,\
    \ 1982. Jun, K.-S. and Zhang, C. Crush optimism with pessimism: Structured bandits\
    \ beyond asymptotic optimality. Advances in Neural Information Processing Systems,\
    \ 33: 6366-6376, 2020. Katz-Samuels, J., Zhang, J., Jain, L., and Jamieson, K.\
    \ Improved algorithms for agnostic pool-based active classification. In International\
    \ Conference onMachine Learning, pp. 5334-5344. PMLR, 2021. Keskin G\xFCndo\u011F\
    du, T., Deniz, I., \xC7ali\u015Fkan, G., \u015Eahin, E. S., and Azbar, N. Experimental\
    \ design methods for bioengineering applications. Critical reviews in biotechnology,\
    \ 36(2):368-388, 2016. Kiefer, J. and Wolfowitz, J. The equivalence of two extremum\
    \ problems. Canadian Journal of Mathematics, 12: 363-366, 1960. Kleinberg, R.,\
    \ Slivkins, A., and Upfal, E. Bandits and experts in metric spaces. Journal of\
    \ the ACM (JACM), 66 (4):1-77, 2019. Lattimore, T. and Szepesv\xE1ri, C. Bandit\
    \ algorithms. Cambridge University Press, 2020. Mhammedi, Z., Block, A., Foster,\
    \ D. J., and Rakhlin, A. Efficient model-free exploration in low-rank mdps. arXiv\
    \ preprint arXiv:2307.03997, 2023. Modi, A., Jiang, N., Tewari, A., and Singh,\
    \ S. Sample complexity of reinforcement learning using linearly combined model\
    \ ensembles. In International Conference on Artificial Intelligence and Statistics,\
    \ pp. 2010-2020. PMLR, 2020. Montgomery, D. C. Design and analysis of experiments.\
    \ John wiley & sons, 2017. Pronzato, L. and P\xE1zman, A. Design of experiments\
    \ in nonlinear models. Lecture notes in statistics, 212:1, 2013. Puterman, M.\
    \ L. Markov decision processes: discrete stochastic dynamic programming. John\
    \ Wiley & Sons, 2014. Russo, D. and Van Roy, B. Eluder dimension and the sample\
    \ complexity of optimistic exploration. Advances in Neural Information Processing\
    \ Systems, 26, 2013. Shor, N. Z. Minimization methods for nondifferentiable functions,\
    \ volume 3. Springer Science & Business Media, 2012. Smith, K. On the standard\
    \ deviations of adjusted and interpolated values of an observed polynomial function\
    \ and its constants and the guidance they give towards a proper choice of the\
    \ distribution of observations. Biometrika, 12 (1/2):1-85, 1918. Tikhomirov, V.\
    \ M. \u03B5-entropy and \u03B5-capacity of sets in functional spaces. Selected\
    \ Works of AN Kolmogorov: Volume III: Information Theory and the Theory of Algorithms,\
    \ pp. 86-170, 1993. Todd, M. J. Minimum-volume ellipsoids: Theory and algorithms.\
    \ SIAM, 2016. Vaart, A. v. d. and Wellner, J. A. Empirical processes. In Weak\
    \ Convergence and Empirical Processes: With Applications to Statistics, pp. 127-384.\
    \ Springer, 2023. Vapnik, V. N. Estimation of dependences based on empirical data,\
    \ volume 40. Springer-Verlag New York, 1982. Wagenmaker, A., Katz-Samuels, J.,\
    \ and Jamieson, K. Experimental design for regret minimization in linear bandits.\
    \ In International Conference on Artificial Intelligence and Statistics, pp. 3088-3096.\
    \ PMLR, 2021. Wainwright, M. J. High-dimensional statistics: A nonasymptotic viewpoint,\
    \ volume 48. Cambridge University Press, 2019. Walsh, S. J. Overview of optimal\
    \ experimental design and a survey of its expanse in application to agricultural\
    \ studies. 2022. Wang, J. and Yang, L. Polynomial spline confidence bands for\
    \ regression curves. Statistica Sinica, pp. 325-342, 2009. Xia, Y. Bias-corrected\
    \ confidence bands in nonparametric regression. Journal of the Royal Statistical\
    \ Society: Series B (Statistical Methodology), 60(4):797-811, 1998. Zhang, C.\
    \ and Li, Y. Improved algorithms for efficient active learning halfspaces with\
    \ massart and tsybakov noise. In Conference on Learning Theory, pp. 4526-4527.\
    \ PMLR, 2021. Zhang, T. Feel-good thompson sampling for contextual bandits and\
    \ reinforcement learning. SIAM Journal on Mathematics of Data Science, 4(2):834-857,\
    \ 2022. Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang,\
    \ T. Gec: A unified framework for interactive decision making in mdp, pomdp, and\
    \ beyond. arXiv preprint arXiv:2211.01962, 2022. Zhu, Y., Foster, D. J., Langford,\
    \ J., and Mineiro, P. Contextual bandits with large action spaces: Made practical.\
    \ In International Conference on Machine Learning, pp. 27428-27453. PMLR, 2022."
  title: The non-linear F-design and applications to interactive learning
  url: https://dl.acm.org/doi/10.5555/3692070.3692087
- abstract: Reinforcement Learning (RL) for constrained MDPs (CMDPs) is an increasingly
    important problem for various applications. Often, the average criterion is more
    suitable than the discounted criterion. Yet, RL for average-CMDPs (ACMDPs) remains
    a challenging problem. Algorithms designed for discounted constrained RL problems
    often do not perform well for the average CMDP setting. In this paper, we introduce
    a new policy optimization with function approximation algorithm for constrained
    MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO)
    algorithm is inspired by trust region-based policy optimization algorithms. We
    develop basic sensitivity theory for average CMDPs, and then use the corresponding
    bounds in the design of the algorithm. We provide theoretical guarantees on its
    performance, and through extensive experimental work in various challenging OpenAI
    Gym environments, show its superior empirical performance when compared to other
    state-of-the-art algorithms adapted for the ACMDPs.
  keywords: Computing methodologies, Machine learning, Learning paradigms, Reinforcement
    learning, Sequential decision making, Supervised learning, Machine learning algorithms,
    Dynamic programming for Markov decision processes, Temporal difference learning,
    Machine learning approaches
  references: "Abounadi, J., Bertsekas, D., and Borkar, V. S. Learning algorithms\
    \ for markov decision processes with average cost. SIAM Journal on Control and\
    \ Optimization, 40(3):681-698, 2001. Achiam, J. UC Berkeley CS 285 (Fall 2017),\
    \ Advanced Policy Gradients, 2017. URL: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf.\
    \ Last visited on 2020/05/24. Achiam, J., Held, D., Tamar, A., and Abbeel, P.\
    \ Constrained policy optimization. In Proceedings of the 34th International Conference\
    \ on Machine Learning-Volume 70, pp. 22-31. JMLR. org, 2017. Achiam, J., Adler,\
    \ S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt,\
    \ J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,\
    \ 2023. Agnihotri, A., Saraf, P., and Bapnad, K. R. A convolutional neural network\
    \ approach towards self-driving cars. In 2019 IEEE 16th India Council International\
    \ Conference (INDICON), pp. 1-4, 2019. Akkaya, I., Andrychowicz, M., Chociej,\
    \ M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G.,\
    \ Ribas, R., et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113,\
    \ 2019. Altman, E. Constrained Markov decision processes, volume 7. CRC Press,\
    \ 1999. Aractingi, M., L\xE9ziart, P.-A., Flayols, T., Perez, J., Silander, T.,\
    \ and Sou\xE8res, P. Controlling the solo12 quadruped robot with deep reinforcement\
    \ learning. scientific Reports, 13(1):11945, 2023. Bhatnagar, S. and Lakshmanan,\
    \ K. An Online Actor-Critic Algorithm with Function Approximation for Constrained\
    \ Markov Decision Processes., 2012. [Accessed 08-10-2023]. Borkar, V. S. A convex\
    \ analytic approach to markov decision processes. Probability Theory and Related\
    \ Fields, 78(4):583-602, 1988. Calvo-Fullana, M., Paternain, S., Chamon, L. F.,\
    \ and Ribeiro, A. State augmented constrained reinforcement learning: Overcoming\
    \ the limitations of learning with rewards. IEEE Transactions on Automatic Control,\
    \ 2023. Chen, L., Jain, R., and Luo, H. Learning infinite-horizon average-reward\
    \ Markov decision process with constraints. In Chaudhuri, K., Jegelka, S., Song,\
    \ L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th\
    \ International Conference on Machine Learning, volume 162 of Proceedings of Machine\
    \ Learning Research, pp. 3246-3270. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/chen22i.html.\
    \ Cho, G. E. and Meyer, C. D. Comparison of perturbation bounds for the stationary\
    \ distribution of a markov chain. Linear Algebra and its Applications, 335(1-3):137-150,\
    \ 2001. Doyle, P. G. The kemeny constant of a markov chain. arXiv preprint arXiv:0909.2636,\
    \ 2009. Gao, Y., Xu, H., Lin, J., Yu, F., Levine, S., and Darrell, T. Reinforcement\
    \ learning from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.\
    \ Garcia, J. and Fernandez, F. A comprehensive survey on safe reinforcement learning.\
    \ Journal of Machine Learning Research, 16(1):1437-1480, 2015. Hordijk, A. and\
    \ Kallenberg, L. Linear programming and markov decision chains. Management Science,\
    \ 25(4):352-362, 1979. Hu, H., Liu, Z., Chitlangia, S., Agnihotri, A., and Zhao,\
    \ D. Investigating the impact of multi-lidar placement on object detection for\
    \ autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision\
    \ and Pattern Recognition (CVPR), pp. 2550-2559, June 2022. Hunter, J. J. Stationary\
    \ distributions and mean first passage times of perturbed markov chains. Linear\
    \ Algebra and its Applications, 410:217-243, 2005. Hunter, J. J. Mathematical\
    \ techniques of applied probability: Discrete time models: Basic theory, volume\
    \ 1. Academic Press, 2014. Kakade, S. and Langford, J. Approximately optimal approximate\
    \ reinforcement learning. In International Conference on Machine Learning, volume\
    \ 2, pp. 267-274, 2002. Kemeny, J. and Snell, I. Finite Markov Chains. Van Nostrand,\
    \ New Jersey, 1960. Levene, M. and Loizou, G. Kemeny's constant and the random\
    \ surfer. The American mathematical monthly, 109(8):741-745, 2002. Levine, S.,\
    \ Finn, C., Darrell, T., and Abbeel, P. End-to-end training of deep visuomotor\
    \ policies. Journal of Machine Learning Research, 17(1):1334-1373, 2016. Liao,\
    \ P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S. A. Batch policy learning in\
    \ average reward markov decision processes. The Annals of Statistics, 50(6):3364-3387,\
    \ 2022. Ma, X., Tang, X., Xia, L., Yang, J., and Zhao, Q. Average-reward reinforcement\
    \ learning with trust region methods. arXiv preprint arXiv:2106.03442, 2021. Manne,\
    \ A. S. Linear programming and sequential decisions. Management Science, 6(3):259-267,\
    \ 1960. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,\
    \ M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level\
    \ control through deep reinforcement learning. nature, 518(7540):529-533, 2015.\
    \ Moskovitz, T., Singh, A. K., Strouse, D., Sandholm, T., Salakhutdinov, R., Dragan,\
    \ A. D., and McAleer, S. Confronting reward model overoptimization with constrained\
    \ rlhf. arXiv preprint arXiv:2310.04373, 2023. Rajeswaran, A., Kumar, V., Gupta,\
    \ A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S. Learning complex\
    \ dexterous manipulation with deep reinforcement learning and demonstrations.\
    \ arXiv preprint arXiv:1709.10087, 2017. Satija, H., Amortila, P., and Pineau,\
    \ J. Constrained Markov decision processes via backward value functions. In III,\
    \ H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference\
    \ on Machine Learning, volume 119 of Proceedings of Machine Learning Research,\
    \ pp. 8502-8511. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/satija20a.html.\
    \ Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region\
    \ policy optimization. In International Conference on Machine Learning, pp. 1889-1897,\
    \ 2015. Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional\
    \ continuous control using generalized advantage estimation. International Conference\
    \ on Learning Representations (ICLR), 2016. Schulman, J., Wolski, F., Dhariwal,\
    \ P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv\
    \ preprint arXiv:1707.06347, 2017. Silver, D., Schrittwieser, J., Simonyan, K.,\
    \ Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton,\
    \ A., et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359,\
    \ 2017. Song, H. F., Abdolmaleki, A., Springenberg, J. T., Clark, A., Soyer, H.,\
    \ Rae, J. W., Noury, S., Ahuja, A., Liu, S., Tirumala, D., et al. V-mpo: on-policy\
    \ maximum a posteriori policy optimization for discrete and continuous control.\
    \ International Conference on Learning Representations, 2020. Tessler, C., Mankowitz,\
    \ D. J., and Mannor, S. Reward constrained policy optimization. International\
    \ Conference on Learning Representation (ICLR), 2019. Tibshirani, R. J. Dykstra's\
    \ algorithm, admm, and coordinate descent: Connections, insights, and extensions.\
    \ Advances in Neural Information Processing Systems, 30, 2017. Todorov, E., Erez,\
    \ T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012\
    \ IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.\
    \ IEEE, 2012. Vinitsky, E., Kreidieh, A., Le Flem, L., Kheterpal, N., Jang, K.,\
    \ Wu, F., Liaw, R., Liang, E., and Bayen, A. M. Benchmarks for reinforcement learning\
    \ in mixed-autonomy traffic. In Proceedings of Conference on Robot Learning, pp.\
    \ 399-409, 2018. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik,\
    \ A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster\
    \ level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354,\
    \ 2019. Vuong, Q., Zhang, Y., and Ross, K. W. Supervised policy update for deep\
    \ reinforcement learning. In International Conference on Learning Representation\
    \ (ICLR), 2019. Wan, Y., Naik, A., and Sutton, R. S. Learning and planning in\
    \ average-reward markov decision processes. In International Conference on Machine\
    \ Learning, pp. 10653-10662. PMLR, 2021. Wei, C.-Y., Jahromi, M. J., Luo, H.,\
    \ and Jain, R. Learning infinite-horizon average-reward mdps with linear function\
    \ approximation. In International Conference on Artificial Intelligence and Statistics,\
    \ pp. 3007-3015. PMLR, 2021. Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., and\
    \ Ba, J. Scalable trust-region method for deep reinforcement learning using kronecker-factored\
    \ approximation. In Advances in neural information processing systems (NIPS),\
    \ pp. 5285-5294, 2017. Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.\
    \ J. Projection-based constrained policy optimization. In International Conference\
    \ on Learning Representation (ICLR), 2020. Zhang, S., Wan, Y., Sutton, R. S.,\
    \ and Whiteson, S. Average-reward off-policy policy evaluation with function approximation.\
    \ arXiv preprint arXiv:2101.02808, 2021. Zhang, Y. and Ross, K. Average reward\
    \ reinforcement learning with monotonic policy improvement. Preprint, 2020."
  title: 'ACPO: a policy optimization algorithm for average MDPs with constraints'
  url: https://dl.acm.org/doi/10.5555/3692070.3692088
- abstract: While neural networks allow highly accurate predictions in many tasks,
    their lack of robustness towards even slight input perturbations often hampers
    their deployment. Adversarial attacks such as the seminal projected gradient descent
    (PGD) offer an effective means to evaluate a model's robustness and dedicated
    solutions have been proposed for attacks on semantic segmentation or optical flow
    estimation. While they attempt to increase the attack's efficiency, a further
    objective is to balance its effect, so that it acts on the entire image domain
    instead of isolated point-wise predictions. This often comes at the cost of optimization
    stability and thus efficiency. Here, we propose CosPGD, an attack that encourages
    more balanced errors over the entire image domain while increasing the attack's
    overall efficiency. To this end, CosPGD leverages a simple alignment score computed
    from any pixel-wise prediction and its target to scale the loss in a smooth and
    fully differentiable way. It leads to efficient evaluations of a model's robustness
    for semantic segmentation as well as regression models (such as optical flow,
    disparity estimation, or image restoration), and it allows it to outperform the
    previous SotA attack on semantic segmentation. We provide code for the CosPGD
    algorithm and example usage at https://github.com/shashankskagnihotri/cospgd.
  keywords: Computing methodologies, Artificial intelligence, Computer vision, Computer
    vision problems, Object detection, Computer vision representations, Image representations,
    Computer vision tasks, Machine learning, Machine learning approaches, Neural networks,
    Security and privacy
  references: "Abdelhamed, A., Lin, S., and Brown, M. S. A high-quality denoising\
    \ dataset for smartphone cameras. In 2018 IEEE/CVF Conference on Computer Vision\
    \ and Pattern Recognition, pp. 1692-1700, 2018. Agnihotri, S., Gandikota, K. V.,\
    \ Grabinski, J., Chandramouli, P., and Keuper, M. On the unreasonable vulnerability\
    \ of transformers for image restoration - and an easy fix, 2023a. Agnihotri, S.,\
    \ Grabinski, J., and Keuper, M. Improving stability during upsampling-on the importance\
    \ of spatial context. arXiv preprint arXiv:2311.17524, 2023b. Andriushchenko,\
    \ M., Croce, F., Flammarion, N., and Hein, M. Square attack: a query-efficient\
    \ black-box adversarial attack via random search. In European conference on computer\
    \ vision, pp. 484-501. Springer, 2020. Arnab, A., Miksik, O., and Torr, P. H.\
    \ S. On the robustness of semantic segmentation models to adversarial attacks,\
    \ 2017. URL https://arxiv.org/abs/1711.09856. Brown, T. B., Man\xE9, D., Roy,\
    \ A., Abadi, M., and Gilmer, J. Adversarial patch, 2017. URL https://arxiv.org/abs/1712.09665.\
    \ Buhrmester, V., M\xFCnch, D., and Arens, M. Analysis of explainers of black\
    \ box deep neural networks for computer vision: A survey, 2019. URL https://arxiv.org/abs/1911.12116.\
    \ Butler, D. J., Wulff, J., Stanley, G. B., and Black, M. J. A naturalistic open\
    \ source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.) (ed.),\
    \ European Conf. on Computer Vision (ECCV), Part IV, LNCS 7577, pp. 611-625. Springer-Verlag,\
    \ October 2012. Carlini, N. and Wagner, D. Towards evaluating the robustness of\
    \ neural networks. In 2017 ieee symposium on security and privacy (sp), pp. 39-57.\
    \ IEEE, 2017. Chen, L., Chu, X., Zhang, X., and Sun, J. Simple baselines for image\
    \ restoration, 2022. Chen, L.-C., Papandreou, G., Schroff, F., and Adam, H. Rethinking\
    \ atrous convolution for semantic image segmentation, 2017. Cordts, M., Omran,\
    \ M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S.,\
    \ and Schiele, B. The cityscapes dataset for semantic urban scene understanding,\
    \ 2016. Croce, F. and Hein, M. Reliable evaluation of adversarial robustness with\
    \ an ensemble of diverse parameter-free attacks. In ICML, 2020. Croce, F., Andriushchenko,\
    \ M., Sehwag, V., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench:\
    \ a standardized adversarial robustness benchmark. CoRR, abs/2010.09670, 2020.\
    \ URL https://arxiv.org/abs/2010.09670. Croce, F., Andriushchenko, M., Sehwag,\
    \ V., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench:\
    \ a standardized adversarial robustness benchmark. In Thirty-fifth Conference\
    \ on Neural Information Processing Systems Datasets and Benchmarks Track (Round\
    \ 2), 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. Croce, F., Singh,\
    \ N. D., and Hein, M. Robust semantic segmentation: Strong adversarial attacks\
    \ and fast training of robust models, 2023. URL https://arxiv.org/abs/2306.12941.\
    \ Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li, J. Boosting adversarial\
    \ attacks with momentum. In 2018 IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition (CVPR), pp. 9185-9193, Los Alamitos, CA, USA, jun 2018. IEEE Computer\
    \ Society. Dosovitskiy, A., Fischer, P., Ilg, E., H\xE4usser, P., Hazirbas, C.,\
    \ Golkov, V., v.d. Smagt, P., Cremers, D., and Brox, T. Flownet: Learning optical\
    \ flow with convolutional networks. In IEEE International Conference on Computer\
    \ Vision (ICCV), 2015. URL http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15.\
    \ Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A.\
    \ The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html,\
    \ 2012. Fischer, P., Dosovitskiy, A., Ilg, E., H\xE4usser, P., Hazirba\u015F,\
    \ C., Golkov, V., van der Smagt, P., Cremers, D., and Brox, T. Flownet: Learning\
    \ optical flow with convolutional networks, 2015. URL https://arxiv.org/abs/1504.06852.\
    \ Gajjar, S., Hati, A., Bhilare, S., and Mandal, S. Generating targeted adversarial\
    \ attacks and assessing their effectiveness in fooling deep neural networks. In\
    \ 2022 IEEE International Conference on Signal Processing and Communications (SPCOM),\
    \ pp. 1-5, 2022. Gavrikov, P., Lukasik, J., Jung, S., Geirhos, R., Lamm, B., Mirza,\
    \ M. J., Keuper, M., and Keuper, J. Are vision language models texture or shape\
    \ biased and can we steer them? arXiv preprint arXiv:2403.09193, 2024. Geirhos,\
    \ R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W.\
    \ Imagenet-trained cnns are biased towards texture; increasing shape bias improves\
    \ accuracy and robustness, 2018. URL https://arxiv.org/abs/1811.12231. Geirhos,\
    \ R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and\
    \ Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence,\
    \ 2(11):665-673, nov 2020. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\
    \ and harnessing adversarial examples, 2014. URL https://arxiv.org/abs/1412.6572.\
    \ Grabinski, J., Jung, S., Keuper, J., and Keuper, M. Frequencylowcut pooling-plug\
    \ & play against catastrophic overfitting. arXiv preprint arXiv:2204.00491, 2022.\
    \ Grabinski, J., Keuper, J., and Keuper, M. Fix your down-sampling asap! be natively\
    \ more robust via aliasing and spectral artifact free pooling, 2023. Gu, J., Zhao,\
    \ H., Tresp, V., and Torr, P. H. Segpgd: An effective and efficient adversarial\
    \ attack for evaluating and boosting segmentation robustness. In European Conference\
    \ on Computer Vision, pp. 308-325. Springer, 2022. Hariharan, B., Arbelaez, P.,\
    \ Bourdev, L., Maji, S., and Malik, J. Semantic contours from inverse detectors.\
    \ In International Conference on Computer Vision (ICCV), 2011. Hariharan, B.,\
    \ Arbel\xE1ez, P., Girshick, R., and Malik, J. Hypercolumns for object segmentation\
    \ and fine-grained localization. In 2015 IEEE Conference on Computer Vision and\
    \ Pattern Recognition (CVPR), pp. 447-456, 2015. He, K., Zhang, X., Ren, S., and\
    \ Sun, J. Deep residual learning for image recognition, 2015. URL https://arxiv.org/abs/1512.03385.\
    \ Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common\
    \ corruptions and perturbations, 2019. URL https://arxiv.org/abs/1903.12261. Hendrycks,\
    \ D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples,\
    \ 2019. URL https://arxiv.org/abs/1907.07174. Hoffmann, J., Agnihotri, S., Saikia,\
    \ T., and Brox, T. Towards improving robustness of compressed cnns. In ICML Workshop\
    \ on Uncertainty and Robustness in Deep Learning (UDL), 2021. Ilg, E., Mayer,\
    \ N., Saikia, T., Keuper, M., Dosovitskiy, A., and Brox, T. Flownet 2.0: Evolution\
    \ of optical flow estimation with deep networks, 2016. URL https://arxiv.org/abs/1612.01925.\
    \ Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box adversarial attacks\
    \ with limited queries and information. In Proceedings of the 35th International\
    \ Conference on Machine Learning, ICML 2018, July 2018. URL https://arxiv.org/abs/1804.08598.\
    \ Iyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L. Adversarial example\
    \ generation with syntactically controlled paraphrase networks. In Proceedings\
    \ of the 2018 Conference of the North American Chapter of the Association for\
    \ Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),\
    \ pp. 1875-1885, New Orleans, Louisiana, June 2018. Association for Computational\
    \ Linguistics. URL https://aclanthology.org/N18-1170. Jia, J., Qu, W., and Gong,\
    \ N. Multiguard: Provably robust multi-label classification against adversarial\
    \ examples. Advances in Neural Information Processing Systems, 35: 10150-10163,\
    \ 2022. Jiang, S., Campbell, D., Lu, Y., Li, H., and Hartley, R. Learning to estimate\
    \ hidden motions with global motion aggregation, 2021. Jung, S. and Keuper, M.\
    \ Spectral distribution aware image generation, 2020. URL https://arxiv.org/abs/2012.03110.\
    \ Jung, S. and Keuper, M. Internalized biases in fr\xE9chet inception distance.\
    \ In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications,\
    \ 2021. Jung, S., Ziegler, S., Kardoost, A., and Keuper, M. Optimizing edge detection\
    \ for image segmentation with multicut penalties. In DAGM German Conference on\
    \ Pattern Recognition, pp. 182-197. Springer, 2022. Jung, S., Lukasik, J., and\
    \ Keuper, M. Neural architecture design and robustness: A dataset. arXiv preprint\
    \ arXiv:2306.06712, 2023a. Jung, S., Schwedhelm, J. C., Schillings, C., and Keuper,\
    \ M. Happy people-image synthesis as black-box optimization problem in the discrete\
    \ latent space of deep generative models. arXiv preprint arXiv:2306.06684, 2023b.\
    \ Kang, D., Sun, Y., Hendrycks, D., Brown, T., and Steinhardt, J. Testing robustness\
    \ against unforeseen adversaries, 2019. URL https://arxiv.org/abs/1908.08016.\
    \ Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\
    \ T., Whitehead, S., Berg, A. C., Lo, W.-Y., Doll\xE1r, P., and Girshick, R. Segment\
    \ anything. arXiv:2304.02643, 2023. Krizhevsky, A., Sutskever, I., and Hinton,\
    \ G. E. Imagenet classification with deep convolutional neural networks. In Pereira,\
    \ F., Burges, C., Bottou, L., and Weinberger, K. (eds.), Advances in Neural Information\
    \ Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\
    \ Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial examples in the physical\
    \ world, 2016. URL https://arxiv.org/abs/1607.02533. Kurakin, A., Goodfellow,\
    \ I., and Bengio, S. Adversarial machine learning at scale, 2017. URL. Li, Z.,\
    \ Liu, X., Drenkow, N., Ding, A., Creighton, F. X., Taylor, R. H., and Unberath,\
    \ M. Revisiting stereo depth estimation from a sequence-to-sequence perspective\
    \ with transformers. In Proceedings of the IEEE/CVF International Conference on\
    \ Computer Vision (ICCV), pp. 6197-6206, October 2021. Liu, Z., Mao, H., Wu, C.-Y.,\
    \ Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. In Proceedings\
    \ of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11976-11986,\
    \ 2022. Lukasik, J., Jung, S., and Keuper, M. Learning where to look-generative\
    \ nas is surprisingly efficient. In European Conference on Computer Vision, pp.\
    \ 257-273. Springer, 2022. Lukasik, J., Gavrikov, P., Keuper, J., and Keuper,\
    \ M. Improving native cnn robustness with filter frequency regularization. Transactions\
    \ on Machine Learning Research, 2023a. Lukasik, J., Moeller, M., and Keuper, M.\
    \ An evaluation of zero-cost proxies-from neural architecture performance prediction\
    \ to model robustness. In DAGM German Conference on Pattern Recognition, pp. 624-638.\
    \ Springer, 2023b. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu,\
    \ A. Towards deep learning models resistant to adversarial attacks, 2017. URL\
    \ https://arxiv.org/abs/1706.06083. Mayer, N., Ilg, E., H\xE4usser, P., Fischer,\
    \ P., Cremers, D., Dosovitskiy, A., and Brox, T. A large dataset to train convolutional\
    \ networks for disparity, optical flow, and scene flow estimation. In IEEE International\
    \ Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL http://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16.\
    \ arXiv:1512.02134. Mehl, L., Schmalfuss, J., Jahedi, A., Nalivayko, Y., and Bruhn,\
    \ A. Spring: A high-resolution high-detail dataset and benchmark for scene flow,\
    \ optical flow and stereo. In Proceedings of the IEEE/CVF Conference on Computer\
    \ Vision and Pattern Recognition, pp. 4981-4991, 2023. Menze, M. and Geiger, A.\
    \ Object scene flow for autonomous vehicles. In Conference on Computer Vision\
    \ and Pattern Recognition (CVPR), 2015. Moosavi-Dezfooli, S.-M., Fawzi, A., and\
    \ Frossard, P. Deepfool: a simple and accurate method to fool deep neural networks,\
    \ 2015. URL https://arxiv.org/abs/1511.04599. Morris, J. X., Lifland, E., Yoo,\
    \ J. Y., Grigsby, J., Jin, D., and Qi, Y. Textattack: A framework for adversarial\
    \ attacks, data augmentation, and adversarial training in nlp, 2020. URL https://arxiv.org/abs/2005.05909.\
    \ Nah, S., Kim, T. H., and Lee, K. M. Deep multi-scale convolutional neural network\
    \ for dynamic scene deblurring. In CVPR, July 2017. Qu, W., Li, Y., and Wang,\
    \ B. A certified radius-guided attack framework to image segmentation models.\
    \ arXiv preprint arXiv:2304.02693, 2023. Ranjan, A. and Black, M. J. Optical flow\
    \ estimation using a spatial pyramid network. In Proceedings of the IEEE Conference\
    \ on Computer Vision and Pattern Recognition, 2017. Ribeiro, M. T., Singh, S.,\
    \ and Guestrin, C. Semantically equivalent adversarial rules for debugging NLP\
    \ models. In Proceedings of the 56th Annual Meeting of the Association for Computational\
    \ Linguistics (Volume 1: Long Papers), pp. 856-865, Melbourne, Australia, July\
    \ 2018. Association for Computational Linguistics. URL https://aclanthology.org/P18-1079.\
    \ Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for\
    \ biomedical image segmentation, 2015. URL https://arxiv.org/abs/1505.04597. Rony,\
    \ J., Hafemann, L. G., Oliveira, L. S., Ayed, I. B., Sabourin, R., and Granger,\
    \ E. Decoupling direction and norm for efficient gradient-based l2 adversarial\
    \ attacks and defenses, 2019. Rony, J., Pesquet, J., and Ayed, I. Proximal splitting\
    \ adversarial attack for semantic segmentation. In 2023 IEEE/CVF Conference on\
    \ Computer Vision and Pattern Recognition (CVPR), pp. 20524-20533, Los Alamitos,\
    \ CA, USA, jun 2023. IEEE Computer Society. URL Scheurer, E., Schmalfuss, J.,\
    \ Lis, A., and Bruhn, A. Detection defenses: An empty promise against adversarial\
    \ patch attacks on optical flow. In Proceedings of the IEEE/CVF Winter Conference\
    \ on Applications of Computer Vision, pp. 6489-6498, 2024. Schmalfuss, J., Mehl,\
    \ L., and Bruhn, A. Attacking motion estimation with adversarial snow. arXiv preprint\
    \ arXiv:2210.11242, 2022a. Schmalfuss, J., Scholze, P., and Bruhn, A. A perturbation-constrained\
    \ adversarial attack for evaluating the robustness of optical flow. In European\
    \ Conference on Computer Vision, pp. 183-200. Springer, 2022b. Schmalfuss, J.,\
    \ Mehl, L., and Bruhn, A. Distracting downpour: Adversarial weather attacks for\
    \ motion estimation. In Proceedings of the IEEE/CVF International Conference on\
    \ Computer Vision, pp. 10106-10116, 2023. Schrodi, S., Saikia, T., and Brox, T.\
    \ Towards understanding adversarial robustness of optical flow networks. In Proceedings\
    \ of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8916-8924,\
    \ 2022. Sommerhoff, H., Agnihotri, S., Saleh, M., Moeller, M., Keuper, M., and\
    \ Kolb, A. Differentiable sensor layouts for end-to-end learning of task-specific\
    \ camera parameters. arXiv preprint arXiv:2304.14736, 2023. Sun, D., Yang, X.,\
    \ Liu, M.-Y., and Kautz, J. Pwc-net: Cnns for optical flow using pyramid, warping,\
    \ and cost volume, 2018. Sun, Y., Chen, F., Chen, Z., and Wang, M. Local aggressive\
    \ adversarial attacks on 3d point cloud, 2021. URL https://arxiv.org/abs/2105.09090.\
    \ Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,\
    \ and Fergus, R. Intriguing properties of neural networks, 2014. Teed, Z. and\
    \ Deng, J. Raft: Recurrent all-pairs field transforms for optical flow, 2020.\
    \ URL https://arxiv.org/abs/2003.12039. username: mberkay0, B. M. mberkay0/pretrainedbackbones-unet.\
    \ https://github.com/mberkay0/pretrained-backbones-unet, 2023. Vo, J., Xie, J.,\
    \ and Patel, S. Multiclass asma vs targeted pgd attack in image segmentation,\
    \ 2022. URL https://arxiv.org/abs/2208.01844. Wang, Z., Bovik, A., Sheikh, H.,\
    \ and Simoncelli, E. Image quality assessment: from error visibility to structural\
    \ similarity. IEEE Transactions on Image Processing, 13 (4):600-612, 2004. Wang,\
    \ Z., Pang, T., Du, C., Lin, M., Liu, W., and Yan, S. Better diffusion models\
    \ further improve adversarial training, 2023. Wong, A., Cicek, S., and Soatto,\
    \ S. Targeted adversarial perturbations for monocular depth prediction. In Advances\
    \ in neural information processing systems, 2020a. Wong, E., Rice, L., and Kolter,\
    \ J. Z. Fast is better than free: Revisiting adversarial training, 2020b. URL\
    \ https://arxiv.org/abs/2001.03994. Wulff, J., Butler, D. J., Stanley, G. B.,\
    \ and Black, M. J. Lessons and insights from creating a synthetic optical flow\
    \ benchmark. In A. Fusiello et al. (Eds.) (ed.), ECCV Workshop on Unsolved Problems\
    \ in Optical Flow and Stereo Estimation, Part II, LNCS 7584, pp. 168-177. Springer-Verlag,\
    \ October 2012. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified perceptual\
    \ parsing for scene understanding. In European Conference on Computer Vision.\
    \ Springer, 2018. Xie, C., Wu, Y., van der Maaten, L., Yuille, A., and He, K.\
    \ Feature denoising for improving adversarial robustness, 2019. Xie, E., Wang,\
    \ W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. Segformer: Simple and\
    \ efficient design for semantic segmentation with transformers. Advances in neural\
    \ information processing systems, 34:12077-12090, 2021. Xie, S., Girshick, R.,\
    \ Doll\xE1r, P., Tu, Z., and He, K. Aggregated residual transformations for deep\
    \ neural networks, 2016. URL https://arxiv.org/abs/1611.05431. Xu, X., Zhao, H.,\
    \ and Jia, J. Dynamic divide-and-conquer adversarial training for robust semantic\
    \ segmentation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV),\
    \ pp. 7466-7475, 2021. Zamir, S. W., Arora, A., Khan, S., Hayat, M., Khan, F.\
    \ S., and Yang, M.-H. Restormer: Efficient transformer for high-resolution image\
    \ restoration. In CVPR, 2022. Zhang, J., Chen, L., Liu, B., Ouyang, B., Xie, Q.,\
    \ Zhu, J., Li, W., and Meng, Y. 3d adversarial attacks beyond point cloud, 2021.\
    \ URL https://arxiv.org/abs/2104.12146. Zhao, H. semseg. https://github.com/hszhao/semseg,\
    \ 2019. Zhao, H., Shi, J., Qi, X., Wang, X., and Jia, J. Pyramid scene parsing\
    \ network. In CVPR, 2017. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso,\
    \ A., and Torralba, A. Scene parsing through ade20k dataset. In Proceedings of\
    \ the IEEE Conference on Computer Vision and Pattern Recognition, 2017. Zhou,\
    \ B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A.\
    \ Semantic understanding of scenes through the ade20k dataset. International Journal\
    \ of Computer Vision, 127(3):302-321, 2019."
  title: 'CosPGD: an efficient white-box adversarial attack for pixel-wise prediction
    tasks'
  url: https://dl.acm.org/doi/10.5555/3692070.3692089
- abstract: A promising approach to preserving model performance in linearized transformers
    is to employ position-based re-weighting functions. However, state-of-the-art
    re-weighting functions rely heavily on target sequence lengths, making it difficult
    or impossible to apply them to autoregressive and simultaneous tasks, where the
    target and sometimes even the input sequence length are unknown. To address this
    issue, we propose Learned Proportions (LeaP) and LeaPformers. Our contribution
    is built on two major components. First, we generalize the dependence on explicit
    positional representations and sequence lengths into dependence on sequence proportions
    for re-weighting. Second, we replace static positional representations with dynamic
    proportions derived via a compact module, enabling more flexible attention concentration
    patterns. We evaluate LeaPformer against eight representative efficient transformers
    on the Long-Range Arena benchmark, showing that LeaPformer achieves the best quality-throughput
    trade-off, as well as LeaPformer to Wikitext-103 autoregressive language modeling
    and simultaneous speech-to-text translation for two language pairs, achieving
    competitive results.
  keywords: Applied computing, Arts and humanities, Language translation, Computing
    methodologies, Artificial intelligence, Natural language processing, Information
    extraction, Machine translation, Natural language generation, Machine learning,
    Learning paradigms, Supervised learning, Supervised learning by regression
  references: "Agostinelli, V. and Chen, L. Improving autoregressive nlp tasks via\
    \ modular linearized attention. In Koutra, D., Plant, C., Gomez Rodriguez, M.,\
    \ Baralis, E., and Bonchi, F. (eds.), Machine Learning and Knowledge Discovery\
    \ in Databases: Research Track, pp. 90-106, Cham, 2023. Springer Nature Switzerland.\
    \ ISBN 978-3-031-43421-1. Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher,\
    \ Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. Etc: Encoding\
    \ long and structured inputs in transformers, 2020. URL https://arxiv.org/abs/2004.08483.\
    \ Baevski, A. and Auli, M. Adaptive input representations for neural language\
    \ modeling. In International Conference on Learning Representations, 2019. URL\
    \ https://openreview.net/forum?id=ByxZX20qFQ. Beltagy, I., Peters, M. E., and\
    \ Cohan, A. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150.\
    \ Cattoni, R., Di Gangi, M. A., Bentivogli, L., Negri, M., and Turchi, M. Must-c:\
    \ A multilingual corpus for end-to-end speech translation. Computer Speech & Language,\
    \ 66:101155, 2021. ISSN 0885-2308. URL https://www.sciencedirect.com/science/article/pii/S0885230820300887.\
    \ Chen, Y., Zeng, Q., Ji, H., and Yang, Y. Skyformer: Remodel self-attention with\
    \ gaussian kernel and nystr\xF6m method. In Advances in Neural Information Processing\
    \ Systems 35: Annual Conference on Neural Information Processing Systems 2021,\
    \ NeurIPS 2021, December 6-14, 2021, virtual, 2021. Child, R., Gray, S., Radford,\
    \ A., and Sutskever, I. Generating long sequences with sparse transformers, 2019.\
    \ URL https://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan,\
    \ D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser,\
    \ L., Belanger, D., Colwell, L., and Weller, A. Rethinking attention with performers,\
    \ 2020. URL https://arxiv.org/abs/2009.14794. Dai, Z., Yang, Z., Yang, Y., Carbonell,\
    \ J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models\
    \ beyond a fixed-length context, 2019. URL https://arxiv.org/abs/1901.02860. Dosovitskiy,\
    \ A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\
    \ M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An\
    \ image is worth 16x16 words: Transformers for image recognition at scale, 2021.\
    \ Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state\
    \ spaces, 2023. Gu, A., Goel, K., and R\xE9, C. Efficiently modeling long sequences\
    \ with structured state spaces. In The International Conference on Learning Representations\
    \ (ICLR), 2022. Inaguma, H., Kiyono, S., Duh, K., Karita, S., Soplin, N. E. Y.,\
    \ Hayashi, T., and Watanabe, S. Espnet-st: Allin-one speech translation toolkit,\
    \ 2020. URL https://arxiv.org/abs/2004.10234. Katharopoulos, A., Vyas, A., Pappas,\
    \ N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers\
    \ with linear attention, 2020. URL https://arxiv.org/abs/2006.16236. Kitaev, N.,\
    \ Kaiser, L., and Levskaya, A. Reformer: The efficient transformer, 2020. URL\
    \ https://arxiv.org/abs/2001.04451. Liu, Z., Li, D., Lu, K., Qin, Z., Sun, W.,\
    \ Xu, J., and Zhong, Y. Neural architecture search on efficient transformers and\
    \ beyond, 2022. Liutkus, A., C\xEDfka, O., Wu, S.-L., \u015Eim\u015Fekli, U.,\
    \ Yang, Y.-H., and Richard, G. Relative positional encoding for Transformers with\
    \ linear complexity. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th\
    \ International Conference on Machine Learning, volume 139 of Proceedings of Machine\
    \ Learning Research, pp. 7067-7079. PMLR, 18-24 Jul 2021. URL http://proceedings.mlr.press/v139/liutkus21a.html.\
    \ Ma, M., Huang, L., Xiong, H., Zheng, R., Liu, K., Zheng, B., Zhang, C., He,\
    \ Z., Liu, H., Li, X., Wu, H., and Wang, H. Stacl: Simultaneous translation with\
    \ implicit anticipation and controllable latency using prefix-to-prefix framework.\
    \ In Proceedings of the 57th Annual Meeting of the Association for Computational\
    \ Linguistics, pp. 3025-3036, Florence, Italy, 2019. Association for Computational\
    \ Linguistics (ACL). Ma, X., Dousti, M. J., Wang, C., Gu, J., and Pino, J. Simuleval:\
    \ An evaluation toolkit for simultaneous translation, 2020a. URL https://arxiv.org/abs/2007.16193.\
    \ Ma, X., Pino, J., Cross, J., Puzon, L., and Gu, J. Monotonic multihead attention.\
    \ In International Conference on Learning Representations, 2020b. Ma, X., Pino,\
    \ J., Cross, J., Puzon, L., and Gu, J. Simulmt to simulst: Adapting simultaneous\
    \ text translation to end-to-end simultaneous speech translation. In Proceedings\
    \ of 2020 Asia-Pacific Chapter of the Association for Computational Linguistics\
    \ and the International Joint Conference on Natural Language Processing, 2020c.\
    \ Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture\
    \ models. ArXiv, abs/1609.07843, 2016. URL https://api.semanticscholar.org/CorpusID:16299141.\
    \ Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D.,\
    \ and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling, 2019.\
    \ URL https://arxiv.org/abs/1904.01038. Parmar, N., Vaswani, A., Uszkoreit, J.,\
    \ Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer, 2018. URL https://arxiv.org/abs/1802.05751.\
    \ Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L.\
    \ Random feature attention, 2021. URL https://arxiv.org/abs/2103.02143. Post,\
    \ M. A call for clarity in reporting BLEU scores. In Proceedings of the Third\
    \ Conference on Machine Translation: Research Papers, pp. 186-191, Belgium, Brussels,\
    \ October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-6319.\
    \ Press, O., Smith, N., and Lewis, M. Train short, test long: Attention with linear\
    \ biases enables input length extrapolation. In International Conference on Learning\
    \ Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Qin,\
    \ Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., and Zhong, Y. The devil\
    \ in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods\
    \ in Natural Language Processing, pp. 7025-7041, Abu Dhabi, United Arab Emirates,\
    \ December 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.473.\
    \ Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and\
    \ Zhong, Y. cosformer: Rethinking softmax in attention. In International Conference\
    \ on Learning Representations, 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4.\
    \ Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\
    \ Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a\
    \ unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN\
    \ 1532-4435. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer:\
    \ Enhanced transformer with rotary position embedding, 2022. Tay, Y., Bahri, D.,\
    \ Yang, L., Metzler, D., and Juan, D.- C. Sparse sinkhorn attention, 2020. URL\
    \ https://arxiv.org/abs/2002.11296. Tay, Y., Dehghani, M., Abnar, S., Shen, Y.,\
    \ Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range\
    \ arena : A benchmark for efficient transformers. In International Conference\
    \ on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.\
    \ Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A\
    \ survey. ACM Comput. Surv., 55(6), dec 2022. ISSN 0360-0300. Vaswani, A., Shazeer,\
    \ N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin,\
    \ I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,\
    \ H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural\
    \ Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL\
    \ https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\
    \ Wang, B., Zhao, D., Lioma, C., Li, Q., Zhang, P., and Simonsen, J. G. Encoding\
    \ word order in complex embeddings. ArXiv, abs/1912.12333, 2019. URL https://api.semanticscholar.org/CorpusID:209516262.\
    \ Wang, C., Pino, J., Wu, A., and Gu, J. CoVoST: A diverse multilingual speech-to-text\
    \ translation corpus. In Proceedings of The 12th Language Resources and Evaluation\
    \ Conference, pp. 4197-4203, Marseille, France, May 2020a. European Language Resources\
    \ Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-1.517.\
    \ Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention\
    \ with linear complexity, 2020b. URL https://arxiv.org/abs/2006.04768. Wang, Y.-A.\
    \ and Chen, Y.-N. What do position embeddings learn? an empirical study of pre-trained\
    \ language model positional encoding. In Webber, B., Cohn, T., He, Y., and Liu,\
    \ Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural\
    \ Language Processing (EMNLP), pp. 6840-6849, Online, November 2020. Association\
    \ for Computational Linguistics. URL https://aclanthology.org/2020.emnlp-main.555.\
    \ Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., and Yu, Z. Memformer: A memory-augmented\
    \ transformer for sequence modeling, 2020. URL https://arxiv.org/abs/2010.06891.\
    \ Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh,\
    \ V. Nystr\xF6mformer: A nystr\xF6m-based algorithm for approximating self-attention.\
    \ 2021. Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon,\
    \ S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: Transformers\
    \ for longer sequences. 2020. URL https://arxiv.org/abs/2007.14062. Zhong, G.,\
    \ Lin, X., Chen, K., Li, Q., and Huang, K. Long short-term attention. In Ren,\
    \ J., Hussain, A., Zhao, H., Huang, K., Zheng, J., Cai, J., Chen, R., and Xiao,\
    \ Y. (eds.), Advances in Brain Inspired Cognitive Systems, pp. 45-54, Cham, 2020.\
    \ Springer International Publishing. ISBN 978-3-030-39431-8."
  title: 'LeaPformer: enabling linear transformers for autoregressive and simultaneous
    tasks via learned proportions'
  url: https://dl.acm.org/doi/10.5555/3692070.3692090
