{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import evaluate\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832f8a8",
   "metadata": {},
   "source": [
    "## Data Loading / Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ba352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_jsonl(input_file,\n",
    "                train_file=\"train.jsonl\",\n",
    "                valid_file=\"valid.jsonl\",\n",
    "                test_file=\"test.jsonl\",\n",
    "                ratios=(0.8, 0.1, 0.1),\n",
    "                seed=42):\n",
    "    \"\"\"\n",
    "    Split a single JSONL file into train/valid/test files.\n",
    "    ratios must sum to 1.0 (otherwise they'll be normalized).\n",
    "    Returns a tuple with the counts (train, valid, test).\n",
    "    \"\"\"\n",
    "    # normalize ratios\n",
    "    total = sum(ratios)\n",
    "    if total <= 0:\n",
    "        raise ValueError(\"ratios must sum to a positive number\")\n",
    "    r = [x / total for x in ratios]\n",
    "\n",
    "    p = Path(input_file)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{input_file} not found\")\n",
    "\n",
    "    # read all non-empty lines (preserve original JSON lines)\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.rstrip(\"\\n\") for ln in f if ln.strip()]\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(lines)\n",
    "    print(lines[0])\n",
    "\n",
    "    n = len(lines)\n",
    "    n_train = int(n * r[0])\n",
    "    n_valid = int(n * r[1])\n",
    "\n",
    "    train_lines = lines[:n_train]\n",
    "    valid_lines = lines[n_train:n_train + n_valid]\n",
    "    test_lines = lines[n_train + n_valid:]\n",
    "\n",
    "    # write out files (ensure trailing newline if non-empty)\n",
    "    def write_lines(path, arr):\n",
    "        path = Path(path)\n",
    "        if arr:\n",
    "            path.write_text(\"\\n\".join(arr) + \"\\n\", encoding=\"utf-8\")\n",
    "        else:\n",
    "            # create empty file\n",
    "            path.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "    write_lines(train_file, train_lines)\n",
    "    write_lines(valid_file, valid_lines)\n",
    "    write_lines(test_file, test_lines)\n",
    "\n",
    "    return (len(train_lines), len(valid_lines), len(test_lines))\n",
    "\n",
    "# Example usage:\n",
    "# split_jsonl(\"dataset.jsonl\", \"train.jsonl\", \"valid.jsonl\", \"test.jsonl\", ratios=(0.8,0.1,0.1), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\",\n",
    "                                           \"validation\": \"valid.jsonl\",\n",
    "                                           \"test\": \"test.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],\n",
    "                     padding=\"max_length\",\n",
    "                     truncation=True,\n",
    "                     max_length=512)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "# tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized.set_format(\"torch\",\n",
    "                     columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7678ed0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Build a deterministic mapping from string labels to integers and apply it to `tokenized`\u001b[39;00m\n\u001b[32m      2\u001b[39m unique_labels = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized\u001b[49m:\n\u001b[32m      4\u001b[39m     unique_labels.update(\u001b[38;5;28mset\u001b[39m(tokenized[split][\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m      6\u001b[39m label_list = \u001b[38;5;28msorted\u001b[39m(unique_labels)  \u001b[38;5;66;03m# deterministic order\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "# Build a deterministic mapping from string labels to integers and apply it to `tokenized`\n",
    "unique_labels = set()\n",
    "for split in tokenized:\n",
    "    unique_labels.update(set(tokenized[split][\"label\"]))\n",
    "\n",
    "label_list = sorted(unique_labels)  # deterministic order\n",
    "label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "\n",
    "def _map_label(example):\n",
    "    lab = example[\"label\"]\n",
    "    # if already integer, keep as is\n",
    "    if isinstance(lab, int):\n",
    "        return example\n",
    "    example[\"label\"] = label2id[lab]\n",
    "    return example\n",
    "\n",
    "tokenized = tokenized.map(_map_label)\n",
    "\n",
    "# ensure torch format (re-apply to be safe)\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# update num_labels variable\n",
    "num_labels = len(label2id)\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "\n",
    "id2label = {\"LABEL_\" + str(v): k for k, v in label2id.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be47e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(dataset[\"train\"][\"label\"])\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec137d1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de039990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_out\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ed864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e6224",
   "metadata": {},
   "source": [
    "## Testing / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff54428",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "preds = trainer.predict(tokenized[\"test\"])\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "y_true = preds.label_ids\n",
    "\n",
    "print(y_pred)\n",
    "print(y_true)\n",
    "\n",
    "acc = accuracy.compute(predictions=y_pred, references=y_true)\n",
    "print(acc)\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "preds = trainer.predict(tokenized[\"test\"])\n",
    "f1_score = f1.compute(predictions=preds.predictions.argmax(-1),\n",
    "                      references=preds.label_ids,\n",
    "                      average=\"macro\")\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"final_model\")\n",
    "tokenizer.save_pretrained(\"final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
