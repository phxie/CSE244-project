{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82678fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xuedo\\onedrive\\desktop\\cse244-project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xuedo\\onedrive\\desktop\\cse244-project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 15.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 15.6 MB/s  0:00:00\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading numpy-2.3.5-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.6/12.8 MB 16.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 15.4 MB/s  0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyyaml, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/5 [pytz]\n",
      "   -------- ------------------------------- 1/5 [tzdata]\n",
      "   -------- ------------------------------- 1/5 [tzdata]\n",
      "   ---------------- ----------------------- 2/5 [pyyaml]\n",
      "   ---------------- ----------------------- 2/5 [pyyaml]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [numpy]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   -------------------------------- ------- 4/5 [pandas]\n",
      "   ---------------------------------------- 5/5 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.5 pandas-2.3.3 pytz-2025.2 pyyaml-6.0.3 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2422499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your YAML file (change as needed)\n",
    "file_path = \"C:\\\\Users\\\\xuedo\\\\OneDrive\\\\Desktop\\\\CSE244-project\\\\dataset_prep\\\\old_data\\\\Databases.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"YAML file not found: {file_path}\")\n",
    "\n",
    "# If the YAML is a list of dicts, convert to a DataFrame for easier inspection\n",
    "if isinstance(data[\"papers\"], list) and all(isinstance(item, dict) for item in data[\"papers\"]):\n",
    "    df_yaml = pd.DataFrame(data[\"papers\"])\n",
    "    df_yaml  # last expression will display the DataFrame in Jupyter\n",
    "else:\n",
    "    # pretty-print the loaded YAML structure\n",
    "    print(yaml.safe_dump(data, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303119ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract  \\\n",
      "0  The end-to-end lookup latency of a hierarchica...   \n",
      "1  k-closest pair (KCP for short) search is a fun...   \n",
      "2  Compiler optimization plays an increasingly im...   \n",
      "3  Given a graph G, a cost associated with each n...   \n",
      "4  As an important cohesive subgraph model in bip...   \n",
      "\n",
      "                                            keywords  \\\n",
      "0  Information systems, Data management systems, ...   \n",
      "1  Information systems, Data management systems, ...   \n",
      "2  Computer systems organization, Architectures, ...   \n",
      "3  Theory of computation, Design and analysis of ...   \n",
      "4  Mathematics of computing, Discrete mathematics...   \n",
      "\n",
      "                                          references  \\\n",
      "0  [n.d.]. https://github.com/illinoisdata/airind...   \n",
      "1  Michiel H. M. Smid. Closest-point problems in ...   \n",
      "2  Cited April 2023. Stablehlo, backward compatib...   \n",
      "3  Cigdem Aslay, Francesco Bonchi, Laks VS Lakshm...   \n",
      "4  Aman Abidi, Lu Chen, Rui Zhou, and Chengfei Li...   \n",
      "\n",
      "                                               title  \\\n",
      "0  AirIndex: Versatile Index Tuning Through Data ...   \n",
      "1              Closest Pairs Search Over Data Stream   \n",
      "2  BladeDISC: Optimizing Dynamic Shape Machine Le...   \n",
      "3  Efficient Algorithm for Budgeted Adaptive Infl...   \n",
      "4  Efficient Core Maintenance in Large Bipartite ...   \n",
      "\n",
      "                                      url  \n",
      "0  https://dl.acm.org/doi/10.1145/3617308  \n",
      "1  https://dl.acm.org/doi/10.1145/3617326  \n",
      "2  https://dl.acm.org/doi/10.1145/3617327  \n",
      "3  https://dl.acm.org/doi/10.1145/3617328  \n",
      "4  https://dl.acm.org/doi/10.1145/3617329  \n"
     ]
    }
   ],
   "source": [
    "print(df_yaml.head())  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff7c631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ArtificialIntelligence.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerNetworks.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerSecurity.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerVision.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\Databases.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\MachineLearning.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\WWWInformationRetrieval.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# use existing df_yaml and file_path variables from the notebook\n",
    "p = Path(file_path)\n",
    "\n",
    "old_data_dir = p.parent  # existing variable from notebook\n",
    "entries = sorted(old_data_dir.iterdir(), key=lambda x: x.name.lower())\n",
    "\n",
    "# Print each entry and whether it's a file or directory\n",
    "out_path = \"dataset.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    for entry in entries:\n",
    "        with open(entry, \"r\") as f:\n",
    "            print(entry)\n",
    "            data = yaml.safe_load(f)\n",
    "            if isinstance(data[\"papers\"], list) and all(isinstance(item, dict) for item in data[\"papers\"]):\n",
    "                df_yaml = pd.DataFrame(data[\"papers\"])\n",
    "                for abstract in df_yaml[\"abstract\"].astype(str):\n",
    "                    json.dump({\"input_text\": \"What computer science subject does the following paper belong to? \" +abstract, \"output_text\": entry.stem} , fh, ensure_ascii=False)\n",
    "                    fh.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59859023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ArtificialIntelligence.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerArchitecture.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerNetworks.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerSecurity.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\ComputerVision.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\Databases.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\Databases_abstracts.jsonl\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\MachineLearning.yaml\n",
      "C:\\Users\\xuedo\\OneDrive\\Desktop\\CSE244-project\\dataset_prep\\old_data\\WWWInformationRetrieval.yaml\n"
     ]
    }
   ],
   "source": [
    "old_data_dir = p.parent  # existing variable from notebook\n",
    "entries = sorted(old_data_dir.iterdir(), key=lambda x: x.name.lower())\n",
    "\n",
    "# Print each entry and whether it's a file or directory\n",
    "for entry in entries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12838523",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "897c3ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "{'text': 'Kleene algebra modulo theories: a framework for concrete KATs Kleene algebras with tests (KATs) offer sound, complete, and decidable equational reasoning about regularly structured programs. Interest in KATs has increased greatly since NetKAT demonstrated how well extensions of KATs with domain-specific primitives and extra axioms apply to computer networks. Unfortunately, extending a KAT to a new domain by adding custom primitives, proving its equational theory sound and complete, and coming up with an efficient implementation is still an expert’s task. Abstruse metatheory is holding back KAT’s potential. We offer a fast path to a “minimum viable model” of a KAT, formally or in code through our framework, Kleene algebra modulo theories (KMT). Given primitives and a notion of state, we can automatically derive a corresponding KAT’s semantics, prove its equational theory sound and complete with respect to a tracing semantics (programs are denoted as traces of states), and derive a normalization-based decision procedure for equivalence checking. Our framework is based on pushback, a generalization of weakest preconditions that specifies how predicates and actions interact. We offer several case studies, showing tracing variants of theories from the literature (bitvectors, NetKAT) along with novel compositional theories (products, temporal logic, and sets). We derive new results over unbounded state, reasoning about monotonically increasing, unbounded natural numbers. Our OCaml implementation closely matches the theory: users define and compose KATs with the module system.', 'label': 'prog_languages'}\n"
     ]
    }
   ],
   "source": [
    "# Prompt for paper title and multi-line abstract (end abstract input with an empty line)\n",
    "paper_title = input(\"Paper title: \").strip()\n",
    "\n",
    "paper_abstract = input(\"Paper abstract: \").strip()\n",
    "\n",
    "label = \"prog_languages\"\n",
    "\n",
    "# show stored values\n",
    "# paper_title, paper_abstract\n",
    "\n",
    "papers.append({\"text\": paper_title + \" \" + paper_abstract, \"label\": label})\n",
    "print(len(papers))\n",
    "print(papers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4650c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Serenade - Low-Latency Session-Based Recommendation in e-Commerce at Scale Session-based recommendation predicts the next item with which a user will interact, given a sequence of her past interactions with other items. This machine learning problem targets a core scenario in e-commerce platforms, which aim to recommend interesting items to buy to users browsing the site. Session-based recommenders are difficult to scale due to their exponentially large input space of potential sessions. This impedes offline precomputation of the recommendations, and implies the necessity to maintain state during the online computation of next-item recommendations. We propose VMIS-kNN, an adaptation of a state-of-the-art nearest neighbor approach to session-based recommendation, which leverages a prebuilt index to compute next-item recommendations with low latency in scenarios with hundreds of millions of clicks to search through. Based on this approach, we design and implement the scalable session-based recommender system Serenade, which is in production usage at bol.com, a large European e-commerce platform. We evaluate the predictive performance of VMIS-kNN, and show that Serenade can answer a thousand recommendation requests per second with a 90th percentile latency of less than seven milliseconds in scenarios with millions of items to recommend. Furthermore, we present results from a three week long online A/B test with up to 600 requests per second for 6.5 million distinct items on more than 45 million user sessions from our e-commerce platform. To the best of our knowledge, we provide the first empirical evidence that the superior predictive performance of nearest neighbor approaches to session-based recommendation in offline evaluations translates to superior performance in a real world e-commerce setting.', 'label': 'databases'}, {'text': 'Neural Subgraph Counting with Wasserstein Estimator Subgraph counting is a fundamental graph analysis task which has been widely used in many applications. As the problem of subgraph counting is NP-complete and hence intractable, approximate solutions have been widely studied, which fail to work with large and complex query graphs. Alternatively, Machine Learning techniques have been recently applied for this problem, yet the existing ML approaches either only support very small data graphs or cannot make full use of the data graph information, which inherently limits their scalability, estimation accuracies and robustness. In this paper, we propose a novel approximate subgraph counting algorithm, NeurSC, that can exploit and combine information from both the query graphs and the data graphs effectively and efficiently. It consists of two components: (1) an extraction module that adaptively generates simple yet representative substructures from data graph for each query graph and (2) an estimator WEst that first computes the representations from individual and joint distributions of query and data graphs and then estimates subgraph counts with the learned representations. Furthermore, we design a novel Wasserstein discriminator in WEst to minimize the Wasserstein distance between query and data graphs by updating the parameters in network with the vertex correspondence relationship between query and data graphs. By doing this, WEst can better capture the correlation between query and data graphs which is essential to the quality of the estimation. We conduct experimental studies on seven large real-life labeled graphs to demonstrate the superior performance of NeurSC in terms of estimation accuracy and robustness.', 'label': 'databases'}, {'text': \"Statistical Schema Learning with Occam's Razor A judiciously normalized database schema can increase data interpretability, reduce data size, and improve data integrity. However, real world data sets are often stored or shared in a denormalized state. We examine the problem of automatically creating a good schema for a denormalized table, approaching it as an unsupervised machine learning problem which must learn an optimal schema from the data. This differs from past rule-based approaches that focus on normalization into a canonical form. We define a principled schema optimization criterion, based on Occam's razor, that is robust to noise and extensible---allowing users to easily specify desirable properties of the resulting schema. We develop an efficient learning algorithm for this criterion and empirically demonstrate that it is 3 to 100 times faster than previous work and produces higher quality schemas with 1/5th the errors.\", 'label': 'databases'}, {'text': 'Understanding Queries by Conditional Instances A powerful way to understand a complex query is by observing how it operates on data instances. However, specific database instances are not ideal for such observations: they often include large amounts of superfluous details that are not only irrelevant to understanding the query but also cause cognitive overload; and one specific database may not be enough. Given a relational query, is it possible to provide a simple and generic \"representative\\'\\' instance that (1) illustrates how the query can be satisfied, (2) summarizes all specific instances that would satisfy the query in the same way by abstracting away unnecessary details? Furthermore, is it possible to find a collection of such representative instances that together completely characterize all possible ways in which the query can be satisfied? This paper takes initial steps towards answering these questions. We design what these representative instances look like, define what they stand for, and formalize what it means for them to satisfy a query in \"all possible ways.\" We argue that this problem is undecidable for general domain relational calculus queries, and develop practical algorithms for computing a minimum collection of such instances subject to other constraints. We evaluate the efficiency of our approach experimentally, and show its effectiveness in helping users debug relational queries through a user study.', 'label': 'databases'}, {'text': \"Complaint-Driven Training Data Debugging at Interactive Speeds Modern databases support queries that perform model inference (inference queries). Although powerful and widely used, inference queries are susceptible to incorrect results if the model is biased due to training data errors. Recently, prior work Rain proposed complaint-driven data debugging which uses user-specified errors in the output of inference queries (Complaints) to rank erroneous training examples that most likely caused the complaint. This can help users better interpret results and debug training sets. Rain combined influence analysis from the ML literature with relaxed query provenance polynomials from the DB literature to approximate the derivative of complaints w.r.t. training examples. Although effective, the runtime is O(|T|d), where T and d are the training set and model sizes, due to its reliance on the model's second order derivatives (the Hessian). On a Wide Resnet Network (WRN) model with 1.5 million parameters, it takes >1 minute to debug a complaint. We observe that most complaint debugging costs are independent of the complaint, and that modern models are overparameterized. In response, Rain++ uses precomputation techniques, based on non-trivial insights unique to data debugging, to reduce debugging latencies to a constant factor independent of model size. We also develop optimizations when the queried database is known apriori, and for standing queries over streaming databases. Combining these optimizations in Rain++ ensures interactive debugging latencies (~1ms) on models with millions of parameters.\", 'label': 'databases'}]\n"
     ]
    }
   ],
   "source": [
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "533d6d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 15 record(s) to prog_languages.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def write_jsonl(dicts, out_path=\"output.jsonl\", mode=\"w\", ensure_ascii=False, indent=None):\n",
    "    \"\"\"\n",
    "    Write a list of dictionaries to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        dicts (list): list of dict objects to write.\n",
    "        out_path (str or Path): output file path.\n",
    "        mode (str): file open mode, 'w' to overwrite or 'a' to append.\n",
    "        ensure_ascii (bool): passed to json.dump.\n",
    "        indent (int or None): passed to json.dump for pretty printing (each record still on one line).\n",
    "    \"\"\"\n",
    "    if not isinstance(dicts, list):\n",
    "        raise TypeError(\"dicts must be a list of dicts\")\n",
    "    with open(out_path, mode, encoding=\"utf-8\") as fh:\n",
    "        for item in dicts:\n",
    "            if not isinstance(item, dict):\n",
    "                raise TypeError(\"each item in dicts must be a dict\")\n",
    "            json.dump(item, fh, ensure_ascii=ensure_ascii, indent=indent)\n",
    "            fh.write(\"\\n\")\n",
    "\n",
    "# Example usage (uses existing variables in the notebook: paper_title, paper_abstract)\n",
    "# records = [\n",
    "#     {\n",
    "#         \"input_text\": \"What computer science subject does the following paper belong to? \" + paper_abstract,\n",
    "#         \"output_text\": paper_title\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "write_jsonl(papers, out_path=\"prog_languages.jsonl\")\n",
    "print(f\"Wrote {len(papers)} record(s) to prog_languages.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
