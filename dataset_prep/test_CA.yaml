papers:
- title: 'CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark'
  abstract: 'We introduce \texttt{CASS}, the first large-scale dataset and model suite
    for

    cross-architecture GPU code transpilation, targeting both source-level

    (CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia

    SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k

    verified code pairs across host and device, addressing a critical gap in

    low-level GPU code portability. Leveraging this resource, we train the

    \texttt{CASS} family of domain-specific language models, achieving 95\% source

    translation accuracy and 37.5\% assembly translation accuracy, substantially

    outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our

    generated code matches native performance in over 85\% of test cases,

    preserving runtime and memory behavior. To support rigorous evaluation, we

    introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with

    ground-truth execution. All data, models, and evaluation tools are released as

    open source to foster progress in GPU compiler tooling, binary compatibility,

    and LLM-guided hardware translation. Dataset and benchmark are on

    \href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},

    with code at

    \href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.'
  url: http://arxiv.org/abs/2505.16968v1
  keywords: ''
  document: "![](_page_0_Picture_0.jpeg)\n\nAhmed Heakl<sup>1</sup> Sarim Hashmi†<sup>1</sup>\
    \ Gustavo Bertolo Stahl†<sup>1</sup> Seung Hun Eddie Han<sup>1</sup> Salman Khan1,<sup>2</sup>\
    \ Abdulrahman Mahmoud1<sup>∗</sup>\n\n<sup>1</sup>MBZUAI <sup>2</sup>Australian\
    \ National University\n\n§ <https://github.com/GustavoStahl/CASS> <https://huggingface.co/datasets/MBZUAI/cass>\n\
    \n## Abstract\n\nWe introduce CASS, the first large-scale dataset and model suite\
    \ for crossarchitecture GPU code transpilation, targeting both source-level (CUDA\
    \ ↔ HIP) and assembly-level (Nvidia SASS ↔ AMD RDNA3) translation. The dataset\
    \ comprises 70k verified code pairs across host and device, addressing a critical\
    \ gap in low-level GPU code portability. Leveraging this resource, we train the\
    \ CASS family of domain-specific language models, achieving 95% source translation\
    \ accuracy and 37.5% assembly translation accuracy, substantially outperforming\
    \ commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code\
    \ matches native performance in over 85% of test cases, preserving runtime and\
    \ memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a\
    \ curated benchmark spanning 16 GPU domains with ground-truth execution. All data,\
    \ models, and evaluation tools are released as open source to foster progress\
    \ in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.\
    \ Dataset and benchmark are on [HuggingFace,](https://huggingface.co/datasets/MBZUAI/cass)\
    \ with code at [GitHub.](https://github.com/GustavoStahl/CASS)\n\n## 1 Introduction\n\
    \nGraphics Processing Units (GPUs) are foundational to modern machine learning\
    \ and scientific computing workloads due to their high-throughput parallelism.\
    \ Nvidia's Compute Unified Device Architecture (CUDA) [\\[1\\]](#page-9-0) has\
    \ become the dominant programming model for GPU acceleration, but its tight coupling\
    \ to proprietary hardware introduces severe vendor lock-in: CUDA code cannot run\
    \ on non-Nvidia GPUs due to incompatible instruction set architectures (ISAs)\
    \ [\\[2\\]](#page-9-1). As a result, organizations with large CUDA-based codebases\
    \ face steep engineering costs when migrating to alternative platforms. Meanwhile,\
    \ AMD GPUs, offering potential favorable performance-perdollar [\\[3,](#page-9-2)\
    \ [4\\]](#page-9-3), are increasingly adopted across both data centers and consumer\
    \ devices [\\[5\\]](#page-9-4), creating a growing need to execute legacy CUDA\
    \ programs on AMD hardware without full rewrites in software [\\[6\\]](#page-9-5).\n\
    \nIn response, AMD introduced the Heterogeneous-computing Interface for Portability\
    \ (HIP) [\\[7\\]](#page-9-6), a C++ GPU API built into the ROCm stack [\\[8\\\
    ]](#page-9-7), designed to mirror CUDA's functionality while supporting cross-platform\
    \ development. HIP enables a unified codebase for both Nvidia and AMD GPUs. Tools\
    \ like HIPIFY [\\[9\\]](#page-9-8), a static translator, assist migration by converting\
    \ CUDA-specific constructs into\n\n<sup>∗</sup> † Equal contribution.\n\ntheir\
    \ HIP equivalents, streamlining adoption of the ROCm stack. However, HIPIFY only\
    \ operates at the source level and cannot execute precompiled CUDA binaries. Furthermore,\
    \ it exhibits a high failure rate when converting CUDA programs, highlighting\
    \ the need for more reliable and lower-level transpilation approaches [\\[10\\\
    ]](#page-9-9).\n\nTranslating GPU assembly across vendors is hindered by divergent\
    \ ISAs and compilation pipelines. Nvidia employs a proprietary toolchain centered\
    \ on nvcc, producing PTX and low-level SASS [\\[1\\]](#page-9-0), while AMD uses\
    \ GCN/RDNA architectures compiled via the open-source ROCm stack using hipcc [\\\
    [8\\]](#page-9-7) (Figure [2](#page-5-0) provides a detailed breakdown of the\
    \ alternative stacks). Bridging this gap at the assembly level is critical for\
    \ democratizing the hardware computing landscape, transfer of hardware-specific\
    \ optimizations across vendors, and enabling automation beyond source-level rewrites,\
    \ especially for legacy CUDA codebases rich in low-level tuning. Our model introduces\
    \ the first foundation for Nvidia-to-AMD assembly and source translation, focusing\
    \ on correctness and alignment. While not optimization-aware yet, it paves the\
    \ way for future systems that preserve and adapt performance-critical patterns\
    \ across GPU backends.\n\nTo address the lack of cross-architecture GPU translation\
    \ datasets, we introduce CASS (CUDA–AMD ASsembly and Source Mapping), a large-scale\
    \ corpus of 70k semantically aligned CUDA–HIP source pairs and their corresponding\
    \ host (CPU – x86 ISA) and device (GPU) assemblies for Nvidia (SASS) and AMD (RDNA3)\
    \ platforms. Each sample comprises functionally equivalent low-level code across\
    \ vendors, verified through successful compilation and execution, enabling instruction-level\
    \ analysis across execution boundaries. Unlike generic code corpora like The Stack\
    \ [\\[11\\]](#page-9-10), which lack GPU-aligned and compilable content, CASS\
    \ provides complete source and binary representations across both GPU compute\
    \ stacks. To construct CASS, we developed a fully open-source pipeline that scrapes,\
    \ synthesizes, translates (via HIPIFY [\\[9\\]](#page-9-8)), compiles, and aligns\
    \ GPU code. We evaluate CASS along two dimensions: (1) instruction coverage, capturing\
    \ diverse SASS and RDNA3 opcodes; and (2) domain coverage, spanning real-world\
    \ compute kernels from ML, graphics, and HPC. CASS is the first dataset to enable\
    \ source- and assembly-level translation research for GPU architectures.\n\nTo\
    \ validate the utility of our dataset, we introduce the CASS model family, a suite\
    \ of domain-specific large language models fine-tuned for both source and assembly-level\
    \ GPU code translation. These models are trained on our curated corpus and demonstrate\
    \ significant improvements over SoTA proprietary systems such as GPT-4o [\\[12\\\
    ]](#page-9-11), Claude-3.7 [\\[13\\]](#page-9-12), and traditional tools like\
    \ HIPIFY [\\[9\\]](#page-9-8)—achieving 95% accuracy in source-level translation\
    \ and 37.5% in assembly translation. To ensure rigorous evaluation, we further\
    \ contribute CASS-Bench, the first benchmark tailored to cross-architecture GPU\
    \ transpilation. It spans 16 diverse GPU domains with execution-verified source\
    \ and assembly pairs, providing a standardized testbed for future work in low-level\
    \ translation and performance-aware code generation.\n\nOur contributions are\
    \ summarized as follows:\n\n- CASS Dataset. We introduce CASS, the first large-scale\
    \ dataset for GPU transpilation, containing 70k semantically aligned Nvidia ↔\
    \ AMD pairs at both the source (CUDA ↔ HIP) and assembly levels (SASS ↔ RDNA3),\
    \ covering 16 real-world GPU domains.\n- CASS-Bench. We contribute the first evaluation\
    \ benchmark for cross-architecture GPU translation, with 40 curated tasks across\
    \ 16 domains, including functionally verified outputs and aligned CUDA/HIP source\
    \ and SASS/RDNA3 assembly.\n- CASS Models. We release domain-specialized CASS\
    \ LLMs trained for cross-architecture code translation. Our 7B model achieves\
    \ 95% source and 37.5% assembly accuracy, outperforming GPT-4o and Claude (0%)\
    \ on CASS-Bench. Crucially, 85% of translated assemblies preserve execution runtime\
    \ and memory compared to native, confirming semantic and performance fidelity.\n\
    - CASS Dataset Pipeline. We designed a scalable pipeline for scraping, synthesizing,\
    \ transpiling, and compiling CUDA/HIP code into aligned host and device assemblies\
    \ across Nvidia and AMD GPUs.\n\nThe rest of the paper is organized as follows:\
    \ [§2](#page-2-0) reviews prior work on Nvidia-to-AMD and assembly translation.\
    \ [§3](#page-3-0) describes our data collection, conversion, and filtering pipeline.\
    \ [§4](#page-4-0) analyzes dataset structure and coverage. [§5](#page-7-0) outlines\
    \ model training and evaluation, with results and ablations in [§6.](#page-7-1)\
    \ Finally, [§7](#page-8-0) lists limitations and future work, followed by [§8](#page-8-1)\
    \ concluding remarks.\n\n<span id=\"page-2-4\"></span>\n\n| Domain/<br>Characteristics\
    \ | ComputeEval<br>NVIDIA[19] | Rodinia<br>Bench[20] | SHOC<br>[21] | Poly<br>Bench[22]\
    \ | Babel<br>Stream[23] | Ours |\n|----------------------------|---------------------------|----------------------|--------------|-------------------|---------------------|------|\n\
    | CUDA (source)              | ✓                         | ✓                 \
    \   | ✓            | ✓                 | ✓                   | ✓    |\n| SASS\
    \ (assembly)            | ✗                         | ✗                    | ✗\
    \            | ✗                 | ✗                   | ✓    |\n| RDNA3 (assembly)\
    \           | ✗                         | ✗                    | ✗           \
    \ | ✗                 | ✗                   | ✓    |\n| OpenCL (source)      \
    \      | ✗                         | ✓                    | ✓            | ✓ \
    \                | ✓                   | ✓    |\n\nTable 1: Comparison of Domain/Characteristics\
    \ across Different Datasets\n\n# <span id=\"page-2-0\"></span>2 Related Works\n\
    \nIn this section, we describe prior work in GPU translation efforts ([§2.1\\\
    )](#page-2-1), assembly-level transpilation ([§2.2\\)](#page-2-2), and related\
    \ benchmarks (and their shortcomings) in the space ([§2.3\\)](#page-2-3)\n\n##\
    \ <span id=\"page-2-1\"></span>2.1 Translating from Nvidia to AMD\n\nThe fragmentation\
    \ of GPU software ecosystems has driven the need for robust CUDA-to-HIP translation\
    \ tools. HIPIFY [\\[14\\]](#page-9-15) statically converts CUDA source code into\
    \ HIP, enabling ROCm compatibility via direct syntax substitution. Operating at\
    \ a lower abstraction, CuPBoP-AMD [\\[15\\]](#page-9-16) translates NVVM IR to\
    \ HIP-compatible LLVM IR using the LLVM toolchain [\\[16,](#page-9-17) [17\\]](#page-9-18),\
    \ offering more flexible intermediate-level interoperability. Earlier, GPU Ocelot\
    \ [\\[18\\]](#page-9-19) explored dynamic binary translation, recompiling CUDA\
    \ to AMD/x86 ISAs at runtime. Although innovative, it was limited by poor scalability\
    \ and high overhead, making it impractical for modern GPU workloads. All these\
    \ tools have lacked consistent updates to keep up with CUDA advances, suffer from\
    \ usability issues, and operate only at the source level.\n\nMore recently, ZLUDA\
    \ [\\[6\\]](#page-9-5) introduced a runtime system for executing unmodified CUDA\
    \ binaries on AMD GPUs without source access by intercepting CUDA APIs and translating\
    \ PTX/SASS into AMD-compatible code via LLVM. Originally targeting Intel, it now\
    \ supports AMD RDNA3 through runtime patching. ZLUDA operates at the LLVM IR level\
    \ rather than the hardware assembly. While a reasonable level in the stack to\
    \ target, ZLUDA would not be able to benefit from low-level, backend Nvidia optimizations\
    \ (operating below the PTX level), and is limited to the AMD stacks backend optimizations.\
    \ In our work, we target assembly-to-assembly translation, in an effort to leverage\
    \ hardware-specific optimizations below the intermediate representation (IR) level,\
    \ that may be missing altogether in the corresponding AMD codebase.\n\n# <span\
    \ id=\"page-2-2\"></span>2.2 Assembly-to-Assembly Translation\n\nTranslating assembly\
    \ across ISAs is challenging due to divergent instruction sets and execution models.\
    \ Recent work employs language models for this task, including CRT [\\[24\\]](#page-10-3),\
    \ a lightweight transpiler from x86 assembly (CISC) to ARM (RISC), and Guess &\
    \ Sketch [\\[25\\]](#page-10-4), which integrates language models with symbolic\
    \ reasoning to translate between ARMv8 and RISC-V. These recent successes open\
    \ the door for assembly-to-assembly translation in the unexplored GPU-to-GPU space.\
    \ A key contributing factor to their success is the large CPU-centric dataset\
    \ enabling training from one ISA to another. Given the lack of such a rich dataset\
    \ in the GPU space, a primary goal of this work is to enable such an exploration\
    \ and transpilation across GPU vendors, democratizing compute in the critical\
    \ GPU and ML-acceleration landscape, where Nvidia/CUDA currently dominate the\
    \ market.\n\n### <span id=\"page-2-3\"></span>2.3 Datasets and Benchmarks for\
    \ CUDA and HIP\n\nAs shown in table [1,](#page-2-4) existing benchmarks in the\
    \ GPU space generally focus on runtime performance, do none target the assembly\
    \ level, and do not have paired/aligned data across Nvidia/AMD codebases. ComputeEval\
    \ [\\[19\\]](#page-9-13) includes only CUDA code for hardware evaluation. Rodinia\
    \ [\\[20\\]](#page-9-14) and SHOC [\\[21\\]](#page-10-0) provide heterogeneous\
    \ benchmarks using CUDA/OpenCL/OpenMP but omit AMD code and assembly. PolyBench\
    \ [\\[22\\]](#page-10-1) evaluates compilers with CUDA/OpenCL kernels, yet lacks\
    \ assembly-level or AMD support. BabelStream [\\[23\\]](#page-10-2) benchmarks\
    \ HIP/CUDA/OpenCL memory bandwidth but excludes assembly and domain diversity.\
    \ Hetero-Mark [\\[26\\]](#page-10-5) targets CPU–GPU workloads where GPU code\
    \ is minimal. The Stack [\\[11,](#page-9-10) [27\\]](#page-10-6) dataset nearly\
    \ 200k CUDA files but no AMD coverage or aligned\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_0.jpeg)\n\
    \nFigure 1: CASS Pipeline: We collect CUDA code from public repositories and synthesize\
    \ additional samples via prompt-based LLM generation. After filtering and deduplication,\
    \ all CUDA files are translated to HIP using HIPIFY, then compiled to extract\
    \ host and device assembly. Matched outputs form the CASS dataset with aligned\
    \ source and assembly pairs across Nvidia and AMD stacks.\n\nassembly. In contrast,\
    \ CASS uniquely offers 70k semantically aligned CUDA–HIP source and SASS–RDNA3\
    \ assembly pairs across both host and device, enabling instruction-level analysis\
    \ and forming the first dataset purpose-built for cross-vendor GPU assembly translation.\n\
    \nTo the best of our knowledge, no existing dataset provides *paired* source-\
    \ and assembly-level Nvidia-AMD code, hindering effective training and benchmarking.\n\
    \n# <span id=\"page-3-0\"></span>3 Methods\n\nThis section outlines the end-to-end\
    \ methodology behind CASS-Instruct, including data collection, code conversion,\
    \ and compilation for Nvidia and AMD GPUs. We built the low-level assembly corpus\
    \ from high-level CUDA code using three strategies: scraping public repositories,\
    \ generating synthetic samples, and applying targeted code generation frameworks.\n\
    \n### 3.1 CUDA Code Scraping\n\nWe leveraged the Stackv2 dataset [\\[27\\]](#page-10-6)\
    \ to extract CUDA source files. This dataset, curated from a vast array of public\
    \ code repositories, offers deduplicated and license-compliant samples, facilitating\
    \ the assembly of a diverse corpus of GPU-oriented code. To maximize the number\
    \ of compiled files in the later stage, we used the dataset's metadata to identify\
    \ and download the top 200 repositories with the highest number of CUDA files.\
    \ This repository-level download preserved the original directory structure and\
    \ relative imports, as shown in Figure [1,](#page-3-1) and improved compilation\
    \ success by 23.7% compared to isolated file scraping. After extraction, we applied\
    \ additional filtering to remove overly long files (> 7k lines), trivially short\
    \ files (<10 lines), naive boilerplate samples (e.g., \"Hello World\"), and files\
    \ lacking CUDA kernel definitions. This process resulted in a final set of 24k\
    \ usable samples.\n\n### 3.2 Synthetic Data Generation\n\nWe employed a coding-oriented\
    \ large language model (Qwen2.5-Coder32B) to synthesize CUDA kernel code using\
    \ our variable-augmented persona strategy. The process begins by defining a set\
    \ of natural language prompt templates with variable placeholders. For example,\
    \ a template might read:\n\n#### *Generate a CUDA kernel for cloth simulation\
    \ with a {size}X{size} grid. Optimize for {optimization}.*\n\nTo fill these templates,\
    \ we prepared predefined lists of variable values. For instance, {size} was instantiated\
    \ with values such as 32, 64, and 128, while {optimization} was sampled from options\
    \ like \"memory bandwidth\", \"register usage\", and \"multi-GPU scaling\". This\
    \ allowed us to systematically generate a broad range of prompts, each specifying\
    \ different values for the placeholders in the templates. Refer to the Appendix\
    \ [A.5](#page-15-0) for full details.\n\nThese prompts were then passed to the\
    \ LLM, which generated CUDA source files accordingly. While this method introduced\
    \ some functional inconsistencies that required significant post-generation\n\n\
    filtering (syntactic errors, missing definitions, or invalid memory operations),\
    \ it enabled the creation of rich and diverse CUDA samples. In total, we generated\
    \ 85k CUDA samples, of which 49.1% compiled successfully, yielding a final set\
    \ of 46.3k valid files.\n\n### 3.3 Transpilation and Compilation\n\nAfter collecting\
    \ CUDA files from the previous stages, we performed deduplication to ensure all\
    \ samples are unique in our dataset. We then used AMD's Hipify tool [\\[9\\]](#page-9-8)\
    \ to convert CUDA source files by replacing CUDA-specific API calls with HIP equivalents.\
    \ Files that failed conversion (approx. 43.9%) were discarded. Once CUDA–HIP pairs\
    \ were available, we compiled them to host and device assemblies using -Os compilation\
    \ flag to reduce code size, achieving a 9.3% average token reduction compared\
    \ to O3. Given the architectural divergence of the two stacks (see figure [2\\\
    )](#page-5-0), their compilation pipelines differed substantially, requiring significant\
    \ effort to engineer and standardize our described workflow.\n\nIn figure [2,](#page-5-0)\
    \ a key distinction between the CUDA and HIP compilation pipelines lies in how\
    \ they manage host and device assembly separation. In ROCm, the device binary\
    \ is typically embedded into the host binary during the BitCode-to-assembly transition.\
    \ We modified this behavior by deferring insertion until after host assembly was\
    \ converted to object code, enabling: (1) independent extraction of pure host\
    \ and device assemblies, and (2) selective recombination for controlled translation\
    \ and evaluation.\n\nConversely, Nvidia provides no access to its binary injection\
    \ process, device and host assemblies remain intertwined, with no official method\
    \ for extraction or reintegration [\\[28\\]](#page-10-7). Since our goal was to\
    \ support host-to-host and device-to-device transpilation, recombination on the\
    \ CUDA side was unnecessary. Instead, we developed a regex-based filtering pipeline\
    \ to disentangle host and device assembly sections during CUDA compilation.\n\n\
    After compiling both stacks to SASS and RDNA3, we retained only samples that compiled\
    \ successfully on both Nvidia and AMD pipelines, accounting for asymmetric failures.\
    \ The final dataset includes matched CUDA–HIP source pairs, SASS–RDNA3 device\
    \ assemblies, and host assemblies. We got 64k samples from these steps.\n\n###\
    \ 3.4 OpenCL Pipeline\n\nOpenCL stands as an independent pipeline in generating\
    \ Nvidia to AMD mapping datasets outside of the CUDA/HIP framework. In other words,\
    \ it alsos compiling down to the assembly level without going through the aforementioned\
    \ stacks, operating as a single \"source\" for GPU code deveolpment [\\[29\\]](#page-10-8).\
    \ Approximately 6k OpenCL code snippets were collected from the Stack dataset\
    \ and compiled down to the device assemblies. On the Nvidia stack, a wrapper C++\
    \ function was used to encapsulate the clBuildProgram library provided by OpenCL\
    \ [\\[30\\]](#page-10-9) and convert them into PTX, after which the CUDA stack\
    \ was used to compile them down to assemblies. On the AMD stack, clang was used\
    \ to directly transpile the OpenCL files into device assemblies whilst forcing\
    \ it to emit intermediate LLVM during this process [\\[17\\]](#page-9-18).\n\n\
    In total, these pipelines produced 70k aligned assembly samples, with the final\
    \ distribution detailed in Table [2.](#page-6-0) All compilations were performed\
    \ on an Nvidia A100 PCIe machine for the CUDA stack (SASS sm85 ISA) and on AMD\
    \ Radeon RX 7900 XT GPUs (RDNA3 ISA) for the AMD stack.\n\n# <span id=\"page-4-0\"\
    ></span>4 CASS-Instruct and CASS-Bench Datasets\n\nThe final instruction training\
    \ dataset (CASS-Instruct) comprises 70,694 samples spanning a broad range of domains\
    \ as seen in Figure [3,](#page-5-1) with a primary focus on GPU compute and GPU-related\
    \ data structures. The dataset also includes corresponding CUDA and HIP source\
    \ code alongside their compiled assembly representations. All samples have been\
    \ verified to compile successfully and have pairwise source/assembly alignments.\n\
    \n<span id=\"page-5-0\"></span>![](_page_5_Figure_0.jpeg)\n\nFigure 2: The Nvidia\
    \ (left) and AMD (right) stacks illustrate the compilation process for CUDA and\
    \ HIP. Blue denotes device-side steps; green denotes host-side steps. Nvidia's\
    \ stack is opaque; accessing device assembly (SASS) requires first compiling to\
    \ binary, then using cuobjdump. In contrast, AMD's process is transparent, allowing\
    \ direct inspection and modification of device assembly (RDNA3) before host integration.\n\
    \n<span id=\"page-5-1\"></span>![](_page_5_Figure_2.jpeg)\n\nFigure 3: CASS coverage\
    \ across dataset and benchmark (left) domain distribution of training samples\
    \ (right) category distribution in CASS-Bench.\n\n### 4.1 Dataset Analysis\n\n\
    CASS reveals pronounced structural divergence between CUDA and HIP at both source\
    \ and assembly levels, underscoring the inherent complexity of cross-architecture\
    \ GPU transpilation. We analyze this by looking at the length of the assembly\
    \ files, their syntactic similarity, and opcode diversity.\n\nLength of Assembly\
    \ Files. Figure [4](#page-6-1) (left) shows that AMD device assembly is, on average,\
    \ twice as long as Nvidia's in both synthetic and Stack subsets, while Nvidia's\
    \ device assembly exceeds HIP device assembly by 50% in the OpenCL set. We found\
    \ an exponential relationship between source complexity and assembly size, with\
    \ CUDA producing more verbose outputs than HIP for equivalent code. This highlights\
    \ the growing difficulty of assembly-level translation as code complexity scales.\
    \ See appendix [A.4.1](#page-14-0) for full details.\n\n<span id=\"page-6-1\"\
    ></span>![](_page_6_Figure_0.jpeg)\n\nFigure 4: Comparison of structural and syntactic\
    \ patterns in CASS: (a) verbosity across subsets and backends; (b) syntactic similarity\
    \ of translated code.\n\nCode Efficiency and Analysis. Assembly accuracy varies\
    \ across domains 0% in math, data structures, and graph tasks, 25–50% in linear\
    \ algebra and memory operations, and up to 100% in physics simulations—highlighting\
    \ the challenge of preserving low-level semantics. Despite this, the translated\
    \ code closely matches the original in execution: memory usage deviates by less\
    \ than ±0.3%, and execution time stays within ±11.8%, with over 85% of samples\
    \ falling within ±5.6% for both metrics.\n\nSyntax Similarity. As illustrated\
    \ in figure [4](#page-6-1) (right), the CHRF [\\[31\\]](#page-10-10) score indicates\
    \ that HIP and CUDA assembly exhibit low syntactic similarity for both device\
    \ and medium similarity to host code, particularly in the OpenCL and Stackv2 subsets.\
    \ In contrast, the source code translations, especially in the synthetic subset,\
    \ show high overlap, highlighting that surface-level syntax is better preserved\
    \ in the source code than in the compiled assembly representations.\n\nOpcode\
    \ Diversity. We noticed that tensor operations dominate both CUDA and HIP assembly,\
    \ especially in device code, with memory-related instructions such as mov and\
    \ call appearing most frequently (refer to appendix [A.4\\)](#page-14-1). Additionally,\
    \ HIP opcodes like s\\_mov\\_b32 and v\\_add\\_co\\_u32 are used extensively reflecting\
    \ low-level vector and memory operations unique to AMD's ISA, while Nvidia is\
    \ dominated by its own variant of common instructions such as movq, call, and\
    \ jmp, with greater host-side integration (refer to appendix [A.4\\)](#page-14-1).\
    \ Both stacks share common control and memory ops (e.g., mov, test), but HIP provides\
    \ finer-grained access to GPU internals,\n\n<span id=\"page-6-0\"></span>\n\n\
    | Table 2:        | Dataset composition by |  |\n|-----------------|------------------------|--|\n\
    | source and size |                        |  |\n\n| Dataset   | Collected | Final\
    \ |\n|-----------|-----------|-------|\n| Synthetic | 85k       | 40.k  |\n| Stack\
    \     | 124k      | 24k   |\n| OpenCL    | 6k        | 6k    |\n| Total     |\
    \ –         | 70k   |\n\nrevealing deeper visibility into parallelism. The synthetic\
    \ subset emphasizes memory-oriented instructions, aligning with LLM-driven template\
    \ optimizations. Figure [6](#page-8-2) further shows t-SNE clusters of opcode\
    \ embeddings (via BERTCoder), suggesting that despite backend differences, Nvidia\
    \ and AMD share semantically aligned opcode distributions across device and host\
    \ levels.\n\n### 4.2 CASS-Bench\n\nCASS-Bench is a 40-sample evaluation suite\
    \ spanning 16 GPU-centric domains, each represented by 1–5 curated prompts. For\
    \ each, we (1) used Claude-3.7 to generate a CUDA implementation; (2) compiled\
    \ and executed on Nvidia hardware to obtain reference outputs; then (3) prompted\
    \ Claude-3.7 to generate the corresponding AMD code. If outputs mismatched due\
    \ to compilation errors, formatting differences or random generators variance,\
    \ the AMD code was regenerated. Only samples with manually verified output equivalence\
    \ were included. All final Nvidia–AMD pairs were processed using our pipeline\
    \ (Section [3\\)](#page-3-0) to extract aligned host and device assembly. Figure\
    \ [3](#page-5-1) (right) shows the category distribution.\n\n|          | Model\
    \                                                                            \
    \ | Assembly Accuracy (%)   | Source-to-Source Accuracy (%)    |\n|----------|-----------------------------------------------------------------------------------|-------------------------|----------------------------------|\n\
    | Tools    | ZLUDA [6]                                                       \
    \                  | 2.5%                    | 27.5%                         \
    \   |\n|          | Hipify [9]                                               \
    \                         | –                       | 87.5%                  \
    \          |\n| Ms<br>LL | GPT-4o [12]<br>Gemini-2.0-Flash [38]<br>Claude-3.7\
    \ [13]<br>Qwen2.5-Coder-32B [32] | 0%<br>0%<br>0%<br>25.0% | 90.0%<br>80.0%<br>90.0%<br>85.0%\
    \ |\n| Ours     | CASS-1.5B                                                  \
    \                       | 12.5%                   | 90.0%                    \
    \        |\n|          | CASS-3B                                             \
    \                              | 20.0%                   | 92.5%             \
    \               |\n|          | CASS-7B                                      \
    \                                     | 37.5%                   | 95.0%      \
    \                      |\n\n<span id=\"page-7-2\"></span>Table 3: Performance\
    \ of different models on our CASS-Bench. Bold cells refer to the best results.\n\
    \n# <span id=\"page-7-0\"></span>5 Experiments\n\nWe evaluate the CASS dataset\
    \ by instruction-supervised fine-tuning the Qwen2.5-Coder [\\[32\\]](#page-10-11)\
    \ models at various parameter scales. Two variants are developed: one for assembly\
    \ translation and another for source translation. We benchmark these models against\
    \ both proprietary and open-source baselines, including larger-scale systems.\n\
    \nInstruction Supervised Finetuning. To ensure that input samples fit within the\
    \ 16K-token context window of the LLM, we normalized CUDA assembly code by removing\
    \ redundant whitespace and comments, which reduced token count by roughly 15%.\
    \ No preprocessing was applied to HIP assembly code due to its sensitivity to\
    \ whitespace changes.\n\nWe fine-tuned the Qwen2.5-Coder [\\[32\\]](#page-10-11)\
    \ models at 1.5B, 3B and 7B parameter scales on 4xA100 GPUs, using a batch size\
    \ of 4, gradient accumulation of 32 (effective batch size of 512) and a learning\
    \ rate of 1×10<sup>−</sup><sup>5</sup> . The relatively aggressive learning rate\
    \ was selected due to the dataset's distributional divergence from the models'\
    \ pretraining corpus.\n\nTraining employed DeepSpeed [\\[33\\]](#page-10-12) with\
    \ optimizer state sharding to maximize hardware efficiency, achieving 98% GPU\
    \ utilization. Additionally, we incorporated Liger Kernel [\\[34\\]](#page-10-13)\
    \ and Paged Adam optimizer [\\[35\\]](#page-10-14) to accelerate training and\
    \ manage memory more effectively. We utilized LLaMA-Factory [\\[36\\]](#page-10-15)\
    \ to implement all of these optimizations.\n\nAll models were trained with a 16K-token\
    \ context window. At inference time, we applied RoPE [\\[37\\]](#page-11-1) extrapolation\
    \ to support up to 32.7K tokens. Inference was efficient, requiring approximately\
    \ 56 seconds per a 16K-token sample.\n\nEvaluation Protocol. For both source and\
    \ assembly transpilation, the LLM-generated code (HIP source or host/device assembly)\
    \ was compiled and executed. The resulting outputs were then compared against\
    \ the ground truth from CASS-Bench to verify functional correctness.\n\n# <span\
    \ id=\"page-7-1\"></span>6 Results\n\nAssembly-to-Assembly Performance. Table\
    \ [3](#page-7-2) reports CASS-Bench results across LLMs and tools. All baselines,\
    \ including proprietary and large open models, failed with 0% accuracy, except\
    \ Qwen2.5- Coder-32B, which reached 25% . ZLUDA, a runtime-level system, achieved\
    \ only 2.5% assembly accuracy despite operating directly on compiled binaries\
    \ which be attributed to its compatibility with RNDA1. In contrast, our CASS models\
    \ reached up to 37.5%, highlighting that our dataset imparts essential assembly-level\
    \ knowledge absent from existing tools and models.\n\nSource-to-Source Performance.\
    \ To further validate the utility of the dataset, we also evaluated source transpilation\
    \ performance as shown in table [3.](#page-7-2) This task aligns more closely\
    \ with some of the pretraining objectives of many proprietary models, as reflected\
    \ in their relatively strong performance (ranging from 80% to 90%). Nonetheless,\
    \ even the smallest CASS model (1.5B) significantly outperformed all baselines,\
    \ achieving 90% accuracy. The 7B variant showed an outstanding state-of-\n\n<span\
    \ id=\"page-8-3\"></span>\n\n| ∆ Impact |\n|----------|\n|          |\n| +12.5%\
    \   |\n| +2.5%    |\n| +5.0%    |\n|          |\n\nTable 4: Ablation study on\
    \ the impact of different data types.\n\nthe-art performance of 95% accuracy.\
    \ Although our CUDA dataset was entirely translated by Hipify and we retained\
    \ only semantically aligned samples, our model surpassed Hipify by 7.5%.\n\n<span\
    \ id=\"page-8-2\"></span>![](_page_8_Figure_3.jpeg)\n\nFigure 5: Source and assembly-level\
    \ accuracy across categories. Figure 6: t-SNE projection of CUDA and HIP assembly\
    \ embeddings.\n\nAblation Study. Table [4](#page-8-3) shows that using only Stack\
    \ data yields 17.5% assembly accuracy. Adding synthetic data improves it by +12.5%,\
    \ highlighting its role in learning low-level patterns. OpenCL adds +2.5%, providing\
    \ complementary coverage, while RoPE extrapolation pushes accuracy to 37.5% by\
    \ extending context capacity.\n\n# <span id=\"page-8-0\"></span>7 Limitations\
    \ and Future Work\n\nDespite achieving state-of-the-art assembly transpilation,\
    \ current performance is inadequate for production due to limited accuracy in\
    \ complex or underrepresented domains. Expanding category diversity is essential\
    \ to address this. The dataset currently covers only one host/device pair per\
    \ vendor (RTX 4090 and RX7900), limiting generalizability across GPU architectures\
    \ with varying ISAs. Broader architectural representation is needed to support\
    \ real-world deployment. Finally, dataset size was minimized to fit within 16K-token\
    \ context windows, excluding many vendor-specific low-level optimizations. Incorporating\
    \ these will require future models with larger context capacity or more advanced\
    \ chunking and attention strategies.\n\n## <span id=\"page-8-1\"></span>8 Conclusion\n\
    \nWe present CASS, the first large-scale dataset and model suite for cross-architecture\
    \ GPU code transpilation, encompassing 70k aligned pairs of source and assembly\
    \ code for both Nvidia and AMD platforms. Our dataset uniquely bridges both source-to-source\
    \ (CUDA to HIP) and assembly-toassembly (SASS to RDNA3) mappings, addressing a\
    \ critical gap in low-level code portability. To validate its effectiveness, we\
    \ train the CASS model family, which achieves 95% accuracy in source translation\
    \ and 37. 5% in assembly translation, substantially outperforming both proprietary\
    \ and open-source baselines. Furthermore, our transpiled code preserves functional\
    \ behavior: over 85% of samples match native execution in both memory usage and\
    \ runtime. We also introduce CASS-Bench, a purpose-built evaluation suite spanning\
    \ 16 GPU-centric domains. All models, data, and benchmarks are released as open-source\
    \ resources, establishing a foundation for future research in compiler tooling,\
    \ hardware interoperability, and performance-aware code generation.\n\n## References\n\
    \n- <span id=\"page-9-0\"></span>[1] Mark Harris. An even easier introduction\
    \ to cuda. *NVIDIA Developer Blog*, 2024.\n- <span id=\"page-9-1\"></span>[2]\
    \ NVIDIA Corporation. *Turing Compatibility Guide for CUDA Applications*, 2021.\
    \ Version 11.4.2.\n- <span id=\"page-9-2\"></span>[3] AMD. Gaming gpu benchmarks.\
    \ [https://www.amd.com/en/products/graphics/](https://www.amd.com/en/products/graphics/gaming/gaming-benchmarks.html)\
    \ [gaming/gaming-benchmarks.html](https://www.amd.com/en/products/graphics/gaming/gaming-benchmarks.html),\
    \ 2024. Accessed: 2025-05-15.\n- <span id=\"page-9-3\"></span>[4] The Verge. Amd\
    \ radeon rx 9070 xt review: performance that beats the price. [https://www.theverge.com/gpu-reviews/624423/](https://www.theverge.com/gpu-reviews/624423/amd-radeon-rx-9070-xt-review-benchmarks-price)\
    \ [amd-radeon-rx-9070-xt-review-benchmarks-price](https://www.theverge.com/gpu-reviews/624423/amd-radeon-rx-9070-xt-review-benchmarks-price),\
    \ 2024. Accessed: 2025-05- 15.\n- <span id=\"page-9-4\"></span>[5] Financial Times.\
    \ Nvidia's rivals take aim at its software dominance. 2024. Accessed: 2025-05-\
    \ 14.\n- <span id=\"page-9-5\"></span>[6] Andrzej Janik. Zluda: Cuda on non-nvidia\
    \ gpus. <https://github.com/vosen/ZLUDA>, 2024. Accessed: 2025-04-28.\n- <span\
    \ id=\"page-9-6\"></span>[7] AMD. HIP: Heterogeneous-computing Interface for Portability.\
    \ [https://github.com/](https://github.com/ROCm-Developer-Tools/HIP) [ROCm-Developer-Tools/HIP](https://github.com/ROCm-Developer-Tools/HIP),\
    \ 2024. Accessed: 2025-04-30.\n- <span id=\"page-9-7\"></span>[8] Advanced Micro\
    \ Devices (AMD). Amd rocm™ 6: Open software platform for gpu computing. Technical\
    \ report, Advanced Micro Devices, Inc., 2024.\n- <span id=\"page-9-8\"></span>[9]\
    \ Advanced Micro Devices, Inc. *HIPIFY Documentation*, 2025. Accessed: 2025-04-28.\n\
    - <span id=\"page-9-9\"></span>[10] Anwar Hossain Zahid, Ignacio Laguna, and Wei\
    \ Le. Testing gpu numerics: Finding numerical differences between nvidia and amd\
    \ gpus. In *SC24-W: Workshops of the International Conference for High Performance\
    \ Computing, Networking, Storage and Analysis*, pages 547–557. IEEE, 2024.\n-\
    \ <span id=\"page-9-10\"></span>[11] Anton Lozhkov, Raymond Li, Loubna Ben Allal,\
    \ Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar,\
    \ Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation.\
    \ *arXiv preprint arXiv:2402.19173*, 2024.\n- <span id=\"page-9-11\"></span>[12]\
    \ Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan\
    \ Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system\
    \ card. *arXiv preprint arXiv:2410.21276*, 2024.\n- <span id=\"page-9-12\"></span>[13]\
    \ Anthropic. Claude 3.7 sonnet and claude code, February 2025. Accessed: 2025-05-14.\n\
    - <span id=\"page-9-15\"></span>[14] AMD ROCm Documentation. *HIP Porting Guide*,\
    \ 2024. Accessed: 2025-01-29.\n- <span id=\"page-9-16\"></span>[15] Jun Chen,\
    \ Xule Zhou, and Hyesoon Kim. Cupbop-amd: Extending cuda to amd platforms. In\
    \ *Proceedings of the SC'23 Workshops of The International Conference on High\
    \ Performance Computing, Network, Storage, and Analysis*, pages 1093–1104, 2023.\n\
    - <span id=\"page-9-17\"></span>[16] Chris Lattner and Vikram Adve. LLVM: A compilation\
    \ framework for lifelong program analysis and transformation. In *Proceedings\
    \ of the International Symposium on Code Generation and Optimization*, pages 75–86,\
    \ San Jose, CA, USA, 2004. IEEE Computer Society.\n- <span id=\"page-9-18\"></span>[17]\
    \ The Clang Team. Clang: a c language family frontend for llvm, 2025. Accessed:\
    \ 2025-01-29.\n- <span id=\"page-9-19\"></span>[18] Gregory Diamos, Andrew Kerr,\
    \ and Sudhakar Yalamanchili. Gpuocelot: A dynamic compilation framework for ptx.\
    \ <https://github.com/gtcasl/gpuocelot>, 2009. Accessed: 2025-04- 28.\n- <span\
    \ id=\"page-9-13\"></span>[19] NVIDIA. Computeeval: Evaluating large language\
    \ models for cuda code generation. [https:](https://github.com/NVIDIA/compute-eval)\
    \ [//github.com/NVIDIA/compute-eval](https://github.com/NVIDIA/compute-eval),\
    \ 2024. Accessed: May 2025.\n- <span id=\"page-9-14\"></span>[20] Shuai Che, Michael\
    \ Boyer, Jiayuan Meng, David Tarjan, Jeremy W Sheaffer, Sang-Ha Lee, and Kevin\
    \ Skadron. Rodinia: A benchmark suite for heterogeneous computing. In *2009 IEEE\
    \ international symposium on workload characterization (IISWC)*, pages 44–54.\
    \ Ieee, 2009.\n- <span id=\"page-10-0\"></span>[21] Anthony Danalis, Gabriel Marin,\
    \ Collin McCurdy, Jeremy S Meredith, Philip C Roth, Kyle Spafford, Vinod Tipparaju,\
    \ and Jeffrey S Vetter. The scalable heterogeneous computing (shoc) benchmark\
    \ suite. In *Proceedings of the 3rd workshop on general-purpose computation on\
    \ graphics processing units*, pages 63–74, 2010.\n- <span id=\"page-10-1\"></span>[22]\
    \ Scott Grauer-Gray, Lifan Xu, Robert Searles, Sudhee Ayalasomayajula, and John\
    \ Cavazos. Auto-tuning a high-level language targeted to gpu codes. in 2012 innovative\
    \ parallel computing (inpar). *IEEE, Piscataway, NJ, USA*, pages 1–10, 2012.\n\
    - <span id=\"page-10-2\"></span>[23] Tom Deakin, James Price, Matt Martineau,\
    \ and Simon McIntosh-Smith. Gpu-stream v2. 0: Benchmarking the achievable memory\
    \ bandwidth of many-core processors across diverse parallel programming models.\
    \ In *High Performance Computing: ISC High Performance 2016 International Workshops,\
    \ ExaComm, E-MuCoCoS, HPC-IODC, IXPUG, IWOPH, Pˆ 3MA, VHPC, WOPSSS, Frankfurt,\
    \ Germany, June 19–23, 2016, Revised Selected Papers 31*, pages 489–507. Springer,\
    \ 2016.\n- <span id=\"page-10-3\"></span>[24] Ahmed Heakl, Chaimaa Abi, Rania\
    \ Hossam, and Abdulrahman Mahmoud. From cisc to risc: language-model guided assembly\
    \ transpilation. *arXiv preprint arXiv:2411.16341*, 2024.\n- <span id=\"page-10-4\"\
    ></span>[25] Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni,\
    \ David Brooks, Stephen Chong, Gu-Yeon Wei, and Alexander M Rush. Guess & sketch:\
    \ Language model guided transpilation. *arXiv preprint arXiv:2309.14396*, 2023.\n\
    - <span id=\"page-10-5\"></span>[26] Yifan Sun, Xiang Gong, Amir Kavyan Ziabari,\
    \ Leiming Yu, Xiangyu Li, Saoni Mukherjee, Carter McCardwell, Alejandro Villegas,\
    \ and David Kaeli. Hetero-mark, a benchmark suite for cpu-gpu collaborative computing.\
    \ In *2016 IEEE International Symposium on Workload Characterization (IISWC)*,\
    \ pages 1–10. IEEE, 2016.\n- <span id=\"page-10-6\"></span>[27] Anton Lozhkov,\
    \ Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane\
    \ Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and\
    \ the stack v2: The next generation. *arXiv preprint arXiv:2402.19173*, 2024.\n\
    - <span id=\"page-10-7\"></span>[28] NVIDIA Corporation. *CUDA Binary Utilities*,\
    \ 2025. [https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)\
    \ [cuda-binary-utilities/index.html](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html).\n\
    - <span id=\"page-10-8\"></span>[29] The Khronos Group. Opencl guide. <https://github.com/KhronosGroup/OpenCL-Guide>,\
    \ 2025. Accessed: 2025-05-14.\n- <span id=\"page-10-9\"></span>[30] Khronos Group.\
    \ clbuildprogram - opencl 3.0 reference pages. [https://registry.khronos.](https://registry.khronos.org/OpenCL/sdk/3.0/docs/man/html/clBuildProgram.html)\
    \ [org/OpenCL/sdk/3.0/docs/man/html/clBuildProgram.html](https://registry.khronos.org/OpenCL/sdk/3.0/docs/man/html/clBuildProgram.html),\
    \ 2020. Accessed: 2025- 05-14.\n- <span id=\"page-10-10\"></span>[31] Maja Popovic.\
    \ chrf: character n-gram f-score for automatic mt evaluation. In ´ *Proceedings\
    \ of the tenth workshop on statistical machine translation*, pages 392–395, 2015.\n\
    - <span id=\"page-10-11\"></span>[32] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi\
    \ Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu,\
    \ et al. Qwen2. 5-coder technical report. *arXiv preprint arXiv:2409.12186*, 2024.\n\
    - <span id=\"page-10-12\"></span>[33] Jeff Rasley, Samyam Rajbhandari, Olatunji\
    \ Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep\
    \ learning models with over 100 billion parameters. In *Proceedings of the 26th\
    \ ACM SIGKDD international conference on knowledge discovery & data mining*, pages\
    \ 3505–3506, 2020.\n- <span id=\"page-10-13\"></span>[34] Pin-Lun Hsu, Yun Dai,\
    \ Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam\
    \ Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels\
    \ for llm training. *arXiv preprint arXiv:2410.10989*, 2024.\n- <span id=\"page-10-14\"\
    ></span>[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\
    \ *arXiv preprint arXiv:1711.05101*, 2017.\n- <span id=\"page-10-15\"></span>[36]\
    \ Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng,\
    \ and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language\
    \ models. *arXiv preprint arXiv:2403.13372*, 2024.\n- <span id=\"page-11-1\"></span>[37]\
    \ Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\
    \ Enhanced transformer with rotary position embedding. *Neurocomputing*, 568:127063,\
    \ 2024.\n- <span id=\"page-11-0\"></span>[38] Demis Hassabis and Koray Kavukcuoglu.\
    \ Introducing gemini 2.0: our new ai model for the agentic era, December 2024.\
    \ Accessed: 2025-05-14.\n\n# A Appendix\n\n## A.1 Evaluation on ZLUDA\n\nTo assess\
    \ ZLUDA's ability to execute CUDA code on AMD GPUs, we designed a two-track evaluation\
    \ strategy targeting both source-level and binary-level workflows (the latter\
    \ being akin to assembly-level translation). In the source-to-source setting,\
    \ we leveraged access to the original CUDA source files to manually compile them\
    \ into PTX using nvcc. These PTX files were then ingested by ZLUDA, which translated\
    \ them into AMD-compatible LLVM IR before lowering them into native executables\
    \ targeting RDNA3 hardware. In the assembly-to-assembly setting, we instead compiled\
    \ the CUDA source into a complete executable and invoked it directly. ZLUDA intercepted\
    \ the CUDA runtime calls, dynamically translated the embedded PTX or SASS, and\
    \ executed the resulting code on the AMD backend. This dual strategy allowed us\
    \ to assess both ZLUDA's static translation capabilities and its runtime interoperability\
    \ under realistic execution conditions.\n\n# A.2 Hardware used\n\nAll experiments\
    \ were conducted on two distinct machines to generate architecture-specific outputs.\
    \ For AMD-related compilation and execution, we used a workstation equipped with\
    \ an Intel i7- 14700KF CPU and an AMD Radeon RX 7900 XT GPU. For NVIDIA-related\
    \ outputs, we used a server with an AMD EPYC 9654 CPU and an Nvidia A100 (80GB)\
    \ GPU. Although the Nvidia system features a high-end data center GPU, all CUDA\
    \ code was compiled targeting the compute capabilities of a standard consumer-grade\
    \ GPU (e.g., RTX 4090) to maintain parity with the AMD hardware. Furthermore,\
    \ to ensure consistency and reproducibility across platforms, all file generation\
    \ was performed within Docker containers tailored to each architecture.\n\n<span\
    \ id=\"page-12-0\"></span>![](_page_12_Figure_5.jpeg)\n\nFigure 7: Relationship\
    \ between source and assembly-level LoC in the CASS dataset. Scatter plot comparing\
    \ source code lines of code (LoC) to the corresponding assembly LoC for both CUDA\
    \ and HIP backends across the Stackv2 and Synthetic subsets. Trend lines and density\
    \ contours illustrate that CUDA typically produces more verbose assembly output\
    \ than HIP for equivalent source sizes.\n\n![](_page_13_Figure_0.jpeg)\n\nFigure\
    \ 8: Opcode Category Distribution by Dataset and Code Type. Stacked bar chart\
    \ showing the distribution of assembly instructions across 10 opcode categories\
    \ for device and host code in the Synthetic, Stackv2, and OpenCL subsets. Each\
    \ bar represents a (dataset, code type) pair, illustrating the functional composition\
    \ of the code across memory, tensor, control flow, synchronization, and other\
    \ operations.\n\n<span id=\"page-13-0\"></span>![](_page_13_Figure_2.jpeg)\n\n\
    (a) HIP assembly opcodes. (b) CUDA assembly opcodes.\n\nFigure 9: Most frequent\
    \ opcodes in HIP and CUDA assembly. Word clouds depicting the most common opcodes\
    \ in HIP and CUDA assembly files. The size of each opcode reflects its relative\
    \ frequency in the compiled dataset, highlighting structural and architectural\
    \ differences between the two backends.\n\n![](_page_13_Figure_6.jpeg)\n\nFigure\
    \ 10: Accuracy vs. training steps for source/assembly across CASS model scales\
    \ (1.5B, 3B, 7B).\n\n<span id=\"page-14-2\"></span>![](_page_14_Figure_0.jpeg)\n\
    \nFigure 11: Comparison of memory usage (left) and execution time (right) between\
    \ predicted and ground truth HIP programs, measured via compilation and runtime\
    \ profiling.\n\n## A.3 CASS Domain Coverage\n\nTo obtain the domain-level breakdown\
    \ shown in Figure [3,](#page-5-1) we developed a static analysis pipeline that\
    \ categorizes each source file based on its content. The classification is performed\
    \ by matching the file's text against curated sets of domain-specific keywords\
    \ corresponding to seven high-level categories: *general compute, simulation,\
    \ data structure, machine learning, graphics, cryptography,* and *scientific computing*.\
    \ Each keyword set includes terms commonly associated with the respective domain;\
    \ for example, the *machine learning* category includes terms such as neural,\
    \ gradient, and activation, while *cryptography* includes hash, encrypt, and signature.\
    \ For a given file, the domain with the highest keyword match count is assigned.\
    \ If no keywords are matched, a default label (e.g., *general compute*) is applied.\
    \ After all files are processed, their assignments are aggregated to produce the\
    \ final domain distribution. This process provides a simple yet straightforward\
    \ and interpretable way of grouping source files by their functional domain.\n\
    \n### <span id=\"page-14-1\"></span>A.4 Extra Data Analysis\n\n### <span id=\"\
    page-14-0\"></span>A.4.1 Length of Assembly Files\n\nAs shown in the figure [7](#page-12-0)\
    \ We found an exponential relationship between source complexity and assembly\
    \ size, with CUDA producing more verbose outputs than HIP for equivalent code.\
    \ This highlights the growing difficulty of assembly-level translation as code\
    \ complexity scales.\n\n### A.4.2 Code efficiency\n\nAs shown figure [5,](#page-8-2)\
    \ assembly accuracy is inconsistent, 0% in Math, Data Structures, and Graph, 25–50%\
    \ in linear algebra and memory operations. This reflects the challenge of low-level\
    \ semantic preservation. Only physics simulation achieves 100%, likely due to\
    \ simpler or repetitive control flows. As shown in figures [11,](#page-14-2) the\
    \ translated code exhibits tight fidelity to the ground truth. Memory usage deviates\
    \ by less than ±0.3% for all files, with 18 files using more memory (max +0.3%)\
    \ and 22 using less (min −0.3%). Execution time differences are similarly small:\
    \ 11 files are slower (max +11.8%), 8 are faster (min −10.0%), and the rest are\
    \ unchanged. Over 85% of samples fall within ±5.6% across both metrics, confirming\
    \ that our model preserves both memory and runtime efficiency during assembly\
    \ translation. Each test was executed 20 times, and the reported values reflect\
    \ the average across runs to mitigate noise and ensure statistical reliability.\n\
    \n# A.4.3 Opcode Diversity\n\nTaking a deeper dive into the low-level instructions\
    \ representation shown in figure [9,](#page-13-0) a few extra insights can be\
    \ drawn. In the HIP case, many opcodes, such as s\\_mov\\_b32, v\\_add\\_co\\\
    _u32, and s\\_waitcnt, come directly from AMD's GPU instruction set. These reflect\
    \ fine-grained control over the hardware, including scalar and vector operations\
    \ and synchronization. On the other hand, the CUDA assembly is mostly made up\
    \ of x86-64 instructions like movq, call, jmp, and pushq, which are typically\
    \ used on the CPU. This suggests that the CUDA output includes more host-side\
    \ code or that GPU instructions are hidden behind a higher level of abstraction.\
    \ Still, both stacks share common instructions like mov and test, showing that\
    \ some basic control and memory operations are similar. In general, HIP provides\
    \ more visibility into what the GPU is doing, while CUDA hides many of those low-level\
    \ details behind a more unified host-device model.\n\n#### <span id=\"page-15-0\"\
    ></span>A.5 Sythetic Generation\n\nTo generate large-scale, diverse CUDA programs,\
    \ we design a multiprocessing Python pipeline that interacts with a locally hosted\
    \ large language model via a chat-based API. The pipeline leverages a wide array\
    \ of handcrafted prompt templates, each parameterized with variables such as problem\
    \ size, optimization target, algorithm type, and architectural features (see Appendix\
    \ [A.5.1\\)](#page-16-0). At runtime, these templates are instantiated with randomly\
    \ sampled values from curated sets covering domains like matrix operations, graph\
    \ algorithms, scientific computing, machine learning, and sparse computation (see\
    \ Table [5\\)](#page-15-1). Each worker process independently generated prompts,\
    \ sends them to the model, extracts valid CUDA code from the response, and saves\
    \ the output in a structured format. Robust faulttolerance mechanisms—including\
    \ retry logic, output validation, and file existence checks—ensure resilience\
    \ to model failures and concurrent access. The system supports parallel generation\
    \ with controlled API concurrency and automatic resumption from previous checkpoints,\
    \ enabling scalable and efficient generation of compilable CUDA code samples suitable\
    \ for downstream benchmarking or training.\n\n| Placeholder              | Example\
    \ Values                                                 |\n|--------------------------|----------------------------------------------------------------|\n\
    | {size}                   | 64, 1024, 16384                                 \
    \               |\n| {dimension}              | 1, 3, 6                      \
    \                                  |\n| {optimization}           | memory coalescing,\
    \ shared memory usage, warp-level programming |\n| {operation}              |\
    \ sum, histogram, L2 norm                                        |\n| {algorithm}\
    \              | matrix multiplication, radix sort, BFS                      \
    \   |\n| {radius}                 | 1, 5, 13                                 \
    \                      |\n| {graph_format}           | adjacency matrix, CSR,\
    \ edge list                               |\n| {md_algorithm}           | Verlet\
    \ integration, leapfrog, Runge-Kutta                      |\n| {linear_solver}\
    \          | conjugate gradient, Jacobi, multigrid                          |\n\
    | {numerical_method}       | finite difference, spectral, Crank-Nicolson     \
    \               |\n| {factorization_method}   | SVD, LU, eigenvalue decomposition\
    \                              |\n| {conv_layer_count}       | 2, 6, 12      \
    \                                                 |\n| {neuron_count}        \
    \   | 64, 512, 2048                                                  |\n| {sparse_format}\
    \          | CSR, ELL, HYB                                                  |\n\
    | {nbody_algorithm}        | Barnes-Hut, brute force, particle mesh          \
    \               |\n| {filter_type}            | Gaussian, Sobel, Gabor       \
    \                                  |\n| {filter_size}            | 3, 7, 15  \
    \                                                     |\n| {resolution}      \
    \       | 720p, 1080p, 4K                                                |\n|\
    \ {segmentation_algorithm} | watershed, region growing, U-Net                \
    \               |\n| {signal_transform}       | FFT, wavelet, Hilbert        \
    \                                  |\n| {optimization_algorithm} | Adam, simulated\
    \ annealing, particle swarm                      |\n| {crypto_algorithm}     \
    \  | AES, RSA, Argon2                                               |\n| {cracking_method}\
    \        | brute force, dictionary attack, rainbow table                  |\n\
    | {hash_algorithm}         | SHA-256, BLAKE3, Bcrypt                         \
    \               |\n| {data_structure}         | binary tree, hash table, bloom\
    \ filter                          |\n| {collision_strategy}     | linear probing,\
    \ cuckoo hashing, separate chaining              |\n\n<span id=\"page-15-1\"></span>Table\
    \ 5: Representative values for prompt placeholders used in the synthetic code\
    \ generation.\n\n### <span id=\"page-16-0\"></span>A.5.1 Prompt Templates for\
    \ Synthetic CUDA Code Generation\n\n#### Basic Operations\n\n- 1. Implement a\
    \ CUDA kernel for {size}D FFT (Fast Fourier Transform). Optimize for { optimization}.\n\
    - 2. Generate a CUDA implementation for {size}D stencil computation with radius\
    \ { radius}. Optimize for {optimization}.\n- 3. Write a CUDA kernel for parallel\
    \ reduction to compute the {operation} of an array of size {size}. Focus on {optimization}.\n\
    - 4. Create a CUDA implementation for convolution operation with a {size}x{size}\
    \ filter. Focus on {optimization} optimization.\n- 5. Generate a CUDA kernel for\
    \ matrix multiplication of two matrices A and B of size {size}x{size}. Include\
    \ error handling and optimize for {optimization}.\n\n#### Graph Algorithms\n\n\
    - 1. Write a CUDA implementation for graph coloring of a graph with {size} nodes.\
    \ Focus on {optimization}.\n- 2. Implement a CUDA kernel for community detection\
    \ in a graph with {size} nodes using the {community\\_algorithm} algorithm.\n\
    - 3. Implement a CUDA kernel for graph processing that computes {algorithm} on\
    \ a graph with {size} nodes. Optimize for {optimization}.\n- 4. Generate a CUDA\
    \ kernel for finding strongly connected components in a directed graph with {size}\
    \ nodes. Optimize for {optimization}.\n- 5. Create a CUDA implementation for breadth-first\
    \ traversal on a graph with {size} nodes stored in {graph\\_format}. Optimize\
    \ for {optimization}.\n\n#### Scientific Computing\n\n- 1. Write a CUDA implementation\
    \ for {size}D fluid simulation using {method}. Focus on {optimization}.\n- 2.\
    \ Create a CUDA kernel for Monte Carlo simulation of {size} paths for option pricing.\
    \ Focus on {optimization}.\n- 3. Implement a CUDA solver for {size}x{size} sparse\
    \ linear system using { linear\\_solver}. Focus on {optimization}.\n- 4. Generate\
    \ a CUDA implementation for {size}D heat equation solver using { numerical\\_method}.\
    \ Optimize for {optimization}.\n- 5. Create a CUDA kernel for molecular dynamics\
    \ simulation of {size} particles using {md\\_algorithm}. Optimize for {optimization}.\n\
    \n#### Machine Learning\n\n- 1. Generate a CUDA kernel for k-means clustering\
    \ of {size} data points in {dimension }D space. Optimize for {optimization}.\n\
    - 2. Implement a CUDA kernel for {size}x{size} matrix factorization using { factorization\\\
    _method}. Optimize for {optimization}.\n- 3. Create a CUDA implementation for\
    \ computing attention mechanism in a transformer with {size} tokens. Focus on\
    \ {optimization}.\n- 4. Implement a CUDA kernel for backpropagation in a convolutional\
    \ neural network with {conv\\_layer\\_count} conv layers. Optimize for {optimization}.\n\
    - 5. Write a CUDA implementation for training a neural network with {layer\\_count}\
    \ layers and {neuron\\_count} neurons per layer. Focus on {optimization}.\n\n\
    #### Sparse Operations\n\n- 1. Generate a CUDA kernel for sparse FFT computation.\
    \ Optimize for {optimization}.\n- 2. Implement a CUDA kernel for sparse tensor\
    \ operations with {size} non-zero elements. Optimize for {optimization}.\n- 3.\
    \ Write a CUDA implementation for sparse convolution with {size}x{size} filter\
    \ on sparse input. Focus on {optimization}.\n- 4. Create a CUDA implementation\
    \ for sparse matrix-matrix multiplication in { sparse\\_format} format. Focus\
    \ on {optimization}.\n- 5. Generate a CUDA kernel for sparse matrix-vector multiplication\
    \ where the matrix has approximately {size} non-zero elements. Optimize for {optimization}.\n\
    \n#### Simulation\n\n- 1. Generate a CUDA kernel for cloth simulation with {size}x{size}\
    \ grid. Optimize for {optimization}.\n- 2. Write a CUDA implementation for raytracing\
    \ of a scene with {size} objects. Focus on {optimization}.\n- 3. Create a CUDA\
    \ implementation for {algorithm} of {size} particles in a {dimension} D space.\
    \ Focus on {optimization}.\n- 4. Create a CUDA implementation for fluid-structure\
    \ interaction with {size} boundary elements. Focus on {optimization}.\n- 5. Implement\
    \ a CUDA kernel for N-body simulation of {size} particles using { nbody\\_algorithm}.\
    \ Optimize for {optimization}.\n\n#### Image and Signal Processing\n\n- 1. Create\
    \ a CUDA implementation for feature extraction from {size}x{size} images. Focus\
    \ on {optimization}.\n- 2. Generate a CUDA kernel for image segmentation using\
    \ {segmentation\\_algorithm}. Optimize for {optimization}.\n- 3. Write a CUDA\
    \ implementation for real-time video processing of {resolution} frames . Focus\
    \ on {optimization}.\n- 4. Implement a CUDA kernel for signal processing with\
    \ {size}-point {signal\\_transform }. Optimize for {optimization}.\n- 5. Implement\
    \ a CUDA kernel for image filtering using {filter\\_type} filter of size { filter\\\
    _size}x{filter\\_size}. Optimize for {optimization}.\n\n#### Optimization Algorithms\n\
    \n- 1. Implement a CUDA kernel for simulated annealing with {size} states. Optimize\
    \ for {optimization}.\n- 2. Generate a CUDA kernel for genetic algorithm with\
    \ population size {size}. Optimize for {optimization}.\n- 3. Write a CUDA implementation\
    \ for {optimization\\_algorithm} with {size} variables. Focus on {optimization}.\n\
    - 4. Write a CUDA implementation for gradient descent optimization with {size}\
    \ parameters. Focus on {optimization}.\n- 5. Create a CUDA implementation for\
    \ particle swarm optimization with {size} particles in {dimension}D space. Focus\
    \ on {optimization}.\n\n#### Cryptography and Security\n\n- 1. Generate a CUDA\
    \ kernel for homomorphic encryption operations. Optimize for { optimization}.\n\
    - 2. Write a CUDA implementation for secure hashing using {hash\\_algorithm}.\
    \ Focus on { optimization}.\n- 3. Generate a CUDA kernel for {crypto\\_algorithm}\
    \ encryption/decryption. Optimize for {optimization}.\n- 4. Create a CUDA implementation\
    \ for blockchain mining with difficulty {size}. Focus on {optimization}.\n- 5.\
    \ Implement a CUDA kernel for password cracking using {cracking\\_method}. Optimize\
    \ for {optimization}.\n\n#### Data Structures\n\n- 1. Create a CUDA implementation\
    \ for priority queue with {size} elements. Focus on { optimization}.\n- 2. Create\
    \ a CUDA implementation for {data\\_structure} with {size} elements. Focus on\
    \ {optimization}.\n- 3. Implement a CUDA kernel for operations on a B-tree with\
    \ {size} nodes. Optimize for {optimization}.\n- 4. Generate a CUDA kernel for\
    \ skip list operations with {size} elements. Optimize for {optimization}.\n- 5.\
    \ Write a CUDA implementation for hash table with {size} buckets using { collision\\\
    _strategy}. Focus on {optimization}.\n\n#### A.5.2 Qualitative Comparison with\
    \ Other LLMs\n\nWe highlight several cases where CASS-7B outperforms existing\
    \ LLMs such as Claude, Qwen-Coder, and GPT-4o in faithfully transpiling CUDA to\
    \ HIP. For example, in one instance, CASS-7B correctly transpiled the CUDA code\
    \ while preserving the exact string constants from the original program, including\
    \ the label CUDA in the output format string. Maintaining these strings is essential\
    \ for preserving the intended user-facing behavior, particularly in logging or\
    \ debugging scenarios where clarity and consistency matter. In contrast, Claude,\
    \ Qwen-Coder, and GPT4o unnecessarily altered the string to say HIP, despite the\
    \ output still originating from a CUDA kernel. This substitution introduces a\
    \ semantic error, as the original string refers to CUDA, not HIP, and should remain\
    \ unchanged.\n\n### CASS-7B\n\n```\nprintf(\"tanh(%f) = %f CUDA vs %f (CPU)\\\
    n\",\n       h_input[idx], h_output[idx], tanh(h_input[idx]));\n```\n### Claude,\
    \ Qwen-Coder, GPT4o\n\n```\nprintf(\"tanh(%f) = %f (HIP) vs %f (CPU)\\n\",\n \
    \      h_input[idx], h_output[idx], tanh(h_input[idx]));\n```\nIn another example,\
    \ CASS-7B retained the classical CUDA-style kernel launch syntax using triple\
    \ angle brackets (<<<...>>>), while also ensuring that the generated code remained\
    \ compilable by correctly including the required HIP header <hip/hip\\_runtime.h>.\
    \ This demonstrates a high degree of structural fidelity to the source code, which\
    \ is especially important for developers familiar with standard CUDA conventions.\
    \ In contrast, other models such as Claude and Qwen-Coder replaced the launch\
    \ expression with the HIP-specific macro hipLaunchKernelGGL, which, while functionally\
    \ valid, deviates from the original representation. More critically, they failed\
    \ to include the necessary HIP header, rendering the output uncompilable. This\
    \ example highlights how CASS-7B goes beyond syntactic accuracy to produce code\
    \ that is both faithful to the original structure and immediately usable in a\
    \ real compilation setting.\n\nCASS-7B\n\n```\n#include <hip/hip_runtime.h>\n\
    #include <iostream>\n...\nadd<<<(N + 255) / 256, 256>>>(d_a, d_b, d_c, N);\n```\n\
    Claude, Qwen-Coder\n\n```\n#include <iostream>\n...\nhipLaunchKernelGGL(add, (N\
    \ + 255) / 256, 256, 0, 0, d_a, d_b, d_c, N);\n```\nLastly, when verifying numerical\
    \ correctness, CASS-7B preserved the original logging behavior by correctly emitting\
    \ output to std::cout, as in the source code. This choice maintains consistency\
    \ with the original program's semantics, especially in distinguishing between\
    \ standard output and error streams; important in contexts where output may be\
    \ redirected or parsed. In contrast, GPT-4o unnecessarily altered the output stream\
    \ to std::cerr, which, while syntactically valid, changes the runtime behavior\
    \ of the program. Such a change could lead to unexpected side effects in downstream\
    \ tools or logging pipelines. This example further demonstrates CASS-7B's attention\
    \ to both structural and behavioral fidelity in its translations.\n\nCASS-7B\n\
    \n| std::cout << \"Error at element \" << i << \": \" << h_output[I] |  |  | \
    \ |  |  |  |                                                     |  |\n|--------------------------------------------------------------|--|--|--|--|--|--|-----------------------------------------------------|--|\n\
    |                                                              |  |  |  |  | \
    \ |  | << \" vs. expected \" << h_reference[i] << std::endl; |  |\n\nGPT4o\n\n\
    ```\nstd::cerr << \"Error at element \" << i << \": \" << h_output[i]\n      \
    \    << \" vs expected \" << h_reference[i] << std::endl;\n```"
- title: "DAS-MP: Enabling High-Quality Macro Placement with Enhanced Dataflow\n \
    \ Awareness"
  abstract: 'Dataflow is a critical yet underexplored factor in automatic macro placement,

    which is becoming increasingly important for developing intelligent design

    automation techniques that minimize reliance on manual adjustments and reduce

    design iterations. Existing macro or mixed-size placers with dataflow awareness

    primarily focus on intrinsic relationships among macros, overlooking the

    crucial influence of standard cell clusters on macro placement. To address

    this, we propose DAS-MP, which extracts hidden connections between macros and

    standard cells and incorporates a series of algorithms to enhance dataflow

    awareness, integrating them into placement constraints for improved macro

    placement. To further optimize placement results, we introduce two fine-tuning

    steps: (1) congestion optimization by taking macro area into consideration, and

    (2) flipping decisions to determine the optimal macro orientation based on the

    extracted dataflow information. By integrating enhanced dataflow awareness into

    placement constraints and applying these fine-tuning steps, the proposed

    approach achieves an average 7.9% improvement in half-perimeter wirelength

    (HPWL) across multiple widely used benchmark designs compared to a

    state-of-the-art dataflow-aware macro placer. Additionally, it significantly

    improves congestion, reducing overflow by an average of 82.5%, and achieves

    improvements of 36.97% in Worst Negative Slack (WNS) and 59.44% in Total

    Negative Slack (TNS). The approach also maintains efficient runtime throughout

    the entire placement process, incurring less than a 1.5% runtime overhead.

    These results show that the proposed dataflow-driven methodology, combined with

    the fine-tuning steps, provides an effective foundation for macro placement and

    can be seamlessly integrated into existing design flows to enhance placement

    quality.'
  url: http://arxiv.org/abs/2505.16445v1
  keywords: Floorplanning, Macro placement, Standard cells, Dataflow awareness, QoR,
    Fine-tuning
  document: "## I. INTRODUCTION\n\nWITH increasing process technology complexity and\
    \ higher levels of chip integration, System on Chip (SoC) has become the mainstream\
    \ design model which relies on a large number of IPs (internal or external) to\
    \ improve the\n\nThis work has been submitted to the IEEE for possible publication.\
    \ Copyright may be transferred without notice, after which this version may no\
    \ longer be accessible.\n\nThis work was supported in part by the National Science\
    \ and Technology Major Project (Grant No. 2021ZD0114701), the State Key Laboratory\
    \ of Integrated Chips and Systems (SKLICS) open fund and a SJTU Explore-X grant.\n\
    \nXiaotian Zhao, Zixuan Li, Yichen Cai, Tianju Wang, Jiayin Chen and Xinfei Guo\
    \ are with the University of Michigan – Shanghai Jiao Tong University Joint Institute,\
    \ Shanghai Jiao Tong University, Shanghai 200240, China (E-mails: {xiaotian.zhao,\
    \ cai yichen, wangtianju, chenjiayin, xinfei.guo}@sjtu.edu.cn, lzixuan136@gmail.com).\n\
    \nYushan Pan is with the Xi'an Jiaotong-Liverpool University, Suzhou 215000, China\
    \ (E-mail: Yushan.Pan@xjtlu.edu.cn).\n\nCorresponding author: Xinfei Guo.\n\n\
    ![](_page_0_Figure_12.jpeg)\n\n<span id=\"page-0-0\"></span>Fig. 1. The illustration\
    \ shows all dataflow connections, with the uncovered dataflow connections indicated\
    \ by shaded lines.\n\nreuse of modules, reducing the cost of repeated development\
    \ and shortening the design cycle. From a logic function perspective, IPs comprise\
    \ various hardware modules and circuit components, such as memory cells, analog\
    \ modules (e.g., ADCs, PLLs), and even entire processor cores. However, from a\
    \ physical design perspective, these IPs come in the form of macros, which are\
    \ typically larger, pin-dominant, and have a greater impact on timing, congestion,\
    \ and design rule checking (DRC) violations than standard cells. The quality of\
    \ macro placement in the floorplanning phase will directly impact how standard\
    \ cells are placed and routed in later stages, and ultimately the design's quality-of-results\
    \ (QoR). However, as design scales continue to grow, the complexity of macro placement\
    \ has significantly increased, becoming a critical bottleneck that affects both\
    \ design efficiency and quality.\n\nIn the current physical design process, macro\
    \ placement typically relies on experienced engineers performing manual placements,\
    \ often requiring multiple iterations to achieve an acceptable placement to some\
    \ extent. As a result, fully automating macro placement is highly challenging.\
    \ This is due to the conflict between the very limited design and physical information\
    \ available in the early stages and the vast search space, making it nearly impossible\
    \ to accurately predict the timing and other relevant information required for\
    \ the subsequent placement and routing process (P&R), thus hindering placement\
    \ optimization. Although Reinforcement Learning (RL), Deep Learning (DL), or recently\
    \ proposed large-language model (LLM)-based methods, such as [ [1\\]](#page-13-0)–[\
    \ [4\\]](#page-13-1), have shown promise in automating macro placement, they face\
    \ several significant challenges. First, these methods often struggle with legalization,\
    \ as the generated placements may violate design rules or physical constraints.\
    \ Second, the training process for RL/DL/LLM models is computationally intensive,\
    \ requiring substantial resources and time. Third, obtaining highquality and diverse\
    \ training datasets is difficult, especially for complex designs with unique characteristics.\
    \ Additionally, some of these techniques rely on a good initial placement as a\
    \ starting point [\\[4\\]](#page-13-1), which limits their applicability in scenarios\
    \ where such prior knowledge is unavailable. These limitations highlight the need\
    \ for complementary techniques to achieve human-quality macro placement results.\n\
    \nTo automate macro placement and mimic manual strategies, it is crucial to identify\
    \ the features that physical engineers consider during placement. One prominent\
    \ feature is dataflow, which engineers use to optimize macro positioning. Dataflow\
    \ defines the movement of data between macros and standard cells, influencing\
    \ the circuit's timing and power consumption. Physical design engineers invest\
    \ significant effort in analyzing flylines and collaborating with logic design\
    \ engineers to gain a deep understanding of dataflow information, underscoring\
    \ the importance of dataflow awareness in macro placement. Since the interactions\
    \ between macros and other modules can be analyzed from the synthesized netlist.\
    \ Furthermore, the number of macros is significantly smaller than standard cells\
    \ in modern designs, making it computationally feasible to extract detailed dataflow\
    \ connections involving macros. This has inspired dataflow-aware macro placers\
    \ [\\[5\\]](#page-13-2)–[\\[11\\]](#page-13-3), where the dataflow is analyzed\
    \ and incorporated as constraints to guide macro placement.\n\nCurrent macro placers\
    \ or mixed-size placers incorporate some level of dataflow analysis, but they\
    \ mainly only focus on the virtual or direct connections between macros (i.e.\
    \ macro-macro connections), as the placement of standard cells is typically performed\
    \ after the macro placement. However, an important consideration is that standard\
    \ cells are often placed in clusters based on their logic hierarchy. If a cluster\
    \ is large enough, it can be modeled as a \"macro\". Therefore, the placement\
    \ of standard cell clusters in turn impact how macros are placed. This is reflected\
    \ in the current design process, where physical design teams typically rely on\
    \ feedback from front-end design teams to create guide or fence regions for standard\
    \ cell clusters, which are crucial for floorplanning and constraint-based cell\
    \ placement. The impact of standard cells on macro placement cannot be neglected.\
    \ This is illustrated in Fig. [1,](#page-0-0) where routing resources can be optimized\
    \ by adjusting the location of *Macro D*, considering macro-cell connections and\
    \ dataflow among these modules. Additionally, the orientation of the macro plays\
    \ a key role in determining overall QoR, as different numbers of pin accesses\
    \ are available on each side of the macro, leading to different wirelengths. This\
    \ is also shown in Fig. [1,](#page-0-0) where *Macro D* is flipped along the Y-axis\
    \ to reduce the distance to the logically connected cell clusters. Inspired by\
    \ these observations, we propose DAS-MP, which incorporates a series of incremental\
    \ optimization strategies for high-quality macro placement. First, recognizing\
    \ the importance of dataflow awareness, we introduce a novel methodology that\
    \ efficiently extracts four distinct types of \"hidden\" relationships between\
    \ macros and cell clusters. To further refine the macro placement after the initial\
    \ optimiza-\n\n<span id=\"page-1-1\"></span>TABLE I A SUMMARY OF RECENTLY PROPOSED\
    \ MACRO PLACERS\n\n| Type        | Work       | Key<br>Method             | Dataflow<br>Consideration\
    \ | Flipping<br>Optimization |\n|-------------|------------|---------------------------|---------------------------|--------------------------|\n\
    |             | [1]        | RL + GNN                  | ✗                   \
    \      | ✗                        |\n|             | [2]        | Graph Attention\
    \           | ✗                         | ✗                        |\n|      \
    \       | [12]       | RL                        | ✗                         |\
    \ ✗                        |\n| [13]        |            | RL + SA           \
    \        | ✗                         | ✗                        |\n| ML-based\
    \    | [3]        | NN + Bayesian             | ✗                         | ✗\
    \                        |\n| Methods     | [14]       | NN                  \
    \      | ✗                         | ✗                        |\n|           \
    \  | [15]       | Bayesian                  | ✗                         | ✗  \
    \                      |\n|             | [16]       | SVM + NN              \
    \    | ✓                         | ✗                        |\n|             |\
    \ [17]       | Training-based Prediction | ✗                         | ✗     \
    \                   |\n|             | [18]       | Gradient Optimization    \
    \ | ✗                         | ✗                        |\n|             | [19]\
    \       | Priority-queue clustering | ✗                         | ✗          \
    \              |\n|             | [20]       | Linear Programming        | ✗ \
    \                        | ✗                        |\n|             | [21]  \
    \     | MP-tree                   | ✗                         | ✗            \
    \            |\n|             | [22]       | Electrostatics-based      | ✗   \
    \                      | ✗                        |\n|             | [23]    \
    \   | Density Function          | ✗                         | ✗              \
    \          |\n|             | [24]       | Linear Programming        | ✗     \
    \                    | ✗                        |\n| Traditional | [25], [26]\
    \ | CP-tree                   | ✗                         | ✗                \
    \        |\n| Methods     | [27]       | Hierarchical Approach     | ✗       \
    \                  | ✗                        |\n|             | [5]        |\
    \ Dataflow Graph            | ✓                         | ✗                  \
    \      |\n|             | [6], [9]   | Dataflow Graph            | ✓         \
    \                | ✗                        |\n|             | [8]        | Simulating\
    \ Human          | ✓                         | ✓                        |\n| \
    \            | [10]       | Hierarchy Clustering      | ✓                    \
    \     | ✓                        |\n|             | [11]       | Dataflow Awareness\
    \        | ✓                         | ✗                        |\n\ntion, we\
    \ integrate macro area consideration and orientation optimization into the fine-tuning\
    \ process. In this process, macros of varying sizes are adjusted based on dataflow\
    \ intensity, and macro orientation is optimized to align with the dataflow direction.\
    \ By refining the placement in this way, the proposed methodology achieve better\
    \ correlation between dataflowaware placement and practical physical design constraints,\
    \ leading to improved overall performance. Furthermore, the proposed DAS-MP methodology\
    \ demonstrates excellent portability and efficiency; it can be seamlessly integrated\
    \ as a plugin with commercial tools, thereby enhancing macro placement outcomes.\n\
    \nThe rest of the paper is organized as follows. Section [II](#page-1-0) reviews\
    \ state-of-the-art macro placers and outlines the motivation behind the proposed\
    \ work. Section [III](#page-3-0) provides an overview of the DAS-MP methodology.\
    \ Sections [IV](#page-4-0) and [V](#page-5-0) detail the extraction algorithms.\
    \ The fine-tuning methods are discussed in Section [VI.](#page-6-0) Evaluation\
    \ results are presented in Section [VII.](#page-8-0) Finally, Section [VIII](#page-12-0)\
    \ concludes the paper.\n\n## II. BACKGROUND AND RELATED WORK\n\n## <span id=\"\
    page-1-0\"></span>*A. Related Work*\n\nMacro placement and floorplanning have\
    \ long been fundamental challenges in the field of electronic design automation\
    \ (EDA). Traditionally, macro placement has been modeled as an objective-driven\
    \ problem and tackled using analytical methods or iterative-perturbative techniques.\
    \ The recent rise of machine learning (ML) has spurred interest in applying these\
    \ techniques to macro placement, leading to the development of reinforcement learning\
    \ (RL) and deep learning (DL)-based methods. As summarized in Table [I,](#page-1-1)\
    \ we categorize these techniques into two groups: ML-based methods and traditional\
    \ methods, which refer to non-ML-based approaches.\n\nFor ML-based methods, Google\
    \ [\\[1\\]](#page-13-0) utilized Graph Neural Network (GNN) to process netlists\
    \ and applied Reinforcement Learning to constrain the placement of macro blocks\
    \ while considering some parameter constraints. This approach introduced a new\
    \ dimension for macro placement and has generated significant interest in the\
    \ field. In [\\[12\\]](#page-13-5), building on Google's method, a Convolutional\
    \ Neural Network (CNN) was used to capture global information, and a GNN was used\
    \ to extract more detailed information, and then fully connected into policy to\
    \ achieve multi-perspective optimization. Similarly, [\\[2\\]](#page-13-4) presented\
    \ a graph attention-based floorplanner to learn an optimized mapping between circuit\
    \ connection and physical wirelength. While in [\\[13\\]](#page-13-6), a combined\
    \ approach using simulated annealing and RL was employed for macro placement.\
    \ In [\\[3\\]](#page-13-7), AutoDMP was proposed as an optimization framework\
    \ built on DREAMPlace [\\[14\\]](#page-13-8), incorporating multi-objective Bayesian\
    \ optimization into the design space search. It demonstrated the ability to further\
    \ improve the results obtained from DREAMPlace. Similarly, [\\[15\\]](#page-13-9)\
    \ modeled the macro placement problem as Bayesian optimization over sequence pairs,\
    \ concluding that Bayesian optimization is more sample-efficient than RL and can\
    \ accommodate more realistic objectives. In [\\[16\\]](#page-13-10), dataflow\
    \ was modeled as a graph and solved through iterative optimization using Support\
    \ Vector Machines (SVM) and Neural Networks (NN). In [\\[17\\]](#page-13-11),\
    \ a machine learning-based method was introduced to predict halfperimeter wirelength\
    \ (HPWL) and routing congestion directly after macro placement.\n\nFor traditional\
    \ methods, macro placement has been viewed as an object-driven optimization problem.\
    \ In [\\[19\\]](#page-13-13), a priority-queue data structure was used to repeatedly\
    \ cluster the globally best pair of objects. [\\[20\\]](#page-13-14) proposed\
    \ a linear programming method, combining the grid-based method and the path-based\
    \ method, by considering the critical path of timing disturbances during the placement\
    \ optimization process. [\\[21\\]](#page-13-15) proposed a multi-packing tree\
    \ (MP-tree) representation to handle mixed-size designs, which can effectively\
    \ optimize macro locations and facilitate standard cell placement and routing.\
    \ In ePlace [\\[22\\]](#page-13-16), the Nesterov method was used as the nonlinear\
    \ solver, and the Lipschitz constant dynamically predicted the step size. While\
    \ RePlace [\\[23\\]](#page-13-17) mainly optimized density and presented a method\
    \ for adapting density penalties through improved dynamic step size adaptation.\
    \ [\\[24\\]](#page-13-18) proposed a new adaptive SA process that uses smoothing\
    \ techniques for cost evaluation to reduce the number of linear program solutions.\
    \ [\\[25\\]](#page-13-19), [\\[26\\]](#page-13-20) constructed circular-packing\
    \ trees (CP-trees) that can circularly position macros along the chip boundaries.\
    \ In [\\[27\\]](#page-13-21), a hierarchical approach was proposed to accelerate\
    \ turnaround time.\n\nDataflow has been one of the key features used by engineers\
    \ to optimize macro positioning. In [\\[16\\]](#page-13-10), the dataflow was\
    \ modeled as a graph and solved through SVM and NN iterative optimization. In\
    \ [\\[5\\]](#page-13-2), optimized data path layout was generated by utilizing\
    \ rule information directly extracted from dataflow graphs. [\\[7\\]](#page-13-26)\
    \ also leverage dataflow to address the problem of preplaced macros, which cannot\
    \ be handled by simulated annealing (SA). A recently released macro placer named\
    \ RTL-MP [\\[8\\]](#page-13-24) considered the macro-to-macro dataflow and proved\
    \ that such dataflow is helpful to get a human-quality layout. An updated version\
    \ of this placer was presented in [\\[10\\]](#page-13-25). These two method also\
    \ considerting macro flipping for further macro position optimization. Recently,\
    \ commercial tool vendors also started to pay attention to dataflow analysis and\
    \ equip some of their tools with such capability [\\[28\\]](#page-13-27)–[\\[30\\\
    ]](#page-13-28). However, among these dataflow-aware macro placers, only a few\
    \ have considered the impact of macro-to-cell or cell-to-cell dataflow on macro\
    \ placement. Many approaches have failed to effectively leverage this information\
    \ to constrain subsequent steps, resulting in little to no improvement—or even\
    \ a decline—in design quality. In [\\[11\\]](#page-13-3), a methodology was proposed\
    \ to extract various dataflow connections and incorporate this information into\
    \ macro placement using simulated annealing (SA). While this work demonstrated\
    \ the effectiveness of dataflow, it treated all macros equally without considering\
    \ differences in macro area and failed to account for macro orientation, which\
    \ is critical for QoR optimization.\n\n# *B. Design Principles and Contributions*\n\
    \nThe proposed work differs from the existing macro placers in the following design\
    \ principles. The proposed methodology involves four different types of connections\
    \ including cell connections and placement in the macro placement process and\
    \ defines a new dataflow-aware optimization target. The proposed dataflow extraction\
    \ methodology offers a good starting point for any macro placers or mixed-size\
    \ placers such as [\\[31\\]](#page-13-29). It complements the existing tools with\
    \ newly discovered \"hidden\" connections that are essential for the final QoR.\
    \ The key contributions from DAS-MP are summarized as follows.\n\n- Introducing\
    \ New Cell Dataflow Relationships: Unlike previous work that primarily focuses\
    \ on macro-to-macro dataflow, the proposed work recognizes that cells occupy substantial\
    \ area, and their placement significantly influences macro positioning. Therefore,\
    \ we propose to extract several new dataflow relationships that take into account\
    \ for cell positions. This intuition is illustrated in Fig. [1,](#page-0-0) where\
    \ routing resources are optimized by adjusting the location of *Macro D*, considering\
    \ macro-cell connections and the flow of data among these modules.\n- Expanding\
    \ Macro-Macro Dataflow: Beyond macro-cell relationships, we extend our approach\
    \ by introducing multi-hop connections, such as macro-cell-cell interactions.\
    \ Additionally, we incorporate indirect macro-macro relationships to capture more\
    \ complex dependencies between macros. This comprehensive dataflow model enhances\
    \ the optimization process during macro placement.\n- Introducing Two-step Incremental\
    \ Fine-tuning: Finetuning is essential to further refine the macro placement after\
    \ the initial optimization, addressing local inefficiencies that may not be captured\
    \ during global optimization. The fine-tuning process is divided into two key\
    \ stages.\n\t- First, we introduce a feedback mechanism to adjust the weights\
    \ of macro connections. This mechanism refines placement by accounting for the\
    \ effect of macro area on congestion. Larger macros, which tend to increase congestion,\
    \ are strategically placed to minimize their impact, while smaller macros are\
    \ positioned more flexibly to improve overall placement quality. This feedback\
    \ mechanism ensures that placement optimization considers both connectivity and\
    \ macro size, enhancing layout efficiency.\n- Second, we optimize the orientation\
    \ of macros to reduce wirelength and improve placement efficiency. Directional\
    \ flipping is considered for each macro, with the flipping order determined based\
    \ on topological relationships in the dataflow, focusing on minimizing congestion\
    \ and aligning macros with the dataflow direction.\n- Demonstrated Placement Outcome:\
    \ Extensive experiments on diverse benchmarks demonstrate significant improvements:\
    \ an average improvement of 7.9% in HPWL compared to a state-of-the-art dataflow-aware\
    \ macro placer RTL-MP [\\[8\\]](#page-13-24), with a maximum improvement of 12.1%.\
    \ Congestion overflow is reduced by 82.5% on average, with a runtime overhead\
    \ of less than 1.5% of the total macro placement time. As a result, these improvements\
    \ translate to approximately 36.97% and 59.44% improvement on WNS and TNS compared\
    \ to RTL-MP [\\[8\\]](#page-13-24).\n\n## III. DAS-MP METHODOLOGY OVERVIEW\n\n\
    <span id=\"page-3-0\"></span>The proposed DAS-MP methodology and fine-tuning process,\
    \ which incorporates the proposed dataflow connection extraction, is depicted\
    \ in Fig. [2,](#page-3-1) with the key steps explained below.\n\nThe first step\
    \ is threshold-limited hierarchical clustering, where the synthesized netlist\
    \ is fed into a clustering engine adapted from a recently proposed hierarchical\
    \ auto-clustering method in [\\[8\\]](#page-13-24). This engine converts the structural\
    \ netlist representation of the RTL design into a clustered netlist. Clusters\
    \ are split and merged when the number of macro or standard cells exceeds or falls\
    \ below a preset threshold. Bundled I/O pins are modeled as macro clusters in\
    \ this process. Clustering also reduces the search cost for later connection extraction\
    \ steps. Each cluster contains either macros or standard cells, and a cross-cluster\
    \ connection is defined as a one-hop connection.\n\nWith the generated macro or\
    \ cell clusters, we explore the dataflow extraction in three categories, as shown\
    \ in the top box of Fig. [2.](#page-3-1) The first category includes connections\
    \ among macro clusters, where most prior work focused on the one-hop direct connections\
    \ between macros. We enhance this connection extraction proces by also considering\
    \ potential indirect connections between macro clusters resulting from shared\
    \ connections with the same cell clusters. The second category of dataflow connections\
    \ captures dataflow between macro clusters and cell clusters, where logical connections\
    \ and their strengths are extracted and stored for later use in placement optimization.\
    \ The third category involves connections among cell clusters, which were typically\
    \ overlooked in previous macro placers but still indirectly affect macro placement\
    \ and are therefore incorporated into our model. The connections between macro\
    \ cluster-cell cluster and cell cluster-cell cluster are combined into a unified\
    \ macro cluster-cell cluster-cell cluster dataflow guidance to further improve\
    \ macro placement.\n\nWith the extracted dataflow, a fine-tuning step (denoted\
    \ as \"Finetuning Step 1\" in the figure) refines the dataflow by incorporating\
    \ the area characteristics of each macro. For a 2-hop\n\n![](_page_3_Figure_7.jpeg)\n\
    \n<span id=\"page-3-1\"></span>Fig. 2. The overview of the overall DAS-MP methodology.\
    \ The first part is the proposed enhanced dataflow extraction process where light\
    \ yellow rectangles outline all the newly proposed features in dataflow extraction.\
    \ The second and third parts are the proposed fine-tuning stages where light blue\
    \ rectangles list the macro specificity analysis and orientation optimization.\n\
    \ndataflow connection, represented as macro cluster-cell cluster-cell cluster,\
    \ we propose a weight feedback model. In this model, the weight of the second-hop\
    \ cell cluster is directly applied to the macro requiring optimization. Additionally,\
    \ a new feature representing macro area is introduced to account for the impact\
    \ of different macro sizes on the final placement.\n\nAfter the first fine-tuning\
    \ process, the enhanced dataflow awareness provides a comprehensive view of data\
    \ movement, considering macro uniqueness within the design. Since macro placement\
    \ depends heavily on cell clusters, we first conduct a global placement (GP) to\
    \ position the cell clusters. Macros are then placed by incorporating the extracted\
    \ dataflow information into a loss function, which is optimized using the Simulated\
    \ Annealing (SA) algorithm [\\[8\\]](#page-13-24), [\\[32\\]](#page-13-30) and\
    \ represented in the netlist using Sequence Pair [\\[33\\]](#page-13-31). Depending\
    \ on the proximity to the convergence point, GP and macro placement can be run\
    \ iteratively.\n\nOnce the dataflow-guided placement is complete, a second\n\n\
    <span id=\"page-4-1\"></span>![](_page_4_Figure_0.jpeg)\n\n(a) One-hop direct\
    \ dataflow connection of macro cluster-cell cluster.\n\n![](_page_4_Figure_2.jpeg)\n\
    \n<span id=\"page-4-2\"></span>(b) one-hop indirect dataflow connection from macro\
    \ cluster-macro cluster.\n\nFig. 3. One-hop direct dataflow connection of macro\
    \ cluster-cell cluster. The strength is given based on the bit width of the extracted\
    \ connections. Illustration of one-hop indirect dataflow connection from macro\
    \ cluster-macro cluster. The left figure treats the cell cluster as the smallest\
    \ unit, and the right figure treats the single cell instance as the smallest unit.\n\
    \nfine-tuning process (denoted as \"Finetuning Step 2\" in the figure) is performed\
    \ to refine macro orientation, which significantly impacts placement quality.\
    \ Our method's extracted dataflow enables the determination of both dataflow direction\
    \ and weight. This process involves three steps: first, weighted dataflow decomposition;\
    \ second, calculation of values along the axes; and third, orientation adjustment\
    \ for each macro. Once this fine-tuning is complete, the macro placement from\
    \ DAS-MP is finalized, allowing subsequent physical design stages to proceed.\n\
    \n# <span id=\"page-4-0\"></span>IV. DATAFLOW BETWEEN MACRO CLUSTERS AND CELL\
    \ CLUSTERS\n\nExisting macro placers primarily focus on direct connections among\
    \ macros. However, given the vast number of cell instances in the design, analyzing\
    \ the logical relationships between macro clusters and cell clusters is essential.\
    \ The macro cluster-cell cluster connections can reveal indirect and virtual links\
    \ between macros. This section details the connection extraction process.\n\n\
    # <span id=\"page-4-5\"></span>*A. One-Hop Direct Macro Cluster-Cell Cluster Dataflow*\n\
    \nThe dataflow from the netlist is converted into a graph, where the prior work\
    \ mostly modeled the macro cluster-macro cluster connections as undirected. As\
    \ can be seen in Fig. [3\\(a\\),](#page-4-1) in addition to the undirected macro\
    \ cluster-macro cluster connections, we also consider directionality when extracting\
    \ connections between macros and cells. This approach enables the identification\
    \ of more types of logical relationships. Although brute force search (BFS) is\
    \ used, we introduce pruning in the search process and optimize\n\n# Algorithm\
    \ 1 Indirect Macro Dataflow Connection Extraction Algorithm\n\n<span id=\"page-4-4\"\
    ></span>\n\n|     | Require: .lef, .lib, .v, constraints                   |\n\
    |-----|--------------------------------------------------------|\n|     | Ensure:\
    \ two types of indirect macro connections        |\n| 1:  | ▷ At cell cluster\
    \ level (Fig. 3(b) left)               |\n|     | 2: for each cell cluster (in\
    \ cell-macro connection) do |\n| 3:  | vector : cell connected macro vec     \
    \                 |\n| 4:  | num = cell connected macro vec.size             \
    \       |\n| 5:  | for i = 0 to num do                                    |\n\
    | 6:  | for j = i to num do                                    |\n| 7:  | macro\
    \ src = cell connected macro vec[i]                |\n| 8:  | macro sink = cell\
    \ connected macro vec[j]               |\n| 9:  | addconnection (macro src, macro\
    \ sink)                  |\n| 10: | end for                                  \
    \              |\n| 11: | end for                                            \
    \    |\n|     | 12: end for                                            |\n| 13:\
    \ | ▷ At cell instance level (Fig. 3(b) right)             |\n|     | 14: for\
    \ each vertex in cell fanin map do               |\n| 15: | vector : same vertex\
    \ macro vec                         |\n| 16: | for each pin (in vertex pin list)\
    \ do                   |\n| 17: | id = find the macro id where pin located   \
    \            |\n| 18: | if id is in a macro cluster then                     \
    \  |\n| 19: | same vertex macro vec.push back(id)                    |\n| 20:\
    \ | end if                                                 |\n| 21: | end for<br>▷\
    \ Find macros connected to the same cell    |\n| 22: | total = same vertex macro\
    \ vec.size                     |\n| 23: | for i = 0 to total − 1 do          \
    \                    |\n| 24: | for j = i+1 to total do                      \
    \          |\n| 25: | macro src = same vertex macro vec[i]                   |\n\
    | 26: | macro sink = same vertex macro vec[j]                  |\n| 27: | addconnection\
    \ (macro src, macro sink)                  |\n| 28: | end for                \
    \                                |\n| 29: | end for                          \
    \                      |\n|     | 30: end for                                \
    \            |\n|     | 31: return indirect macro connections                \
    \  |\n\ndata storage during graph building and traversal. The strength of connections\
    \ is defined based on the data bit-width. To incorporate the impact of the dataflow,\
    \ a loss function is defined in Equation [1.](#page-4-3) This loss function is\
    \ utilized to guide the subsequent solution search process, which is iterated\
    \ to optimize the wirelenghth as the final quality indicator. In Equation [1,](#page-4-3)\
    \ W L is half perimeter wire length (HPWL) and w<sup>x</sup> is a weight factor\
    \ which is defined by dataflow bit width between the clusters.\n\n<span id=\"\
    page-4-3\"></span>\n$$\\begin{aligned} w\\_0 &= bit\\\\_width(macro\\\\_cluster\\\
    _1, macro\\\\_cluster\\_2) \\\\ w\\_1 &= bit\\\\_width(macro\\\\_cluster, cell\\\
    \\_cluster) \\\\ loss &= w\\_0 \\* WL\\_{macro-macro} + w\\_1 \\* WL\\_{macro-cell}\
    \ \\end{aligned} \\quad (1)$$\n\n# *B. One-Hop Indirect Macro-Macro Dataflow*\n\
    \nFor certain designs, macro clusters may not have a direct connection among themselves,\
    \ but they may share connections with the same cell clusters. Such connections\
    \ can indicate indirect dataflow among macro clusters. Then these \"hidden\" relations\
    \ can be converted into virtual connections to guide macro placement. This type\
    \ of connection is challenging to find due to the scale of the search and the\
    \ complexity involved in excluding common signals that are shared among all macros,\
    \ such as clock and reset signals. In the proposed methodology, we extract these\
    \ virtual connections in a hierarchical way. This has been shown in Fig. [3\\\
    (b\\).](#page-4-2)\n\nFirstly, we treat the *cell cluster* as the smallest unit\
    \ and examine all macro connections to or from the same cell cluster\n\n![](_page_5_Figure_0.jpeg)\n\
    \nFig. 4. Illustration of newly established two-hop dataflow connection macro\
    \ cluster-cell cluster-cell cluster.\n\n(shown on the left part of Fig. [3\\(b\\\
    )\\)](#page-4-2). We view these macros as being virtually connected and convert\
    \ these connections into virtual dataflow among macro clusters. This process has\
    \ been shown in lines 2-12 of Algorithm [1.](#page-4-4) After traversing all the\
    \ clusters, we establish virtual connections between macros that share common\
    \ connections to the same cell cluster (shown Fig. [3\\(b\\)](#page-4-2) left).\
    \ Direct connections from a cell cluster to multiple macro clusters are then transformed\
    \ into indirect connections between macro clusters within the cluster-based graph.\
    \ In this graph, vertices represent the mapping nodes of fanout and pin, and the\
    \ addconnection function creates the required virtual connections.\n\nIn the second\
    \ part of the extraction process, we treat the *single standard cell* instance\
    \ as the smallest unit and check their macro fanouts. If a single cell drives\
    \ multiple macros, we view these macros as being virtually connected (Fig. [3\\\
    (b\\)](#page-4-2) right). Line 14-21 in Algorithm [1](#page-4-4) details the extraction\
    \ process of such connections. By far, we have supplemented the relationship among\
    \ macro clusters by considering all direct and indirect connections between macro\
    \ clusters. It is worth mentioning that in this example, we traverse all clusters\
    \ to find these virtual connections. However, one can also selectively extract\
    \ only cluster-level connections to reduce the search complexity.\n\nWe also define\
    \ the new loss function for the indirect macro cluster connection in Equation\
    \ [3,](#page-5-1) where w<sup>i</sup> represents the sum of dataflow bit width\
    \ between two macro clusters to cell cluster and W L denotes the HPWL.\n\n<span\
    \ id=\"page-5-7\"></span>\n$$\\begin{split} w\\_i &= bit\\\\_width(macro\\\\_cluster\\\
    _1, cell\\\\_cluster) \\\\ &+ bit\\\\_width(macro\\\\_cluster\\_2, cell\\\\_cluster)\
    \ \\end{split} \\quad (2)$$\n\nlossindirect macro = w<sup>i</sup> ∗ W Lindirect\
    \ macro−macro (3)\n\n## V. DATAFLOW AMONG CELL CLUSTERS\n\n<span id=\"page-5-0\"\
    ></span>The ultimate goal of this work is to establish macro cluster-cell cluster-cell\
    \ cluster dataflow to further analyze the impact of multi-hop dataflow connection.\
    \ This dataflow consists of two parts: the macro cluster-cell cluster connection\
    \ discussed in Section [IV-A](#page-4-5) and the cell cluster-cell cluster connection.\
    \ The size of cell clusters is also considered, as larger clusters typically have\
    \ a greater impact on the positioning of virtually connected macros. Therefore,\
    \ the connection between cell clusters is weighted\n\n![](_page_5_Figure_9.jpeg)\n\
    \n<span id=\"page-5-5\"></span>Fig. 5. Comparison between one-hop connected cell\
    \ cluster (only macro cluster-cell cluster) and two-hop connected cell cluster\
    \ (after considering macro cluster-cell cluster-cell cluster) in an example design\
    \ swerv\\_wrapper.\n\n# <span id=\"page-5-2\"></span>Algorithm 2 Macro Cluster-Cell\
    \ Cluster-Cell Cluster Connection Extraction Algorithm (Fig. [4\\)](#page-5-2)\n\
    \n<span id=\"page-5-4\"></span>\n\n| Require: .lef, .lib, .v, constraints    \
    \                                 |\n|--------------------------------------------------------------------------|\n\
    | Ensure: connections of macro cluster-cell cluster-cell cluster           |\n\
    | 1: map : macro cell connect                                              |\n\
    | 2: for each connection do                                                |\n\
    | 3:<br>if src is macro && sink is cell then<br>▷ First connect macro-cell |\n\
    | 4:<br>addconnection (macro src, cell sink)                               |\n\
    | 5:<br>macro cell connect[sink.id].f irst = 1                             |\n\
    | 6:<br>macro cell connect[sink.id].second = src id                        |\n\
    | 7:<br>end if                                                             |\n\
    | if src is cell && sink is cell then<br>▷ Then connect cell-cell<br>8:    |\n\
    | 9:<br>if macro cell connect[src id].first == 1 then                      |\n\
    | ▷ Checking whether this cell is connected to a macro<br>10:              |\n\
    | 11:<br>if src id != sink.first then                                      |\n\
    | 12:<br>macro id = macro cell connect[src id].second                      |\n\
    | 13:<br>addconnection (macro id, cell sink)                               |\n\
    | 14:<br>addconnection (cell src, cell sink)                               |\n\
    | 15:<br>end if                                                            |\n\
    | end if<br>16:                                                            |\n\
    | 17:<br>end if                                                            |\n\
    | 18: end for                                                              |\n\
    | 19: return macro cluster-cell cluster-cell cluster connections           |\n\
    |                                                                          |\n\
    \nusing the product of a constant k, bit width, area, and instance number, as\
    \ shown in Equation [4.](#page-5-3)\n\n<span id=\"page-5-3\"></span>\n$$w\\_j\
    \ = k \\* bit\\\\_width \\* cluster\\_{area} \\* cluster\\_{number} \\qquad (4)$$\n\
    \nAfter extracting the dataflow connections among cell clusters using Algorithm\
    \ [2,](#page-5-4) we convert them into two-hop connections between macro clusters\
    \ and cell clusters, as shown in Fig. [4.](#page-5-2) The conversion assigns higher\
    \ weights to two-hop virtual connections compared to one-hop direct connections.\
    \ In lines 3–7, we first label the macro clusters directly connected to the cell\
    \ cluster. After traversing the connections between cell clusters, we check for\
    \ any labeled macro clusters. Once identified, a macro cluster-cell cluster-cell\
    \ cluster dataflow connection is established. Fig. [5](#page-5-5) illustrates\
    \ the impact of a twohop connection using the swerv\\_wrapper design, showing\
    \ that green clusters with second-hop connections to macros occupy a much larger\
    \ area and will impact macro positioning. The final updated definition of the\
    \ loss function is shown in Equation [5,](#page-5-6)\n\n<span id=\"page-5-6\"\
    ></span><span id=\"page-5-1\"></span>\n$$loss = w\\_0 \\* WL\\_{m-m} + w\\_1 \\\
    * WL\\_{m-c} + w\\_2 \\* WL\\_{m-c-c} \\tag{5}$$\n\nwhere w0, w<sup>1</sup> and\
    \ w<sup>2</sup> are defined in Equation [2,](#page-5-7) Equation [1](#page-4-3)\
    \ and Equation [4,](#page-5-3) and W Lm−<sup>m</sup> refers to total HPWL of macro\
    \ cluster-macro cluster connections, W Lm−<sup>c</sup> means HPWL of macro cluster-cell\
    \ cluster connections, W Lm−c−<sup>c</sup> represents HPWL of the second-hop connection\
    \ from macro cluster-cell cluster-cell cluster.\n\n![](_page_6_Figure_0.jpeg)\n\
    \n(a) 2-hop backward-feedback weight update.\n\n![](_page_6_Figure_2.jpeg)\n\n\
    (b) 2-hop area-driven placement optimization.\n\nFig. 6. Fine-tuning for Macro\
    \ Specificity Analysis and Optimization.\n\n## <span id=\"page-6-0\"></span>VI.\
    \ FINE-TUNING PROCESSES BASED ON DATAFLOW\n\n## *A. Macro Specificity Analysis\
    \ and Optimization*\n\nSo far all macros were treated equally when extracting\
    \ dataflow relationships, without considering their individual characteristics,\
    \ especially area. Larger macros typically occupy more chip area and are more\
    \ prone to causing congestion or other placement violations. To address this,\
    \ we propose a feedback model that incorporates macro characteristics into the\
    \ loss function during the Simulated Annealing process, refining connectivity\
    \ and placement based on macro size. Specifically, the model adjusts the weights\
    \ of two-hop connections in a cascading manner, where the influence of second-hop\
    \ cell clusters is weighted differently based on their connection to source macros.\
    \ Additionally, macro area within the cluster is factored into the placement optimization,\
    \ as larger macros are more likely to impact floorplan congestion, while smaller\
    \ ones allow greater placement flexibility. The final placement is further refined\
    \ based on these weighted adjustments, producing an optimized floorplan. While\
    \ other features, such as the number of cell clusters, could also influence placement,\
    \ they are omitted here to maintain variable uniformity. The following sections\
    \ detail the methodology and implementation.\n\n*1) Macro Characteristic Backward-feedback\
    \ Model:* As discussed in Section [V,](#page-5-0) the 2-hop macro-cell-cell connectivity\
    \ relationship is reflected by w<sup>2</sup> ∗ W Lm−c−<sup>c</sup> in the loss\
    \ function in Equation [5.](#page-5-6) Here, only the strength of the connectivity\
    \ between cell clusters was considered. While this approach links macros to two-hop\
    \ cell clusters, it only captures the connection strength between one-hop and\
    \ two-hop cell clusters without distinguishing the unique influence of different\
    \ source macros. As shown in Fig. [6\\(](#page-6-1)a), the connection strength\
    \ between the B1 cell cluster and the two macros (M1 and M2) remains identical,\
    \ despite the differences in the source macros' characteristics. Since the goal\
    \ is to improve macro placement, it is essential to account for the varying connectivity\
    \ strength between the same two-hop cell cluster and different macros. To address\
    \ this, we propose coupling the weights of one-hop macro-cell clusters with the\
    \ weights of two-hop cell-cell clusters. This allows the feedback from the source\
    \ macro's characteristics to propagate through the connectivity network, creating\
    \ differentiated connection strengths for different source macros, as shown in\
    \ Equation [6.](#page-6-2) This coupling reflects both the local connection strength\
    \ and the global impact of macro characteristics, enhancing the accuracy of the\
    \ placement optimization process.\n\n<span id=\"page-6-2\"></span>\n$$loss\\_{m-c-c}\
    \ = w\\_1 \\* w\\_2 \\* WL\\_{m-c-c} \\tag{6}$$\n\n<span id=\"page-6-1\"></span>*2)\
    \ Feature-driven Placement Optimization:* Beyond multihop connection strength,\
    \ macro area significantly impacts floorplan congestion and overall placement\
    \ quality. Based on back-end design experience, larger macros should be placed\
    \ near the chip boundary to reserve more space for cell clusters, while smaller\
    \ macros should be positioned closer to cell clusters to enhance placement performance.\
    \ To integrate this consideration, macro sizes are incorporated into the final\
    \ loss function. As illustrated in Fig. [6\\(](#page-6-1)b), the larger macro\
    \ cluster M1 is strategically positioned at the chip boundary, while the smaller\
    \ macro cluster M2 is placed closer to the cell cluster, influenced by the cell\
    \ cluster's pull. This area-based placement strategy ensures optimal utilization\
    \ of available space and reduces routing congestion, thereby improving overall\
    \ design efficiency.\n\n*3) New Loss Function for Optimizing the Macro Placement:*\
    \ Based on the fine-tuning processes described in the prior sections, we introduce\
    \ a refined loss function that incorporates both the connection weights of 2-hop\
    \ macro clustercell cluster-cell cluster connections and macro area into the simulated\
    \ annealing process. In Equation [7,](#page-6-3) the macro area Ai is normalized\
    \ to the interval [1,2] based on the maximum and minimum macro areas in the design,\
    \ yielding A′ i . The loss function for the 2-hop connection is then updated by\
    \ adjusting the weights and incorporating the area constraints, as shown in Equation\
    \ [8.](#page-6-4)\n\n<span id=\"page-6-4\"></span><span id=\"page-6-3\"></span>\n\
    $$A'\\_i = 1 + \\frac{(A\\_i - A\\_{\\min})}{A\\_{\\max} - A\\_{\\min}} \\tag{7}$$\n\
    \n$$loss\\_{m-c-c} = \\frac{\\sqrt{w\\_1 \\* w\\_2}}{A\\_i'} \\* WL\\_{m-c-c}\
    \ \\tag{8}$$\n\nIn Equation [7,](#page-6-3) the macro area A<sup>i</sup> is normalized\
    \ to the interval [1,2] based on the maximum and minimum macro areas in the design,\
    \ resulting in A′ i . The loss function for the 2-hop connection is then updated\
    \ by incorporating the revised weights and area constraints, as shown in Equation\
    \ [8.](#page-6-4) By leveraging the final SA loss function, we obtain SAbased\
    \ macro placement results, which serve as the foundation for further macro orientation\
    \ optimization through flipping.\n\n# *B. Macro Orientation Optimization*\n\n\
    Most automatic floorplanning tools focus on macro placement, often neglecting\
    \ macro flipping, which can effectively reduce wirelength and improve timing and\
    \ other design metrics [\\[34\\]](#page-13-32). In advanced technology nodes,\
    \ macro orientation is typically restricted to 0 and 180 degrees to prevent layer\
    \ misalignment. If rotation is necessary, it is limited to flipping along the\
    \ x and y axes, effectively capping adjustments at 180 degrees. We define four\
    \ flipping modes: no flipping (N), xaxis flipping (FN), y-axis flipping (FS),\
    \ and both x and y-axis flipping (S).\n\nIn the fine-tuning process, we begin\
    \ from a selected point based on the dataflow connection to the I/O pins. The\
    \ flipping order of subsequent macros is determined by their topological relationships\
    \ within the dataflow while respecting dataflowbased order constraints. To model\
    \ the macro flipping problem accurately, we perform dataflow vectorization and\
    \ decomposition, using superimposed vectors to guide the flipping direction.\n\
    \nSince dataflow inherently encodes the connection information of a design through\
    \ its directional and weighted nature, making it well-suited for vector-based\
    \ representation. By mapping dataflow into a vector space (V<sup>T</sup> ), we\
    \ identify three distinct vector types relevant to macro flipping:\n\n- Vmm vectors\
    \ between macros\n- Vmc vectors from a macro to cell clusters\n- Vmcc vectors\
    \ from a macro to cell clusters at a further hop.\n\nAs discussed in Section [V,](#page-5-0)\
    \ we limit the analysis to two hops from macros, as experiments show that standard\
    \ cells beyond this range have minimal influence on macro positioning. We then\
    \ analyze the vector projections along the x- and ydimensions, denoted as xV<sup>T</sup>\
    \ and yV<sup>T</sup> , respectively.\n\n*1) Dataflow Decomposition of* Vmm*:*\
    \ A key step in decomposing vectorized dataflow is identifying the vector's starting\
    \ and ending points. In the proposed method, the origin for decomposition coordinates\
    \ is set at the midpoint of all pin positions on the target macro. By focusing\
    \ on pin positions, the macro flipping problem is effectively reduced to accurately\
    \ positioning these pins within each macro. In many designs, physical connections\
    \ between macros are rarely strictly horizontal or vertical and often involve\
    \ multiple connections, necessitating dataflow decomposition, as shown in Fig.\
    \ [7\\(](#page-7-0)a). During decomposition, vertical orientation guides upward\
    \ or downward flipping, while horizontal orientation guides left or right flipping.\
    \ The vector direction between macros is determined by the position of the target\
    \ macro's pin relative to the connected macro's pin, with its magnitude calculated\
    \ as the product of the dataflow exchange's bitwidth and the physical distance\
    \ between pin centers (see Equation [9,](#page-7-1) where m2 is the center macro).\
    \ For macros with multiple out-degree connections, the additive property of vectors\
    \ allows the overlay of decomposed vectors along the same axis, enabling the calculation\
    \ of dataflow magnitude in all directions relative to the coordinate axes, as\
    \ defined in Equation [10,](#page-7-2) where n is the number of connections for\
    \ the macro of interest.\n\n<span id=\"page-7-1\"></span>\n$$V\\_{mm} = bitwidth\
    \ \\ast \\left( (x\\_{m\\_1}, y\\_{m\\_1}) - (x\\_{m\\_2}, y\\_{m\\_2}) \\right)\
    \ \\qquad (9)$$\n\n<span id=\"page-7-2\"></span>\n$$\\begin{aligned} x\\{V\\_{mm}\\\
    } &= x\\{V\\_{mm\\_1}\\} + x\\{V\\_{mm\\_2}\\} + \\dots + x\\{V\\_{mm\\_n}\\}\\\
    \\ y\\{V\\_{mm}\\} &= y\\{V\\_{mm\\_1}\\} + y\\{V\\_{mm\\_2}\\} + \\dots + y\\\
    {V\\_{mm\\_n}\\} \\end{aligned} \\quad (10)$$\n\n<span id=\"page-7-5\"></span>![](_page_7_Figure_10.jpeg)\n\
    \n(a) Macro-Macro dataflow vector (Vmm) decomposition.\n\n![](_page_7_Figure_12.jpeg)\n\
    \n<span id=\"page-7-6\"></span><span id=\"page-7-0\"></span>(b) Macro-Cell (Vmc)\
    \ and Macro-Cell-Cell (Vmcc) dataflow vector decomposition.\n\nFig. 7. Macro orientation\
    \ optimization guided by enhanced dataflow awareness.\n\n*2) Dataflow Decomposition\
    \ of* Vmc *and* Vmcc*:* The approach to decomposing dataflow between a macro and\
    \ cells is similar to what has been used for macro-to-macro dataflow. After obtaining\
    \ the cell cluster, we estimate its geometric center using Equation [11,](#page-7-3)\
    \ as detailed in line 14 of Algorithm [3,](#page-8-1) where c is the number of\
    \ cell instances in a given standard cell cluster, and x<sup>i</sup> and y<sup>i</sup>\
    \ are the coordinates of each cell after initial global placement. The dataflow\
    \ connection between the target macro and the cell cluster is determined by the\
    \ midpoint of the macro's pin and the cell cluster's geometric center, with its\
    \ weight defined by the dataflow bitwidth. Similar to Vmm, we project the Vmc\
    \ dataflow vectors onto the x- and y-axes, superimposing these projections as\
    \ shown in Fig. [7\\(](#page-7-0)b) and Equation [12.](#page-7-4) For scenarios\
    \ where a macro is indirectly connected to standard cell clusters through other\
    \ clusters (multihop connections), we first compute each cluster's geometric center\
    \ and then determine virtual centers based on these for the multi-hop connection.\
    \ The Vmcc vector projection is carried out in the same way, with the results\
    \ superimposed.\n\n<span id=\"page-7-3\"></span>\n$$x\\_{geo\\text{\\\\_center}}\
    \ = \\frac{1}{c} \\* \\sum\\_{i=1}^{c} x\\_i ; \\ y\\_{geo\\text{\\\\_center}}\
    \ = \\frac{1}{c} \\* \\sum\\_{i=1}^{c} y\\_i \\qquad (11)$$\n\n<span id=\"page-7-4\"\
    ></span>\n$$\\begin{aligned} x\\{V\\_{mc}\\} &= x\\{V\\_{mc\\_1}\\} + x\\{V\\\
    _{mc\\_2}\\} + \\dots + x\\{V\\_{mc\\_n}\\} \\\\ y\\{V\\_{mc}\\} &= y\\{V\\_{mc\\\
    _1}\\} + y\\{V\\_{mc\\_2}\\} + \\dots + y\\{V\\_{mc\\_n}\\} \\end{aligned} \\\
    quad (12)$$\n\n*3) Making Flipping Decisions:* Using the dataflow decomposition\
    \ methods, we obtain three distinct dataflow vectors: Vmm, Vmc, and Vmcc. The\
    \ Equation [13](#page-8-2) represents the weighted\n\nAlgorithm 3 Dataflow decomposition-based\
    \ macro flipping algorithm (using flipping on x-axis as an example, y-axis is\
    \ the same)\n\n<span id=\"page-8-1\"></span>\n\n| Input: macros ← pre-placed macros\
    \                                     |\n|-----------------------------------------------------------------------|\n\
    | Output: macros ← macros flipped by dataflow decomposition             |\n| 1:\
    \ for each macro as center macro do                                  |\n| 2:<br>center\
    \ macro x = pin center x of center macro                   |\n| 3:<br>total f\
    \ orce x = 0                                              |\n| ▷ Macro to Macro\
    \ out degree (Fig. 7(a))<br>4:                         |\n| 5:<br>for each connected\
    \ macro do                                     |\n| macro x = pin center x of\
    \ connected macro<br>6:                       |\n| sub f orce = w1 ∗ (macro x\
    \ − center macro x)<br>7:                    |\n| 8:<br>total f orce x+ = sub\
    \ f orce                                    |\n| 9:<br>end for               \
    \                                          |\n| 10:<br>▷ Macro to Cell Cluster\
    \ out degree (Fig. 7(b) left)            |\n| for each connected cell cluster\
    \ do<br>11:                             |\n| cell x = cluster geometric center<br>12:\
    \                              |\n| sub f orce = w2 ∗ (cell x − center macro x)<br>13:\
    \                    |\n| 14:<br>total f orce x+ = sub f orce                \
    \                   |\n| 15:<br>end for                                      \
    \                  |\n| 16:<br>▷ Macro to Multi-hop Cell Cluster out degree (Fig.\
    \ 7(b) right) |\n| 17:<br>for each connected two-hop cell cluster do         \
    \            |\n| 18:<br>cell 1 x = cluster 1 geometric center               \
    \           |\n| cell 2 x = cluster 2 geometric center<br>19:                \
    \          |\n| cell center x = 0.5 ∗ (cell 1 x + cell 2 x)<br>20:           \
    \         |\n| sub f orce = w3 ∗ (cell center x − center macro x)<br>21:     \
    \        |\n| 22:<br>total f orce x+ = sub f orce                            \
    \       |\n| 23:<br>end for                                                  \
    \      |\n| 24: end for                                                      \
    \     |\n| 25: return Selected Macros for Flipping                           \
    \    |\n\nsums of these vectors' projections along the x axis and y axis, forming\
    \ the projections of V<sup>T</sup> on these axes:\n\n<span id=\"page-8-2\"></span>\n\
    $$\\begin{aligned} x\\{V\\_T\\} &= \\alpha \\ast x \\{V\\_{mm}\\} + \\beta \\\
    ast x \\{V\\_{mc}\\} + \\gamma \\ast x \\{V\\_{mcc}\\} \\\\ y\\{V\\_T\\} &= \\\
    alpha \\ast y \\{V\\_{mm}\\} + \\beta \\ast y \\{V\\_{mc}\\} + \\gamma \\ast y\
    \ \\{V\\_{mcc}\\} \\end{aligned} \\tag{13}$$\n\nThe weights α, β, and γ are hyperparameters\
    \ that can be adjusted based on the strength of connectivity. In our evaluations,\
    \ we determined their values based on multiple trials across various design scales,\
    \ and set α = 0.55, β = 0.3 and γ = 0.15. With this hyperparameters setting, design\
    \ delivers a good optimization performance. In designs with only macro-cell connections,\
    \ Vmc is particularly critical. While Vmcc includes Vmc cases, direct connections\
    \ between cell clusters have a more significant impact on macro layout than indirect\
    \ 2-hop connections, as observed in real physical design [\\[11\\]](#page-13-3).\
    \ Therefore, both Vmc and Vmcc are considered in our calculations.\n\nThe flipping\
    \ decisions for the macro are made by comparing the magnitudes of x{V<sup>T</sup>\
    \ } which guide the left and right flip and y{V<sup>T</sup> } which guide the\
    \ up and down flip. The direction with the larger value indicates a stronger influence\
    \ of the dataflow in that direction, similar to principal component analysis in\
    \ linear algebra. The macro is then flipped accordingly.\n\n## VII. EVALUATION\
    \ RESULTS\n\n# <span id=\"page-8-0\"></span>*A. Experiment Setup*\n\nEvaluation\
    \ Flow. DAS-MP is implemented in C++ and is compatible with the database used\
    \ in the latest OpenROAD release [\\[35\\]](#page-13-33), [\\[36\\]](#page-13-34).\
    \ To evaluate its capabilities, we tested DAS-MP through the full design flow,\
    \ including placement and routing. The experiments were conducted on an Intel\
    \ Core i7- 11700 CPU with 128GB of memory. The netlist was generated using the\
    \ Yosys synthesis tool [\\[37\\]](#page-13-35). We compare the DAS-MP whole-process\
    \ approach against Triton Macro Placer (TMP) [\\[38\\]](#page-13-36), the default\
    \ macro placer in OpenROAD, RTL-MP [\\[8\\]](#page-13-24), a recently released\
    \ dataflow-aware macro placer, and Hier-RTLMP [\\[10\\]](#page-13-25), a state-of-the-art\
    \ macro placer in OpenROAD. To better understand the impact of each optimization\
    \ stage, we evaluate two versions of DAS-MP:\n\n- DAS-MP (DE): Incorporates only\
    \ the dataflow connection extraction methods [\\[11\\]](#page-13-3) between macros\
    \ and standard cells.\n- DAS-MP (DE+FT): Includes both dataflow connection extraction\
    \ and fine-tuning optimizations, such as macrospecificity and macro orientation\
    \ adjustments.\n\nComparison Metrics. To evaluate macro placement and finetuning\
    \ quality, we primarily use HPWL, a widely accepted metric in placement and floorplanning\
    \ research, referenced in recent macro placers [\\[1\\]](#page-13-0), [\\[2\\\
    ]](#page-13-4), [\\[8\\]](#page-13-24), [\\[10\\]](#page-13-25). Since HPWL alone\
    \ does not fully reflect the final QoR [\\[39\\]](#page-13-37), we also assess\
    \ postrouting PPA metrics, including WNS, TNS, power, and area. Runtime is recorded\
    \ to evaluate the framework's efficiency.\n\nBenchmark Selection. We select the\
    \ benchmarks based on the following criteria. Firstly, they need to be representative\
    \ and widely accepted in the community. Secondly, they should exhibit diverse\
    \ behaviors related to macros. For example, some designs should be macro-dominant,\
    \ and some should have more connections among macros and cell clusters. Last but\
    \ not least, they need to be new enough to reflect the current design scale and\
    \ complexity. Based on the above principles, we run extensive experiments and\
    \ end up picking seven benchmark designs for the evaluation of the proposed methodology.\
    \ They are selected based on commonly used design cases in recently published\
    \ literature or newly released test suites for evaluating macro placement [\\\
    [36\\]](#page-13-34), [\\[40\\]](#page-13-38). We test the designs in NanGate45\
    \ [\\[41\\]](#page-13-39) technology node, a popularly used open-source standard\
    \ cell libraries in 45nm.\n\n# *B. Dataflow Connection Relationship Analysis*\n\
    \nTable [II](#page-9-0) summarizes the benchmark characteristics and the number\
    \ of unique dataflow connections extracted using DAS-MP. TinyRocket is a small\
    \ design with only two macros of the same type and a limited number of standard\
    \ cells, but it exhibits a high number of cell cluster-cell cluster connections.\
    \ bp\\_multi has a macro count comparable to black parrot, but their dataflow\
    \ patterns differ significantly. bp\\_multi has substantially fewer macro cluster-cell\
    \ cluster and cell cluster-macro cluster connections than black parrot, with an\
    \ even greater disparity in cell cluster-cell cluster connections. Ariane133,\
    \ selected from the macro placement test suite [\\[40\\]](#page-13-38), represents\
    \ a macroheavy design but features only one unique macro type, resulting in fewer\
    \ unique connections. For instance, its number of macro cluster-cell cluster connections\
    \ is lower than that of black parrot. This analysis enables more design-specific\
    \ macro placement, which is particularly valuable when implementing a new design\
    \ without prior experience.\n\n# *C. HPWL Result Analysis*\n\nTable [III](#page-9-1)\
    \ presents HPWL results for TMP [\\[35\\]](#page-13-33), RTL-MP [\\[8\\]](#page-13-24),\
    \ Hier-RTLMP [\\[10\\]](#page-13-25), DAS-MP (DE) with dataflow extraction\n\n\
    TABLE II BENCHMARK INFORMATION AND EXTRACTED DETAILED CONNECTIONS FOR EACH BENCHMARK\
    \ DESIGN\n\n<span id=\"page-9-0\"></span>\n\n| Design Name   | Std Cells<br>Count\
    \ | Macros<br>Count | Macro<br>Type 1 | # of<br>IOs | Total Cluster<br>Count |\
    \ Macro Cluster<br>Macro Cluster * | Macro Cluster<br>Cell Cluster * 2 | Cell\
    \ Cluster<br>Macro Cluster * 2 | Cell Cluster<br>Cell Cluster * | Macro Cluster<br>Cell\
    \ Cluster-Cell Cluster * |\n|---------------|--------------------|-----------------|-----------------|-------------|------------------------|----------------------------------|-----------------------------------|-----------------------------------|--------------------------------|----------------------------------------------|\n\
    | TinyRocket    | 27217              | 2               | 1               | 269\
    \         | 65                     | 0                                | 2    \
    \                             | 35                                | 829      \
    \                      | 67                                           |\n| bp_be\
    \         | 59882              | 10              | 3               | 3029    \
    \    | 139                    | 25                               | 120       \
    \                        | 364                               | 1815          \
    \                 | 251                                          |\n| bp_fe  \
    \       | 29993              | 11              | 3               | 2511      \
    \  | 82                     | 24                               | 132         \
    \                      | 260                               | 406             \
    \               | 196                                          |\n| black parrot\
    \  | 427501             | 24              | 5               | 1198        | 763\
    \                    | 2                                | 565                \
    \               | 1095                              | 14649                  \
    \        | 3375                                         |\n| bp_multi      | 209086\
    \             | 26              | 6               | 1453        | 432        \
    \            | 2                                | 351                        \
    \       | 828                               | 4962                           |\
    \ 1427                                         |\n| swerv_wrapper | 99750    \
    \          | 28              | 3               | 1416        | 518           \
    \         | 0                                | 232                           \
    \    | 741                               | 15300                          | 594\
    \                                          |\n| ariane133     | 165953       \
    \      | 133             | 1               | 495         | 314               \
    \     | 44                               | 380                               |\
    \ 1947                              | 9963                           | 1708  \
    \                                       |\n\n\\* # of unique connections.\n\n\
    <sup>1</sup> This includes macros with different sizes and functionalities.\n\n\
    <span id=\"page-9-1\"></span><sup>2</sup> Bi-directional connections.\n\nTABLE\
    \ III EXPERIMENTAL RESULTS\n\n| Design Name      | TMP [35] |                \
    \        | RTL-MP [8] (baseline) |                        | Hier-RTLMP [10] |\
    \                        | DAS-MP (DE) [11] (Proposed) |                     \
    \   |                   |                        | DAS-MP (DE+FT) (Proposed) |\
    \                        |\n|------------------|----------|------------------------|-----------------------|------------------------|-----------------|------------------------|-----------------------------|------------------------|-------------------|------------------------|---------------------------|------------------------|\n\
    |                  | HPWL (m) | Congestion<br>Overflow | HPWL (m)            \
    \  | Congestion<br>Overflow | HPWL (m)        | Congestion<br>Overflow | M-C<br>HPWL\
    \ (m)             | Congestion<br>Overflow | M-C-C<br>HPWL (m) | Congestion<br>Overflow\
    \ | HPWL (m)                  | Congestion<br>Overflow |\n| TinyRocket       |\
    \ 830.03   | 73                     | 803.80                | 74             \
    \        | 786.75          | 0                      | 758.98                 \
    \     | 26                     | 752.80            | 19                     |\
    \ 744.29                    | 16                     |\n| bp_be            | 4151.71\
    \  | 66553                  | 4445.60               | 40537                  |\
    \ 4798.22         | 3738                   | 4218.92                     | 2224\
    \                   | 4113.33           | 454                    | 4003.06   \
    \                | 433                    |\n| bp_fe            | 2772.04  | 3373\
    \                   | 2789.94               | 4445                   | 3130.68\
    \         | 0                      | 2664.61                     | 3165      \
    \             | 2519.63           | 3156                   | 2463.79         \
    \          | 1530                   |\n| black parrot     | 12966.52 | 142   \
    \                 | 12930.57              | 981                    | 13430.91\
    \        | 1570                   | 12600.53                    | 272        \
    \            | 11820.00          | 130                    | 11632.06         \
    \         | 115                    |\n| bp_multi         | 7496.04  | 2854   \
    \                | 7252.76               | 3965                   | 7630.34  \
    \       | 62699                  | 7235.15                     | 1328        \
    \           | 7146.48           | 1470                   | 6999.51           \
    \        | 173                    |\n| swerv_wrapper    | 5199.62  | 3384    \
    \               | 4745.86               | 2533                   | 5400.98   \
    \      | 633                    | 4718.80                     | 2299         \
    \          | 4648.02           | 887                    | 4366.34            \
    \       | 848                    |\n| ariane133        | 8775.61  | 3421     \
    \              | 8857.12               | 23876                  | 9525.40    \
    \     | 183261                 | 8737.35                     | 3842          \
    \         | 8624.48           | 3919                   | 8378.74             \
    \      | 3707                   |\n| Avg. Improvement | 0.9% ↓   | 4.4% ↓    \
    \             | 0%                    | 0%                     | 6.9% ↓      \
    \    | 26.3% ↓                | 2.8%↑                       | 62.9% ↑        \
    \        | 5.5% ↑            | 73.4% ↑                | 7.9% ↑               \
    \     | 82.5% ↑                |\n\n<span id=\"page-9-3\"></span><span id=\"page-9-2\"\
    ></span>![](_page_9_Figure_7.jpeg)\n\nFig. 8. Layouts of black parrot with congestion\
    \ map. The highlighted green cells are from the one-hop connected cell cluster.\n\
    \nonly, and DAS-MP (DE+FT) with full optimization. The best result for each design\
    \ is highlighted in bold, with percentage improvements over RTL-MP provided, as\
    \ RTL-MP generally outperforms TMP and Hier-RTLMP.\n\n*1) HPWL Improvement from\
    \ Dataflow-aware Placement (w/o Fine-tuning):* The proposed DAS-MP (DE) method\
    \ outperforms RTL-MP across all designs, achieving HPWL improvements ranging from\
    \ 0.2% to 9.7%. Using black parrot as an example, as illustrated in Fig. [8\\\
    (a\\)](#page-9-2) and Fig. [8\\(b\\),](#page-9-3) DAS-MP (DE) achieves lower HPWL\
    \ by placing cells and macros closer together. Notably, the previous macro placer\
    \ [\\[8\\]](#page-13-24) failed to recognize certain critical cell cluster connections\
    \ near the pin access region, leading to increased wire length. Our method corrects\
    \ this by accurately identifying these connections and adjusting macro placement\
    \ accordingly. While blocking pin access may seem counterintuitive, our experiments\
    \ show that it can sometimes enhance placement quality. In simpler designs with\
    \ fewer macros, macro placement is generally more straightforward, but overlooking\
    \ macro-cell connections can still result in unnecessary design iterations. Additionally,\
    \ automatic macro placers can be misled by poorly defined constraints, a challenge\
    \ DAS-MP (DE) overcomes with its dataflow-guided approach.\n\n*2) HPWL Improvement\
    \ from Dataflow-aware Placement (w/ Fine-tuning):* The right two columns of Table\
    \ [III](#page-9-1) present the HPWL and congestion overflow results of DAS-MP\n\
    \n![](_page_9_Figure_12.jpeg)\n\n<span id=\"page-9-4\"></span>Fig. 9. The proposed\
    \ method placement result of TinyRocket with \"push boundary\" and \"not push\
    \ boundary\" actions.\n\n(DE+FT), which incorporates fine-tuning on top of the\
    \ dataflow extraction-only strategy. The results demonstrate that fine-tuning\
    \ further improves HPWL, achieving an average reduction of 7.9% compared to RTL-MP\
    \ across all designs, while also significantly reducing congestion overflow by\
    \ 82.5% on average. Notably, all designs outperform the dataflow-only approach\
    \ (DAS-MP (DE)). The improvement is particularly evident in designs such as bp\\\
    _fe and swerv\\_wrapper, which achieve HPWL reductions of 2.8% and 6.1%, respectively.\
    \ In bp\\_be, fine-tuning reduces HPWL by approximately 9% compared to RTL-MP,\
    \ optimizing wirelength by bringing cells and macros closer together while minimizing\
    \ unnecessary routing. Similarly, swerv\\_wrapper shows significant gains in both\
    \ HPWL and congestion overflow, confirming that fine-tuning substantially enhances\
    \ placement quality, especially in designs with more complex connectivity.\n\n\
    *3) The Impact of \"Push Boundary\" Action:* A common practice among physical\
    \ designers is to push macros closer to the boundary to create more space for\
    \ standard cells. This approach assumes that standard cells are typically placed\
    \ after macro locations are fully or partially fixed. However, DAS-MP reveals\
    \ that this \"rule\" is not always optimal. Stronger dataflow connections can\
    \ increase wirelength if macros are\n\n![](_page_10_Figure_1.jpeg)\n\n![](_page_10_Figure_2.jpeg)\n\
    \nFig. 11. Congestion maps of different macro placers for swerv\\_wrapper.\n\n\
    pushed to the edge, far from the connected standard cells. Therefore, DAS-MP relaxes\
    \ this constraint and observes the resulting placement quality. An example is\
    \ shown in Fig. [9](#page-9-4) using TinyRocket, the simplest design in the suite,\
    \ which is cell-dominant and has a large number of cell cluster-cell cluster connections.\
    \ The table summarizes the HPWL differences when enabling or disabling the pushing-boundary\
    \ constraint. When considering only one-hop connections, \"pushing boundary\"\
    \ results in better HPWL due to fewer cell clusters involved. However, for two-hop\
    \ connections, \"not pushing boundary\" achieves better HPWL, leading to an overall\
    \ 6.3% improvement over RTL-MP. This demonstrates that DAS-MP can uncover optimization\
    \ opportunities that may be overlooked by conventional design practices.\n\n##\
    \ *D. Congestion Overflow Result Analysis*\n\nBy considering \"hidden\" connections,\
    \ congestion conditions can be significantly improved. This improvement is reflected\
    \ in the reduced congestion overflow reported in Table [III.](#page-9-1) The proposed\
    \ DAS-MP (DE+FT) achieves an average congestion overflow reduction of 82.5%, outperforming\
    \ all other compared placers. This is due to the placement of macros and their\
    \ associated cell clusters in closer proximity, which helps avoid long and zigzag\
    \ wires. Additionally, macros are adjusted and flipped to their optimal orientations,\
    \ further reducing wirelength and improving routing efficiency. As a result, more\
    \ routing resources become available, leading to overall better placement quality.\n\
    \nSince bp\\_fe and swerv\\_wrapper demonstrated relatively significant optimization,\
    \ we take them as examples. Table [III](#page-9-1) shows that DAS-MP (DE+FT) not\
    \ only achieves superior HPWL results but also excels in other quality metrics.\
    \ This is further illustrated in Fig. [10](#page-10-0) and Fig. [11,](#page-10-1)\
    \ where the placement results and congestion maps are presented. It is evident\
    \ that DAS-MP (DE+FT) effectively places cells closer to their associated macros,\
    \ leveraging extracted connections to guide <span id=\"page-10-1\"></span><span\
    \ id=\"page-10-0\"></span>the placement of both types of instances. The macro\
    \ placement achieved by DAS-MP (DE+FT) closely aligns with manual efforts, where\
    \ macros belonging to the same hierarchy are typically placed in clusters and\
    \ oriented towards the dataflow, leading to improved legalization. The congestion\
    \ map in Fig. [11](#page-10-1) further highlights the advantage of DAS-MP, as\
    \ it produces a layout with fewer congestion hotspots compared to the other methods.\
    \ Additionally, DAS-MP improves resource utilization, leaving more empty space\
    \ available for subsequent optimizations in detailed placement, routing, and other\
    \ stages of the design flow.\n\n## *E. Post-routing PPA Result Analysis*\n\nThe\
    \ optimization of HPWL can have a direct impact on timing. In most cases, reducing\
    \ wirelength leads to shorter signal propagation delays, which in turn helps to\
    \ improve timing metrics such as Total Negative Slack (TNS) and Worst Negative\
    \ Slack (WNS). Table [IV](#page-11-0) summarizes the PPA results after routing\
    \ with various placement strategies. Since TNS and WNS are conventionally expressed\
    \ as negative values, calculations are based on their actual (negative) values.\
    \ A positive \"improvement\" value indicates an increase in TNS or WNS, signifying\
    \ a degradation in performance. Conversely, a negative \"improvement\" value denotes\
    \ a reduction in TNS or WNS, indicating an enhancement in timing performance.\n\
    \nTable [IV](#page-11-0) clearly demonstrates a significant increase in both WNS\
    \ and TNS for TMP and Hier-RTLMP compared to RTL-MP, indicating that their timing\
    \ issues have worsened. On the other hand, WNS and TNS significantly decrease\
    \ (in absolute terms) for DAS-MP (DE) with the 1-hop and 2-hop methods, as well\
    \ as for DAS-MP (DE+FT). The proposed DAS-MP (DE+FT) method achieves an average\
    \ WNS improvement of 36.97%, while TNS improves by an average of 59.44%. When\
    \ comparing the HPWL data, it becomes evident that the substantial reduction in\
    \ HPWL achieved by the proposed methods aligns logically with the significant\
    \ improvements observed in\n\nTABLE IV POST-ROUTING PPA RESULTS\n\n<span id=\"\
    page-11-0\"></span>\n\n| Design        | Flow                                \
    \               | WNS (ns)         | TNS (ns)              | Power (mw)     |\
    \ Area (µm2<br>)   |\n|---------------|----------------------------------------------------|------------------|-----------------------|----------------|------------------|\n\
    | TinyRocket    | TMP [35]                                           | -0.270\
    \           | -88.695               | 74             | 59528            |\n| \
    \              | RTL-MP [8]                                         | -0.259 \
    \          | -83.636               | 73             | 59538            |\n|  \
    \             | Hier-RTLMP [10]                                    | -0.230  \
    \         | -69.035               | 191            | 59510            |\n|   \
    \            | DAS-MP (DE) (M-C) [11]                             | -0.236   \
    \        | -73.763               | 74             | 59536            |\n|    \
    \           | DAS-MP (DE) (M-C-C) [11]                           | -0.228    \
    \       | -68.877               | 73             | 59539            |\n|     \
    \          | DAS-MP (DE+FT)                                     | -0.220     \
    \      | -68.533               | 73             | 59535            |\n|      \
    \         | TMP [35]                                           | -1.081      \
    \     | -1238.863             | 419            | 269138           |\n|       \
    \        | RTL-MP [8]                                         | -0.830       \
    \    | -1005.879             | 414            | 269204           |\n| bp_be  \
    \       | Hier-RTLMP [10]                                    | -1.139        \
    \   | -1237.849             | 410            | 268252           |\n|         \
    \      | DAS-MP (DE) (M-C) [11]                             | -0.638         \
    \  | -667.922              | 423            | 269116           |\n|          \
    \     | DAS-MP (DE) (M-C-C) [11]                           | -0.618          \
    \ | -467.990              | 411            | 268987           |\n|           \
    \    | DAS-MP (DE+FT)                                     | -0.610           |\
    \ -313.790              | 389            | 268357           |\n|             \
    \  | TMP [35]                                           | -0.533           | -27.161\
    \               | 275            | 222466           |\n|               | RTL-MP\
    \ [8]                                         | -0.702           | -37.492   \
    \            | 264            | 221949           |\n| bp_fe         | Hier-RTLMP\
    \ [10]                                    | -3.388           | -9802.706     \
    \        | 1010           | 212604           |\n|               | DAS-MP (DE)\
    \ (M-C) [11]                             | -0.717           | -42.103        \
    \       | 259            | 221769           |\n|               | DAS-MP (DE) (M-C-C)\
    \ [11]                           | -0.531           | -22.327               |\
    \ 259            | 221774           |\n|               | DAS-MP (DE+FT)      \
    \                               | -0.487           | -17.855               | 270\
    \            | 222251           |\n|               | TMP [35]                \
    \                           | -0.108           | -0.108                | 168 \
    \           | 877591           |\n|               | RTL-MP [8]               \
    \                          | -0.147           | -0.147                | 169  \
    \          | 877590           |\n| black parrot  | Hier-RTLMP [10]           \
    \                         | -0.119           | -0.119                | 206   \
    \         | 877248           |\n|               | DAS-MP (DE) (M-C) [11]     \
    \                        | -0.128           | -0.128                | 157    \
    \        | 877563           |\n|               | DAS-MP (DE) (M-C-C) [11]    \
    \                       | -0.095           | -0.095                | 154     \
    \       | 877477           |\n|               | DAS-MP (DE+FT)               \
    \                      | -0.110           | -0.110                | 169      \
    \      | 877803           |\n|               | TMP [35]                      \
    \                     | -0.223           | -77.322               | 218       \
    \     | 614172           |\n|               | RTL-MP [8]                     \
    \                    | -0.25            | -121.213              | 218        \
    \    | 614173           |\n| bp_multi      | Hier-RTLMP [10]                 \
    \                   | -0.261           | -105.773              | 254         \
    \   | 613882           |\n|               | DAS-MP (DE) (M-C) [11]           \
    \                  | -0.241           | -83.441               | 214          \
    \  | 615344           |\n|               | DAS-MP (DE) (M-C-C) [11]          \
    \                 | -0.214           | -75.588               | 207           \
    \ | 615342           |\n|               | DAS-MP (DE+FT)                     \
    \                | -0.21            | -0.21                 | 218            |\
    \ 614539           |\n| swerv_wrapper | TMP [35]                             \
    \              | -1.098           | -1441.25              | 26600          | 511702\
    \           |\n|               | RTL-MP [8]                                  \
    \       | -1.831           | -3063.155             | 26600          | 511649 \
    \          |\n|               | Hier-RTLMP [10]                              \
    \      | -1.563           | -2563.767             | 26600          | 511694  \
    \         |\n|               | DAS-MP (DE) (M-C) [11]<br>DAS-MP (DE) (M-C-C) [11]\
    \ | -0.958           | -2035.965             | 27600          | 511798       \
    \    |\n|               | DAS-MP (DE+FT)                                     |\
    \ -0.853<br>-0.798 | -1793.158<br>-1403.92 | 26600<br>26600 | 511691<br>511673\
    \ |\n|               | TMP [35]                                           | -0.199\
    \           | -16.54                | 207            | 739424           |\n| \
    \              | RTL-MP [8]                                         | -0.109 \
    \          | -2.26                 | 187            | 739303           |\n| Ariana133\
    \     | Hier-RTLMP [10]                                    | -0.056          \
    \ | -0.859                | 422            | 738914           |\n|           \
    \    | DAS-MP (DE) (M-C) [11]                             | -0.076           |\
    \ -1.364                | 186            | 739451           |\n|             \
    \  | DAS-MP (DE) (M-C-C) [11]                           | -0.014           | -0.063\
    \                | 186            | 739408           |\n|               | DAS-MP\
    \ (DE+FT)                                     | -0.012           | -0.053    \
    \            | 224            | 739307           |\n|               | TMP [35]\
    \                                           | 2.23%↓           | 73.97%↓     \
    \          | 2.37% ↓        | 0.03% ↓          |\n| Avg. Improv.* | RTL-MP [8]\
    \                                         | 0.00%            | 0.00%         \
    \        | 0.00%          | 0.00%            |\n|               | Hier-RTLMP [10]\
    \                                    | 47.25%↓          | 3705.95%↓          \
    \   | 85.56%↓        | 0.68% ↑          |\n|               | DAS-MP (DE) (M-C)\
    \ [11]                             | 17.77% ↑         | 21.48%↑              \
    \ | 0.66% ↑        | 0.017% ↓         |\n|               | DAS-MP (DE) (M-C-C)\
    \ [11]                           | 36.03% ↑         | 46.18% ↑              |\
    \ 2.48% ↑        | 0.006% ↓         |\n|               | DAS-MP (DE+FT)      \
    \                               | 36.97% ↑         | 59.44% ↑              | 2.24%\
    \ ↓        | 1.3%↓            |\n\n\\* The TNS and WNS numbers are always negative,\
    \ and the closer they are to 0, the more effective the placement. In this table,\
    \ improvements are derived using actual data, which includes negative numbers.\
    \ Therefore, negative improvement values of TNS and WNS indicate better performance\
    \ of the method.\n\nTNS and WNS, reinforcing the correlation between wirelength\
    \ and timing optimization.\n\nSince the primary focus of this work is HPWL optimization,\
    \ extensive changes for area and power results have been avoided. Nonetheless,\
    \ experimental results indicate that the approach does not negatively impact area\
    \ and power metrics. The increase in area is minimal, amounting to only 1.3%,\
    \ with a corresponding power increase of 2.24%.\n\nThe analysis of the PPA experimental\
    \ results reveals that the proposed method demonstrates stronger performance in\
    \ terms of TNS and WNS while avoiding significant negative effects on power and\
    \ area.\n\n## *F. Optimization Runtime Analysis*\n\nThe runtime of the proposed\
    \ methodology is compared against RTL-MP [\\[8\\]](#page-13-24), with a detailed\
    \ breakdown summarized in Table [V.](#page-12-1) It can be observed that DAS-MP\
    \ (DE), which includes the macro cluster-cell cluster and macro cluster-cell cluster-cell\
    \ cluster connections, results in an average runtime increase of 2.13× and 2.91×,\
    \ respectively. The full methodology, DAS-MP (DE+FT), incorporating optimized\
    \ macro cluster-cell cluster-cell cluster extraction and macro flipping, leads\
    \ to an average runtime increase of 2.83× and 3.21×, respectively. As shown in\
    \ Fig. [12\\(a\\),](#page-11-1) despite the increased runtime in the extraction\
    \ and fine-tuning phase, the incurred overhead\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_8.jpeg)\n\
    \n(a) Runtime occupation ratio on the swerv\\_wrapper (left) and ariane133 (right)\
    \ test cases.\n\n![](_page_11_Figure_10.jpeg)\n\n<span id=\"page-11-3\"></span><span\
    \ id=\"page-11-2\"></span>Fig. 12. Runtime breakdown comparison between RTL-MP\
    \ [\\[8\\]](#page-13-24), DAS-MP (DE) [\\[11\\]](#page-13-3) and DAS-MP (DE+FT)\
    \ for designs swerv\\_wrapper (left) and ariane133 (right). The time spent by\
    \ each component is normalized to the total runtime of DAS-MP (DE) [\\[11\\]](#page-13-3).\n\
    \nremains less than 1.5% of the total runtime of the entire macro placement process.\
    \ Notably, the extraction and fine-tuning runtime scales sub-linearly with the\
    \ number of connections, providing significant design quality gains with only\
    \ a marginal runtime penalty.\n\nFor the macro cluster-cell cluster-cell cluster\
    \ extraction within DAS-MP (DE+FT), runtime optimization reduces the overhead\
    \ to 2.83× due to the introduction of a feedback mechanism. This mechanism not\
    \ only enhances PPA but also improves runtime efficiency. The acceleration is\
    \ attributed to the mechanism's ability to differentiate macros based on area\
    \ considerations, reducing the overall weight of 2 hop connections and thereby\
    \ optimizing runtime. Additionally, as illustrated in Fig. [12\\(b\\)](#page-11-2)\
    \ and Fig. [12\\(c\\),](#page-11-3) we analyzed the runtime of the macro flipping\
    \ process and found that it accounts for only 10.2% and 16.0% of the extraction\
    \ step. This demonstrates that the flipping action achieves significant PPA optimization\
    \ with minimal computational cost.\n\n## *G. Ablation Study for Fine-tuning Processes*\n\
    \nTo further analyze the impact of different fine-tuning strategies in our macro\
    \ placement optimization, we conduct an ablation study, summarized in Table [VI,](#page-12-2)\
    \ by evaluating two key components: area-based fine-tuning and orientation-based\
    \ fine-tuning. The table presents quantitative results comparing the baseline\
    \ RTL-MP, DAS-MP (DE), and the proposed DAS-MP (DE+FT) approaches.\n\nFor area-based\
    \ fine-tuning, our strategy effectively improves congestion and timing metrics.\
    \ On average, congestion overflow is reduced by 1.43%, with a maximum improvement\
    \ of 17.23% compared to DAS-MP (DE) [\\[11\\]](#page-13-3). Timing performance\
    \ also benefits, achieving an average improvement of 0.58% in WNS and 10.33% in\
    \ TNS, demonstrating the effectiveness of incorporating macro area awareness in\
    \ placement refinement.\n\nTABLE V RUNTIME ANALYSIS\n\n<span id=\"page-12-1\"\
    ></span>\n\n| Design Name         | RTL-MP [8]    |                          \
    \    | DAS-MP (DE) [11] Runtime (s)             | DAS-MP (DE+FT) Runtime (s) |\
    \                        |  |\n|---------------------|---------------|------------------------------|------------------------------------------|----------------------------|------------------------|--|\n\
    |                     | Runtime (s) 1 | Macro Cluster-Cell Cluster 2 | Macro Cluster-Cell\
    \ Cluster-Cell Cluster3 | Optimized M-C-C Extraction | Flipping with Dataflow\
    \ |  |\n| TinyRocket          | 1.48          | 6.59                         |\
    \ 9.93 (+3.33)                             | 9.91 (+3.32)               | 10.22\
    \ (+0.31)          |  |\n| bp_be               | 6.53          | 17.04       \
    \                 | 23.78 (+6.75)                            | 23.66 (+6.62) \
    \             | 25.64 (+1.98)          |  |\n| bp_fe               | 1.85    \
    \      | 4.49                         | 6.11 (+1.63)                         \
    \    | 6.13 (+1.64)               | 8.28 (+2.15)           |  |\n| black parrot\
    \        | 326.10        | 421.98                       | 490.42 (+68.44)    \
    \                      | 481.65 (+59.67)            | 493.63 (+11.94)        |\
    \  |\n| bp_multi            | 56.29         | 101.50                       | 131.52\
    \ (+30.02)                          | 131.37 (+29.88)            | 139.57 (+8.20)\
    \         |  |\n| swerv_wrapper       | 20.24         | 68.14                \
    \        | 100.40 (+32.27)                          | 100.38 (+32.24)        \
    \    | 110.59 (+10.22)        |  |\n| ariane133           | 55.48         | 377.69\
    \                       | 603.81 (+226.12)                         | 576.06 (+198.37)\
    \           | 673.02 (+96.96)        |  |\n| Average Runtime (s) | 66.99 (1.00×)\
    \ | 142.49 (2.13×)               | 195.28 (2.91×)                           |\
    \ 189.88 (2.83×)             | 208.71 (3.12×)         |  |\n\n<sup>1</sup> Extracting\
    \ macro cluster-macro cluster connections only within RTL-MP [\\[8\\]](#page-13-24).\n\
    \n2 Includes extracting connections among macro clusters.\n\n<span id=\"page-12-2\"\
    ></span>3 Includes extracting previous two types of connections in 1 and 2.\n\n\
    TABLE VI ABLATION STUDY RESULTS FOR FINE-TUNING PROCESSES\n\n| Design        \
    \   | Type                                     | HPWL     | Congestion<br>Overflow\
    \ | WNS (ns) | TNS (ns) | Power (mW) | Area (µm2<br>) |\n|------------------|------------------------------------------|----------|------------------------|----------|----------|------------|----------------|\n\
    | TinyRocket       | RTL-MP [8]                               | 803.80   | 74\
    \                     | -0.26    | -83.64   | 73.4       | 59538          |\n\
    |                  | DAS-MP (DE) [11]                         | 752.80   | 19\
    \                     | -0.23    | -68.88   | 73.2       | 59539          |\n\
    |                  | DAS-MP (DE+FT) (Area Fine-tuning)        | 744.63   | 16\
    \                     | -0.22    | -68.53   | 73.2       | 59535          |\n\
    |                  | DAS-MP (DE+FT) (Orientation Fine-tuning) | 746.62   | 16\
    \                     | -0.22    | -68.53   | 73.2       | 59535          |\n\
    | bp_be            | RTL-MP                                   | 4445.60  | 40537\
    \                  | -0.83    | -1005.88 | 414        | 269204         |\n|  \
    \                | DAS-MP (DE) [11]                         | 4113.33  | 454 \
    \                   | -0.62    | -467.99  | 411        | 268987         |\n| \
    \                 | DAS-MP (DE+FT) (Area Fine-tuning)        | 4073.92  | 447\
    \                    | -0.61    | -377.34  | 389        | 268990         |\n|\
    \                  | DAS-MP (DE+FT) (Orientation Fine-tuning) | 4006.38  | 432\
    \                    | -0.61    | -318.23  | 389        | 268390         |\n|\
    \ bp_fe            | RTL-MP                                   | 2789.94  | 4445\
    \                   | -0.70    | -37.49   | 264        | 221949         |\n| \
    \                 | DAS-MP (DE) [11]                         | 2519.63  | 3156\
    \                   | -0.53    | -22.33   | 259        | 221774         |\n| \
    \                 | DAS-MP (DE+FT) (Area Fine-tuning)        | 2470.9   | 1742\
    \                   | -0.52    | -18.95   | 270        | 222250         |\n| \
    \                 | DAS-MP (DE+FT) (Orientation Fine-tuning) | 2468.93  | 1543\
    \                   | -0.49    | -18.69   | 270        | 222250         |\n| black\
    \ parrot     | RTL-MP                                   | 12930.57 | 981     \
    \               | -0.14    | -0.15    | 169        | 877590         |\n|     \
    \             | DAS-MP (DE) [11]                         | 11820.00 | 130    \
    \                | -0.10    | -0.11    | 154        | 877477         |\n|    \
    \              | DAS-MP (DE+FT) (Area Fine-tuning)        | 11673.80 | 142   \
    \                 | -0.10    | -0.11    | 162        | 844700         |\n|   \
    \               | DAS-MP (DE+FT) (Orientation Fine-tuning) | 11632.06 | 112  \
    \                  | -0.11    | -0.11    | 169        | 877830         |\n| bp_multi\
    \         | RTL-MP                                   | 7252.76  | 3965       \
    \            | -0.25    | -121.21  | 218        | 614173         |\n|        \
    \          | DAS-MP (DE) [11]                         | 7146.48  | 1470      \
    \             | -0.21    | -75.59   | 207        | 615342         |\n|       \
    \           | DAS-MP (DE+FT) (Area Fine-tuning)        | 7048.47  | 963      \
    \              | -0.21    | -0.21    | 221        | 614818         |\n|      \
    \            | DAS-MP (DE+FT) (Orientation Fine-tuning) | 7024.28  | 846     \
    \               | -0.21    | -0.21    | 218        | 614231         |\n| swerv_wrapper\
    \    | RTL-MP                                   | 4745.86  | 2533            \
    \       | -1.83    | -3063.16 | 26600      | 511649         |\n|             \
    \     | DAS-MP (DE) [11]                         | 4648.02  | 887            \
    \        | -0.85    | -1793.16 | 26600      | 511691         |\n|            \
    \      | DAS-MP (DE+FT) (Area Fine-tuning)        | 4570.32  | 1053          \
    \         | -0.85    | -1762.82 | 26600      | 511670         |\n|           \
    \       | DAS-MP (DE+FT) (Orientation Fine-tuning) | 4501.14  | 997          \
    \          | -0.82    | -1410.25 | 26600      | 511670         |\n| ariana133\
    \        | RTL-MP                                   | 8857.12  | 23876       \
    \           | -0.109   | -2.26    | 187        | 739303         |\n|         \
    \         | DAS-MP (DE) [11]                         | 8624.48  | 3919       \
    \            | -0.014   | -0.06    | 186        | 739408         |\n|        \
    \          | DAS-MP (DE+FT) (Area Fine-tuning)        | 8475.60  | 3778      \
    \             | -0.014   | -0.06    | 222        | 739309         |\n|       \
    \           | DAS-MP (DE+FT) (Orientation Fine-tuning) | 8404.55  | 3678     \
    \              | -0.012   | -0.05    | 222        | 739310         |\n| Avg. Improv.\
    \ (%) | RTL-MP (Baseline)                        | 0.00%    | 0.00%          \
    \        | 0.00%    | 0.00%    | 0.00%      | 0.00%          |\n|            \
    \      | DAS-MP (DE) [11]                         | 5.46%    | 71.49%        \
    \         | 36.03%   | 46.18%   | 2.48%      | -0.01%         |\n|           \
    \       | DAS-MP (DE+FT) (Area Fine-tuning)        | 6.81%    | 77.42%       \
    \          | 36.61%   | 56.38%   | -1.70%     | 0.51%          |\n|          \
    \        | DAS-MP (DE+FT) (Orientation Fine-tuning) | 7.28%    | 79.30%      \
    \           | 38.14%   | 59.03%   | -2.10%     | 0.01%          |\n\nOrientation-based\
    \ fine-tuning enhances macro placement by leveraging orientation adjustments.\
    \ As shown in Table [VI,](#page-12-2) compared to DAS-MP (DE) [\\[11\\]](#page-13-3),\
    \ congestion overflow improves by 2.06% on average, with a peak improvement of\
    \ 24.29%, highlighting the effectiveness of macro orientation refinement. Timing\
    \ metrics also show notable gains, with WNS and TNS improving by 2.38% and 17.40%,\
    \ respectively. Compared to area-based fine-tuning, orientation-based finetuning\
    \ delivers more consistent improvements, though a slight degradation of 3.06%\
    \ is observed in some cases.\n\nThese results indicate that both fine-tuning strategies\
    \ independently contribute to optimization. Their complementary roles underscore\
    \ the necessity of incorporating both techniques to enhance different aspects\
    \ of macro placement.\n\n## VIII. CONCLUSIONS\n\n<span id=\"page-12-0\"></span>In\
    \ this paper, we identified the importance of dataflow analysis in macro placement\
    \ and proposed DAS-MP, a methodology centered around dataflow extraction and adoption.\
    \ The methodology first extracts detailed dataflow connections among macro clusters\
    \ and between macro and cell clusters. These extracted connections are then incorporated\
    \ into the loss function to guide subsequent macro placement steps. Building upon\
    \ this foundation, two fine-tuning steps are applied. The first adjusts weights\
    \ by considering the impact of macro area, aiming to optimize congestion based\
    \ on engineering practices. Additionally, we recognize the importance of orientation\
    \ and introduce dataflow-aware approaches to fine-tune macro orientations. Through\
    \ a diverse set of benchmarks, we demonstrate that the proposed methodology outperforms\
    \ the recently released academic dataflow-aware macro placer in terms of HPWL\
    \ and congestion reduction. These improvements are further reflected in significant\
    \ enhancements in timing results. Moreover, the methodology operates efficiently,\
    \ with an acceptable overhead that trades computational cost for substantial quality\
    \ improvements. This work highlights the necessity of detailed and comprehensive\
    \ dataflow analysis in developing automatic macro placers. It provides an opportunity\
    \ for co-optimizing macro and standard cell placement, fostering further advancements\
    \ in placement strategies.\n\n## REFERENCES\n\n- <span id=\"page-13-0\"></span>[1]\
    \ A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y. J.\
    \ Lee, E. Johnson, O. Pathak, A. Nazi, J. Pak, A. Tong, K. Srinivasa, W. Hang,\
    \ E. Tuncer, Q. V. Le, J. Laudon, R. Ho, R. Carpenter, and J. Dean, \"A graph\
    \ placement methodology for fast chip design,\" *Nature*, vol. 594, no. 7862,\
    \ pp. 207–212, 2021. [Online]. Available: <http://dx.doi.org/10.1038/s41586-021-03544-w>\n\
    - <span id=\"page-13-4\"></span>[2] Y. Liu, Z. Ju, Z. Li, M. Dong, H. Zhou, J.\
    \ Wang, F. Yang, X. Zeng, and L. Shang, \"Floorplanning with graph attention,\"\
    \ in *Proceedings of the 59th ACM/IEEE Design Automation Conference*, ser. DAC\
    \ '22. Association for Computing Machinery, 2022, p. 1303–1308.\n- <span id=\"\
    page-13-7\"></span>[3] A. Agnesina, P. Rajvanshi, T. Yang, G. Pradipta, A. Jiao,\
    \ B. Keller, B. Khailany, and H. Ren, \"Autodmp: Automated dreamplace-based macro\
    \ placement,\" in *Proceedings of the 2023 International Symposium on Physical\
    \ Design*, 2023, pp. 149–157.\n- <span id=\"page-13-1\"></span>[4] C.-K. Cheng,\
    \ A. B. Kahng, S. Kundu, Y. Wang, and Z. Wang, \"Assessment of reinforcement learning\
    \ for macro placement,\" *arXiv preprint arXiv:2302.11014*, 2023.\n- <span id=\"\
    page-13-2\"></span>[5] T. T. Ye and G. De Micheli, \"Data path placement with\
    \ regularity,\" *IEEE/ACM International Conference on Computer-Aided Design, Digest\
    \ of Technical Papers, ICCAD*, vol. 2000-Janua, pp. 264–270, 2000.\n- <span id=\"\
    page-13-22\"></span>[6] A. Vidal-Obiols, J. Cortadella, J. Petit, M. Galceran-Oms,\
    \ and F. Martorell, \"RTL-Aware Dataflow-Driven Macro Placement,\" *Proceedings\
    \ of the 2019 Design, Automation and Test in Europe Conference and Exhibition,\
    \ DATE 2019*, pp. 186–191, 2019.\n- <span id=\"page-13-26\"></span>[7] J.-M. Lin,\
    \ Y.-L. Deng, Y.-C. Yang, J.-J. Chen, and P.-C. Lu, \"Dataflowaware macro placement\
    \ based on simulated evolution algorithm for mixed-size designs,\" *IEEE Transactions\
    \ on Very Large Scale Integration (VLSI) Systems*, vol. 29, no. 5, pp. 973–984,\
    \ 2021.\n- <span id=\"page-13-24\"></span>[8] A. B. Kahng, R. Varadarajan, and\
    \ Z. Wang, \"Rtl-mp: toward practical, human-quality chip planning and macro placement,\"\
    \ in *Proceedings of the 2022 International Symposium on Physical Design*, 2022,\
    \ pp. 3–11.\n- <span id=\"page-13-23\"></span>[9] A. Vidal-Obiols, J. Cortadella,\
    \ J. Petit, M. Galceran-Oms, and F. Martorell, \"Multilevel dataflow-driven macro\
    \ placement guided by rtl structure and analytical methods,\" *IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems*, vol. 40, no. 12,\
    \ pp. 2542– 2555, 2021.\n- <span id=\"page-13-25\"></span>[10] A. B. Kahng, R.\
    \ Varadarajan, and Z. Wang, \"Hier-rtlmp: A hierarchical automatic macro placer\
    \ for large-scale complex ip blocks,\" 2023.\n- <span id=\"page-13-3\"></span>[11]\
    \ X. Zhao, T. Wang, R. Jiao, and X. Guo, \"Standard cells do matter: Uncovering\
    \ hidden connections for high-quality macro placement,\" in *2024 Design, Automation\
    \ and Test in Europe Conference (DATE)*, 2024, pp. 1–6.\n- <span id=\"page-13-5\"\
    ></span>[12] R. Cheng and J. Yan, \"On Joint Learning for Solving Placement and\
    \ Routing in Chip Design,\" *arXiv*, vol. 1, no. NeurIPS, pp. 1–12, 2021. [Online].\
    \ Available: <http://arxiv.org/abs/2111.00234>\n- <span id=\"page-13-6\"></span>[13]\
    \ D. Vashisht, H. Rampal, H. Liao, Y. Lu, D. Shanbhag, E. Fallon, and L. B. Kara,\
    \ \"Placement in Integrated Circuits using Cyclic Reinforcement Learning and Simulated\
    \ Annealing,\" *arXiv*, no. NeurIPS, pp. 1–8, 2020. [Online]. Available: <http://arxiv.org/abs/2011.07577>\n\
    - <span id=\"page-13-8\"></span>[14] Y. Lin, Z. Jiang, J. Gu, W. Li, S. Dhar,\
    \ H. Ren, B. Khailany, and D. Z. Pan, \"DREAMPlace: Deep Learning Toolkit-Enabled\
    \ GPU Acceleration for Modern VLSI Placement,\" *IEEE Transactions on Computer-Aided\
    \ Design of Integrated Circuits and Systems*, vol. 40, no. 4, pp. 748–761, 2021.\n\
    - <span id=\"page-13-9\"></span>[15] C. Oh, R. Bondesan, D. Kianfar, R. Ahmed,\
    \ R. Khurana, P. Agarwal, R. Lepert, M. Sriram, and M. Welling, \"Bayesian optimization\
    \ for macro placement,\" *arXiv preprint arXiv:2207.08398*, 2022.\n- <span id=\"\
    page-13-10\"></span>[16] S. Ward, D. Ding, and D. Z. Pan, \"PADE: A high-performance\
    \ placer with automatic datapath extraction and evaluation through high dimensional\
    \ data learning,\" *Proceedings - Design Automation Conference*, pp. 756–761,\
    \ 2012.\n- <span id=\"page-13-11\"></span>[17] W. K. Cheng, Y. Y. Guo, and C.\
    \ S. Wu, \"Evaluation of routabilitydriven macro placement with machine-learning\
    \ technique,\" *Proceedings - 2018 7th International Symposium on Next-Generation\
    \ Electronics, ISNE 2018*, vol. 1, no. Isne, pp. 1–3, 2018.\n- <span id=\"page-13-12\"\
    ></span>[18] S. Liu, Q. Sun, P. Liao, Y. Lin, and B. Yu, \"Global Placement with\
    \ Deep Learning-Enabled Explicit Routability Optimization,\" *Proceedings -Design,\
    \ Automation and Test in Europe, DATE*, vol. 2021-Febru, pp. 1821–1824, 2021.\n\
    - <span id=\"page-13-13\"></span>[19] C. Alpert, A. Kahng, G.-J. Nam, S. Reda,\
    \ and P. Villarrubia, \"A semipersistent clustering technique for vlsi circuit\
    \ placement,\" in *Proceedings of the 2005 international symposium on Physical\
    \ design*, 2005, pp. 200– 207.\n- <span id=\"page-13-14\"></span>[20] T. Luo,\
    \ D. Newmark, and D. Z. Pan, \"A new lp based incremental timing driven placement\
    \ for high performance designs,\" in *Proceedings of the 43rd annual Design Automation\
    \ Conference*, 2006, pp. 1115–1120.\n- <span id=\"page-13-15\"></span>[21] T.-C.\
    \ Chen, P.-H. Yuh, Y.-W. Chang, F.-J. Huang, and D. Liu, \"Mp-trees: A packing-based\
    \ macro placement algorithm for mixed-size designs,\" in *Proceedings of the 44th\
    \ annual Design Automation Conference*, 2007, pp. 447–452.\n- <span id=\"page-13-16\"\
    ></span>[22] J. Lu, P. Chen, C.-C. Chang, L. Sha, D. J.-H. Huang, C.-C. Teng,\
    \ and C.-K. Cheng, \"eplace: Electrostatics based placement using nesterov's method,\"\
    \ in *2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)*, 2014, pp. 1–6.\n\
    - <span id=\"page-13-17\"></span>[23] C. K. Cheng, A. B. Kahng, I. Kang, and L.\
    \ Wang, \"RePlAce: Advancing Solution Quality and Routability Validation in Global\
    \ Placement,\" *IEEE Transactions on Computer-Aided Design of Integrated Circuits\
    \ and Systems*, vol. 38, no. 9, pp. 1717–1730, 2019.\n- <span id=\"page-13-18\"\
    ></span>[24] H. C. Chen, Y. L. Chuang, Y. W. Chang, and Y. C. Chang, \"Constraint\
    \ graph-based macro placement for modern mixed-size circuit designs,\" *IEEE/ACM\
    \ International Conference on Computer-Aided Design, Digest of Technical Papers,\
    \ ICCAD*, pp. 218–223, 2008.\n- <span id=\"page-13-19\"></span>[25] Y.-F. Chen,\
    \ C.-C. Huang, C.-H. Chiou, Y.-W. Chang, and C.-J. Wang, \"Routability-driven\
    \ blockage-aware macro placement,\" in *Proceedings of the 51st Annual Design\
    \ Automation Conference*, 2014, pp. 1–6.\n- <span id=\"page-13-20\"></span>[26]\
    \ C.-H. Chiou, C.-H. Chang, S.-T. Chen, and Y.-W. Chang, \"Circularcontour-based\
    \ obstacle-aware macro placement,\" in *2016 21st Asia and South Pacific Design\
    \ Automation Conference (ASP-DAC)*. IEEE, 2016, pp. 172–177.\n- <span id=\"page-13-21\"\
    ></span>[27] M.-K. Hsu, Y.-F. Chen, C.-C. Huang, T.-C. Chen, and Y.-W. Chang,\
    \ \"Routability-driven placement for hierarchical mixed-size circuit designs,\"\
    \ in *Proceedings of the 50th Annual Design Automation Conference*, 2013, pp.\
    \ 1–6.\n- <span id=\"page-13-27\"></span>[28] Synopsys, \"Maxplace (aquired by\
    \ synopsys),\" [https://www.](https://www.synopsys.com/implementation-and-signoff/physical-implementation/fusion-compiler.html)\
    \ [synopsys.com/implementation-and-signoff/physical-implementation/](https://www.synopsys.com/implementation-and-signoff/physical-implementation/fusion-compiler.html)\
    \ [fusion-compiler.html.](https://www.synopsys.com/implementation-and-signoff/physical-implementation/fusion-compiler.html)\n\
    - [29] ——, \"Freeform macro placement technology,\" [https://www.synopsys.](https://www.synopsys.com/implementation-and-signoff/physical-implementation/ic-compiler.html)\
    \ [com/implementation-and-signoff/physical-implementation/ic-compiler.](https://www.synopsys.com/implementation-and-signoff/physical-implementation/ic-compiler.html)\
    \ [html.](https://www.synopsys.com/implementation-and-signoff/physical-implementation/ic-compiler.html)\n\
    - <span id=\"page-13-28\"></span>[30] Cadence, \"Innovus mixed placer,\" [https://community.cadence.com/](https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/innovus-mixed-placer)\
    \ cadence blogs [8/b/breakfast-bytes/posts/innovus-mixed-placer.](https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/innovus-mixed-placer)\n\
    - <span id=\"page-13-29\"></span>[31] J. Lu, H. Zhuang, P. Chen, H. Chang, C.-C.\
    \ Chang, Y.-C. Wong, L. Sha, D. Huang, Y. Luo, C.-C. Teng, and C.-K. Cheng, \"\
    eplace-ms: Electrostatics-based placement for mixed-size circuits,\" *IEEE Transactions\
    \ on Computer-Aided Design of Integrated Circuits and Systems*, vol. 34, no. 5,\
    \ pp. 685–698, 2015.\n- <span id=\"page-13-30\"></span>[32] S. Kirkpatrick *et\
    \ al.*, \"Optimization by simulated annealing,\" *Science*, vol. 220, no. 4598,\
    \ pp. 671–680, 1983.\n- <span id=\"page-13-31\"></span>[33] H. Murata *et al.*,\
    \ \"Vlsi module placement based on rectangle-packing by the sequence-pair,\" *IEEE\
    \ Transactions on Computer-Aided Design of Integrated Circuits and Systems*, vol.\
    \ 15, no. 12, pp. 1518–1524, 1996.\n- <span id=\"page-13-32\"></span>[34] X. Zhao,\
    \ J. Chen, Z. Li, Y. Cai, and G. Xinfei, \"Incredflip: Incremental dataflow-driven\
    \ macro flipping for efficient macro placement refinement,\" in *2025 3rd International\
    \ Symposium of Electronics Design Automation (ISEDA)*, 2025.\n- <span id=\"page-13-33\"\
    ></span>[35] A. B. Kahng and T. Spyrou, \"The openroad project: Unleashing hardware\
    \ innovation,\" in *Proc. GOMAC*, 2021.\n- <span id=\"page-13-34\"></span>[36]\
    \ \"Openroad-flow-script,\" [https://github.com/The-OpenROAD-Project/](https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts)\
    \ [OpenROAD-flow-scripts.](https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts)\n\
    - <span id=\"page-13-35\"></span>[37] C. Wolf, \"Yosys open synthesis suite,\"\
    \ [https://github.com/YosysHQ/](https://github.com/YosysHQ/yosys) [yosys,](https://github.com/YosysHQ/yosys)\
    \ 2016.\n- <span id=\"page-13-36\"></span>[38] \"Triton macro placer (tmp),\"\
    \ [https://github.com/](https://github.com/The-OpenROAD-Project/TritonMacroPlace)\
    \ [The-OpenROAD-Project/TritonMacroPlace.](https://github.com/The-OpenROAD-Project/TritonMacroPlace)\n\
    - <span id=\"page-13-37\"></span>[39] A. Kahng, S. Kang, S. Kundu, K. Min, S.\
    \ Park, and B. Pramanik, \"Ppa-relevant clustering-driven placement for large-scale\
    \ vlsi designs,\" in *Proceedings of the 61st ACM/IEEE Design Automation Conference*,\
    \ 2024, pp. 1–6.\n- <span id=\"page-13-38\"></span>[40] \"Tilos macro placement\
    \ benchmark,\" [https://github.com/](https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Testcases)\
    \ [TILOS-AI-Institute/MacroPlacement/tree/main/Testcases.](https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Testcases)\n\
    - <span id=\"page-13-39\"></span>[41] \"Nangate 45nm library,\" [http://www.nangate.com/.](http://www.nangate.com/)"
- title: How to keep pushing ML accelerator performance? Know your rooflines!
  abstract: 'The rapidly growing importance of Machine Learning (ML) applications,
    coupled

    with their ever-increasing model size and inference energy footprint, has

    created a strong need for specialized ML hardware architectures. Numerous ML

    accelerators have been explored and implemented, primarily to increase

    task-level throughput per unit area and reduce task-level energy consumption.

    This paper surveys key trends toward these objectives for more efficient ML

    accelerators and provides a unifying framework to understand how compute and

    memory technologies/architectures interact to enhance system-level efficiency

    and performance. To achieve this, the paper introduces an enhanced version of

    the roofline model and applies it to ML accelerators as an effective tool for

    understanding where various execution regimes fall within roofline bounds and

    how to maximize performance and efficiency under the rooline. Key concepts are

    illustrated with examples from state-of-the-art designs, with a view towards

    open research opportunities to further advance accelerator performance.'
  url: http://arxiv.org/abs/2505.16346v1
  keywords: ML accelerators, energy efficiency, throughput, chip design, quantization,
    sparsity, processor architectures, memory hierarchy, roofline model.
  document: '#### I. INTRODUCTION AND MOTIVATION


    The size of machine learning models has been growing rapidly across the last decade,
    as illustrated in Figure [1.](#page-0-0) This growth led to an explosion in the
    required number of operations per ML training or inference tasks, as well as growing
    memory footprints. The growth rate in complexity of these models substantially
    outpaces Moore''s law (Figure [1\)](#page-0-0). As a result, tracking the requirements
    in energy efficiency and throughput requires agility in design innovation and
    clarity around design trade-offs.


    This sustained innovation pressure has led to a myriad of dedicated hardware accelerators,
    exploiting specialized efficiency-boosting techniques, tailored for the ML domain.
    This push towards specialized architectures is challenged by a growing diversity
    in workloads. The rapid evolution of ML models, in terms of their topologies (layer
    dimensions, connectivity), precision requirements, modalities and embedding/deembedding
    approaches, etc., require specialization yet without


    M. Verhelst is with KU Leuven, Belgium and with imec, Belgium. L. Benini is with
    ETH Zurich and Universita di Bologna. N. Verma is with ` Princeton University
    and EnCharge AI. The work has received funding from the European Research Council
    (ERC), the Flanders AI Research Program (FAIR), the Swiss State Secretariat for
    Education, Research, and Innovation (SERI) under the SwissChips initiative and
    the DARPA OPTIMA agreement no. HR00112490300.


    Color versions of one or more figures in this article are available at XXXX. Digital
    Object Identifier XXXX


    ![](_page_0_Figure_12.jpeg)


    <span id="page-0-0"></span>Fig. 1. Evolution of ML model size, GPU performance
    and Moore''s law across the last decade. Data from [\[1\]](#page-15-0) and [\[2\]](#page-15-1).


    giving up on programmability, necessitating careful balance between flexibility
    and efficiency.


    Industrial and academic research have responded to the challenging needs, achieving
    impressive nearly 100 × performance improvement every 24 months, maintained over
    the past 8 years as seen in Figure [1.](#page-0-0) Despite the wide variety of
    ML accelerator architectures in the NPU''s (Neural Processing Units) of commercial
    products and published in the literature, a large part of their performance improvements
    can, however, be attributed to different variants of a common list of effective
    optimizations, including: a.) maximizing parallelism; b.) exploiting (spatial
    and temporal) data reuse; c.) quantization; d.) sparsity; and e.) near- or in-memory
    compute. This paper surveys these techniques, and illustrates their key insights
    and trade-offs with various examples from state-of-the-art (SOTA) designs.


    As these techniques are deployed in widely diverging platforms and at different
    scale, from tiny/extreme-edge devices, to edge processors, to cloud systems, it
    is difficult to assess them relative to each other. We require a unifying framework
    to assess their impact and to gain structured understanding of what sets efficiency
    and performance, as well as what the associated trade-offs are. This paper attempts
    to do this, leveraging and enhancing the roofline model for throughput and energy
    efficiency. The goal is to clearly visualize the impact of the different architectural
    techniques through the roofline representation, thereby enhancing contextual/comparative
    understanding. Finally, this will lead to insights into fundamental trade-off
    between the different techniques impacting performance, ultimately leading to
    open questions and directions for future research.


    This paper is organized as follows: Section [II](#page-1-0) outlines the components
    contributing to the energy and latency of ML


    Received XXX 2024; revised XXX 2024; accepted XXX 2025. This article was approved
    by Associate Editor XXXXX.


    ![](_page_1_Figure_1.jpeg)


    <span id="page-1-1"></span>Fig. 2. Typical architecture of an ML accelerator.
    Note that the amount of PE''s in the datapath, the number of memory levels in
    the memory hierarchy and type of memory at each level (registers, SRAM, DRAM)
    can vary from system to system. Illustrative bandwidth and energy numbers are
    from a 22 nm technology.


    accelerator execution, leading to a visualization comprising two different rooflines.
    The most impactful efficiency enhancement techniques from recent research are
    subsequently surveyed and assessed in light of the roofline models in Section
    [III,](#page-4-0) illustrated by several examples from recent SOTA. Next, section
    IV dives deeper into some fundamental tradeoffs between the presented techniques,
    and how they have a conflicting impact on the performance and efficiency rooflines.
    This will finally lead to an outlook on the future, and directions for further
    research in Section V.


    # <span id="page-1-0"></span>II. THE ROOFLINE MODELS FOR THROUGHPUT AND ENERGY
    EFFICIENCY


    # *A. Where does the energy and latency go?*


    Almost all ML accelerators, regardless of whether standalone units, data-path
    extensions to a processor, or tensor cores integrate in a GPU, can be characterized
    by a common baseline architecture, depicted in Figure [2.](#page-1-1) A regular
    array of processing elements are each capable of one or more multiplyaccumulate
    (MAC) operations. The data for these operations are fed from an on-chip memory
    system, typically consisting of a hierarchy of registers and one or more levels
    of SRAM scratch pads. Additionally, the memory system typically couples to off-chip
    memory and/or storage subsystems.


    To optimize ML accelerators for throughput and energy efficiency, one has to understand
    where the energy and latency are spent in such systems. While one could focus
    on the efficiency of the MAC operation itself, this often only contributes to
    a portion of the total system energy and latency. Specifically, the energy per
    inference task Etask and latency per inference task Ltask are heavily impacted
    by the necessary data movement operations to bring the required data on chip,
    and to move the data between the different on-chip memory levels.


    For a given number of bytes NLi that must be accessed in each level Li of the
    memory hierarchy in order to perform a given number of multiply and/or accumulate
    operations Nop per inference, the total energy per inference can be computed as:


    $$E\_{task} = N\_{op}.E\_{op} + \sum\_{i} N\_{Li}.E\_{Li} \tag{1}$$


    with ELi being the energy per byte of memory access at memory level Li, and Eop
    being the energy per multiply or accumulate operation.


    Latency cannot be derived with a similar sum of the latency impact of the different
    memory and compute components: As ML accelerators are typically designed to perform
    compute and data-movement operations in parallel, compute operations can be hidden
    behind memory transfers or vice versa. As a result, the total latency per inference
    task is typically obtained by:


    $$L\_{task} = \frac{1}{f\_{clk}}.max(\frac{N\_{Ln}}{B\_{Ln}}, ..., \frac{N\_{L1}}{B\_{L1}},
    \frac{N\_{op}}{A\_{op}})\tag{2}$$


    with BLi being the bandwidth (bytes/cycle) of memory level Li, Aop being the amount
    of arithmetic operators physically on the chip, and fclk being the chip''s operating
    frequency.


    To determine which contribution dominates the total energy and latency of an ML
    accelerator, one can fill in energy and memory bandwidth values of a target technology,
    as given in Figure [2](#page-1-1) for a 22 nm CMOS. Doing so, it quickly becomes
    apparent that in both the energy and latency models, the ratios between the operation
    counts Nop [1](#page-1-2) and the different memory access counts NLi strongly
    determine the relative contribution of the different components towards system-level
    efficiency. In practice, these ratios are determined by the workload and denoted
    as the "Arithmetic Intensity" AI (also referred to as the "Operational Intensity",
    or "Compute Intensity") of the target workload for memory level Li [\[3\]](#page-16-0):


    $$AI\_{Li} = \frac{N\_{op}}{N\_{Li}}\tag{3}$$


    For neural network workloads, the compute intensity typically increases steeply
    towards the upper levels of the memory stack. A single MAC can have a AI of less
    than one towards its internal registers, as maximally 1 operand can remain stationary
    [\[4\]](#page-16-1) and hence multiple words have to be read/written per operation.
    Yet, higher up in the memory hierarchy, the compute intensity can be drastically
    higher due to various data reuse opportunities, as will be discussed further in
    Section III.


    It is important to note that the achievable arithmetic intensities towards the
    different memory levels are highly network and layer dependent [\[5\]](#page-16-2).
    While CONV operators with large input/output tensors might be able to achieve
    high AI''s in the upper level memories, others such as fully connected layers
    in batch-1 mode, experience low AI''s which are rather flat along the memory hierarchy.
    For this reason, it is important to assess performance as a function of AI, which
    will be discussed in the next subsection.


    ### <span id="page-1-3"></span>*B. Not one, but two roofline models*


    To assess throughput across a wide range of potential arithmetic intensities,
    the computer architecture community introduced the "roofline model" [\[3\]](#page-16-0)
    [\[6\]](#page-16-3). This model was later also applied to Large Language Model
    (LLM) inference on GPUs [\[7\]](#page-16-4). It provides a visualization of the
    attainable


    <span id="page-1-2"></span><sup>1</sup>Throughout the complete paper, only addition
    and multiply operations are considered towards the number of operations (ops),
    where a MAC (multiply accumulate) counts for 2 operations.


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 3. Example roofline models for performance and
    energy efficiency. (Assumptions: Nop=2048, Eop=0.50pJ/op, EL1=0.100pJ/byte, EL2=3pJ/byte,
    EL3=100pJ/byte, BL1=128B/cycle, BL2=32B/cycle, BL3=8B/cycle and for the aggregated
    energy roofline: AI=AIL3/16=AIL2=AIL<sup>1</sup> ∗ 16.)


    processor performance as a function of arithmetic intensity, to quickly assess
    which system components limit performance for a given workload. The roofline models
    presented earlier, however, typically focus on the throughput performance of a
    processor with a single memory level, assuming the AI of the studied workloads
    remains almost identical across the memory hierarchy, rendering the highest level
    memory interface the most limiting one. An important distinction in ML accelerators
    is that the AI of neural networks typically varies widely between memory levels.
    The opportunities to multi-cast input and weight data, and reduce (aggregate)
    output data typically cause NL<sup>3</sup> < NL<sup>2</sup> < NL<sup>1</sup> and
    hence AIL<sup>3</sup> > AIL<sup>2</sup> > AIL<sup>1</sup> (see also Section [III.](#page-4-0)B
    and [\[8\]](#page-16-5)). This requires us to derive the roofline equation for
    total attainable throughput performance PT P (expressed in operations/sec), starting
    from Eqn. (2), as:


    $$P\_{TP} = \frac{N\_{op}}{L\_{task}} = f\_{clk} \cdot \min(A I\_{Ln}, B\_{Ln},
    \dots, A I\_{L1}, B\_{L1}, A\_{op}) \tag{4}$$


    Visualizing this, results in Figure [3](#page-2-0) (left), in which the memory
    architecture sets the sloped performance boundary for lower arithmetic intensities,
    while the MAC compute architecture sets the flat performance boundary for higher
    arithmetic intensities. Depending on the relative memory bandwidths and arithmetic
    intensities of the different memory levels, one memory level will be the limiting
    one and form the overall system roofline (e.g. Level1 in Fig. [3](#page-2-0) (left)).


    While not as common in the literature, it is also valuable to define an *energy*
    performance roofline [\[9\]](#page-16-6) for ML accelerators. Energy efficiency
    is, however, not computed with a min/max operator, but as the sum of the contributing
    system components (Eqn. (1)), resulting in the following expression for the equivalent
    Energy performance PE, expressed in ops/seconds/Watt or ops/Joule[2](#page-2-1)
    :


    $$P\_E = \frac{N\_{op}}{E\_{task}} = \frac{1}{E\_{op} + \sum\_{i} \frac{E\_{Li}}{AI\_{Li}}}
    \tag{5}$$


    <span id="page-2-1"></span>2 In the remainder of this paper, we will consistently
    express energy efficiency in terms of TOPS/W = 1012ops/seconds/W att, which is
    equivalent to ops/pJoule


    Visualizing this roofline, will no longer give the typical roof shape seen in
    the well-known throughput curve, yet, will result in a curved roofline, with a
    low-AI region where memory energy dominates the total efficiency; a high-AI region
    when compute energy dominates; and a middle region, where the curve is smooth
    because both components comparably impact total efficiency. The location of the
    bending point depends on the relative ratio of the different <sup>E</sup>Li AILi
    terms for the memory levels Li.


    It is important to note that the cut off point of both rooflines can lie at very
    different arithmetic intensities. While the performance roofline has its transition
    from the memory-bound regime to the compute bound regime at AI = Nop/BLi, the
    energy roofline curves at AI = ELi/Eop for a given memory level L<sup>i</sup>
    . As both depend on very different parameters, an accelerator can for a certain
    range of arithmetic intensities simultaneously operate in the compute bound regime
    for throughput performance, and in the memory-bound regime for energy efficiency.
    As a result, optimizations can differ significantly depending on whether one cares
    more for throughput or energy efficiency. Finally note that the throughput roofline
    only has a sharp transition as long as the compute and memory latency can overlap
    (min-operation in eq. (4)). In case total latency depends on a linear combination
    of both, also here a rounded roofline curve would appear similar to the energy
    roofline, as observed in benchmarks in [\[10\]](#page-16-7).


    While these roofline plots help us in determining the maximum attainable throughput
    and energy efficiencies for a given AI, neither allow us to derive the *effective*
    efficiency when executing realistic workload. The actual performance will fall
    below the roofline curve, the extent of which depends on the effective *memory
    and compute utilizations*, Umem and UMAC , respectively. The performance in the
    flat part of the roofline, will be lowered by the compute utilization UMAC , characterizing
    the ratio of the average number of useful MAC operations performed per cycle,
    to the total amount of MAC operations available in the ML accelerator. The performance
    in the sloped part of the roofline, on the other hand, will be lowered by the
    memory utilization Umem, representing the ratio of the average number of useful
    data bytes accessed from a memory level per cycle, to the peak memory bandwidth
    at that level.


    # *C. Optimizing performance under the rooflines*


    The goal of ML accelerator designers, system architects and workload mappers is
    to maximize throughput and energy performance. From the discussions in Section
    [II-B](#page-1-3) it is clear that this can be done through: (1) raising the rooflines
    themselves; and/or (2) by moving to the right, to the compute bound regime and/or
    (3) by more closely approaching the rooflines (Figure [4\)](#page-3-0). Here,
    we briefly summarize the most relevant SOTA methods towards these three approaches
    and visualize their effect in Figure [4.](#page-3-0) We will dive into more detail
    on each of them in the next Section.


    *1) Raising the rooflines:* On the one hand, increasing the compute boundary peak
    performance (operations per cycle, Nop) and peak MAC efficiency (operations per
    second per


    ![](_page_3_Figure_1.jpeg)


    <span id="page-3-0"></span>Fig. 4. Overview of various architectural and mapping
    techniques to maximize performance by raising the rooflines and more closely approaching
    the rooflines.


    Watt, 1/Eop) allows us to raise the flat part of the roofline. Figure [4](#page-3-0)
    outlines various architectural techniques (parallelization for large compute arrays
    and multi-processors, analog compute, etc.), as well as algorithmic techniques
    (quantization, sparsity, etc.). On the other hand, increasing the memory boundary
    in terms of memory bandwidth (bits/cycle, Bmem) and memory efficiency (energy
    per access, 1/Emem) allows us to raise the diagonal part of the roofline and move
    the knee point more to the left, i.e., to lower AI. This, for instance, is possible
    by optimizing memory architectures for more efficient parallel access, as well
    as through near/in-memory compute.


    *2) Move to the compute-bound regime:* As long as workloads operate on the left
    of the roofline knee point, compute performance and energy efficiency are bounded
    by the memory bandwidth of the system. To maximally exploit a systems hardware
    resources, operation should take place as close to the knee point as possible.
    This typically requires to move right on the horizontal axis, shifting to larger
    AI or in other words doing more compute per fetched data element. One technique
    to increase AI, is to perform more aggressive quantization, as reducing the precision
    of the operands increases the number of operations per data byte. However, also
    the hardware architecture can influence the AI. While AI is often considered to
    only be a function of the considered workload and not of the hardware architecture,
    we will show in this article that this is not the case for NPU''s. The physical
    compute architecture can enable a larger AI by for example providing larger on-chip
    caches or hardware support for spatial and temporal data reuse, or by supporting
    quantization and/or sparsity exploitation.


    *3) Approaching the roofline:* Under a fixed throughput or energy performance
    roofline and for a given Arithmetic Intensity, effective performance can still
    be situated well below the roofline. Both in the memory-bounded and in the compute-bounded
    region effective performance can be limited by utilization. For memory-bounded
    workloads, this stems from the fact that the memory bandwidth is not used to the
    fullest. For compute-bounded workloads, this stems from idle cycles or idle MACs
    on the compute array. In order to maximize performance these utilization losses
    should be eliminated to move as close as possible to the roofline. This requires
    minimizing all efficiency losses stemming from memory and compute under-utilization,
    by for example foreseeing support for double buffering to avoid memory stalls
    or datapath reconfigurability to maximize utilization.


    Before we dive deeper into those, let''s look at how the different techniques
    come together in a recent NPU evolution. Figure [5](#page-3-1) shows the performance
    of subsequent generations of Google''s TPU. As can be seen, different techniques,
    such as a wider the DRAM bandwidth, increased parallelism and a higher clock frequency
    are used to raise the roofline(1). Secondly, hardware support for quantization
    and data reuse is added to move the workloads to the right of the Arithmetic


    ![](_page_3_Figure_8.jpeg)


    <span id="page-3-1"></span>Fig. 5. Evolution of the rooflines of the different
    TPU generations, together with the impact on Arithmetic Intensity of a typical
    GeMM workload. Data from [\[11\]](#page-16-8)


    Intensity axis(2). For example, the TPU makes use of large 128x128 compute arrays
    for massive spatial data reuse, and is equipped with support for both BF16 and
    INT8 as of the v4 generation. Finally, hardware is modified to pursue a high utilization
    in order to operate close to the roofline curve(3). Specifically, the array size
    is brought from 256x256 to 128x128 to reduce compute utilization losses, while
    all weight registers are double buffered to reduce memory stalls.


    In the combination of these different techniques, it is important to always keep
    an eye on the X-axis location of the throughput and energy knee points. Depending
    on the AI range of the target workload, one or the other might be more relevant.
    While improvements of the AI and utilization have impact on the performance under
    both rooflines, several other architectural enhancements only impact one of the
    two rooflines. Examples are changes in parallelization (Nop) shifting the performance
    roofline, versus changes in operation or memory energy (Eop or ELi) only impacting
    the energy roofline. After this bird''s eye view, let''s dive deeper into each
    of these techniques, to fully understand the opportunities that stem from each
    of them in light of the performance under the roofline.


    # <span id="page-4-0"></span>III. HARDWARE TECHNIQUES TO MAXIMIZE PERFORMANCE
    UNDER THE ROOFLINE


    This section will dive deeper into various hardware architectural and implementation
    techniques aiming to increase throughput and/or energy efficiency.


    #### <span id="page-4-2"></span>*A. Maximizing parallelism*


    A direct knob to push the performance roofline up is to increase the amount of
    parallelism in the ML accelerator. The majority of the neural network operators
    consist mainly of "multiply-accumulate" (MAC) operations. To raise the flat part
    of the roofline, the number of parallel multiply-accumulate units in the processing
    core should hence be increased. Indeed, this trend can clearly be observed over
    the past decade: From Eyeriss'' 168 and Envision''s 256 MAC units in 2016 and
    2017 [\[4\]](#page-16-1) [\[12\]](#page-16-9), over Samsung''s and Tesla''s ∼10,000
    of MACs in 2019 and 2020 [\[13\]](#page-16-10) [\[14\]](#page-16-11), to today''s
    massively parallel near- and in-memory compute engines with 100,000''s of parallel
    MAC operators [\[15\]](#page-16-12)–[\[19\]](#page-16-13), and often beyond this
    in the case of multi-core compute engines for providing additional parallelism
    at the full system level.


    Rising to these levels of parallelism, however, comes with important challenges.
    First, a mapper is required to determine how to spatially and temporally map a
    target workload across the many parallel processing elements. Assume for example
    a system with N cores, each consisting of a 2-dimensional array of M × M MAC operators;
    and assume as target workload a sequence of L batched neural network layers, each
    represented as a matrix multiplication (MatMul) of dimension [B × C] ∗ [C × K]
    = [B × K]. This workload can be parallelized across the different cores under
    various parallelization schemes: *DataParallelism* replicates the network across
    the cores, and feeds each replication with different data from the batch dimension
    B; *PipelineParallelism* assigns different layers L to different cores, such that
    they can feed each other in a pipelined fashion; and *TensorParallelism* splits
    a single layer into multiple chunks along the C or K dimensions, and executes
    each chunk on a separate processor core. Within a core, the tiled matrix multiplication
    assigned to a specific core is then further unrolled on to the core''s internal
    M ×M MAC array, for example, spatially unrolling the remainder of the K loop along
    one dimension and the C loop along another one, while mapping the B loop temporally,
    as illustrated in Figure [6.](#page-4-1)


    It is, however, important to realize that the selected paralellization strategy
    strongly impacts the efficiency with which the parallel resources are used. This
    can be quantified in terms of the *utilization* of the processing elements. As
    detailed in Figure [6,](#page-4-1) the utilization can be broken down into the
    *core utilization* (ratio of active core count to total core count); the *temporal
    utilization* (ratio of compute cycles in which the core is not stalled, versus
    total clock cycles), and *spatial utilization* (ratio of active MAC units versus
    total MAC units within one processor core). Without proper mapping, the combination
    of these three utilization factors can quickly lead to severe compute under-utilization,
    and hence performance far below the theoretical roofline.


    Finally, it is important to realize that the increased parallelism in the cores,
    raise the roofline of the core, which shifts the knee point of the roofline to
    higher AI''s (see figure [4](#page-3-0) (left)), making the system more likely
    to be memory bound, potentially leading to memory-induced reduction of the temporal
    utilization (T U). Indeed, the execution of a large number of MAC''s per clock
    cycle, also demands a large data volume to be dragged into the compute array per
    clock cycle. Although theoretically this memory pressure can be resolved by increasing
    memory bandwidths across all levels of the memory hierarchy to shift the diagonal
    part of the roofline up, this is often not feasible in practice. ML accelerators
    often already maximally exploit distributed register files, widelybanked SRAM
    memories, and high-speed (costly) off-chip DRAM, whereby further bandwidth increases
    come at substantial area and energy costs. Fortunately, it may be possible to
    increase parallelism without having to proportionally increase memory bandwidth,
    by employing mappings that leverage increased data reuse, when/if the workloads
    allow this.


    ![](_page_4_Figure_11.jpeg)


    <span id="page-4-1"></span>Fig. 6. Spatial and temporal under-utilization. Illustration
    for a 5-core system with an (unrealistically small) MAC array of dimension 4x4
    per core, and a MatMul workload of dimensions 32x4x3


    ![](_page_5_Figure_1.jpeg)


    <span id="page-5-0"></span>Fig. 7. Spatial data reuse concepts


    ## *B. Exploiting spatial and temporal data reuse*


    The operands of the MAC operations in neural networks have a potential data reuse
    factor which can easily go up to 10,000''s. For example, the weights in a Conv
    layer or the operands of a large MatMul operation can be fetched once, and subsequently
    reused in many dotproduct operations. With some exceptions (e.g. weights in a
    fully connected layer with batch size 1, or inputs in a depthwise layer), neural
    network compute kernels thus exhibit large intrinsic arithmetic intensity. This,
    however, does not automatically result in operating at the top-right corner of
    the roofline model. The processor hardware has to be able to exploit the kernels''
    data reuse opportunities at the datapath level. This involves the alignment of
    the execution schedule to the hardware implementation, in particular to flexibly
    leverage both spatial and temporal data reuse as will be explained next.


    *Spatial data reuse* refers to reusing data within a given clock cycle across
    more than one MAC unit in the datapath. Figure [7](#page-5-0) illustrates how
    parallel MACs in a MAC array can spatially exploit weight, input, or output data
    reuse, to increase the effective arithmetic intensity. This requires hardware
    modifications, to enable direct data sharing paths between the hardware units.
    Without such provisions, the inherent arithmetic intensity of the compute kernel
    cannot be spatially exploited at the datapath level. All SOTA ML processors exploit
    such spatial data reuse along one or more operand dimensions of their datapath,
    typically consisting of hundreds to thousands of MAC units. The examples of Figure
    [7](#page-5-0) all only contain a single parfor-loop (=spatially parallel for-loop),
    corresponding with one-dimensional spatial data reuse. This can be found in vector
    processors, which are typically equipped with a Fuse Multiply Add (FMA) instruction,
    exploiting output reuse.


    However, AI typically does not improve strongly from reuse across a single operator
    dimension, as the memory accesses of the other operands will quickly start to
    limit reuse. As a result, ML processors are commonly equipped with 2- or even
    3 dimensional datapaths, and hence multiple spatial parfor-loops in their dataflow
    representation, which jointly exploit different forms of the three spatial reuse
    concepts along each MAC array dimension. The Envision processor [\[12\]](#page-16-9),
    for example,


    ![](_page_5_Figure_7.jpeg)


    <span id="page-5-1"></span>Fig. 8. Spatial data reuse within the DaVinci core
    [\[20\]](#page-16-14), and an analog IMC core with 256 rows and columns.


    combines 16-way input reuse and 16-way weight reuse, while the Huawei DaVinci
    cores [\[20\]](#page-16-14) or Nvidia Tensor Cores [\[21\]](#page-16-15) exploit
    all three forms in a 3-dimensional MAC array, as illustrated in Figure [8](#page-5-1)
    (top).


    Such multi-dimensional spatial reuse, reduces the memory pressure from being linearly
    dependent on the number of MAC elements Aop, to only scaling with p<sup>3</sup>
    Aop, substantially increasing arithmetic intensity. More importantly, the larger
    the compute array is, the more spatial data reuse can be accommodated. As a result,
    larger compute arrays can typically operate at higher arithmetic intensities in
    the compute-dominated region of the energy roofline (Figure [3](#page-2-0) (right)),
    which represents the most energy-efficient region of operation. Such very large
    and efficient compute arrays can be found in recently emerging in-memory compute
    architectures. Typically these architectures (Figure [8](#page-5-1) (bottom))
    only exploit spatial data reuse across 2 dimensions: While they can massively
    reuse inputs and output data along the memory rows and columns, each weight is
    used only once each clock cycle. As long as the weights do not have to be reloaded,
    this allows them to operate at very high arithmetic intensity. Loading new weights
    every clock cycle, however, would drastically limit the spatial AI benefits, as
    numerically illustrated in Figure [8](#page-5-1) (bottom). This is, however, avoided
    through temporal data reuse, discussed next.


    *Temporal data reuse* can further increase the effective arithmetic intensity.
    Specifically, it reduces the number of data accesses per compute operation, not
    by reusing data within one clock cycle (spatial data reuse), but by reusing data
    *across* clock cycles without repeated memory fetching. When studying the nested
    for-loops embedding the ML MAC operations (e.g. see Figure [7](#page-5-0) and
    [8\)](#page-5-1), it is clear that for each for-loop one operand remains constant
    across different iterations of the for-loop. For example, in the analog IMC


    ![](_page_6_Figure_1.jpeg)


    <span id="page-6-0"></span>Fig. 9. Nested for-loop representation of workload
    mapping and tensor allocations across the memory hierarchy.


    core (Figure [8](#page-5-1) (bottom)) the nested for-loop along the "b" dimension
    reuses the same weight element w[c][k] across all its temporal iterations, as
    the operand is not dependent on the index [b]. This allows us to keep the weights
    *stationary* across the for-loop iterations, also denoted as a *"weight-stationary"
    dataflow*. Such weight-stationarity is exploited in almost all in-memory compute
    cores [\[22\]](#page-16-16)–[\[24\]](#page-16-17).


    Beyond weight-stationary, processors can also be input- or output-stationary,
    together forming the three orthogonal forms of temporal data reuse. When describing
    processor dataflow in terms of their nested "for"- and "parfor"-loops, one can
    easily derive the stationarity of the processor under study. The lowest for-loop
    above the spatial "parfor" loops determines the temporal data reuse towards the
    registers. For example, in the DaVinci-example (Figure [8](#page-5-1) (top)) the
    lowest for-loop is a "c"-loop, and hence irrelevant for the output operand. This
    represented dataflow is hence an *output-stationary dataflow*, which is very common
    in fully digital ML processors. Also the aforementioned Envision chip [\[12\]](#page-16-9),
    Tensor cores [\[21\]](#page-16-15) or Tesla NPU [\[25\]](#page-16-18) are output
    stationary processors. As the outputs typically require a larger word length,
    and hence memory bandwidth, compared to weights or inputs, output stationarity(=temporal
    output reuse) has a significant impact on a processor''s AI.


    In contrast to spatial data reuse when multiple reuse dimensions can be combined,
    only a single stationarity dimension can be exploited per memory level. Indeed,
    it is not possible to keep two operands constant across subsequent MAC computations,
    as this would lead to repeated, redundant operations. It is, however, possible,
    to exploit different forms of temporal data reuse across the memory hierarchy.
    Executing larger tensor operations in a core typically involves tiling the input,
    weight, and output tensors, corresponding to splitting/blocking the different
    for-loops, and allocating them across different memory levels, as done in Figure
    [9.](#page-6-0)


    Here it is clear that each level of the memory hierarchy can exploit a distinct
    form of stationarity, as the lowest forloop tiled within a memory level determines
    which tensor can remain stationary towards the lower level memory. This also explains
    why the arithmetic intensity increases when moving


    ![](_page_6_Figure_7.jpeg)


    <span id="page-6-1"></span>Fig. 10. RISC-V mixed-precision SIMD dot-product unit


    from the L1 to L2 and L3 memory level. Optimal combinations of such spatial and
    temporal unrolling are therefore indispensable towards packing as much as compute
    for a given memory bandwidth.


    # <span id="page-6-2"></span>*C. Exploiting quantization*


    Quantization is currently the most actively exploited technique to boost efficiency
    in ML accelerators. A quantitative estimate of the major performance improvements
    achieved by GPUs thanks to quantization (16x), has been reported by Nvidia [\[26\]](#page-16-19)
    and is confirmed by extensive literature surveys covering a wide range of accelerators
    [\[27\]](#page-16-20)–[\[29\]](#page-16-21).


    From a hardware-centric viewpoint, quantization is easily exploitable and its
    effect on the roofline, as shown in Figures [22](#page-13-0) and [5,](#page-3-1)
    is to raise the achievable peak compute -the height of the compute-bound region
    - as more operations can be performed faster in the same silicon area, and to
    increase the arithmetic intensity of a ML workload, as more operations are performed
    for each byte exchanged with memory. Energy improvements are potentially superlinear,
    due to the fact that key operators in ML workloads, such as MAC units, have time
    and space complexity that is super-linear on the bit-width on their input operands
    [\[30\]](#page-16-22).


    Hence, it is not surprising that the hardware design community has been aggressively
    pursuing quantization. Recent research results have demonstrated that digital
    compute efficiency as high as POPS/W can be achieved with extreme, ternary and
    binary quantization [\[31\]](#page-16-23), [\[32\]](#page-16-24). On the other
    hand, such an extreme quantization comes with significant model accuracy loss.
    The current production models can be compressed down to 4-bit coefficients and
    8-bit activations, but algorithmic research is actively working to lower these
    thresholds [\[33\]](#page-16-25).


    Hardware support for quantization in instruction processors, through dedicated
    instruction set architecture (ISA) extensions, has been extensively explored,
    leveraging the wellknown single-instruction multiple-data (SIMD) approach. As
    an example, the RISC-V ISA has been extended to support all mixed-precision combinations
    of power-of-two subsets of 32 bit registers (namely 2, 4, 8, 16 bit operands)
    [\[34\]](#page-16-26), [\[35\]](#page-16-27), as well as binary and ternary operands.
    Figure [10](#page-6-1) shows the design


    ![](_page_7_Figure_1.jpeg)


    <span id="page-7-1"></span>Fig. 11. (a) Block quantization and dot-product of
    block-quantized vector fragments, (b) OCP microscaling specification of block-quantized
    numbers, (c) from basic quantization to mixed and block quantization, and future
    block and mixed quantization


    of a SIMD *expanding*[3](#page-7-0) dot-product unit for a 32-bit RISC-V core,
    supporting all power-of-two mixed precision quantized operands below 32-bit. The
    slicer&router block splits data coming from the 32b input registers in the appropriate
    narrow operands and routes them the MAC units in the dedicated datapath section.
    Sign-extension is then performed for the smaller operand; Multiplication is performed
    in parallel on 8 5b → 10b multipliers, followed by the 8×10b → 32b adder tree.
    Results are stored in the 32b accumulator register. Note that distinct dot-products
    units are instantiated for each power-of-two (plus sign extension) bit-width to
    allow operand isolation and reduce switching activity.


    The SOTA hardware support for quantization is moving along two main axes of evolution:
    First, fine-grain control of mixed quantization, where large multioperand operators,
    such as large multiplier arrays followed by adder trees for dot-product computation,
    are designed to enable different and configurable bitwidth for different classes
    of operands (activations vs. weights) [\[26\]](#page-16-19). Second, *block-quantization*
    support is getting increasingly emphasized, as operand-by-operand (scalar) quantization
    has plateaued [\[36\]](#page-16-28). Block-quantized data formats for ML are also
    being standardized in an effort to facilitate model portability across different
    hardware platforms: the Microscaling standard formats [\[36\]](#page-16-28), proposed
    by Nvidia, AMD, Intel, Microsoft, Meta, Arm, and Qualcomm in the Open Compute
    Project (OCP), are specified down to 4-bit floating-point for blocks 32 or 64
    numbers, with a shared 8-bit exponent. Figure [11](#page-7-1) provides a visual
    summary of block quantization as standardized by OCP and depicts the trajectory
    from simple scalar and uniform quantization to advanced mixed and block quantization
    schemes. The most recent GPU generation from NVIDIA, Blackwell, supports all these
    formats, including the aggressively scaled MXFP6 and MXFP4 [\[37\]](#page-16-29).


    A representative design of a specialized tensor accelerator


    ![](_page_7_Figure_7.jpeg)


    <span id="page-7-2"></span>Fig. 12. Neureka mixed-precision accelerator architecture
    design


    with finely-tunable block quantization support for weights is Neureka [\[38\]](#page-16-30)
    shown in Figure [12.](#page-7-2) Activations and weights are read in blocks through
    wide ports from L1 memories (not shown in figure). Blocks are buffered in internal
    (L0) latch-based register files, which are preferred over SRAMs because required
    capacity is low and energy-per-access can be minimized more aggressively than
    with SRAMs by voltage scaling, which can be pushed further for logic cells than
    for SRAM macros. The datapath is designed to access bit slices of each weight
    blocks. Multiplication of bits of the weights, with 8-bit activations is performed,
    one bit at a time, and products are accumulated into multi-bit numbers (12 bits),
    which are then locally shifted over a 20-bit range. The shifted output of each
    COL combinational datapath is then further accumulated by a larger adder tree
    that collects multiple local shifted sums. Results are then stored into output
    accumulation registers in an output-stationary dataflow.


    This datapath can handle weights quantized between 2 and 8 bits. The number of
    cycles needed to compute the final output-stationary summation grows linearly
    with Nw, where N<sup>w</sup> is number of bits used to represent weights. Weight
    precision is temporally unrolled, and activation precision is spatially unrolled.
    The bit-by-bit shift and accumulate steps


    <span id="page-7-0"></span><sup>3</sup> this denomination stems from the fact
    that the accumulator is computed and stored at higher precision that the operands,
    32bit in this case.


    ![](_page_8_Figure_1.jpeg)


    <span id="page-8-0"></span>Fig. 13. Nested loop for execution of a 3×3 convolutional
    layer in Neureka. Note that shift-and-add loop is executed sequentially bit-by-bit
    for the 8bit weights


    can be seen as one additional dimension in the nested for-loop notation discussed
    in section [III:](#page-4-0) this is shown in Figure [13.](#page-8-0) In this
    example, Neureka''s datapath accumulates in parallel over inputs and output channels
    and convolutional window size, but sequentially over the bit of the weights.


    Note that Neureka does not offer tunable activation quantization: this extra degree
    of freedom has also been explored in the literature [\[35\]](#page-16-27), but
    quantization of activation below 8 bits is usually not accuracy-neutral. Closing
    this accuracy gap at a high level of robustness is an active research field.


    An implementation of Neureka in 16nm TSMC technology, featuring 36x32 COL units
    achieves end-to-end (including L1 memory access cost) energy efficiency ranging
    from 8.84 TOPS/W (2-bit weights) to 2.68 TOPS/W (8-bit weights) at 0.6V [\[38\]](#page-16-30).
    Note that energy scaling is not perfectly linear in the number of bits, because
    a fraction the execution cycles does not scale with weight bitwidth: a few cycles
    are needed for scale and offset related computation. Nevertheless, significant
    energy efficiency boost is achieved (3.5× when scaling weight bitwidth by a factor
    of 4), by aggressively quantizing weights.


    # <span id="page-8-2"></span>*D. Exploiting Sparsity*


    Sparsity is present in most classical and advanced ML models, as coefficients
    tends to cluster around zero after training, As a consequence, activations are
    also pulled close to zero, albeit with a smoother distribution. Sparsity exploitation
    is also one of the key efficiency boosters for incumbent GPU architectures [\[26\]](#page-16-19),
    although to a lesser degree compared to quantization (2x vs. 16x). On the other
    hand, the impact of sparsity is likely to grow as the trend is toward largescale
    models that are intrinsically highly sparse, not just as a result of training.
    *Mixture-of-experts* models [\[29\]](#page-16-21) are notable examples of this
    trend.


    Hardware support for extremely sparse models with nonstructured sparsity is being
    explored, as the rapidly growing model size trend may soon impose the use of indexed
    data structures to handle high degrees sparsity for the largest "leadership" models.
    An example of accelerator with hardware support for highly sparse tensor computation
    is the Onyx coarse-grained configurable array (CGRA) architecture [\[39\]](#page-16-31).
    The sparse data structure supported by Onyx is known as *fibertree*, and it is
    depicted at the top of Figure [14.](#page-8-1) The


    ![](_page_8_Figure_9.jpeg)


    <span id="page-8-1"></span>Fig. 14. Onyx''s fibertree sparse tensor format and
    hardware unit for intersection and union operators [\[39\]](#page-16-31)


    fibertree is a two-dimensionally-indexed data-structure which indexes non-empty
    rows (or columns) and for each one of them it indexes the respective non-zero
    elements. Only indexes and non-zero data elements are stored in memory, thereby
    achieving large compression factors if sparsity is high and non-structured.


    For efficient execution in the Onyx accelerator, the fibertree is encoded, as
    shown in the bottom of Figure [14,](#page-8-1) in streams of indexes, read sequentially
    by the hardware engine, which ingests parallel streams for the rows/columns of
    the tensors involved in the operation being targeted. In the case of the intersection
    datapath, used for instance for computing the dot products of rows with columns
    needed for matrix multiplication, only equal indexes in both streams (coord<sup>1</sup>
    and coord2) are output by the intersector (coord stream), and the corresponding
    data elements are then loaded from memory for product calculations, which are
    then locally accumulated.


    While the reduction of memory bandwidth to access data is apparent (only the strictly
    needed data elements and their indices are loaded), access to the non-zero elements
    has lower block locality compared to dense tensors; hence efficiently loading
    data from remote, off-chip DRAM memories, which generally benefit from burst access
    to blocks of continuous data, is a challenge. Thus, most hardware optimization
    efforts have focused on *structured sparsity*. N : M sparsity is widely supported,
    where a maximum of N nonzero elements is enforced over an array of M elements
    [\[40\]](#page-17-0). N : M sparsity is hardware friendly, especially if N and
    M are design-time constants: indexes and data have constant size and efficient
    block transfers from/to memory remain viable. As a result many production ML acceleration
    engines, most notably GPUs, support N : M sparsity [\[41\]](#page-17-1) (only
    2:4 sparsity is supported natively in hardware). On-going efforts are exploring
    more advanced structured sparsity models, for instance, multidimensional sparsity
    for tensors [\[42\]](#page-17-2).


    Analyzing sparse workloads and architectural optimizations on the roofline is
    non-trivial. Figure [15,](#page-9-0) derived from [\[10\]](#page-16-7), exemplifies
    the challenge. All the blue symbols correspond to a parallel tensor contraction
    kernel executed on the A100 GPU at different levels of sparsity. The dense version
    of the kernel


    ![](_page_9_Figure_1.jpeg)


    <span id="page-9-0"></span>Fig. 15. Structured and unstructured sparsity - Roofline
    analysis based on data from [\[10\]](#page-16-7). The red dot corresponds to the
    Symphony architecture


    is very close to the roofline in the compute-bound region, indicating high utilization
    and architectural efficiency. The version exploiting structured sparsity is still
    close to the roofline, but slightly more detached and moved to the left (toward
    the memory-bound region). This is motivated by the significantly reduced number
    of operations effectively executed (only those with non-zero operands in both
    tensors) and the less aggressive reduction in memory accesses. The latter is caused
    by the need to load the index of non-zero elements, as well as the fact that non-zero
    elements in one tensor can be loaded and then not used if they do not correspond
    to non-zero elements in the other tensor. Furthermore, the reduced burstiness
    of memory accesses, due to the smaller size of the tensor tiles in compressed
    form makes it harder to fully exploit the wide transfers at the main memory interface
    (HBM). Finally, the highly sparse (non-structured) version of the kernel uses
    indirection, which increases the number of short-burst memory accesses and decreases
    the Arithmetic Intensity further. The result is a reduction in the number of effective
    GOPS that can be executed per second (because of the extra memory stalls and bookkeeping
    instructions needed). This implies a shift toward the memory-bound region of the
    roofline and a very significant detachment from it, due to under-utilization of
    the peak memory bandwidth.


    Architectures with advanced support for sparsity, both hardwired (e.g. [\[39\]](#page-16-31))
    and flexible (e.g. Symphony [\[10\]](#page-16-7), Occamy [\[43\]](#page-17-3)),
    corresponding to the red dot in figure [15,](#page-9-0) can significantly reduce
    the gap from the roofline, thanks to hardwareaccelerated indirection. Unfortunately,
    the kernel remains in the memory-bound region of the roofline because hardware
    optimization does not change the number of executed products and sum operations
    with respect to the memory accesses. The effective Arithmetic Intensity (computed
    with only the nonzero memory accesses and non-zero operations) remains lower than
    for the dense workload, signaling less opportunities for data reuse. Hence, if
    we look at the roofline alone, it may seem that sparsity exploitation is not a
    good idea, as it reduces the effective GOPS/sec. However, the very significant
    reduction of operations may still lead to absolute energy and latency advantages
    with respect to ignoring sparsity, using only dense kernels. Roofline analysis
    alone, albeit useful, is not sufficient


    ![](_page_9_Figure_5.jpeg)


    <span id="page-9-1"></span>Fig. 16. Comparison of traditional, near-, and in-memory
    architectures


    to ascertain the overall end-to-end task speedup and energy efficiency improvements,
    especially when contemplating the addition of dedicated hardware acceleration
    for unstructured sparsity. Recent literature results [\[10\]](#page-16-7) indicate
    that efficiency improvements exceeding one order of magnitude are possible through
    dedicated hardware if tensors are very sparse (nonzero density below 0.39%). Current
    ML models are quite more dense than that. Hence while structured sparsity is immediately
    exploitable for current models with some tuning effort, nonstructured sparsity
    requires model evolutions toward higher level of sparsity.


    It is also important to note that quantization and sparsity may be seen as two
    facets of the same data representation problem: we conjecture that in the future
    they will be combined in advanced data representation schemes with hardware support
    aiming at more aggressively reducing the number of bits needed, on average, to
    store and manipulate weights and activation. On the other hand, the more aggressively
    quantized are the elements of a tensor, the harder is to get additional advantages
    by sparsity exploitation (since indexing overhead becomes relatively larger).
    Optimally balancing sparsity and quantization with efficient hardware, locally
    and at the level of global memory access, is an interesting and open research
    challenge.


    #### *E. Exploiting near-/in-memory compute*


    The need to simultaneously address both the efficiency over a large number of
    operations and the data-movement costs across large amount of data has motivated
    near- and in-memory computing architectures. Fig. [16](#page-9-1) shows the progression
    to such architectures from traditional computing architectures.


    As already considered in detail, the traditional architecture is based on the
    separation of a large amount of computation from a large amount of memory, with
    the hardware for each sized to provide the horizontal (compute-bound) and diagonal
    (memory-bound) rooflines. As we boost these rooflines, the physical size of both
    hardware structures also scales, leading to substantial data communication costs
    (energy, latency) during computations. Representative energies for communicating
    data out of the memory and from the memory to the processor are shown in Fig.
    [16](#page-9-1) for reference.


    The near-memory computing (NMC) architecture is based on finer-grained partitioning
    and proximate placement of the memory and computation hardware, so that the datacommunication
    energy efficiency and aggregate bandwidth are substantially enhanced. This yields
    an upward shift in the diagonal roofline. Again representative energy numbers
    are shown for reference. Such an architecture is amenable under two conditions:
    (1) the computations involve substantial parallelism, so that the computing hardware
    can be partitioned accordingly; (2) the data required for different parallel computations
    is well structured and exhibits high locality, such that it can be readily stored
    in the partitioned memory associated with each parallel computation. As mentioned,
    both of these conditions are fortunately typical in ML workloads (as well as a
    range of other signal-processing workloads), making various forms of NMC a natural
    and commonly employed evolution on traditional integrated architectures. However,
    the NMC architecture has also been adopted within memory-centric (e.g., DRAM)
    technologies, providing integrated computation for data reduction over energy-
    and bandwidth-limited communication channels, opening new processor-memory architectural
    options and trade-offs (not in discussed in detail here) [\[44\]](#page-17-4),
    [\[45\]](#page-17-5).


    The in-memory computing (IMC) architecture eliminates the explicit separation
    of memory and computation hardware, and in doing so eliminates the costs of both
    communicating data out of the memory and of communicating data from the memory
    to a processor. In doing so, IMC introduces the potential for substantial efficiency
    and performance advantages, but through a number of critical circuit and architectural
    trade-offs that impact computation accuracy and hardware utilization. While NMC
    is a straight-forward evolution on traditional architectures, IMC presents a number
    of differentiating considerations, and is the focus of the following section.


    Before analyzing its operation and trade-offs, we note that IMC can be seen as
    an extreme case of two-dimensional (2- D) spatial architectures. Such architectures
    leverage a 2-D arrangement of PEs, each providing local data storage and computation,
    in order to exploit 2-D data reuse in matrix operations. In IMC, PEs are reduced
    to high-density memory bit cells, and the opportunities presented for reducing
    data communication can be viewed by considering each of the operands involved
    in a matrix operation. For inputs and outputs (activations) provided to/from the
    IMC macro, high-density memory bit cells enable greater spatial data reuse, by
    virtue of a greater number of PEs possible within an allocated area. For Weights
    stored in the IMC macro, the integration of computation capabilities within bit
    cells eliminates explicit accessing, at least from a first-level memory, thereby
    eliminating the roofline diagonal associated with the first-level memory. As described
    below, IMC architectures typically require the use of additional level-2 memory
    for AI model scalability. Thus, viewing it as a high-density PE array (enabling
    greater parallelism within an allocated area) with enhanced energy efficiency
    (enabled by analog or custom-digital hardward), IMC has two effects on the roofline
    model: (1) it boosts the compute-bound roofline; (2) enables increased levels
    of reuse, moving workload operating points further into the computebound regime.


    ![](_page_10_Figure_5.jpeg)


    <span id="page-10-0"></span>Fig. 17. Basics of in-memory computing for MVM operations.


    #### *F. In-memory compute (IMC) trade-offs and approaches*


    A range of circuit approaches have been proposed for enhancing the energy efficiency
    of computation, and a number of these have been explored within IMCs architectures,
    as mentioned further below. On the other hand, IMC fundamentally addresses data-movement
    through a specific approach, namely by performing in-memory data reduction. In
    doing so, IMC fundamentally instates a dynamic-range trade-off versus energy efficiency
    and compute density.


    While IMC has been proposed for a range of compute operations, Matrix-Vector Multiplications
    (MVMs), which are of particular importance in ML workloads, provide the most direct
    opportunity for such data reduction. This arises because MVMs first involve parallel
    operations, namely multiplication between input-vector elements and matrix weights,
    followed by a reduction operation, namely accumulation along the matrix inner
    dimension. Fig. [17](#page-10-0) shows a typical in-memory computing architecture.
    Input-vector elements are provided in parallel to the memory rows, via the word
    lines (WLs) or other dedicated inputs, realizing spatial input reuse. The bit
    cells, either via existing or added circuitry, then perform multiplication with
    stored data. Finally, accumulation is then performed along each column, on the
    bit lines or other dedicated outputs, realizing spatial output reuse.


    IMC designs have covered a broad range of approaches to this basic operation.
    Input data has been provided through different signal-modulation schemes, leveraging
    pulse-amplitude modulation [\[46\]](#page-17-6) and pulse-width modulation [\[47\]](#page-17-7).
    Bit-cell storage has been provided through binary [\[46\]](#page-17-6), [\[47\]](#page-17-7)
    and multilevel [\[48\]](#page-17-8), [\[49\]](#page-17-9) cells, while multiplication
    and accumulation has been performed through digital [\[50\]](#page-17-10), [\[51\]](#page-17-11)
    and analog [\[24\]](#page-16-17), [\[46\]](#page-17-6), [\[48\]](#page-17-8) circuits.
    Finally, output readout has been accomplished by direct digital representation
    and analog-todigital conversion from the current [\[52\]](#page-17-12), voltage
    [\[53\]](#page-17-13), and time [\[54\]](#page-17-14) domains. In all cases, IMC
    instates specific fundamental trade-offs, constrained by the storage and compute
    technologies employed, and mediated by critical design parameters described next.


    *1) IMC Fundamental Trade-offs:* From its basic operation, we see that IMC derives
    its gains from high-density PEs (bit cells), providing in-memory data reduction.
    Fig. [18](#page-11-0) analyzes the critical IMC trade-offs determining the achievable
    gains, compared with traditional/NMC architectures, mediated by the row parallelism
    P<sup>R</sup> (where P<sup>C</sup> is the column parallelism). Simplifying the
    more detailed analysis in [\[55\]](#page-17-15), we see that a


    ![](_page_11_Figure_1.jpeg)


    <span id="page-11-0"></span>Fig. 18. Fundamental dynamic-range trade-off of in-memory
    computing [\[55\]](#page-17-15).


    traditional architecture requires energy E × P<sup>R</sup> and latency L×P<sup>R</sup>
    (from P<sup>R</sup> switching cycles), while IMC requires only E and L energy
    and latency, respectively. However, while a traditional architecture accesses
    individual bits of stored data having dynamic range D, IMC access a compute result
    via accumulation over P<sup>R</sup> bits of data, causing dynamic range increase
    to at least D × PR.


    Generally, maximizing density and supporting increased dynamic range has energy
    and performance, as well as, notably, signal-to-noise ratio (SNR) implications,
    particularly when leveraging analog operation. This has direct impacts on ML model
    accuracy, as we also previously saw with quantization, and establishes a critical
    trade-off space for IMC. Quantitatively, the trade-offs depend on the underlying
    technologies employed for data storage and computation, with implementations thus
    ranging from low levels of row of parallelism (2-3 rows, where IMC gains are minimal)
    [\[56\]](#page-17-16), moderate levels of row parallelism (10''s of rows) [\[57\]](#page-17-17),
    and high levels of row parallelism (1000''s of rows) [\[24\]](#page-16-17). We
    therefore see that the choice of underlying technologies is critical to the gains
    possible from IMC.


    It is important to note that the discussion above frames the IMC dynamic range
    trade-off as mediated by the row parallelism PR. However, the dynamic range following
    inmemory data reduction (accumulation) also depends on the dynamic range of multiplication
    operands D<sup>X</sup> and D<sup>W</sup> , inputs and weights respective:


    $$D\_y = (2^{D\_y} + 2^{D\_w} - 1) \times P\_R \tag{6}$$


    Since multiplication precedes the critical IMC accumulation, its contribution
    to the dynamic range can be managed outside of the IMC macro, through bit-sliced
    operation, where multiple input bits are processed in a bit serial manner and
    multiple weight bits are processed in a bit-parallel manner. This causes energy
    and throughput scaling as typically expected in multipliers (i.e., linearly with
    each operand). Though it changes how IMC accumulation impacts SNR, SNR and thus
    model accuracy ultimately remains limited by the IMC accumulation operation [\[53\]](#page-17-13),
    [\[58\]](#page-17-18).


    *2) Technologies and Implementations:* Fig. [19](#page-11-1) considers a generalized
    implementation model for an IMC column, comprising: input drivers, for providing
    row-parallel input data, X; bit cells, for storing weight data W and performing
    multiplication with inputs; mechanism for accumulation; and readout circuitry,
    for providing digitized outputs. The maximum total dynamic range of the system
    (in bits) is set by the sum of input bits, BX, and weight bits, B<sup>W</sup>
    , involved in each multiplication, plus log<sup>2</sup> of the number of rows
    involved in


    ![](_page_11_Figure_9.jpeg)


    <span id="page-11-1"></span>Fig. 19. Generalized column computation in in-memory
    computing.


    accumulation PR. The overall dynamic range may be limited by any of IMC circuit
    components, as discussed below.


    There have been two broad approaches to IMC design: digital IMC (D-IMC), where
    the critical accumulation operation is performed using an adder tree, based on
    digital logic, applied across all memory rows [\[50\]](#page-17-10), [\[51\]](#page-17-11);
    and analog IMC (A-IMC), where the critical accumulation is performed using analog
    current or charge, followed by analog-to-digital conversion [\[55\]](#page-17-15).
    Due to the efficiency with which digital circuits can support dynamic-range scaling,
    D-IMC can readily preserve computation SNR and thus model accuracy. However, by
    employing the same fundamental technology as digital accelerators, it provides
    only incremental gains in energy efficiency and compute density relative to traditional
    digital architectures. As shown in Fig. [20,](#page-12-0) D-IMC architectures
    are typically dominated by their digital circuitry, such that the MVM-level gains
    achieved relative to traditional digital architectures are in the range 2×, arising
    primarily due to the use of custom-layout adder and weight-storage logic circuits.
    To reduce the digital circuitry and increase data-storage density, recent D-IMC
    designs have attempted to time-share the computation logic across multiple sets
    of bit cells, but where the original compute density and energy efficiency remain
    largely unchanged due temporal operation [\[59\]](#page-17-19).


    On the other hand, A-IMC has shown the potential for substantially higher energy
    efficiency than digital architectures [\[60\]](#page-17-20), but has primarily
    been limited in SNR by various sources of analog noise. Many A-IMC approaches
    have attempted to scale the operation of typical memory cells, which communicate
    a stored bit via an output current, in order to minimize the need for additional
    devices. However, A-IMC implementations based on SRAM [\[46\]](#page-17-6), flash
    [\[48\]](#page-17-8), ReRAM [\[57\]](#page-17-17), MRAM [\[54\]](#page-17-14),
    [\[61\]](#page-17-21), PCM [\[62\]](#page-17-22) have all shown that the bit-cell
    current variation and nonlinearity introduce substantial SNR and model-accuracy
    degradation, preventing scaling from single-bit outputs to the larger dynamic
    range following accumulation, required for IMC efficiency and throughput advantage.


    Emerging resistive embedded non-volatile memory (eNVM), particularly ReRAM [\[63\]](#page-17-23)
    and MRAM [\[64\]](#page-17-24), which


    ![](_page_12_Figure_1.jpeg)


    <span id="page-12-0"></span>


    Fig. 20. Analysis of digital in-memory computing versus standard digital architectures.


    have recently begun to be available in foundry CMOS technologies, have been of
    particular research interest due to their potential for density scaling in advanced
    CMOS nodes. However, these technologies tend to provide reduced IMC signal, due
    to low on-to-off resistance ratios, in the range of 2-10× (i.e., well below those
    of MOSFETs used in SRAM technologies, in the range of 104×), and low absolute
    resistance. With high IMC row-parallelism, this leads to low SNR and high current
    for voltage sensing, thus leading to poor readout-circuit power and area efficiency,
    often dominating over the memory array itself. This has prohibitively limited
    the practical efficiency and compute density achievable by eNVM-based IMC.


    The fundamental SNR tradeoff in IMC, has instead motivated switched-capacitor
    (SC) operation [\[24\]](#page-16-17), which moves away from current-based signaling
    of individual bit cells, as prominently used in standard memory operation. As
    shown in Fig. [21,](#page-12-1) SC IMC leverages highly-precise and temperature-stable
    lithographically-processed backend-of-line (BEOL) metal capacitors, formed above
    the bit cells, to enable high SNR charge-domain accumulation. Analysis has shown
    that this can enable row-parallel operation in the tensof-thousands, with practical
    demonstration of over 4k-row implementations [\[24\]](#page-16-17). Further, voltage
    output signals from SC operation enable direct feeding of energy- and area-efficient
    ADC architectures (e.g., capacitive SAR), but still impose energy and area overheads
    ranging from 15-40% [\[53\]](#page-17-13), [\[65\]](#page-17-25).


    SC IMC has led to complete and scalable full-architecture demonstrations, integrating
    A-IMC with digital infrastructure (accelerators, CPUs, on-chip networks, level-2
    memory) [\[66\]](#page-17-26), as well as the highest efficiency IMC macros to
    date (up to 120 TOPS/W for 8-b computation in 28 nm CMOS) [\[65\]](#page-17-25).


    *3) Architectural Implications:* At the architectural level, a critical challenge
    introduced by IMC is that it intrinsically couples storage and compute resources,
    yet maximizing utilization and scalability requires optimizing each of these separately.
    This prevents a fully-weight-static workload map-


    ![](_page_12_Figure_8.jpeg)


    <span id="page-12-1"></span>Fig. 21. Switched-capacitor in-memory computing for
    high-SNR computation.


    <span id="page-12-2"></span>TABLE I ANALYSIS OF IMC, OPTIMIZING FOR STORAGE VS.
    COMPUTE UTILIZATION IN A FULLY-WEIGHT-STATIC MAPPING, CONSIDERING UTILIZATION
    LOSSES JUST DUE TO THE CONFLICT BETWEEN THESE.


    | Model      | Params | Mapping w/o Replication |                        |                        |
    Mapping w/ Replication |                        |                        |

    |------------|--------|-------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|

    |            |        | IMC bits                | Storage<br>Utilization | Compute<br>Utilization
    | IMC<br>Bits            | Storage<br>Utilization | Compute<br>Utilization |

    | ResNet-50  | 23.9 M | 190 M                   | 1.00                   | 0.19                   |
    610 M                  | 0.31                   | 1.00                   |

    | ResNet-101 | 42.8 M | 342 M                   | 1.00                   | 0.18                   |
    1.216 M                | 0.28                   | 1.00                   |

    | BERT       | 110 M  | 1,425 M*                | 1.00                   | 1.00                   |
    1.425 M*               | 1.00                   | 1.00                   |

    | UNet-2D    | 31 M   | 248 M                   | 1.00                   | 0.017                  |
    1. 704 M               | 0.14                   | 1.00                   |


    ping. To illustrate Table [I,](#page-12-2) considers fully-weight-static mappings
    optimizing storage utilization and compute utilization, respectively, while analyzing
    the isolated effect of utilization losses that result from the conflict between
    these (ignoring all other sources of utilization loss, i.e., assuming perfect
    input activation feeding). Columns 3-5, consider workload mappings that optimize
    for storage, by storing each of the model weight bits statically in only one IMC
    bit cell. Since weights are generally involved in highly differing number of operations
    depending on the model architecture, this can lead to extremely low compute utilization,
    due to temporal utilization loss in IMC bit cells storing weights involved in
    fewer operations. As shown, while transformers (e.g., BERT), comprised primarily
    of dense layers, maintain uniform number of operations for all weights in the
    model, and thus preserve high compute utilization, convolutional backbones, including
    ones used in generative vision models (e.g., UNet-2D), exhibit widely varying
    operations, thus substantially limiting compute utilization across workloads.
    Conversely, columns 6-8 consider workload mappings that optimize for compute utilization,
    by replicating weight bits in IMC bit cells in proportion to the number of operations.
    This requires to the excessive number of IMC bit cells shown, effectively leading
    to low spatial storage utilization across workloads, thus substantially limiting
    architectural scalability.


    Such analysis illustrates the need to ultimately break the compute-storage coupling
    through the use of level-2 (L2) memory, so that workload computations can be temporally
    mapped, to address architectural scalability while ensuring high utilization.
    This has important implications at the level of IMC circuit and technological
    design, as well as at the level of architectural design. In terms of circuit and
    technological design, for instance, IMC write energy and bandwidth become critical
    for efficient temporal mapping of workloads, limiting the applicability of eNVM
    technologies, due to their write energies and endurances. In terms of architectural
    design, the need for efficient spatial (parallel) and temporal mapping of workloads
    invokes the many considerations discussed earlier in this paper, but now optimized
    to the distinct circuit and microarchitectural attributes of IMC, in terms of
    area, compute throughput/energy, and weight-loading bandwidth/energy. Architectural
    research in IMC has begun to consider such factors. Doing so has exposed the overheads
    of temporal execution and different forms of parallel execution (data, model,
    pipeline), which reduce the achievable compute rooflines, and also motivated specialized
    architectures to minimize such overheads [\[66\]](#page-17-26).


    #### IV. ROOFLINE MODELS FOR TRADE-SPACE EXPLORATION


    The roofline model represents salient aspects of an architecture, illustrated
    by the various ways the architectural techniques discussed in Section [III](#page-4-0)
    either raise the performance or energy rooflines, or enable workloads to more
    closely approach the performance or energy rooflines. This implies that, in addition
    to characterizing a given architecture, the roofline model can serve as a useful
    tool for architectural design-space exploration and analysis. This section examines
    how the roofline model can drive architectural decisions, by considering how different
    architectural trade-offs impact the roofline model. Ultimately, the workload-level
    performance and efficiency across a relevant set of workloads, would be determined
    by combining the roofline model with the workload mapping, which impacts the arithmetic
    intensity and utilization.


    As it is not straightforward for a designer to find the best trade-off between
    these conflicting design considerations, several performance modeling and design-space
    exploration frameworks have been appearing in the state-of-the-art [\[67\]](#page-17-27)–
    [\[74\]](#page-18-0). These frameworks allow to model many hardware architecture
    variants and can estimate the execution cost of running specific workloads on
    them, while optimizing the efficiency of the spatial and temporal mapping through
    loop optimizations such as unrolling, tilling and reordering. Nonetheless, roofline
    models provide a valuable tool for building intuition and rationale underpinning
    the results of such analysis frameworks.


    #### *A. Compute versus memory area allocation*


    The methods previously suggested for raising the computebound and memory-bound
    rooflines directly impact area and energy, i.e., MAC parallelism and voltage scaling
    in the case of compute-bound performance and efficiency, and memory banking in
    the case of memory-bound performance and efficiency. This suggests an approach
    to architectural optimization under a joint area and energy constraint. As shown
    in Fig. [22,](#page-13-0) the level of compute parallelism and memory parallelism
    (banking) can be optimally set to the arithmetic intensity presented by a target
    workload. The critical challenge, in practice is that architectures must typically
    be designed to support a range of workloads with differing arithmetic intensities,
    and


    <span id="page-13-0"></span>Fig. 22. Roofline optimizations to different workload
    arithmetic intensities.


    where programmability overheads and associated utilization losses will further
    reduce actual efficiency and performance.


    Of recent relevance, the explosion in ML model size has made the total memory
    capacity required a critical challenge even at the level of off-chip DRAM, thus
    extending critical memory-bandwidth and compute-parallelism trade-offs beyond
    a single chip. For instance, the capacity limitations of contemporary DRAM high-bandwidth
    memory (HBM) necessitate executing Large-Language-Models (LLMs) across multiple
    chips, each with attached HBM. This can require interconnection between multiple
    (16-32) chips, making interchip networking an additional key technology, together
    with compute and memory technologies. In fact, to simultaneously meet memory bandwidth
    and capacity requirements, architectures have been proposed based only on high-bandwidth
    onchip SRAM, integrating several hundreds of MB''s on chip and requiring over
    500 chips to be networked even for LLMs of modest size [\[75\]](#page-18-1).


    # *B. Parallelism versus utilization*


    While the previously discussed approach of employing large processing arrays raises
    the rooflines by maximizing parallelism and minimizing the energy per operation,
    the challenges inherent with spatial, temporal, and core utilization presented
    in Section [III-A](#page-4-2) can often prevent operation near the rooflines.
    To analyze workload-level performance the roofline model must be considered together
    with the workload operating point, which itself is determined by the workload
    mapping. As we will see, architectural decisions favorably impacting the rooflines
    can adversely impact the workload operating point.


    For instance, larger spatial arrays make it much harder to maintain full spatial
    utilization across a wide range of workloads. For example, if we map a workload
    with only 128 output channels on the analog IMC core of Figure [8](#page-5-1)
    (bottom), only 32,768 of the 65,536 MACs will be activated during a typical compute
    cycle, resulting in a spatial utilization of just 50%.


    Additionally, large monolithic processing arrays require a massive amount of data
    per clock cycle, limited by memory bandwidth. The severity of such an effect depends
    on the workload mapping, where leveraging greater levels of data reuse enhances
    arithmetic intensity, pushing back to the computebound regime. Larger arrays may
    enable increased data reuse by virtue of storing more data involved in the operations,
    but the maximum level of data reuse is ultimately limited by


    ![](_page_13_Figure_14.jpeg)


    the workload itself. For example, if we have to reload the weights of the IMC
    array of [8](#page-5-1) (bottom) every 1024 cycles, and this weight loading takes
    256 cycles (1 row per cycle) during which no compute can take place, 1/5th of
    the clock cycles is lost to weight transfers resulting in peak temporal utilization
    of just 80%. The total utilization per processor core (the product of the spatial
    and the temporal utilization), determines the distance between the roofline peak
    performance and the obtained performance within that core. Utilization hence puts
    a limit on how close the processor can approach the roofline performance (Figure
    [4](#page-3-0) (right)).


    These tradeoffs become particularly critical with IMC architectures, due to their
    parallelism tradeoffs and relative efficiency of compute versus weight transfers.
    In terms of parallelism, as previously discussed, IMC derives its efficiency advantages
    from high-levels of row parallelism and computedensity advantages from integration
    of many dense bit cells within each core, making spatial (bit cell) utilization
    loss a key challenge. In terms of weight transfers, while row-parallel data reduction
    improves compute efficiency and throughput in IMC, weight loading remains limited
    to bit-by-bit writing, making temporal utilization losses of increased severity.
    Initial demonstrations of IMC architectures have begun to address these challenges,
    by noting that different forms of parallel execution (data/model/pipeline-parallelism)
    yield different tradeoff points, thus integrating low-overhead supports to optimize
    across different forms of parallel execution [\[66\]](#page-17-26).


    The fundamental trade-off stems from the fact that larger monolithic PE arrays
    (e.g. large IMC arrays or widely-parallel digital tensor cores operating at reduced
    precision) are good from a roofline point of view, but challenging from an utilization
    point of view. Smaller tensor cores, on the other hand, are easy to keep well
    utilized both spatially and temporally. Yet, they can only exploit limited data
    reuse with low arithmetic intensity, resulting in a lower peak array performance.
    Such a system would have a lower roofline, but which can be approached quite closely
    across more workloads.


    Similarly, architectural supports for sparsity inevitably cause overheads in the
    computational and memory-accessing control flow, yielding reduced rooflines. However,
    when sparse operations are analyzed as no-operations, such architectures yield
    substantially higher utilization compared to architectures with no sparsity support,
    where the no-operations directly result in utilization loss. In this way, roofline
    models expose the benefits of low overhead approaches to sparsity support within
    an architecture (as discussed in Section [III-D](#page-8-2) and Figure 15), by
    minimally reducing the rooflines while enabling high workload utilization (operating
    points near the rooflines).


    #### *C. Flexibility (programmability) versus specialization*


    All the previous sections have described approaches that involve some degree of
    tailoring hardware to the ML application domain. Hence, the game of increasing
    efficiency is mostly a game of specialization. The multi-order-of-magnitude efficiency
    boost associated with domain specialization has been known for a long time, well
    before the ML hardware explosion [\[76\]](#page-18-2), [\[77\]](#page-18-3). However,
    there are two unique differentiating factors associated with AI that favor specialization.
    First, the application markets are huge, leading to large potential production
    volumes and margins, which can justify the non-recurring engineering cost of a
    specialized architecture. Second, most ML models are dominated, from a computational
    standpoint, by relatively few kernels, namely, linear and few non-linear tensor
    operators [\[78\]](#page-18-4). Furthermore, for all deep neural models the arithmetic
    precision needed for these operators can be much reduced, as seen in section [III-C.](#page-6-2)
    The combination of these factors has led to a "specialization gold rush" with
    a proliferation of ultra-specialized accelerators.


    It is important to note however, that even in this unique context, hyper-specialization
    is a dangerous slope, with the pitfall that aggressively specialized NPU architectures
    may achieve the lowest energy-per-operation at peak utilization, but are subject
    to under-utilization when workloads change and evolve [\[78\]](#page-18-4). Hence,
    a poorly utilized, ultra-specialized NPU can become less efficient at the workload
    level than a highly utilized, but less specialized, architecture (e.g. a GP-GPU).
    If we visualize workloads as points on the roofline plot we ideally would like
    our architecture to have as many points as close as possible to the roofline.


    A concrete example of such workload evolution is the recent transition of leading-edge
    models from convolutional neural networks to attention-based networks (transformers).
    From the computational standpoint the key operator in CNNs, namely the N-channel
    2-D convolution, is a high-locality kernel exemplified in Figure [13.](#page-8-0)
    Convolution can be transformed in general Matrix multiplication through a Toeplitz
    transformation, with a non-negligible memory inflation [\[79\]](#page-18-5). Hence,
    several acceleration engines have been designed for executing convolutions natively,
    with no storage overhead. However, the now widespread transformers are dominated
    by matrix multiplication in the attention block, thus hardware that is specifically
    designed for convolutions faces critical limitations.


    Achieving a high utilization across workloads, including unknown future ML workloads,
    will hence require flexible architectures, capable of adjusting at compile-time
    or runtime to the workload characteristics. Although it is difficult to quantify
    flexibility, without knowing the complete kernel space of interest, recent work
    defined 4 possible axis of flexibility, denoted as TOPS [\[80\]](#page-18-6):
    Tile size (= sizes of the temporal loops); Order (= relative ordering of the temporal
    loops); Parallelization (= which dimensions to unroll in the spatial loops); and
    the Shape (= sizes of the spatial loops). These metrics allow to compare the flexibility
    of existing and envisioned accelerators in terms of their GeMM support.


    Workloads can, however, evolve in other directions than purely in terms of their
    GeMM operations with new alternative operators popping us. This leads us to another
    significant risk factor in specialization linked to the well-known "Amdhal effect":
    if a fraction of a workload f < 1 cannot be accelerated, the end-to-end speedup
    that can be achieved by the best ideal accelerated architecture is upper-bounded
    by 1/f. Hence, to achieve high end-to-end speedup, a NPU has to be flexible enough
    to accelerate a dominant fraction of the computational kernels in a neural network,
    to ensure that f << 1. If these kernels are not homogeneous, multiple "sub-accelerators"
    may be needed within the NPU. Dimensioning to ensure high utilization for all
    sub-accelerators, as required to achieve high efficiency, is a difficult balancing
    act, since silicon area is always limited. Several architectures are being explored
    to address the accelerator utilization challenge [\[78\]](#page-18-4), and this
    is an active area of research: the sweet spot between efficiency and flexibility
    depends not only on workload, but also on technology parameters, such as leakage
    per unit-area, which grows with technology scaling, penalizing under-utilization
    not only from the area, but also from the power viewpoint.


    In terms of the roofline model, flexible architectures incur the typically substantial
    overhead of control flow optimized for software extensibility. This has the impact
    of reducing both the compute and memory-accessing rooflines. The benefits of such
    architectures, on the other hand, become apparent from the utilization achieved
    across a diverse set of workloads (i.e., workload operating points near the roofline).
    Since the architectural overheads of software-optimized control flow are typically
    large, judicious selection of the target workload set is often necessary, making
    ML accelerator design an evolving and application-driven exercise.


    # V. CONCLUSION AND OPEN CHALLENGES: HOW TO KEEP PUSHING ML ACCELERATOR PERFORMANCE?


    The past ten years has seen dramatic enhancements in ML acceleration, amounting
    to roughly 1000× increase in throughput and energy efficiency. These have come
    from two synergistic sources: (1) hardware improvements, at the technological,
    circuit, and architectural levels; (2) algorithmic improvements, in the form of
    number formats and model architectures, which have in turn been leveraged to their
    full extent by the integration of hardware supports.


    Today, we see that both of these are hitting limits, bound in one case by the
    fundamental underlying technologies and in the other case by ultimate task-level
    accuracies. This has placed ML accelerator design squarely in a regime of balancing
    across critical trade-offs, spanning peak efficiency, peak throughput, achievable
    utilization, and workload flexibility. This has made it essential to understand
    the inter-related tradeoffs, in order to effectively design ML accelerators for
    the diverse range of system deployments being envisioned. A key tool for reasoning
    about these trade-offs has been the roofline model, which offers insight into
    how compute performance and memory bandwidth constraints shape accelerator efficiency.
    As ML workloads diversify, accelerator architectures must be designed to not only
    push peak performance (raising the rooflines) but also ensure high utilization
    across a range of workloads (operating near the rooflines).


    Looking beyond today, the rich range of research makes a number of emerging technologies,
    across computation, memory, and networking, as well as emerging algorithmic and
    model architecture innovations, likely to continue to fuel enhancements in ML
    acceleration. Indeed, maximally leveraging these within future ML accelerators,
    at the rapid pace with which they are emerging, also makes it essential to understand
    the trade-offs covered in this paper.


    While a complete survey of emerging technologies is beyond the scope of this paper,
    a few examples can help to illustrate how such technologies may influence the
    tradeoffs discussed. For instance, chiplets, and their associated technologies
    (hybrid integration, die-to-die interconnects), will provide promising pathways
    for scaling compute parallelism and memory bandwidth, and as such raise the rooflines.
    This can be further extended to highly-distributed systems, across many compute
    and memory nodes, where next-generation interconnect technologies, such as optical
    communication. Also leveraging emerging memories can provide greater density and
    enables scale-up of memories to address capacity requirements with ever-increasing
    model sizes. Yet again, the challenge is not only to make the roofline as high
    as possible, but also to operate close to it. Fortunately, such scale out could
    also raise data reuse opportunities and introduce arithmetic intensity relaxations
    between nodes and clusters, which may make new and existing memory technologies
    relevant, in repurposed forms and with optimized interfaces.


    In addition to interconnect and memory technologies, new computing technologies
    will also play a key role. Analog computation, particularly in specific forms
    of in-memory computing, is already showing direct relevance for next-generation
    systems. Its potential is currently being explored on a number of other fronts,
    including alongside new device and materials innovations.


    As mentioned, however, in complement to hardware innovations, maximizing utilization
    of architectures, will remain an important challenge and point of system leverage.
    This will be achievable by more tightly integrating accelerator performance models
    with compiler technology. Rapid development cycles in both hardware and software
    suffer from the current need to micro-code kernels in order to achieve satisfactory
    utilization. In this context, MLIR (Multi-Level Intermediate Representation [\[81\]](#page-18-7)),
    can form an effective abstraction framework for the development and integration
    of tools towards rapid processor emulation and code tuning


    In the end, designing hardware for machine learning will always remain a balancing
    game: Maximizing total system efficiency and throughput, while maintaining sufficient
    flexibility to be prepared for the continuous stream of innovations in ML algorithms.
    The danger is that new powerful algorithms are of such different nature, that
    they do not match well on today''s processor architectures, preventing their breakthrough.
    To avoid this hardware lottery [\[82\]](#page-18-8), we need to find a way to
    not only streamline the design and programming of novel ML accelerator platforms,
    but also their fabrication. The recent trend towards using chiplet technology
    not only for achieving scale-out with good yield, but also toward rapid customization
    seems promising. Future will tell whether this will allows us to again enable
    the next wave of ML algorithms.


    #### REFERENCES


    - <span id="page-15-0"></span>[1] G. Anil, "Visualizing size of large language
    models," 2023, accessed: 2024-10-10. [Online]. Available: [https://medium.com/@georgeanil/](https://medium.com/@georgeanil/visualizing-size-of-large-language-models-ec576caa5557)
    [visualizing-size-of-large-language-models-ec576caa5557](https://medium.com/@georgeanil/visualizing-size-of-large-language-models-ec576caa5557)

    - <span id="page-15-1"></span>[2] B. Dally, "Trends in deep learning hardware,"
    2024, talk, presented by NVIDIA.

    - <span id="page-16-0"></span>[3] S. Williams, A. Waterman, and D. Patterson,
    "Roofline: an insightful visual performance model for multicore architectures,"
    *Communications of the ACM*, vol. 52, no. 4, pp. 65–76, 2009.

    - <span id="page-16-1"></span>[4] Y.-H. Chen, J. Emer, and V. Sze, "Eyeriss: A
    apatial architecture for energy-efficient dataflow for convolutional neural networks,"
    in *2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA)*, 2016, pp. 367–379.

    - <span id="page-16-2"></span>[5] H. Prashanth and M. Rao, "Roofline performance
    analysis of dnn architectures on cpu and gpu systems," in *2024 25th International
    Symposium on Quality Electronic Design (ISQED)*. IEEE, 2024, pp. 1–8.

    - <span id="page-16-3"></span>[6] S. W. Williams, *Book: The roofline model*.
    University of California, 2010.

    - <span id="page-16-4"></span>[7] Z. Yuan, Y. Shang, Y. Zhou, Z. Dong, C. Xue,
    B. Wu, Z. Li, Q. Gu, Y. J. Lee, Y. Yan *et al.*, "Llm inference unveiled: Survey
    and roofline model insights," *arXiv preprint arXiv:2402.16363*, 2024.

    - <span id="page-16-5"></span>[8] H. Kwon, P. Chatarasi, M. Pellauer, A. Parashar,
    V. Sarkar, and T. Krishna, "Understanding reuse, performance, and hardware cost
    of dnn dataflow: A data-centric approach," in *Proceedings of the 52nd Annual
    IEEE/ACM International Symposium on Microarchitecture*, 2019, pp. 754–768.

    - <span id="page-16-6"></span>[9] J. W. Choi, D. Bedard, R. Fowler, and R. Vuduc,
    "A roofline model of energy," in *2013 IEEE 27th International Symposium on Parallel
    and Distributed Processing*. IEEE, 2013, pp. 661–672.

    - <span id="page-16-7"></span>[10] M. Pellauer, J. Clemons, V. Balaji, N. Crago,
    A. Jaleel, D. Lee, M. O''Connor, A. Parashar, S. Treichler, P.-A. Tsai *et al.*,
    "Symphony: Orchestrating sparse and dense tensors with hierarchical heterogeneous
    processing," *ACM Transactions on Computer Systems*, vol. 41, no. 1-4, pp. 1–30,
    2023.

    - <span id="page-16-8"></span>[11] T. P. Morgan, "Lots of questions on Google''s
    "Trillium" TPU v6, a few answers," Oct 2024. [Online]. Available: [https://www.nextplatform.com/2024/06/10/](https://www.nextplatform.com/2024/06/10/lots-of-questions-on-googles-trillium-tpu-v6-a-few-answers/)
    [lots-of-questions-on-googles-trillium-tpu-v6-a-few-answers/](https://www.nextplatform.com/2024/06/10/lots-of-questions-on-googles-trillium-tpu-v6-a-few-answers/)

    - <span id="page-16-9"></span>[12] B. Moons, R. Uytterhoeven, W. Dehaene, and
    M. Verhelst, "Envision: A 0.26-to-10tops/w subword-parallel dynamic-voltage-accuracyfrequency-scalable
    convolutional neural network processor in 28nm fdsoi," in *2017 IEEE International
    Solid-State Circuits Conference (ISSCC)*. IEEE, 2017, pp. 246–247.

    - <span id="page-16-10"></span>[13] J.-S. Park, J.-W. Jang, H. Lee, D. Lee, S.
    Lee, H. Jung, S. Lee, S. Kwon, K. Jeong, J.-H. Song *et al.*, "9.5 a 6k-mac feature-map-sparsity-aware
    neural processing unit in 5nm flagship mobile soc," in *2021 IEEE International
    Solid-State Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021, pp. 152–154.

    - <span id="page-16-11"></span>[14] E. Talpes, D. D. Sarma, G. Venkataramanan,
    P. Bannon, B. McGee, B. Floering, A. Jalote, C. Hsiong, S. Arora, A. Gorti *et
    al.*, "Compute solution for tesla''s full self-driving computer," *IEEE Micro*,
    vol. 40, no. 2, pp. 25–35, 2020.

    - <span id="page-16-12"></span>[15] Y. Jiao, L. Han, R. Jin, Y.-J. Su, C. Ho,
    L. Yin, Y. Li, L. Chen, Z. Chen, L. Liu *et al.*, "7.2 a 12nm programmable convolution-efficient
    neuralprocessing-unit chip achieving 825tops," in *2020 IEEE International Solid-State
    Circuits Conference-(ISSCC)*. IEEE, 2020, pp. 136–140.

    - [16] L. Gwennap, "Groq rocks neural networks," *Microprocessor Report, Tech.
    Rep., jan*, 2020.

    - [17] A. Agrawal, S. K. Lee, J. Silberman, M. Ziegler, M. Kang, S. Venkataramani,
    N. Cao, B. Fleischer, M. Guillorn, M. Cohen, S. Mueller, J. Oh, M. Lutz, J. Jung,
    S. Koswatta, C. Zhou, V. Zalani, J. Bonanno, R. Casatuta, C.-Y. Chen, J. Choi,
    H. Haynie, A. Herbert, R. Jain, M. Kar, K.-H. Kim, Y. Li, Z. Ren, S. Rider, M.
    Schaal, K. Schelm, M. Scheuermann, X. Sun, H. Tran, N. Wang, W. Wang, X. Zhang,
    V. Shah, B. Curran, V. Srinivasan, P.-F. Lu, S. Shukla, L. Chang, and K. Gopalakrishnan,
    "9.1 a 7nm 4-core ai chip with 25.6tflops hybrid fp8 training, 102.4tops int4
    inference and workload-aware throttling," in *2021 IEEE International Solid-State
    Circuits Conference (ISSCC)*, vol. 64, 2021, pp. 144–146.

    - [18] G. Desoli, N. Chawla, T. Boesch, M. Avodhyawasi, H. Rawat, H. Chawla, V.
    Abhijith, P. Zambotti, A. Sharma, C. Cappetta, M. Rossi, A. De Vita, and F. Girardi,
    "16.7 a 40-310tops/w sram-based all-digital up to 4b in-memory computing multi-tiled
    nn accelerator in fd-soi 18nm for deep-learning edge applications," in *2023 IEEE
    International Solid-State Circuits Conference (ISSCC)*, 2023, pp. 260–262.

    - <span id="page-16-13"></span>[19] J. Zhuang, J. Lau, H. Ye, Z. Yang, Y. Du,
    J. Lo, K. Denolf, S. Neuendorffer, A. Jones, J. Hu, D. Chen, J. Cong, and P. Zhou,
    "Charm: Composing heterogeneous accelerators for matrix multiply on versal acap
    architecture," in *Proceedings of the 2023 ACM/SIGDA International Symposium on
    Field Programmable Gate Arrays*, ser. FPGA ''23. New York, NY, USA: Association
    for Computing Machinery, 2023, p. 153–164. [Online]. Available: <https://doi.org/10.1145/3543622.3573210>

    - <span id="page-16-14"></span>[20] H. Liao, J. Tu, J. Xia, and X. Zhou, "Davinci:
    A scalable architecture for neural network computing," in *2019 IEEE Hot Chips
    31 Symposium (HCS)*. IEEE Computer Society, 2019, pp. 1–44.

    - <span id="page-16-15"></span>[21] S. Markidis, S. W. Der Chien, E. Laure, I.
    B. Peng, and J. S. Vetter, "Nvidia tensor core programmability, performance &
    precision," in *2018 IEEE international parallel and distributed processing symposium
    workshops (IPDPSW)*. IEEE, 2018, pp. 522–531.

    - <span id="page-16-16"></span>[22] H. Wang, R. Liu, R. Dorrance, D. Dasalukunte,
    D. Lake, and B. Carlton, "A charge domain sram compute-in-memory macro with c-2c
    ladderbased 8-bit mac unit in 22-nm finfet process for edge inference," *IEEE
    Journal of Solid-State Circuits*, vol. 58, no. 4, pp. 1037–1050, 2023.

    - [23] I. A. Papistas, S. Cosemans, B. Rooseleer, J. Doevenspeck, M.-H. Na, A.
    Mallik, P. Debacker, and D. Verkest, "A 22 nm, 1540 top/s/w, 12.1 top/s/mm 2 in-memory
    analog matrix-vector-multiplier for dnn acceleration," in *2021 IEEE Custom Integrated
    Circuits Conference (CICC)*. IEEE, 2021, pp. 1–2.

    - <span id="page-16-17"></span>[24] H. Valavi, P. J. Ramadge, E. Nestler, and
    N. Verma, "A 64-tile 2.4 mb in-memory-computing cnn accelerator employing charge-domain
    compute," *IEEE Journal of Solid-State Circuits*, vol. 54, no. 6, pp. 1789– 1799,
    2019.

    - <span id="page-16-18"></span>[25] E. Talpes, D. D. Sarma, G. Venkataramanan,
    P. Bannon, B. McGee, B. Floering, A. Jalote, C. Hsiong, S. Arora, A. Gorti, and
    G. S. Sachdev, "Compute Solution for Tesla''s Full Self-Driving Computer," *IEEE
    Micro*, vol. 40, no. 2, pp. 25–35, 2020.

    - <span id="page-16-19"></span>[26] B. Dally, "Hardware for deep learning," in
    *IEEE Hot Chips Symposium (HCS)*, vol. 35. IEEE, 2023, pp. 1–58.

    - <span id="page-16-20"></span>[27] A. Reuther, P. Michaleas, M. Jones, V. Gadepally,
    S. Samsi, and J. Kepner, "Lincoln ai computing survey (laics) update," in *2023
    IEEE High Performance Extreme Computing Conference (HPEC)*. IEEE, 2023, pp. 1–7.

    - [28] K. Guo, W. Li, K. Zhong, Z. Zhu, S. Zeng, T. Xie, S. Han, Y. Xie, P. Debacker,
    M. Verhelst, and Y. Wang, "Neural network accelerator comparison." [Online]. Available:
    [https://nicsefc.ee.tsinghua.](https://nicsefc.ee.tsinghua.edu.cn/project.html)
    [edu.cn/project.html](https://nicsefc.ee.tsinghua.edu.cn/project.html)

    - <span id="page-16-21"></span>[29] Z. Yuan, Y. Shang, Y. Zhou, Z. Dong, Z. Zhou,
    C. Xue, B. Wu, Z. Li, Q. Gu, Y. J. Lee *et al.*, "Llm inference unveiled: Survey
    and roofline model insights," *arXiv preprint arXiv:2402.16363*, 2024.

    - <span id="page-16-22"></span>[30] L. Bertaccini, G. Paulin, M. Cavalcante, T.
    Fischer, S. Mach, and L. Benini, "Minifloats on risc-v cores: Isa extensions with
    mixedprecision short dot products," *IEEE Transactions on Emerging Topics in Computing*,
    2024.

    - <span id="page-16-23"></span>[31] M. Scherer, G. Rutishauser, L. Cavigelli,
    and L. Benini, "Cutie: Beyond petaop/s/w ternary dnn inference acceleration with
    better-than-binary energy efficiency," *IEEE Transactions on Computer-Aided Design
    of Integrated Circuits and Systems*, vol. 41, no. 4, pp. 1020–1033, 2021.

    - <span id="page-16-24"></span>[32] B. Moons, D. Bankman, L. Yang, B. Murmann,
    and M. Verhelst, "Binareye: An always-on energy-accuracy-scalable binary cnn processor
    with all memory on chip in 28nm cmos," in *2018 IEEE Custom Integrated Circuits
    Conference (CICC)*. IEEE, 2018, pp. 1–4.

    - <span id="page-16-25"></span>[33] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang,
    L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei, "Bitnet: Scaling 1-bit transformers
    for large language models," *arXiv preprint arXiv:2310.11453*, 2023.

    - <span id="page-16-26"></span>[34] A. Nadalini, G. Rutishauser, A. Burrello,
    N. Bruschi, A. Garofalo, L. Benini, F. Conti, and D. Rossi, "A 3 tops/w risc-v
    parallel cluster for inference of fine-grain mixed-precision quantized neural
    networks," in *2023 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)*.
    IEEE, 2023, pp. 1–6.

    - <span id="page-16-27"></span>[35] F. Conti, G. Paulin, A. Garofalo, D. Rossi,
    A. Di Mauro, G. Rutishauser, G. Ottavi, M. Eggiman, H. Okuhara, and L. Benini,
    "Marsellus: A heterogeneous risc-v ai-iot end-node soc with 2–8 b dnn acceleration
    and 30%-boost adaptive body biasing," *IEEE Journal of Solid-State Circuits*,
    vol. 59, no. 1, pp. 128–142, 2023.

    - <span id="page-16-28"></span>[36] B. D. Rouhani, R. Zhao, A. More, M. Hall,
    A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf *et
    al.*, "Microscaling data formats for deep learning," *arXiv preprint arXiv:2310.10537*,
    2023.

    - <span id="page-16-29"></span>[37] A. Tirumala and R. Wong, "Nvidia blackwell
    platform: Advancing generative ai and accelerated computing," in *2024 IEEE Hot
    Chips 36 Symposium (HCS)*, 2024, pp. 1–33.

    - <span id="page-16-30"></span>[38] A. S. Prasad, M. Scherer, F. Conti, D. Rossi,
    A. Di Mauro, M. Eggimann, J. T. Gomez, Z. Li, S. S. Sarwar, Z. Wang ´ *et al.*,
    "Siracusa: A 16 nm heterogenous risc-v soc for extended reality with at-mram neural
    engine," *IEEE Journal of Solid-State Circuits*, 2024.

    - <span id="page-16-31"></span>[39] K. Koul, M. Strange, J. Melchert, A. Carsello,
    Y. Mei, O. Hsu, T. Kong, P.-H. Chen, H. Ke, K. Zhang *et al.*, "Onyx: A 12nm 756
    gops/w coarse-grained reconfigurable array for accelerating dense and sparse applications,"
    in *2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)*.
    IEEE, 2024, pp. 1–2.

    - <span id="page-17-0"></span>[40] A. Zhou, Y. Ma, J. Zhu, J. Liu, Z. Zhang, K.
    Yuan, W. Sun, and H. Li, "Learning n: m fine-grained structured sparse neural
    networks from scratch," *arXiv preprint arXiv:2102.04010*, 2021.

    - <span id="page-17-1"></span>[41] J. Choquette, E. Lee, R. Krashinsky, V. Balan,
    and B. Khailany, "3.2 the a100 datacenter gpu and ampere architecture," in *2021
    IEEE International Solid-State Circuits Conference (ISSCC)*, vol. 64. IEEE, 2021,
    pp. 48–50.

    - <span id="page-17-2"></span>[42] R. L. Castro, A. Ivanov, D. Andrade, T. Ben-Nun,
    B. B. Fraguela, and T. Hoefler, "Venom: A vectorized n: M format for unleashing
    the power of sparse tensor cores," in *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis*, 2023, pp. 1–14.

    - <span id="page-17-3"></span>[43] P. Scheffler, T. Benz, V. Potocnik, T. Fischer,
    L. Colagrande, N. Wistoff, Y. Zhang, L. Bertaccini, G. Ottavi, M. Eggimann, M.
    Cavalcante, G. Paulin, F. K. Gurkaynak, D. Rossi, and L. Benini, "Occamy: A ¨
    432-core dual-chiplet dual-hbm2e 768-dp-gflop/s risc-v system for 8 to-64-bit
    dense and sparse computing in 12-nm finfet," *IEEE Journal of Solid-State Circuits*,
    vol. Early Access, pp. 1–15, 2025.

    - <span id="page-17-4"></span>[44] G. Heo, S. Lee, J. Cho, H. Choi, S. Lee, H.
    Ham, G. Kim, D. Mahajan, and J. Park, "Neupims: Npu-pim heterogeneous acceleration
    for batched llm inferencing," in *Proceedings of the 29th ACM International Conference
    on Architectural Support for Programming Languages and Operating Systems, Volume
    3*, ser. ASPLOS ''24. New York, NY, USA: Association for Computing Machinery,
    2024, p. 722-737. [Online]. Available:<https://doi.org/10.1145/3620666.3651380>

    - <span id="page-17-5"></span>[45] J. Alsop, S. Aga, M. Ibrahim, M. Islam, A.
    Mccrabb, and N. Jayasena, "Inclusive-pim: Hardware-software co-design for broad
    acceleration on commercial pim architectures," 2024. [Online]. Available:<https://arxiv.org/abs/2309.07984>

    - <span id="page-17-6"></span>[46] J. Zhang, Z. Wang, and N. Verma, "In-memory
    computation of a machine-learning classifier in a standard 6t sram array," *IEEE
    Journal of Solid-State Circuits*, vol. 52, no. 4, pp. 915–924, 2017.

    - <span id="page-17-7"></span>[47] M. Kang, S. K. Gonugondla, M.-S. Keel, and
    N. R. Shanbhag, "An energy-efficient memory-based high-throughput vlsi architecture
    for convolutional networks," in *2015 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2015, pp. 1037– 1041.

    - <span id="page-17-8"></span>[48] X. Guo, F. M. Bayat, M. Bavandpour, M. Klachko,
    M. R. Mahmoodi, M. Prezioso, K. K. Likharev, and D. B. Strukov, "Fast, energy-efficient,
    robust, and reproducible mixed-signal neuromorphic classifier based on embedded
    nor flash memory technology," in *2017 IEEE International Electron Devices Meeting
    (IEDM)*, 2017, pp. 6.5.1–6.5.4.

    - <span id="page-17-9"></span>[49] L. Fick, D. Blaauw, D. Sylvester, S. Skrzyniarz,
    M. Parikh, and D. Fick, "Analog in-memory subthreshold deep neural network accelerator,"
    in *2017 IEEE Custom Integrated Circuits Conference (CICC)*, 2017, pp. 1–4.

    - <span id="page-17-10"></span>[50] H. Fujiwara, H. Mori, W.-C. Zhao, M.-C. Chuang,
    R. Naous, C.-K. Chuang, T. Hashizume, D. Sun, C.-F. Lee, K. Akarvardar, S. Adham,
    T.- L. Chou, M. E. Sinangil, Y. Wang, Y.-D. Chih, Y.-H. Chen, H.-J. Liao, and
    T.-Y. J. Chang, "A 5-nm 254-tops/w 221-tops/mm2 fully-digital computing-in-memory
    macro supporting wide-range dynamic-voltagefrequency scaling and simultaneous
    mac and write operations," in *2022 IEEE International Solid-State Circuits Conference
    (ISSCC)*, vol. 65, 2022, pp. 1–3.

    - <span id="page-17-11"></span>[51] Y.-D. Chih, P.-H. Lee, H. Fujiwara, Y.-C.
    Shih, C.-F. Lee, R. Naous, Y.-L. Chen, C.-P. Lo, C.-H. Lu, H. Mori, W.-C. Zhao,
    D. Sun, M. E. Sinangil, Y.-H. Chen, T.-L. Chou, K. Akarvardar, H.-J. Liao, Y.
    Wang, M.-F. Chang, and T.-Y. J. Chang, "16.4 an 89tops/w and 16.3tops/mm2 all-digital
    sram-based full-precision compute-in memory macro in 22nm for machine-learning
    edge applications," in *2021 IEEE International Solid-State Circuits Conference
    (ISSCC)*, vol. 64, 2021, pp. 252–254.

    - <span id="page-17-12"></span>[52] P. Deaville, B. Zhang, L.-Y. Chen, and N.
    Verma, "A maximally rowparallel mram in-memory-computing macro addressing readout
    circuit sensitivity and area," in *ESSCIRC 2021 - IEEE 47th European Solid State
    Circuits Conference (ESSCIRC)*, 2021, pp. 75–78.

    - <span id="page-17-13"></span>[53] H. Jia, H. Valavi, Y. Tang, J. Zhang, and
    N. Verma, "A programmable heterogeneous microprocessor based on bit-scalable in-memory
    computing," *IEEE Journal of Solid-State Circuits*, vol. 55, no. 9, pp. 2609–2621,
    2020.

    - <span id="page-17-14"></span>[54] S. Jung, H. Lee, S. Myung, H. Kim, S. K. Yoon,
    S.-W. Kwon, Y. Ju, M. Kim, W. Yi, S. Han, B. Kwon, B. Seo, K. Lee, G.-H. Koh,
    K. Lee, Y. Song, C. Choi, D. Ham, and S. J. Kim, "A crossbar array of magnetoresistive
    memory devices for in-memory computing," *Nature*, vol. 601, no. 7892, pp. 211–216,
    2022. [Online]. Available: <https://doi.org/10.1038/s41586-021-04196-6>

    - <span id="page-17-15"></span>[55] N. Verma, H. Jia, H. Valavi, Y. Tang, M. Ozatay,
    L.-Y. Chen, B. Zhang,


    and P. Deaville, "In-memory computing: Advances and prospects," *IEEE Solid-State
    Circuits Magazine*, vol. 11, no. 3, pp. 43–55, 2019.


    - <span id="page-17-16"></span>[56] J. Wang, X. Wang, C. Eckert, A. Subramaniyan,
    R. Das, D. Blaauw, and D. Sylvester, "14.2 a compute sram with bit-serial integer/floating-point
    operations for programmable in-memory vector acceleration," in *2019 IEEE International
    Solid-State Circuits Conference - (ISSCC)*, 2019, pp. 224–226.

    - <span id="page-17-17"></span>[57] S. D. Spetalnick, M. Chang, B. Crafton, W.-S.
    Khwa, Y.-D. Chih, M.-F. Chang, and A. Raychowdhury, "A 40nm 64kb 26.56tops/w 2.37mb/mm2rram
    binary/compute-in-memory macro with 4.23x improvement in density and > 75% use
    of sensing dynamic range," in *2022 IEEE International Solid-State Circuits Conference
    (ISSCC)*, vol. 65, 2022, pp. 1–3.

    - <span id="page-17-18"></span>[58] S. K. Gonugondla, C. Sakr, H. Dbouk, and N.
    R. Shanbhag, "Fundamental limits on energy-delay-accuracy of in-memory architectures
    in inference applications," *IEEE Transactions on Computer-Aided Design of Integrated
    Circuits and Systems*, vol. 41, no. 10, pp. 3188–3201, 2022.

    - <span id="page-17-19"></span>[59] P. A. Hager, B. Moons, S. Cosemans, I. A.
    Papistas, B. Rooseleer, J. V. Loon, R. Uytterhoeven, F. Zaruba, S. Koumousi, M.
    Stanisavljevic, S. Mach, S. Mutsaards, R. K. Aljameh, G. H. Khov, B. Machiels,
    C. Olar, A. Psarras, S. Geursen, J. Vermeeren, Y. Lu, A. Maringanti, D. Ameta,
    L. Katselas, N. Hutter, M. Schmuck, S. Sivadas, K. Sharma, ¨ M. Oliveira, R. Aerne,
    N. Sharma, T. Soni, B. Bussolino, D. Pesut, M. Pallaro, A. Podlesnii, A. Lyrakis,
    Y. Ruffiner, M. Dazzi, J. Thiele, K. Goetschalckx, N. Bruschi, J. Doevenspeck,
    B. Verhoef, S. Linz, G. Garcea, J. Ferguson, I. Koltsidas, and E. Eleftheriou,
    "11.3 metis aipu: A 12nm 15tops/w 209.6tops soc for cost- and energy-efficient
    inference at the edge," in *2024 IEEE International Solid-State Circuits Conference
    (ISSCC)*, vol. 67, 2024, pp. 212–214.

    - <span id="page-17-20"></span>[60] N. R. Shanbhag and S. K. Roy, "Benchmarking
    in-memory computing architectures," *IEEE Open Journal of the Solid-State Circuits
    Society*, vol. 2, pp. 288–300, 2022.

    - <span id="page-17-21"></span>[61] P. Deaville, B. Zhang, and N. Verma, "A 22nm
    128-kb mram row/column-parallel in-memory computing macro with memoryresistance
    boosting and multi-column adc readout," in *2022 IEEE Symposium on VLSI Technology
    and Circuits (VLSI Technology and Circuits)*, 2022, pp. 268–269.

    - <span id="page-17-22"></span>[62] M. Le Gallo, R. Khaddam-Aljameh, M. Stanisavljevic,
    A. Vasilopoulos, B. Kersting, M. Dazzi, G. Karunaratne, M. Brandli, A. Singh,
    ¨ S. M. Muller, J. B ¨ uchel, X. Timoneda, V. Joshi, M. J. Rasch, ¨ U. Egger,
    A. Garofalo, A. Petropoulos, T. Antonakopoulos, K. Brew, S. Choi, I. Ok, T. Philip,
    V. Chan, C. Silvestre, I. Ahsan, N. Saulnier, V. Narayanan, P. A. Francese, E.
    Eleftheriou, and A. Sebastian, "A 64-core mixed-signal in-memory compute chip
    based on phase-change memory for deep neural network inference," *Nature Electronics*,
    vol. 6, no. 9, pp. 680–693, 2023. [Online]. Available: <https://doi.org/10.1038/s41928-023-01010-1>

    - <span id="page-17-23"></span>[63] C.-C. Chou, Z.-J. Lin, P.-L. Tseng, C.-F.
    Li, C.-Y. Chang, W.-C. Chen, Y.-D. Chih, and T.-Y. J. Chang, "An n40 256k×44 embedded
    rram macro with sl-precharge sa and low-voltage current limiter to improve read
    and write performance," in *2018 IEEE International Solid-State Circuits Conference
    - (ISSCC)*, 2018, pp. 478–480.

    - <span id="page-17-24"></span>[64] D. Shum, D. Houssameddine, S. T. Woo, Y. S.
    You, J. Wong, K. W. Wong, C. C. Wang, K. H. Lee, K. Yamane, V. B. Naik, C. S.
    Seet, T. Tahmasebi, C. Hai, H. W. Yang, N. Thiyagarajah, R. Chao, J. W. Ting,
    N. L. Chung, T. Ling, T. H. Chan, S. Y. Siah, R. Nair, S. Deshpande, R. Whig,
    K. Nagel, S. Aggarwal, M. DeHerrera, J. Janesky, M. Lin, H.-J. Chia, M. Hossain,
    H. Lu, S. Ikegawa, F. B. Mancoff, G. Shimon, J. M. Slaughter, J. J. Sun, M. Tran,
    S. M. Alam, and T. Andre, "Cmosembedded stt-mram arrays in 2x nm nodes for gp-mcu
    applications," in *2017 Symposium on VLSI Technology*, 2017, pp. T208–T209.

    - <span id="page-17-25"></span>[65] J. Lee, B. Zhang, and N. Verma, "A switched-capacitor
    sram in-memory computing macro with high-precision, high-efficiency differential
    architecture," in *2024 European Conference on Solid-State Circuits*, 2024.

    - <span id="page-17-26"></span>[66] H. Jia, M. Ozatay, Y. Tang, H. Valavi, R.
    Pathak, J. Lee, and N. Verma, "Scalable and Programmable Neural Network Inference
    Accelerator Based on In-Memory Computing," *IEEE Journal of Solid-State Circuits*,
    vol. 57, no. 1, pp. 198–211, 2022.

    - <span id="page-17-27"></span>[67] X. Yang, M. Gao, Q. Liu, J. Setter, J. Pu,
    A. Nayak, S. Bell, K. Cao, H. Ha, P. Raina, C. Kozyrakis, and M. Horowitz, "Interstellar:
    Using Halide''s Scheduling Language to Analyze DNN Accelerators," in *Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems*, ser. ASPLOS ''20. New York, NY, USA: Association
    for Computing Machinery, 2020, p. 369–383. [Online]. Available: <https://doi.org/10.1145/3373376.3378514>

    - [68] H. Kwon, P. Chatarasi, V. Sarkar, T. Krishna, M. Pellauer, and A. Parashar,
    "MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware
    Cost of DNN Mappings," *IEEE Micro*, vol. 40, no. 3, pp. 20–29, 2020.

    - [69] A. Parashar, P. Raina, Y. S. Shao, Y.-H. Chen, V. A. Ying, A. Mukkara,
    R. Venkatesan, B. Khailany, S. W. Keckler, and J. Emer, "Timeloop: A Systematic
    Approach to DNN Accelerator Evaluation," in *2019 IEEE International Symposium
    on Performance Analysis of Systems and Software (ISPASS)*, 2019, pp. 304–315.

    - [70] L. Mei, P. Houshmand, V. Jain, S. Giraldo, and M. Verhelst, "ZigZag: Enlarging
    Joint Architecture-Mapping Design Space Exploration for DNN Accelerators," *IEEE
    Transactions on Computers*, vol. 70, no. 8, pp. 1160–1174, 2021.

    - [71] Q. Huang, A. Kalaiah, M. Kang, J. Demmel, G. Dinh, J. Wawrzynek, T. Norell,
    and Y. S. Shao, "CoSA: Scheduling by constrained optimization for spatial accelerators,"
    in *2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
    (ISCA)*, 2021, pp. 554–566.

    - [72] K. Hegde, P.-A. Tsai, S. Huang, V. Chandra, A. Parashar, and C. W. Fletcher,
    "Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space Search,"
    in *Proceedings of the 26th ACM International Conference on Architectural Support
    for Programming Languages and Operating Systems*, ser. ASPLOS 2021. New York,
    NY, USA: Association for Computing Machinery, 2021, p. 943–958. [Online]. Available:<https://doi.org/10.1145/3445814.3446762>

    - [73] S.-C. Kao and T. Krishna, "GAMMA: Automating the HW Mapping of DNN Models
    on Accelerators via Genetic Algorithm," in *Proceedings of the 39th International
    Conference on Computer-Aided Design*, ser. ICCAD ''20. New York, NY, USA: Association
    for Computing Machinery, 2020. [Online]. Available: [https://doi.org/10.1145/3400302.](https://doi.org/10.1145/3400302.3415639)
    [3415639](https://doi.org/10.1145/3400302.3415639)

    - <span id="page-18-0"></span>[74] A. Symons, L. Mei, S. Colleman, P. Houshmand,
    S. Karl, and M. Verhelst, "Stream: Design space exploration of layer-fused dnns
    on heterogeneous dataflow accelerators," *IEEE Transactions on Computers*, 2024.

    - <span id="page-18-1"></span>[75] D. Abts, J. Kim, G. Kimmell, M. Boyd, K. Kang,
    S. Parmar, A. Ling, A. Bitar, I. Ahmed, and J. Ross, "The groq software-defined
    scale-out tensor streaming multiprocessor : From chips-to-systems architectural
    overview," in *2022 IEEE Hot Chips 34 Symposium (HCS)*, 2022, pp. 1–69.

    - <span id="page-18-2"></span>[76] K. Gotz and T. Noll, "Application specific
    instruction processor based ¨ implementation of a gnss receiver on an fpga," in
    *Design & Test in Europe Conference*, 2006, pp. 58–63.

    - <span id="page-18-3"></span>[77] S. Huang, L. Waeijen, and H. Corporaal, "How
    flexible is your computing system?" *ACM Transactions on Embedded Computing Systems
    (TECS)*, vol. 21, no. 4, pp. 1–41, 2022.

    - <span id="page-18-4"></span>[78] S. Ghodrati, S. Kinzer, H. Xu, R. Mahapatra,
    Y. Kim, B. H. Ahn, D. K. Wang, L. Karthikeyan, A. Yazdanbakhsh, J. Park *et al.*,
    "Tandem processor: Grappling with emerging operators in neural networks," in *Proceedings
    of the 29th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, Volume 2*, 2024, pp. 1165–1182.

    - <span id="page-18-5"></span>[79] M. M. Cho and D. Brand, "Mec: memory-efficient
    convolution for deep neural network," in *ICML-34: Proceedings of the International
    Conference on Machine Learning - Volume 70*. JMLR.org, 2017, p. 815–824.

    - <span id="page-18-6"></span>[80] S.-C. Kao, H. Kwon, M. Pellauer, A. Parashar,
    and T. Krishna, "A formalism of dnn accelerator flexibility," *Proceedings of
    the ACM on Measurement and Analysis of Computing Systems*, vol. 6, no. 2, pp.
    1–23, 2022.

    - <span id="page-18-7"></span>[81] C. Lattner, M. Amini, U. Bondhugula, A. Cohen,
    A. Davis, J. Pienaar, R. Riddle, T. Shpeisman, N. Vasilache, and O. Zinenko, "Mlir:
    Scaling compiler infrastructure for domain specific computation," in *2021 IEEE/ACM
    International Symposium on Code Generation and Optimization (CGO)*. IEEE, 2021,
    pp. 2–14.

    - <span id="page-18-8"></span>[82] S. Hooker, "The hardware lottery," *Communications
    of the ACM*, vol. 64, no. 12, pp. 58–65, 2021.


    ![](_page_18_Picture_16.jpeg)


    Marian Verhelst Marian Verhelst is a professor at the MICAS labs of KU Leuven
    and a research director at imec. Her research focuses on embedded machine learning,
    hardware accelerators, and lowpower edge processing. She received a PhD from KU
    Leuven in 2008, and worked as a research scientist at Intel Labs from 2008 till
    2011. Marian is a scientific advisor to multiple startups, member of the board
    of ECSA, member of the Royal Academy of Belgium for Science and Arts, and active
    in the TPC''s of ISSCC, ISCA and ESSERC. She served in the board


    of directors of tinyML, as a member of the Young Academy of Belgium, as an associate
    editor for TVLSI, TCAS-II and JSSC, and as a member of the STEM advisory committee
    to the Flemish Government. She is a science communication enthusiast as an IEEE
    SSCS Distinguished Lecturer, as a regular member of the Nerdland science podcast
    (in Dutch), and as the founding mother of KU Leuven''s InnovationLab high school
    program. Marian received the laureate prize of the Royal Academy of Belgium in
    2016, the 2021 Intel Outstanding Researcher Award, and the Andre Mischke YAE Prize
    ´ for Science and Policy in 2021.


    ![](_page_18_Picture_19.jpeg)


    Luca Benini Luca Benini holds the chair of digital Circuits and systems at ETHZ
    and is Full Professor at the Universita di Bologna. He received a PhD ` from Stanford
    University. His research interests are in energy-efficient parallel computing
    systems, smart sensing micro-systems and machine learning hardware. He is a Fellow
    of the IEEE, of the ACM, a member of the Academia Europaea and of the Italian
    Academy of Engineering and Technology. He is the recipient of the 2016 IEEE CAS
    Mac Van Valkenburg award, the 2020 EDAA achievement


    Award, the 2020 ACM/IEEE A. Richard Newton Award, the 2023 IEEE CS E.J. McCluskey
    Award, and the 2024 IEEE CS Open Source Hardware contribution Award.


    ![](_page_18_Picture_22.jpeg)


    Naveen Verma Naveen Verma received the B.A.Sc. degree in Electrical and Computer
    Engineering from the UBC, Vancouver, Canada in 2003, and the M.S. and Ph.D. degrees
    in Electrical Engineering from MIT in 2005 and 2009 respectively. Since July 2009
    he has been at Princeton University, where he is currently the Ralph H. and Freda
    I. Augustine Professor of Electrical and Computer Engineering. His research focuses
    on advanced sensing and computing systems, exploring how systems for learning,
    inference, and action planning can be enhanced by


    algorithms that exploit new sensing and computing technologies. This includes
    research on large-area flexible sensors, energy-efficient computing architectures
    and circuits, and machine-learning and statistical-signal-processing algorithms.
    Prof. Verma has been involved in a number of technology transfer activities including
    founding start-up companies. Most recently, he co-founded EnCharge AI, together
    with industry leaders in AI computing systems, to commercialize foundational technology
    developed in his lab. Prof. Verma has served as a Distinguished Lecturer of the
    IEEE Solid-State Circuits Society, and on a number of conference program committees
    and advisory groups. Prof. Verma is the recipient of numerous teaching and research
    awards, including several best-paper awards, with his students.'
- title: "Cosmos: A CXL-Based Full In-Memory System for Approximate Nearest\n  Neighbor\
    \ Search"
  abstract: 'Retrieval-Augmented Generation (RAG) is crucial for improving the quality
    of

    large language models by injecting proper contexts extracted from external

    sources. RAG requires high-throughput, low-latency Approximate Nearest Neighbor

    Search (ANNS) over billion-scale vector databases. Conventional DRAM/SSD

    solutions face capacity/latency limits, whereas specialized hardware or RDMA

    clusters lack flexibility or incur network overhead. We present Cosmos,

    integrating general-purpose cores within CXL memory devices for full ANNS

    offload and introducing rank-level parallel distance computation to maximize

    memory bandwidth. We also propose an adjacency-aware data placement that

    balances search loads across CXL devices based on inter-cluster proximity.

    Evaluations on SIFT1B and DEEP1B traces show that Cosmos achieves up to 6.72x

    higher throughput than the baseline CXL system and 2.35x over a

    state-of-the-art CXL-based solution, demonstrating scalability for RAG

    pipelines.'
  url: http://arxiv.org/abs/2505.16096v1
  keywords: CXL, Approximate Nearest Neighbor Search, Processing Near Memory, Retrieval-Augmented
    Generation
  document: '#### I. INTRODUCTION


    R ETRIEVAL-AUGMENTED Generation (RAG) enhances Large Language Models (LLMs) by
    dynamically retrieving relevant information from external databases, enabling
    more accurate and contextually appropriate responses [1]. Techniques such as Agentic
    RAG [2] further improve quality through iterative retrieval. Central to RAG is
    Approximate Nearest Neighbor Search (ANNS), which enables fast retrieval in high-dimensional
    vector spaces by efficiently identifying the top-k most relevant vectors—those
    closest to a given query based on a similarity metric. As datasets grow, efficient
    and scalable ANNS is critical for real-time inference.


    Handling billion-scale ANNS workloads challenges traditional systems. DRAM lacks
    capacity, whereas SSDs suffer from high latency incompatible with fine-grained
    ANNS access patterns [3], [4]. Alternative solutions such as conventional Processing
    Near Memory (PNM) [3], [5] lack flexibility, and RDMA clusters [4] incur network
    latency and complexity.


    Compute Express Link (CXL) offers a promising solution with high-bandwidth, low-latency
    memory expansion [6], beneficial for latency-sensitive RAG [1]. Still, ANNS remains


    Seoyoung Ko, Hyunjeong Shim, Wanju Doh, Sungmin Yun, and Jung Ho Ahn are with
    Seoul National University, Seoul 08826, South Korea. E-mail: {seoyoungko, simhj1212,
    wj.doh, sungmin.yun, gajh}@snu.ac.kr.


    ![](_page_0_Figure_12.jpeg)


    (a) Retrieval -Augmented Generation


    ![](_page_0_Figure_14.jpeg)


    Fig. 1. Overview of retrieval-augmented generation (RAG) and approximate nearest
    neighbor search (ANNS).


    memory-bandwidth bound, dominated by distance calculations [7]. Distributing indices
    across multiple CXL devices requires intelligent data placement to avoid load
    imbalance.


    In this paper, we propose COSMOS, a full in-memory ANNS system using compute-capable
    CXL devices. COSMOS makes the following three key contributions:


    - Full ANNS Offload via CXL GPCs: COSMOS integrates programmable general-purpose
    cores (GPCs) in CXL memory controllers for local ANNS execution, eliminating host
    intervention and PCIe traffic during search.

    - Rank-level Parallel Distance Computation: COSMOS exploits DRAM rank-level parallelism
    for concurrent distance computation, reducing data movement and maximizing memory
    bandwidth.

    - Adjacency-aware Data Placement: A lightweight algorithm uses cluster metadata
    to distribute vectors across CXL devices, balancing load and enabling parallelism
    without runtime profiling.


    #### II. BACKGROUND


    RAG enhances LLMs by retrieving relevant information from external vector databases
    during inference (Fig. 1(a)). This typically involves indexing documents into
    a vector database and, during inference, embedding user queries to retrieve similar
    documents. Recently, Agentic RAG [2] improves quality via iterative search strategies.
    The core retrieval mechanism relies on finding vectors (documents) most similar
    to the query vector.


    Exact Nearest Neighbor Search (ENNS) guarantees the highest accuracy; however,
    its linear scaling with dataset


    This work was partly supported by Samsung Electronics Co., Ltd (IO250301-12185-01)
    and IITP (RS-2021-II211343 and RS-2023-00256081).


    Jinin So, Yongsuk Kwon, Sang-Soo Park, Si-Dong Roh, Minyong Yoon, and Taeksang
    Song are with Samsung Electronics Corporation, Hwaseong-si, Gyeonggi-do 18448,
    South Korea. E-mail: {jinin.so, yssh.kwon, ss23.park, sidong.roh, casper.yoon,
    taeksang.song }@samsung.com.


    ![](_page_1_Figure_0.jpeg)


    Fig. 2. (a) Memory latency hierarchy highlighting the potential of CXLattached
    memory as a new tier between DRAM and RDMA/SSD in terms of latency and capacity.
    (b) Latency breakdown of graph-based ANN search on large-scale datasets (SIFT
    and DEEP with 100M vectors).


    size in computational cost makes it impractical for billionscale data [3]. By
    contrast, ANNS offers a trade-off between accuracy and efficiency, enabling real-time
    search.


    ANNS methods include graph-based and cluster-based approaches [4], [8]. The former
    links similar vectors and searches greedily along edges, but can suffer from irregular
    memory access. The latter partitions data, finds the closest cluster(s) to the
    query, and searches within them, which offers better memory efficiency but suffers
    from potential read amplification. Hybrid approaches combine these, restricting
    graph traversal to relevant clusters, improving efficiency for large datasets
    while maintaining quality in billion-scale datasets. Fig. 1(b) shows an exemplar
    hybrid-based ANNS process when k is 3.


    #### III. MOTIVATION


    # *A. Scalability Challenges in Billion-Scale ANNS*


    Billion-scale ANNS demands vast memory (terabytes) [9], [3], exceeding single-node
    DRAM limits. Using SSDs introduces high latency (tens of microseconds), and their
    coarsegrained access (kilobyte-scale pages) is unsuitable for finegrained ANNS,
    potentially dominating search time [3], [4].


    SSD-based PNM accelerators reduce data movement [3], [5], but they lack flexibility
    against evolving algorithms or parameters. RDMA-based multi-node clusters offer
    lower latency than SSDs but still suffer network overhead (few to several microseconds
    [4], Fig. 2(a)) and add complexity.


    Compute Express Link (CXL), a PCIe-based interconnect standard, has emerged as
    a promising alternative. CXL provides direct load/store access to expanded memory
    with nearnative-DRAM latency (few hundred nanoseconds, Fig. 2(a)) and high bandwidth,
    eliminating network overhead [6]. This low latency is critical for RAG, where
    retrieval time significantly impacts overall performance, especially with iterative
    techniques like Agentic RAG. Studies show that retrieval accounts for 36% of the
    time-to-first-token in a vanilla RAG and up to 97% in scenarios involving frequent
    re-retrieval [1].


    # *B. Leveraging Compute-capable CXL Devices*


    Recent CXL memory devices optionally incorporate nearmemory compute capabilities
    [10], [11]. Performing computation near memory reduces data transfer and alleviates
    bottlenecks for memory-intensive workloads like ANNS. We propose *offloading the
    entire ANNS pipeline to compute-capable*


    TABLE I ANN DATASETS AND SEARCH PARAMETERS


    | Billon-scale ANN Datasets |                                       |      |            |          |

    |---------------------------|---------------------------------------|------|------------|----------|

    |                           | SIFT                                  | DEEP | Text2Image
    | MSSPACEV |

    | Data Type                 | uint8                                 | fp32 | fp32       |
    int8     |

    | Dimension                 | 128                                   | 96   | 200        |
    100      |

    | Search Parameters         |                                       |      |            |          |

    | max degree                | Maximum number of neighbors per node  |      |            |          |

    | cand list len             | Candidate list size                   |      |            |          |

    | num clusters              | Total number of clusters              |      |            |          |

    | num probes                | Number of clusters searched per query |      |            |          |


    *CXL devices* featuring programmable General-Purpose Cores (GPCs). GPCs offer
    flexibility over fixed accelerators, adapting to diverse datasets and parameters
    (see Table I for the BigANN benchmark [12]) without redesign, which mitigates
    over/under-provisioning issues on the fixed accelerators.


    While offloading helps, ANNS remains bottlenecked by memory bandwidth, primarily
    due to distance calculations loading large vectors (Fig. 2(b)). Our architecture
    addresses this by integrating a GPC with a DRAM *rank-level processing unit (PU)*.
    This exploits DRAM''s internal parallelism by partitioning vector dimensions across
    ranks, computing partial distances concurrently within each rank, which reduces
    data movement and improves bandwidth efficiency.


    However, billion-scale datasets require distributing the index across *multiple*
    CXL devices. Na¨ıve distribution can lead to load imbalance when co-accessed data
    resides on the same device. We address this with an *adjacency-aware cluster placement*
    algorithm that assigns clusters based on proximity, balancing load and enabling
    parallel search.


    ## IV. COSMOS: ARCHITECTURE AND MECHANISMS


    # *A. System Architecture and Workflow*


    COSMOS utilizes a CXL-based architecture with a host CPU, CXL switch, and multiple
    CXL memory devices (Fig. 3(a)). Each CXL device consists of a CXL controller with
    a CXL-PNM module and DRAM devices supporting rank-level PUs. The host dispatches
    queries via the switch to relevant CXL devices. Each device performs local ANNS
    using its GPC (performing graph traversal and candidate list management) and returns
    local top-k results. The host aggregates these for the global top-k. Interface
    registers mapped in host memory facilitate host-PNM communication. Intermediate
    results generated during computation are stored in the temporary buffers, minimizing
    unnecessary memory access.


    COSMOS exploits DRAM rank-level parallelism for performance. Data is column-wise
    partitioned across ranks, allowing independent processing within each rank''s
    PU, alleviating channel contention. Rank-level PUs compute partial distances (e.g.,
    for L2 distance and inner product) on 64B sub-vector segments in parallel (Fig.
    3(c)). Unlike prior work (CXL-ANNS [9]), which offloaded only distance calculation
    to a domain-specific accelerator and required host-managed traversal, COSMOS fully
    offloads traversal to the CXL GPC and uses rank-level PUs. This significantly
    reduces PCIe traffic (only local top-k results are transferred) and memory bandwidth
    bottlenecks, enabling scalable ANNS for billion-scale data.


    ![](_page_2_Figure_1.jpeg)


    Fig. 3. (a) Overview of the system architecture. (b) CXL controller architecture
    featuring a general-purpose core for executing graph-based ANN search via a memory-mapped
    host interface. (c) Rank-level distance calculation logic that enables parallel
    L2 and inner product calculations across memory ranks.


    #### Algorithm 1 Adjacency-aware Cluster Placement


    Input: cluster: A cluster to be placed, including .size and a proximity-ordered
    list .adj of the nearby clusters. devices: A list of available CXL devices in
    the system. Output: best d: The best CXL device for placing the cluster.


    |     | 1: best d, max cap, min loss ← −1, 0, ∞          |

    |-----|--------------------------------------------------|

    |     | 2: for d in devices do                           |

    | 3:  | if d.remain ≥ cluster.size then                  |

    | 4:  | loss, proximity ← 0, num devices                 |

    | 5:  | for adj in cluster.adj do                        |

    | 6:  | if adj ∈ d.clusters then                         |

    | 7:  | loss ← loss + proximity                          |

    | 8:  | proximity ← proximity − 1                        |

    | 9:  | if (best d = −1) or (loss < min loss) or         |

    |     | (loss = min loss and d.remain > max cap) then    |

    | 10: | best d, min loss, max cap ← d, loss, d.remain    |

    |     | 11: best d.remain ← best d.remain − cluster.size |

    |     | 12: return best d                                |


    # *B. Memory Space Management*


    COSMOS integrates CXL Host-managed Device Memory (HDM) into the host physical
    address (HPA) space using static mapping, eliminating runtime translation overhead.
    Following [6], the kernel driver maps HDM regions into HPA during enumeration
    and informs the devices. User applications allocate HDM via a namespace interface
    and mmap() system call. A segment table ensures contiguous physical/virtual mappings.


    Given ANNS''s read-only nature after indexing, graphs and embedding data can be
    allocated using a static memory layout. This eliminates the need for dynamic virtual-to-physical
    address translation. During preprocessing, both the graph and embedding data are
    placed in HDM, and their metadata (e.g., base addresses and sizes) is registered
    with the controller. Address calculation becomes simple arithmetic:


    addrnode = addrgraph base + (node index × node stride)


    addrvector = addrembedding base + (vector index × vector stride) To ensure mapping
    validity within the CXL device, the mlock() system call pins HDM regions in physical
    memory. This prevents any swapping or migration of the allocated memory, thereby
    maintaining a consistent address mapping.


    # *C. Adjacency-aware Cluster Placement*


    Partitioning datasets into clusters for parallel search is common; however, it
    risks load imbalance if nearby clusters reside on the same CXL device. This issue
    is exacerbated when multiple queries target similar regions. We propose *adjacencyaware
    cluster placement* (Algorithm 1) to distribute adjacent clusters across different
    devices, enhancing parallelism.


    All clusters are initially sorted by size in descending order, prioritizing the
    placement of larger clusters first. For each cluster, it calculates adjacency
    penalties (referred to as *loss*) for devices with sufficient capacity (line ⃝<sup>3</sup>
    ). Penalties increase based on the proximity of neighboring clusters already on
    a device (lines ⃝∼<sup>5</sup> ⃝<sup>8</sup> ). The cluster is assigned to the
    device with the lowest penalty, the one with greater remaining capacity in case
    of ties (lines ⃝∼<sup>9</sup> ⃝<sup>10</sup> ). As opposed to the CXL-ANNS''s
    hop-count-based round-robin placement that ignores topology, our algorithm considers
    cluster adjacency.


    The host identifies k-nearest clusters via centroids and dispatches searches to
    the corresponding devices. With adjacent clusters distributed, traversals proceed
    in parallel, maximizing utilization. In Section V-C, we analyze the effect of
    our cluster placing algorithm.


    # V. EVALUATION


    # *A. Experimental Setup*


    To evaluate the performance of COSMOS quantitatively, we developed a simulator
    integrated with Ramulator [13]. Our setup models a 1TB CXL memory comprising four
    CXL devices, each with four DDR5-4800 channels and two ranks of 16Gb ×4 DRAM chips
    per channel (256GB per device).


    We used two representative billion-scale datasets, SIFT1B and DEEP1B, from the
    BigANN benchmark [12]. We incorporated a clustering mechanism into DiskANN (in-memory
    mode) [7] and extracted node visit traces from 10,000 queries per dataset to emulate
    realistic access patterns. These traces were used as input to our simulator to
    model the memory access patterns of the three main query processing operations:
    graph traversal, distance calculation, and candidate updates.


    The generated memory requests were injected into Ramulator to measure query latency
    and analyze memory access behavior under various system configurations. We modeled
    a streaming scenario where queries are dispatched to the first available CXL device,
    enabling query-level parallelism. To evaluate data placement, we compared our
    adjacency-aware algorithm with round-robin (RR) placement, which ignores inter-cluster
    proximity.


    ![](_page_3_Figure_1.jpeg)


    Fig. 4. (a) Relative query throughput (Query Per Second, QPS). (b) Breakdown of
    query execution time.


    #### *B. Overall Performance*


    Fig. 4(a) illustrates the relative throughput (Queries Per Second, QPS) of various
    methods normalized to the Base, where all data resides in CXL-memory and computations
    are performed on the host side. The DRAM-only scenario assumes unlimited DRAM
    capacity, placing all data within DRAM. CXL-ANNS [9] improves performance by offloading
    distance computation, applying fine-grained query scheduling, and hop-count-based
    graph caching; we reproduced the first two but excluded caching as it is beyond
    the scope of this work, which only affects graph traversal and has a negligible
    impact on total latency (Fig. 4(b)). To evaluate each component of COSMOS, we
    evaluated three configurations: (1) without rank-level PU (w/o rank.), (2) without
    the data placement algorithm (w/o algo.), and (3) the full system (COSMOS).


    COSMOS achieves the highest performance, improving QPS by 6.72× (SIFT1B) and 5.35×
    (DEEP1B) over Base. While DRAM-only eliminates host-device data transfers, it
    is still bandwidth-limited. CXL-ANNS performs better by leveraging offloading
    and scheduling, but frequent transfers and bandwidth bottlenecks remain. COSMOS
    addresses both issues by fully offloading graph traversal to CXL-side GPCs and
    accelerating distance computation using rank-level PUs.


    Fig. 4(b) shows the single query latency breakdown within a single CXL device,
    excluding the impact of the data placement. COSMOS significantly reduces graph
    traversal and distance calculation latency by combining in-memory execution with
    rank-level parallelism. DRAM-only benefits from reduced data movement, and CXL-ANNS
    reduces latency through scheduling, but neither entirely eliminates bandwidthrelated
    overhead as COSMOS does.


    # *C. Effectiveness of cluster placing*


    Fig. 5 highlights the effectiveness of our adjacency-aware data placement algorithm
    (Algorithm 1). To isolate its impact, we fixed all other system configurations
    and compared against a baseline that distributes clusters across CXL devices in
    a round-robin (RR) manner. Fig. 5(a) shows the load imbalance ratio (LIR) across
    devices, defined as the maximum device load divided by the ideal uniform load
    under perfect distribution. Lower values indicate better load balancing.


    Across varying num probes (4, 8, and 16), COSMOS effectively balances query load
    across devices, showing consistently lower LIR than RR. Fig. 5(b) presents a heatmap
    of cluster assignments handled per device over 10k queries. Unlike RR,


    ![](_page_3_Figure_10.jpeg)


    Fig. 5. (a) Load imbalance ratio according to the increasing number of probes.
    (b) Heatmap showing the number of clusters handled per device.


    which leads to uneven device utilization, COSMOS ensures a uniform distribution.
    By relying solely on centroid distances and cluster sizes, without additional
    profiling, COSMOS effectively balances query load across CXL devices, thereby
    enhancing system scalability and maximizing parallelism.


    #### VI. CONCLUSION


    We have introduced COSMOS, a scalable, full in-memory ANNS system designed to
    overcome the memory bandwidth and data movement bottlenecks inherent in billion-scale
    vector search. By integrating programmable cores and rank-level processing units
    within CXL devices, COSMOS eliminates host intervention during search and maximizes
    memory bandwidth utilization through parallel distance computation. Further, we
    proposed an adjacency-aware data placement algorithm that effectively balances
    search load across CXL devices by strategically distributing neighboring clusters,
    enhancing parallelism and scalability. Our evaluations demonstrated that COSMOS
    significantly outperforms existing DRAM-based and prior CXL-based approaches in
    query throughput and latency.


    #### REFERENCES


    - [1] M. Shen *et al.*, "Towards Understanding Systems Trade-offs in Retrieval-Augmented
    Generation Model Inference," 2024, arXiv:2412.11854.

    - [2] A. Singh *et al.*, "Agentic Retrieval-Augmented Generation: A Survey on
    Agentic RAG," 2025, arXiv:2501.09136.

    - [3] B. Tian *et al.*, "Scalable Billion-point Approximate Nearest Neighbor Search
    Using SmartSSDs," in *USENIX ATC*, 2024.

    - [4] R. Cheng *et al.*, "Characterizing the Dilemma of Performance and Index
    Size in Billion-Scale Vector Search and Breaking It with Second-Tier Memory,"
    2024, arXiv:2405.03267.

    - [5] Y. Wang *et al.*, "NDSEARCH: Accelerating Graph-Traversal-Based Approximate
    Nearest Neighbor Search through Near Data Processing," in *ISCA*, 2024.

    - [6] D. Gouk *et al.*, "Direct Access, High-Performance Memory Disaggregation
    with DirectCXL," in *USENIX ATC*, 2022.

    - [7] S. J. Subramanya *et al.*, "DiskANN: fast accurate billion-point nearest
    neighbor search on a single node," in *NeurIPS*, 2019.

    - [8] W. Jiang *et al.*, "Chameleon: A Heterogeneous and Disaggregated Accelerator
    System for Retrieval-Augmented Language Models," *Proc. VLDB Endowment*, 2024.

    - [9] J. Jang *et al.*, "CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation
    and Computation for Billion-Scale Approximate Nearest Neighbor Search," in *USENIX
    ATC*, 2023.

    - [10] Y. Gu *et al.*, "PIM Is All You Need: A CXL-Enabled GPU-Free System for
    Large Language Model Inference," in *ASPLOS*, 2025.

    - [11] S.-S. Park *et al.*, "An LPDDR-based CXL-PNM Platform for TCOefficient
    Inference of Transformer-based Large Language Models," in *HPCA*, 2024.

    - [12] "Big ANN Benchmarks," 2024. [Online]. Available: https: //big-ann-benchmarks.com

    - [13] Y. Kim, W. Yang, and O. Mutlu, "Ramulator: A Fast and Extensible DRAM Simulator,"
    p. 45–49, 2016.'
- title: "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n\
    \  Graph Databases"
  abstract: 'Large Language Models (LLMs) have demonstrated their potential in hardware

    design tasks, such as Hardware Description Language (HDL) generation and

    debugging. Yet, their performance in real-world, repository-level HDL projects

    with thousands or even tens of thousands of code lines is hindered. To this

    end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval

    Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph

    representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow

    Graphs (DFGs) to capture both code graph view and hardware graph view.

    HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the

    limited recall issues inherent in similarity-based semantic retrieval by

    incorporating structural information, but also enhances its extensibility to

    various real-world tasks by a task-specific retrieval finetuning. Additionally,

    to address the lack of comprehensive HDL search benchmarks, we introduce

    HDLSearch, a multi-granularity evaluation dataset derived from real-world

    repository-level projects. Experimental results demonstrate that HDLxGraph

    significantly improves average search accuracy, debugging efficiency and

    completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based

    RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are

    available at https://github.com/Nick-Zheng-Q/HDLxGraph.'
  url: http://arxiv.org/abs/2505.15701v1
  keywords: Graph RAG, Hardware description language, LLM agent
  document: '#### I. INTRODUCTION


    Recent advances in Large Language Models (LLMs) for software language understanding
    and generation [\[1\]](#page-6-0), [\[2\]](#page-6-1) have inspired efforts to
    extend their capabilities to facilitate Hardware Description Language (HDL) code
    designs. Prior works have demonstrated LLMs'' potential in generating [\[3\]](#page-6-2)–[\[5\]](#page-6-3)
    and debugging [\[6\]](#page-6-4) HDL code [\[7\]](#page-6-5), [\[8\]](#page-6-6).
    However, LLM performance in HDL-related tasks remains hindered by limited training
    data and degradation caused by long prompts. To address these issues, researchers
    have integrated *Retrieval-Augmented Generation (RAG)*, which retrieves relevant
    HDL fragments from high-quality HDL repositories to supplement knowledge gaps
    and reduce prompt length [\[8\]](#page-6-6), [\[9\]](#page-6-7).


    Despite its potential, existing RAG approaches in HDL predominantly rely on similarity-based
    semantic retrieval, which exhibits low recall when encountering intricate queries
    or large, complex HDL repositories. Figure [1](#page-1-0) shows an HDL debugging
    example for a CV32E40P RISC-V HDL implementation, which consists of over 30 modules
    [\[10\]](#page-6-8). The similaritybased RAG approach relies solely on semantic
    similarity between the user query and code module names, making it vulnerable
    to vocabulary mismatches. For instance, a query may contain only an ambiguous
    description, or the relevant code may exist as an unnamed block within HDL repositories.


    Inspired by recent advancements in Graph RAG [\[11\]](#page-6-9), [\[12\]](#page-6-10)
    and the characteristics of HDL codes, we propose integrating graph-based structures
    into HDL-specific RAGs to address the aforementioned challenges. Specifically,
    we introduce HDLx-Graph, a novel hybrid graph-enhanced RAG framework, which incorporates
    two HDL-specific graphs: Abstract Syntax Trees (ASTs) and Data Flow Graphs (DFGs).
    Using ASTs, we partition the HDL repository with about several thousand lines
    of code into a code graph view containing multi-level entity relationships. While
    DFGs provide a more precise hardware graph view of signal-level flow to reflect
    the circuit topology. By integrating structural properties into semantic information,
    HDLxGraph significantly enhances LLMs'' understanding of code structures through
    AST retrieval, enabling multi-level reasoning for complex HDL codes and ambiguous
    queries, while demonstrating extensibility across three downstream applications:
    code search, debugging, and completion, through


    ![](_page_1_Figure_0.jpeg)


    <span id="page-1-0"></span>Fig. 1. (Top) An illustration of the mismatch between
    HDL and natural language in conventional RAG, including structural and vocabulary
    mismatches. And (Bottom) a demonstration of HDLxGraph''s efficiency in bridging
    these mismatches by incorporating graph information, using an HDL debugging example
    for a CV32E40P RISC-V HDL implementation [\[10\]](#page-6-8).


    signal-level task-specific retrieval achieved by DFG. Furthermore, due to the
    identified absence of comprehensive HDL code search benchmarks containing question-answer
    pairs with multi-level relationships (as the example in Figure [1\)](#page-1-0),
    we extend HDLxGraph framework to address this gap, generating a benchmark, dubbed
    as HDLSearch. Our key contributions are summarized as follows:


    - We propose HDLxGraph, a novel LLM-driven RAG framework that leverages a dual-retrieval
    mechanism based on AST and DFG retrieval. Specifically, it takes into account
    the alignment across different hierarchical levels in the AST and incorporates
    task-specific retrieval on signal level within the DFG, thereby enabling more
    fine-grained retrieval compared to conventional RAG and demonstrating extensibility
    across various tasks. To the best of our knowledge, HDLxGraph is the first framework
    to integrate HDLs'' inherent graph structures with RAGs.

    - HDLxGraph implements a repository-level HDL graph database with hybrid graph
    view, where the AST graph provides the code structure view while the DFG graph
    represents the hardware graph view. The database construction also considers cross-file
    relationship, thereby providing a more accurate and consistent graph representation
    of projects at repository level.

    - Based on HDLxGraph, we further construct a new LLMgenerated dataset for HDL
    code search with data cleaning and evaluation, called HDLSearch, which derives
    query benchmark from real-world repository-level HDL projects, to solve the gap
    in insufficient search datasets for HDL codes.

    - Integrating HDLxGraph with three LLMs with various scale and different coding
    abilities, we demonstrate the versatility of HDLxGraph on three real-world HDL
    tasks, i.e., code search, debugging, and completion. Experiments demonstrate that
    our framework exhibits competi-


    tive performance on two widely-used benchmarks [\[13\]](#page-6-11), [\[14\]](#page-6-12)
    for code completion and debugging as well as HDLSearch for code search, respectively.


    The remaining sections are organized as follows. Section [II](#page-1-1) provides
    an overview of the application of LLMs in hardware design, alongside a review
    of conventional Verilog code structural abstractions and graph-based RAG techniques.
    Section [III-B](#page-3-0) presents a detailed explanation of the HDLxGraph workflow
    integrating AST and DFG abstraction and employs a multi-hierarchy approach to
    generate the HDLSearch Benchmark regarding the benchmark gap in hardware searching.
    Section [IV](#page-4-0) reports thorough experimental results on three hardware
    downstream tasks, and finally, Section [V](#page-6-13) concludes the paper.


    ## II. PRELIMINARIES


    ## <span id="page-1-1"></span>*A. LLM-aided HDL Tasks*


    Generation. Although LLMs excel in generating simple HDL designs, they still struggle
    with complex repositorylevel chip designs, as demonstrated in previous work [\[15\]](#page-6-14)–
    [\[23\]](#page-6-15). For example, state-of-the-art (SOTA) works [\[9\]](#page-6-7),
    [\[22\]](#page-6-16)– [\[24\]](#page-6-17) reply on the templates or customed
    RAG dataset provided by human experts, using LLMs to fill in fixed-level content
    while overlooking the entire generation.


    Debugging. Existing work also exhibits certain limitations when using LLMs for
    repository-level complex debugging [\[13\]](#page-6-11), [\[25\]](#page-6-18)–[\[28\]](#page-6-19).
    The LLM4DV [\[25\]](#page-6-18) framework utilizes LLMs to generate test stimuli.
    Though performing well on simple tasks, it fails to achieve high coverage in more
    complex chip designs. Additionally, [\[26\]](#page-6-20) integrates LLMs with
    RAG to identify and patch functional HDL bugs. However, it still relies on manually
    defined error types, limiting LLMs'' potential for understanding-based bug fixing.


    Search. Precise code search is the foundation to RAG for both HDL generation and
    debugging. While no direct work has focused on HDL search, recent studies have
    examined LLMs'' potential in HDL summarization [\[3\]](#page-6-2), which is a
    pre-step for HDL search, as well as EDA Q&A tasks [\[8\]](#page-6-6). However,
    these works do not consider HDL''s inherent hierarchical structure, preventing
    their direct application to precise code searches.


    Additionally, previous work falls short in tasks beyond their targeted objectives,
    limiting generalizability. Our proposed HDLxGraph is a unified RAG-assisted framework
    designed to address these three tasks while exploring LLMs'' potential for repository-level
    HDL codes.


    ## *B. Graph Retrieval Augmented Generation*


    *Graph Retrieval-Augmented Generation* (Graph RAG) leverages the structured nature
    of knowledge graphs and integrates them into the RAG framework [\[11\]](#page-6-9)
    to enable more complex structured reasoning and context-aware responses. Recent
    studies suggest that Graph RAG outperforms classical RAG-based LLM systems in
    certain software code tasks [\[29\]](#page-7-0)– [\[31\]](#page-7-1). Inspired
    by this, our proposed HDLxGraph leverages the unique structure of HDL''s ASTs
    and DFGs to optimize hardware design via Graph RAG. Details shown in Section [III-B.](#page-3-0)


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-1"></span>Fig. 2. The overview of our proposed HDLxGraph framework.


    ## *C. HDL Code Structure*


    Graph code views, such as AST, DFG, and Control Flow Graph (CFG), have been adopted
    for a more comprehensive understanding of software programming language. Although
    sharing syntactic similarities, HDLs introduce unique complexities in representation
    characterized in three aspects: explicit timing modeling, inherent parallelism,
    and rigorous bitwidth specifications [\[32\]](#page-7-2), [\[33\]](#page-7-3).
    Specifically, Verilog''s *always* blocks enable concurrent execution, while the
    *assign* statement facilitates continuous assignment, both differ from constructs
    in software languages. Therefore, directly inheriting the graph views from software
    code is infeasible. We conduct an in-depth study of HDL-specific graphs and propose
    a graph database with hybrid representations by AST and DFG in Section [III-A.](#page-2-0)


    ## *D. Benchmarks/Datasets for LLM-aided Tasks*


    For HDL code generation benchmarks, RTLLM [\[34\]](#page-7-4) consists of 30 designs;
    and VerilogEval [\[14\]](#page-6-12) presents an evaluation dataset consisting
    of 156 problems from HDLBits. For HDL code debugging benchmarks, LLM4SecHW [\[13\]](#page-6-11)
    contains bug localization and repair test sets from the version control data in
    Github; RTLFixer [\[24\]](#page-6-17) introduces a Verilog syntax debugging dataset,
    derived from VerilogEval [\[14\]](#page-6-12); and CirFix [\[35\]](#page-7-5)
    includes a bug repair benchmark with testbenches.


    No existing benchmark has been established for the HDL search, which is an essential
    step for downstream tasks such as generation and debugging. Therefore, we propose
    HDLSearch, the first benchmark for HDL code search, which derives query benchmark
    from real-world repository-level HDL projects.


    ## III. METHODOLOGY


    Figure [2](#page-2-1) illustrates the comprehensive workflow of our proposed HDLxGraph
    framework, which consists of three steps: 1) Graph Database Preparation, 2) Multi-level
    Retrieval, and 3) Downstream Task Completion. Beginning with Step 1, we extract
    ASTs and DFGs from the input code repositories through the AST and DFG parsers,
    then store HDL entities and relationships as nodes and edges in a graph database
    (see Section [III-A\)](#page-2-0). In Step 2 (see Section [III-B\)](#page-3-0),
    HDLxGraph utilizes a Decomposer Agent in AST retrieval to extract the input query
    into structural levels, which are later sent to predefined searching paradigms
    to retrieve relevant fine-grained code snippets. Additionally, code debugging
    and completion tasks trigger DFG retrieval in parallel to narrow the search space
    or enable similarity matching between incomplete and complete code snippets. HDLxGraph
    supports three real-world HDL downstream tasks. Step 3 fuses the retrieved code
    snippets with LLMs to support code debugging, completion, and search, which further
    demonstrates the generality of our framework (see Section [III-B\)](#page-3-0).
    In addition, due to the lack of a code search benchmark in HDL repositories, we
    generated a new benchmark, called HDLSearch, based on HDLxGraph, as shown in Figure
    [5,](#page-4-1) composed of three steps: 1) Manual Filtering, 2) Query Generation,
    3) Benchmark Generation. Details of benchmark generation are presented in Section
    [III-C.](#page-4-2)


    ## <span id="page-2-0"></span>*A. Graph Database Preparation*


    As shown in Step 1 of Figure [2,](#page-2-1) the HDLxGraph RAG framework begins
    with an off-line graph database construction. The graph database represents HDL
    repositories through nodes and edges that correspond to HDL entities and their
    relationships. Without losing representativity, we focus on Verilog, a widely
    used HDL language, in our implementation. Please note that, although different
    HDLs have different syntactic properties, they share the same three-level structural
    abstraction, i.e., (module → block → signal) in Verilog. Specifically, we use
    an AST to support the code graph view that emphasizes multi-level structural relationships
    in HDL, and a DFG to facilitate the hardware graph view focusing on signal flow
    reflecting circuit topology, providing a comprehensive and tailored representation
    of the HDL repository. The AST graph incorporates node types such as MODULE ,
    BLOCK , and SIGNAL connected through CONTAINS and INSTANTIATE edge types, whereas
    the DFG graph introduces TEMP nodes alongside SIGNAL nodes, connected via FLOWS\_TO
    , TRUE , FALSE , and COND edges. When constructing the entire graph database,
    there are three main sub-steps:


    1) Parsing. The graph database construction begins with analyzing individual HDL
    file in the repository using a Pyverilog-based [\[36\]](#page-7-6) AST and DFG
    parser. For AST parsing, we extract the cross-level dependancy information of
    MODULE , BLOCK , and SIGNAL from each Verilog file to represent the fine-grained
    hierarchical code structure. Note that block level (always, assign, initial) here
    represent behavioral abstraction at the register-transfer level, defining concurrent
    hardware operations. Concurrently, we generate the hardware signal flow for DFG
    parsing, which characterizes the transmission and interaction between signals.
    The DFG graph incorporates both the signal directions and the dependency relationships
    between signals, reflecting the functionality and processing flow of a circuit.
    This multi-granularity representation enables our database to store both the code
    structure and the hardware behavior of a single HDL file, thereby facilitating
    a more comprehensive graph abstraction of the HDL, as shown in Figure [3.](#page-3-1)


    2) Meta-data generation. After the parsing of graph data, we generate embeddings
    for nodes (both MODULE and


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-1"></span>Fig. 3. Visualization of an example in the graph database.


    BLOCK ) via code encoding to facilitate semantic search. These embeddings, together
    with the node attributes extracted by the parser, are recorded in the database
    as part of the node meta-data. Typically, meta-data will include multiple attributes
    of each node, for instance, the meta-data for BLOCK nodes contains attributes
    such as block type, code, and embedding, as illustrated in Figure [3.](#page-3-1)
    We use CodeT5+ [\[1\]](#page-6-0), a SOTA code LLM model, to directly generate
    the embedding for our code, avoiding description generation.


    3) Cross-file Relationship Construction. Finally, we address the absense of cross-file
    relationship, which is the module INSTANTIATE relationships. We search for the
    module node with same name recorded in the meta-info instance block to establish
    the cross-file and cross-module relationships.


    The developed graph database provides multi-level code exploration spanning from
    module-level abstractions to signallevel implementations, thereby positioning
    our HDL graph database as a extensible framework for multiple downstream tasks
    due to the modular fashion of database schema management.


    #### <span id="page-3-0"></span>*B. Multi-level Retrieval and Downstream Task
    Completion*


    Multiple-level Retrieval: In real-world hardware project issues, user queries
    always contain rich contextual cues, such as module names, functional descriptions,
    and sometimes brief code snippets, offering hints for retrieval. Specifically,
    user queries can be used to extract multi-level structural information and then
    guide the following multiple-level AST retrieval[1](#page-3-2) . In addition,
    signal-level flow through DFG retrieval is adopted for code completion and debugging
    tasks.


    AST Retrieval. HDLxGraph constructs a hierarchical representation of HDL codebases
    through an AST-based graph, enabling multi-level HDL retrieval as depicted in
    Figure [4.](#page-3-3) For AST retrieval, we follow three sub-steps:


    ![](_page_3_Figure_9.jpeg)


    <span id="page-3-3"></span>Fig. 4. Flow of multi-level retrieval containing AST
    and DFG retrieval.


    1) Query Decomposition. HDLxGraph employs an LLM agent, called *Decomposer*, to
    decompose the original query into three abstraction levels: module, block, and
    signal, thereby extracting structural information. It supports intricate queries
    from various downstream tasks such as: ''Find some certain blocks under a certain
    module'' in Search, or ''Some functions in some certain modules have led to the
    following errors ...'' Therefore, we obtain multi-level queries which have captured
    inherent structural information in the original query.


    2) Top-k Selection and Filtering. Leveraging Verilog''s inherent three-level abstraction
    (module → block → signal), HDLxGraph first retrieves top-k candidate modules and
    blocks which have the highest similarity scores with the decomposed query in corresponding
    levels based on semantic matching, then filters valid module-block pairs through
    containment relationships. To facilitate precise code retrieval in different levels,
    a suite of retrieval APIs is introduced, as detailed in Table [I](#page-8-2) of
    Appendix [A.](#page-8-1) Since we select Neo4j as the graph database, the query
    APIs are written in Cypher to interact with the database.


    3) Cross-level Rerank. Finally, we rerank results using averaged similarity scores.
    Since the signal-level representation lacks the code context, it is challenging
    to directly obtain an accurate similarity score for the signal-level query. Therefore,
    HDLxGraph extracts all filtered module-block pairs that contain the signal and
    computes their average similarity scores as the signal-level similarity score.
    Therefore, we prioritize signal with the highest similarity score to be the retrieved
    signal. This hierarchical approach ensures fine-grained retrieval of HDL''s structural
    information across multiple abstraction layers while maintaining compatibility
    with similarity-based semantic analysis, balancing precision and scalability in
    hardware database exploration.


    DFG Retrieval. The DFG is composed of signal-level variables and relationships.
    As a result, it is useful when signal-level information is needed and the utilization
    method can vary greatly for different downstream tasks. In this work, we utilize
    the signal-level flow to enhance code completion and code debugging tasks, as
    illustrated in Figure [2.](#page-2-1) There are two primary operations for DFG
    retrieval of different tasks, which are *Signal Traverse* and *Similarity-based
    Extract*:


    <span id="page-3-2"></span><sup>1</sup>Figure [9](#page-8-0) in Appendix [A](#page-8-1)
    demonstrates this through two real-world issues submitted to the CVA6 [\[37\]](#page-7-7)
    and OpenTitan [\[38\]](#page-7-8) projects, where highlighted hints guide HDL
    retrieval.


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-1"></span>Fig. 5. HDLSearch benchmark generation flow.


    1) Debugging. For debugging tasks, if a signal mismatch is detected, the debugging
    process can iteratively traverse the DFG upstream with *Signal Traverse* operation
    from the faulty signal, inspecting each node (e.g., operators, multiplexers, or
    instance outputs) to identify where the dataflow diverges from expected behavior.
    This approach guides LLM debugging by focusing only on the subgraph directly influencing
    the problematic signal, filtering out irrelevant code regions. By extracting the
    immediate upstream nodes and their associated code blocks, the system generates
    a concise, context-rich error candidate set.


    2) Completion. While we want to retrieve the similar code with the unfinished
    code, some reference code may look different but still having the similar functionality
    because the hardware (i.e. dataflow) described are very similar. Graph embedding
    offers a viable approach for Verilog code completion by translating code''s structural
    and semantic relationships into a unified mathematical framework. By leveraging
    GraphSAGE [\[39\]](#page-7-9), these graphs are compressed into lowdimensional
    vector representations that preserve contextual patterns, such as recurring HDL
    constructs (e.g., finite state machines, pipelined operations) or common coding
    idioms (e.g., non-blocking assignments in clock-driven blocks). When a developer
    writes partial code, the corresponding subgraph is embedded and compared against
    historical embeddings using similarity metrics, enabling the system to infer likely
    completions—even with incomplete structures—by prioritizing nodes critical to
    the current context. This allows real-time retrieval of relevant patterns from
    large codebases while adhering to Verilog-specific constraints.


    Downstream Task Completion: The propagation trajectory of error signals establishes
    causal dependencies within hardware description constructs, enabling LLMs to trace
    fault origins through backward-chaining analysis. Meanwhile, Dataflow graph analysis
    enables LLMs to identify functionally equivalent code patterns by detecting structural
    similarities in hardware operations, even when surface code syntax differs. This
    approach allows semantic-aware code completion beyond literal text matching.


    #### <span id="page-4-2"></span>*C. HDL Search Benchmark*


    Observing the absence of an HDL code search benchmark, we aim to establish a specific
    benchmark to address this gap. However, manually creating expert-annotated benchmark
    is time-consuming and labor-intensive, posing it economically impractical. Therefore,
    based on the multi-level hierarchical framework of HDLxGraph, we propose to leverage
    LLMs to construct a benchmark dubbed HDLSearch, as shown in Figure [5.](#page-4-1)
    The benchmark generation can be divided into mainly three sub-steps:


    1) Manual Filtering. Our corpus originates from RTL-Repo [\[40\]](#page-7-10),
    a collection of publicly accessible GitHub repositories specializing in HDLs.
    Unlike conventional software repositories, HDL projects tend to lack structured
    documentation and standardized code organization, making automated repository
    filtering particularly challenging. To address this limitation, we first implement
    a manual filtering and select 10 representative repositories at different difficulty
    levels, ranging from educational FPGA projects, interconnection protocols to commercial
    CPUs.


    2) Query Generation. Adopting a hierarchical framework where block serve as the
    fundamental level, we implement a multi-stage generation process. Initial functional
    block descriptions are first generated, then systematically propagated through
    two parallel pathways, which are 1) Signal-level annotation: through contextual
    information, the semantics of a functional block can be inherited by its associated
    signals, thereby effectively annotating these signals with specific functionalities,
    and 2) Module-level abstraction by designing a set of explicit and tailored prompts
    for the LLMs, we enable it to analyze and summarize the interactions among individual
    functional blocks as a module-level description. This dualpath flow ensures consistent
    semantic alignment between finegrained signal behaviors and coarse-grained module
    operations. With all descriptions finished, repo-specific information such as
    module and signal''s names are removed to generate a relatively ambiguous query.


    3) Benchmark Refinement. To further ensure benchmark validity, we employ an iterative
    refinement process using templated instructions (shown in Appendix [B\)](#page-8-3).
    Through multiple rounds of evaluation and regeneration, we gradually remove unsuitable
    queries and align the LLM-generated query outputs with practical engineering requirements
    till it reaches the defined termination count . After that, manual adjustments
    are undertaken to address few gaps between LLM outputs and the actual search intent.


    #### IV. EXPERIMENTS


    #### <span id="page-4-0"></span>*A. Experimental Configuration and Platforms*


    To explore the capabilities of the proposed HDLxGraph framework, we evaluate it
    on three HDL downstream tasks: code search, code debugging, and code completion.
    The taskspecific benchmarks and experimental metrics are detailed in the following
    subsections. We equip HDLxGraph with three LLMs with different model sizes: Claude-3.5-Sonnet
    [\[41\]](#page-7-11), a large model with strong coding ability; Qwen2.5-Coder-7B
    [\[42\]](#page-7-12), a coding-specific model of medium size; and LLAMA-3.1 [\[43\]](#page-7-13),
    a general-purpose model with a relatively small size. We use top-p = 1.0 and temperature
    = 0.7 as our basic configuration. All experiments are run on a 2xA6000 Linux GPU
    Server and all benchmark evaluations conduct 10


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-0"></span>Fig. 6. HDL search MRR comparison with baselines.


    independent experimental trials per task to ensure statistical robustness.


    #### *B. Code Semantic Search*


    Benchmark: Considering the absence of benchmarks for HDL-specific code search,
    we use our proposed HDLSearch (see Section [III-C\)](#page-4-2) as the benchmark
    with termination count = 7 when generation. The generated benchmark comprises
    40 module-level queries, 100 block-level queries and 200 signal-level queries,
    with 6,300 code blocks from 10 repositories serving as distractor, i.e., retrieval
    scope. The evaluation focuses on *block-level* retrieval, which serves as a fundamental
    level with highest extensibility to other downstream tasks as mentioned before.


    Metric: We adopt the widely used mean reciprocal rank (MRR) in RAG as the primary
    metric, which assesses whether the framework is capable of returning correct results
    within the top-ranked outputs:


    $$\text{MRR} = \frac{1}{N} \sum\_{i=1}^{N} \frac{1}{\text{rank}\_i} \tag{1}$$


    Baselines: We compare HDLxGraph against two commonly used similarity-based RAG
    methods, BM25 [\[44\]](#page-7-14) and CodeT5+ embeddings [\[1\]](#page-6-0).


    Evaluation Results: As shown in Figure [6,](#page-5-0) HDLxGraph achieves superior
    performance in block-level search evaluated with all the 100 block-level queries
    (averaged 12.04% MRR improvement), demonstrating its potential in accurate HDL
    search for complex repository-level codes.


    ## <span id="page-5-3"></span>*C. Code Debugging*


    Benchmark: We evaluate HDLxGraph''s capability in handling real-world, repository-level
    debugging challenges and choose LLM4SecHW [\[13\]](#page-6-11) as the benchmark,
    which extracts and refines data from the version control systems of opensource
    repository-level hardware designs. Specifically, we choose the mor1kx repository
    [\[45\]](#page-7-15), an OpenRISC processor IP core, for our evaluation[2](#page-5-1)
    . In the mor1kx repository [\[45\]](#page-7-15), there are 5 git commit SHAs covering
    different debugging issues. Metric: Following LLM4SecHW, we choose ROUGE-N F1
    score [\[46\]](#page-7-16) as the evaluation metric, which refers to the direct
    -gram overlap between a prediction and a reference word considering precision
    and recall. The parameter can be set


    ![](_page_5_Figure_12.jpeg)


    <span id="page-5-2"></span>Fig. 7. HDL debugging comparison with baselines.


    to 1, 2 and L, corresponding to matching at unigram, bigram, and longest common
    subsequence gram, respectively.


    Baselines: We compare our framework with two RAG strategies: the CodeT5+ embedding
    search strategy [\[1\]](#page-6-0), denoted as "Similarity-based RAG", which represents
    the conventional similarity-based RAG approach, and the accurate-RAG debugging
    strategy, denoted as "Accurate-RAG", which relies on human effort to extract the
    exact buggy code segments to be modified, serving as a theoretical top-tier RAG
    baseline.


    Evaluation Results: As illustrated in Figure [7,](#page-5-2) HDLxGraph achieves
    a higher score of ROUGE-1, ROUGE-2 and ROUGE-L compared to similarity-based RAG
    under all scenarios, and exhibits performance approaching that of the top-tier
    baseline. This demonstrates HDLxGraph''s potential in handling realworld debugging
    issues.


    ## *D. Code Completion*


    Benchmark: We evaluate code completion capabilities using VerilogEval-Human v2
    [\[14\]](#page-6-12) with RTLLM [\[34\]](#page-7-4) as a reference implementation.


    Metric: we apply Pass@k metrics [\[47\]](#page-7-17) to assess the generation
    pass rate:


    $$\text{pass} \oplus \mathbf{k} = \mathbb{E}\_{\text{Problems}} \left[ 1 - \frac{\binom{n
    - c\_p}{k}}{\binom{n}{k}} \right] \tag{2}$$


    where is the total number of generations, is the number of successes, and is the
    number of attempts considered. We apply @1 in our experiment.


    Baselines: We compare HDLxGraph against two baselines: direct LLM completion without
    RAG and similarity-based RAG using CodeT5+, as described in Section [IV-C.](#page-5-3)


    Evaluation Results: As shown in Figure [8,](#page-6-21) HDLxGraph consistently
    improves @1 accuracy by 3-10% across various LLMs. While our evaluation framework
    operates at module granularity rather than full repository scope, we strategically
    employ the RTLLM [\[48\]](#page-7-18) codebase as a RAG corpus, thereby maintaining
    repository-level evaluation. The higher accuracy suggests HDLxGraph''s generalizability
    across different abstraction levels, highlighting that structural code understanding
    significantly benefits completion tasks, even at sub-repository granularity.


    <span id="page-5-1"></span><sup>2</sup>Since HDLxGraph''s AST and DFG parsers
    currently do not support SystemVerilog syntax, we leave further debugging evaluation
    on SystemVerilog repositories as future work.


    ![](_page_6_Figure_0.jpeg)


    <span id="page-6-21"></span>Fig. 8. HDL completion Pass@1 comparison with baselines.


    #### V. CONCLUSION AND FUTURE WORK


    <span id="page-6-13"></span>In this work, we propose HDLxGraph, a novel hybrid
    graphenhanced RAG framework that innovatively combines ASTbased structural matching
    with DFG-aware code retrieval. Experimental validation across semantic search,
    debugging, and code completion tasks demonstrates improvements of 12.04%, 12.22%,
    and 5.04% respectively over conventional methods, proving the effectiveness of
    joint structural-semantic retrieval for HDL applications.


    This work establishes graph-enhanced retrieval as a viable paradigm for hardware
    engineering assistance, with broader implications for code-intensive domains requiring
    precise program analysis. Future directions may include multi-view HDL representation
    learning to bridge the semantic gap between natural language specifications and
    circuit implementations. This direction could enable comprehensive support for
    heterogeneous downstream tasks in electronic design automation, from specification
    validation to cross-module optimization.


    #### REFERENCES


    - <span id="page-6-0"></span>[1] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui,
    Junnan Li, and Steven Hoi. CodeT5+: Open code large language models for code understanding
    and generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, December
    2023.

    - <span id="page-6-1"></span>[2] Anton Lozhkov, Raymond Li, Loubna Ben Allal,
    and et al. Starcoder 2 and the stack v2: The next generation. *arXiv*, 2024.

    - <span id="page-6-2"></span>[3] Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin,
    Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan
    Zhang, Zidong Du, Qi Guo, Xing Hu, and Yunji Chen. Codev: Empowering llms for
    verilog generation through multi-level summarization. *arXiv*, 2024.

    - [4] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt,
    Ramesh Karri, and Siddharth Garg. Verigen: A large language model for verilog
    code generation. *arXiv*, 2023.

    - <span id="page-6-3"></span>[5] Xi Wang, Gwok-Waa Wan, Sam-Zaak Wong, Layton
    Zhang, Tianyang Liu, Qi Tian, and Jianmin Ye. Chatcpu: An agile cpu design and
    verification platform with llm. In *Proceedings of the 61st ACM/IEEE Design Automation
    Conference*, DAC ''24, New York, NY, USA, 2024. Association for Computing Machinery.

    - <span id="page-6-4"></span>[6] Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao,
    Mingxuan Yuan, Yu Huang, Lei Chen, and Bei Yu. Hdldebugger: Streamlining hdl debugging
    with large language models. *arXiv*, 2024.

    - <span id="page-6-5"></span>[7] Luyao Shi, Michael Kazda, Bradley Sears, Nick
    Shropshire, and Ruchir Puri. Ask-eda: A design assistant empowered by llm, hybrid
    rag and abbreviation de-hallucination. In *2024 IEEE LLM Aided Design Workshop
    (LAD)*, pages 1–5, 2024.

    - <span id="page-6-6"></span>[8] Yuan Pu, Zhuolun He, Tairu Qiu, Haoyuan Wu, and
    Bei Yu. Customized retrieval augmented generation and benchmarking for eda tool
    documentation qa. *arXiv*, 2024.

    - <span id="page-6-7"></span>[9] Mingzhe Gao, Jieru Zhao, Zhe Lin, Wenchao Ding,
    Xiaofeng Hou, Yu Feng, Chao Li, and Minyi Guo. Autovcoder: A systematic framework
    for automated verilog code generation using llms. *arXiv*, 2024.

    - <span id="page-6-8"></span>[10] Michael Gautschi, Pasquale Davide Schiavone,
    Andreas Traber, Igor Loi, Antonio Pullini, Davide Rossi, Eric Flamand, Frank Gurkaynak,
    and Luca Benini. Near-Threshold RISC-V Core With DSP Extensions for Scalable IoT
    Endpoint Devices, February 2017.

    - <span id="page-6-9"></span>[11] Darren Edge, Ha Trinh, Newman Cheng, Joshua
    Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local
    to global: A graph rag approach to query-focused summarization. *arXiv*, 2024.

    - <span id="page-6-10"></span>[12] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao,
    and Chao Huang. Lightrag: Simple and fast retrieval-augmented generation. *arXiv*,
    2024.

    - <span id="page-6-11"></span>[13] Weimin Fu, Kaichen Yang, Raj Gautam Dutta,
    Xiaolong Guo, and Gang Qu. Llm4sechw: Leveraging domain-specific large language
    model for hardware debugging. In *2023 Asian Hardware Oriented Security and Trust
    Symposium (AsianHOST)*, pages 1–6, 2023.

    - <span id="page-6-12"></span>[14] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany,
    and Haoxing Ren. VerilogEval: evaluating large language models for verilog code
    generation. In *2023 IEEE/ACM International Conference on Computer-Aided Design
    (ICCAD)*, 2023.

    - <span id="page-6-14"></span>[15] Jason Blocklove, Siddharth Garg, Ramesh Karri,
    and Hammond Pearce. Chip-chat: Challenges and opportunities in conversational
    hardware design. *arXiv preprint arXiv:2305.13243*, 2023.

    - [16] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan (Celine)
    Lin. MG-Verilog: multi-grained dataset towards enhanced llm-assisted verilog generation.
    In *The First IEEE International Workshop on LLM-Aided Design (LAD''24)*, 2024.

    - [17] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt,
    Ramesh Karri, and Siddharth Garg. Verigen: A large language model for verilog
    code generation. *ACM Trans. Des. Autom. Electron. Syst.*, 29(3), April 2024.

    - [18] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for
    code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*, 2022.

    - [19] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan,
    Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. Benchmarking large language
    models for automated verilog rtl code generation. In *2023 Design, Automation
    & Test in Europe Conference & Exhibition (DATE)*, pages 1–6. IEEE, 2023.

    - [20] Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, and Zhiyao Xie.
    Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source
    dataset and lightweight solution. In *2024 IEEE LLM Aided Design Workshop (LAD)*,
    pages 1–5. IEEE, 2024.

    - [21] Hasan Genc, Seah Kim, Alon Amid, Ameer Haj-Ali, Vighnesh Iyer, Pranav Prakash,
    Jerry Zhao, Daniel Grubb, Harrison Liew, Howard Mao, Albert Ou, Colin Schmidt,
    Samuel Steffl, John Wright, Ion Stoica, Jonathan Ragan-Kelley, Krste Asanovic,
    Borivoje Nikolic, and Yakun Sophia Shao. Gemmini: Enabling systematic deep-learning
    architecture evaluation via full-stack integration. In *2021 58th ACM/IEEE Design
    Automation Conference (DAC)*, pages 769–774, 2021.

    - <span id="page-6-16"></span>[22] Deepak Vungarala, Mahmoud Nazzal, Mehrdad Morsali,
    Chao Zhang, Arnob Ghosh, Abdallah Khreishah, and Shaahin Angizi. Sa-ds: A dataset
    for large language model-driven ai accelerator design generation, 2024.

    - <span id="page-6-15"></span>[23] Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu
    Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan Celine Lin. Gpt4aigchip: Towards
    next-generation ai accelerator design automation via large language models. In
    *2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)*, pages
    1–9, 2023.

    - <span id="page-6-17"></span>[24] Yun-Da Tsai, Mingjie Liu, and Haoxing Ren.
    Rtlfixer: Automatically fixing rtl syntax errors with large language models. *arXiv*,
    2023.

    - <span id="page-6-18"></span>[25] Zixi Zhang, Greg Chadwick, Hugo McNally, Yiren
    Zhao, and Robert Mullins. Llm4dv: Using large language models for hardware test
    stimuli generation. *ArXiv*, abs/2310.04535, 2023.

    - <span id="page-6-20"></span>[26] Khushboo Qayyum, Muhammad Hassan, Sallar Ahmadi-Pour,
    Chandan Kumar Jha, and Rolf Drechsler. From bugs to fixes: Hdl bug identification
    and patching using llms and rag. In *2024 IEEE LLM Aided Design Workshop (LAD)*,
    pages 1–5, 2024.

    - [27] Ke Xu, Jialin Sun, Yuchen Hu, Xinwei Fang, Weiwei Shan, Xi Wang, and Zhe
    Jiang. Meic: Re-thinking rtl debug automation using llms. *arXiv*, 2024.

    - <span id="page-6-19"></span>[28] Baleegh Ahmad, Shailja Thakur, Benjamin Tan,
    Ramesh Karri, and Hammond Pearce. On hardware security bug code fixes by prompting


    large language models. *IEEE Transactions on Information Forensics and Security*,
    19:4043–4057, 2024.


    - <span id="page-7-0"></span>[29] Kounianhua Du, Jizheng Chen, Renting Rui, Huacan
    Chai, Lingyue Fu, Wei Xia, Yasheng Wang, Ruiming Tang, Yong Yu, and Weinan Zhang.
    Codegrag: Bridging the gap between natural language and programming language via
    graphical retrieval augmented generation. *arXiv*, 2024.

    - [30] Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael
    Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories
    via code graph databases. *arXiv*, 2024.

    - <span id="page-7-1"></span>[31] Ibrahim Abdelaziz, Julian Dolby, James P McCusker,
    and Kavitha Srinivas. A toolkit for generating code knowledge graphs. *The Eleventh
    International Conference on Knowledge Capture (K-CAP)*, 2021.

    - <span id="page-7-2"></span>[32] Ieee standard for verilog hardware description
    language. *IEEE Std 1364- 2005 (Revision of IEEE Std 1364-2001)*, pages 1–590,
    2006.

    - <span id="page-7-3"></span>[33] M. Gordon. The semantic challenge of verilog
    hdl. In *Proceedings of Tenth Annual IEEE Symposium on Logic in Computer Science*,
    pages 136–145, 1995.

    - <span id="page-7-4"></span>[34] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie.
    Rtllm: An opensource benchmark for design rtl generation with large language model.
    In *2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)*,
    pages 722–727. IEEE, 2024.

    - <span id="page-7-5"></span>[35] Hammad Ahmad, Yu Huang, and Westley Weimer.
    Cirfix: automatically repairing defects in hardware design code. In *Proceedings
    of the 27th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems*, ASPLOS ''22, page 990–1003, New York, NY, USA,
    2022.

    - <span id="page-7-6"></span>[36] Shinya Takamaeda-Yamazaki. Pyverilog: A python-based
    hardware design processing toolkit for verilog hdl. In *Applied Reconfigurable
    Computing*, volume 9040 of *Lecture Notes in Computer Science*, pages 451–460.
    Springer International Publishing, Apr 2015.

    - <span id="page-7-7"></span>[37] F. Zaruba and L. Benini. The cost of application-class
    processing: Energy and performance analysis of a linux-ready 1.7-ghz 64-bit risc-v
    core in 22-nm fdsoi technology. *IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems*, 27(11):2629–2640, Nov 2019.

    - <span id="page-7-8"></span>[38] Scott Johnson, Dominic Rizzo, Parthasarathy
    Ranganathan, Jon Mc-Cune, and Richard Ho. Titan: enabling a transparent silicon
    root of trust for cloud. In *Hot Chips: A Symposium on High Performance Chips*,
    volume 194, page 10, 2018.

    - <span id="page-7-9"></span>[39] William L. Hamilton, Rex Ying, and Jure Leskovec.
    Inductive representation learning on large graphs, 2018.

    - <span id="page-7-10"></span>[40] Ahmed Allam and Mohamed Shalan. Rtl-repo: A
    benchmark for evaluating llms on large-scale rtl design projects. *arXiv*, 2024.

    - <span id="page-7-11"></span>[41] Sonnet Anthropic. Model card addendum: Claude
    3.5 haiku and upgraded claude 3.5 sonnet.

    - <span id="page-7-12"></span>[42] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang,
    Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang,
    Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran
    Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin.
    Qwen2.5-coder technical report. *arXiv*, 2024.

    - <span id="page-7-13"></span>[43] Aaron Grattafiori, Abhimanyu Dubey, Abhinav
    Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil
    Mathur, Alan Schelten, Alex Vaughan, Amy Yang, and et al. The llama 3 herd of
    models. *arXiv*, 2024.

    - <span id="page-7-14"></span>[44] Stephen Robertson, Hugo Zaragoza, et al. The
    probabilistic relevance framework: Bm25 and beyond. *Foundations and Trends® in
    Information Retrieval*, 3(4):333–389, 2009.

    - <span id="page-7-15"></span>[45] OpenRISC. mor1kx. [https://github.com/openrisc/mor1kx,](https://github.com/openrisc/mor1kx)
    2022.

    - <span id="page-7-16"></span>[46] Chin-Yew Lin. ROUGE: A package for automatic
    evaluation of summaries. In *Text Summarization Branches Out*, pages 74–81, Barcelona,
    Spain, July 2004. Association for Computational Linguistics.

    - <span id="page-7-17"></span>[47] Andre Nakkab, Sai Qian Zhang, Ramesh Karri,
    and Siddharth Garg. Rome was not built in a single step: Hierarchical prompting
    for llmbased chip design. In *Proceedings of the 2024 ACM/IEEE International Symposium
    on Machine Learning for CAD*, MLCAD ''24, New York, NY, USA, 2024. Association
    for Computing Machinery.

    - <span id="page-7-18"></span>[48] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao
    Xie. Rtllm: An opensource benchmark for design rtl generation with large language
    model. In *2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)*,
    pages 722–727, 2024.


    APPENDIX


    ### <span id="page-8-1"></span>*A. Multi-level Retrieval*


    Figure [9](#page-8-0) demonstrates two real-world repository-level examples used
    for multi-level retrieval, which are CVA6 and Opentitan, respectively. The highlighted
    part demonstrates the retrieval query.


    #### (Issue CVA6-2732)


    #### **[BUG] Cross-privilege TLB leakage through SLS**


    Our microarchitectural fuzzer has found that CVA6 is susceptible to SLS (straight-line
    speculation [1]) and thus allows **leakage** through the **TLB** across privileges.
    Since **speculatively issued loads and stores** from a higher privilege access
    the TLB, their addresses can be recovered from a lower privilege. Thus, privileged
    code that (architecturally) does not leak any sensitive data through its control
    flow or memory operations, leaks transiently to an unprivileged attacker.


    We provide a snippet from the generated test case bellow:.......


    #### (Issue Opentitan-26355)


    **[sram\_ctrl,rtl] Remove macro timing assumptions**


    Some parts of the **sram\_ctrl** design (e.g., **the readback feature**) make
    assumptions about the **timing of the underlying SRAM macro** (e.g., a read always
    comes back at the next cycle).


    We should identify those assumptions and rewrite the design such that the controller
    can handle different SRAM macros.


    <span id="page-8-0"></span>Fig. 9. Two real-world issues posted in repository-level
    projects.


    Table [I](#page-8-2) shows the code retrieval APIs containing mainly five APIs:
    ℎ , ℎ , ℎ , ℎ and ℎ . The output here can be either the name or code of different
    abstraction levels, depending on the API definition.


    TABLE I LIST OF CODE RETRIEVAL APIS


    <span id="page-8-2"></span>


    | API name              | Description                     | Output         |

    |-----------------------|---------------------------------|----------------|

    | 𝑠𝑒𝑎𝑟 𝑐ℎ 𝑚𝑜𝑑𝑢𝑙𝑒        | Search for module               | Name of module |

    | 𝑠𝑒𝑎𝑟 𝑐ℎ 𝑏𝑙𝑜𝑐𝑘         | Search for block                | Code of block  |

    | 𝑠𝑒𝑎𝑟 𝑐ℎ 𝑠𝑖𝑔𝑛𝑎𝑙        | Search for signal               | Name of signal |

    | 𝑠𝑒𝑎𝑟 𝑐ℎ 𝑚𝑜𝑑𝑢𝑙𝑒 𝑏𝑙𝑜𝑐𝑘  | Search for block in the module  | Code of block  |

    | 𝑠𝑒𝑎𝑟 𝑐ℎ 𝑚𝑜𝑑𝑢𝑙𝑒 𝑠𝑖𝑔𝑛𝑎𝑙 | Search for signal in the module | Name of signal |


    Through the annotations, we want to measure how relevant would these results are
    to your Verilog design.


    - You don''t have to be absolutely certain about the correctness of the code.


    - You might be interested in copy-pasting the code, finding a project to use or
    just getting some understanding about how something is implemented.


    - You might be searching within your project (e.g., to reuse modules, signals,
    or testbench constructs) or to understand how a particular digital circuit is
    structured.


    Please annotate the results according to the following scheme:


    - **3: Exact match**. This Verilog snippet is exactly what I was looking for.
    I would directly integrate it into my design with minimal adaptations.


    - **2: Strong match**. The snippet largely meets my requirements. I might use
    it as a backbone for my hardware module, but some modifications or additional
    verification might be needed.


    - **1: Weak match**. Although the snippet is not a perfect fit, it contains useful
    structural elements, coding patterns, or testbench ideas that could guide further
    exploration.


    - **0: Totally irrelevant**. This snippet does not address the query or Verilog
    design challenge at all.


    <span id="page-8-4"></span>Fig. 10. Two real-world issues posted in repository-level
    projects.


    Table [II](#page-8-5) describes the scales and classification of the proposed
    HDLSearch benchmark. The benchmark contains 10 repository-level designs from FPGA
    projects to CPUs, including coffee machine, CNN acc, image compression, AIB, IIC,
    RIFFA, Ethernet, AXIS, MIPS and E203-hbirdv2.


    TABLE II HDLSEARCH BENCHMARK DESCRIPTION AND SCALES


    <span id="page-8-5"></span>


    | Design            | Description                                                   |
    Lines  | Modules | Blocks |

    |-------------------|---------------------------------------------------------------|--------|---------|--------|

    | FPGA Project      |                                                               |        |         |        |

    | coffee machine    | An<br>FPGA-based<br>coffee                                    |
    890    | 8       | 26     |

    |                   | machine control circuit                                       |        |         |        |

    | CNN acc           | An FPGA-based CNN ac                                          |
    866    | 9       | 22     |

    |                   | celerator                                                     |        |         |        |

    | image compression | An<br>FPGA-based<br>JPEG                                      |
    2340   | 18      | 79     |

    |                   | image compression circuit                                     |        |         |        |

    | Interconnection   |                                                               |        |         |        |

    | AIB               | Advanced Interface Bus                                        |
    9994   | 14      | 184    |

    |                   | protocol                                                      |        |         |        |

    | IIC               | Inter-Integrated<br>Circuit                                   |
    1540   | 2       | 11     |

    |                   | protocol                                                      |        |         |        |

    | RIFFA             |                                                               |
    41279  | 5       | 24     |

    |                   | Reusable Integration Framework for FPGA Accelerators protocol
    |        |         |        |

    |                   |                                                               |        |         |        |

    | Ethernet          | Ethernet protocol                                             |
    130457 | 12      | 177    |

    | AXIS              | AXI-Stream protocol                                           |
    15269  | 11      | 132    |

    | CPU               |                                                               |        |         |        |

    | MIPS              | A MIPS RISC-V CPU                                             |
    706    | 9       | 23     |

    | E203-hbirdv2      | The second version of                                         |
    38577  | 6       | 88     |

    |                   | the Hummingbird E203                                          |        |         |        |

    |                   | RISC-V processor                                              |        |         |        |

    |                   |                                                               |        |         |        |


    #### <span id="page-8-3"></span>*B. HDLSearch Benchmark*


    Figure [10](#page-8-4) demonstrates the template instructions used in the HDLSearch
    benchmark refinement.'
- title: "FAV-NSS: An HIL Framework for Accelerating Validation of Automotive\n  Network\
    \ Security Strategies"
  abstract: 'Complex electronic control unit (ECU) architectures, software models
    and

    in-vehicle networks are consistently improving safety and comfort functions in

    modern vehicles. However, the extended functionality and increased connectivity

    introduce new security risks and vulnerabilities that can be exploited on

    legacy automotive networks such as the controller area network (CAN). With the

    rising complexity of vehicular systems and attack vectors, the need for a

    flexible hardware-in-the-loop (HIL) test fixture that can inject attacks and

    validate the performance of countermeasures in near-real-world conditions in

    real time is vital. This paper presents an FPGA-based HIL framework tailored

    towards validating network security approaches (IDS, IPS) and smart integration

    strategies of such capabilities for an automotive CAN bus. FAV-NSS replicates

    an actual vehicular system environment with functional ECUs and network

    infrastructure on an FPGA, allowing functional validation of IDS/IPS

    algorithms, accelerator designs and integration schemes (software task on ECU,

    dedicated accelerator). To show the efficacy of FAV-NSS, we evaluate an IDS

    accelerator integration problem, both as a traditional coupled accelerator (to

    the ECU), and secondly close to the CAN controller (mimicking an extended CAN

    controller). We show that the latter strategy can be fully validated by our

    framework, which would otherwise require integration of specialised CAN modules

    into otherwise standard HIL fixtures with ability to instrument internal

    signals for characterising timing performance. The tests demonstrate a

    promising latency reduction of 6.3x when compared to the traditional coupled

    accelerator. Our case study demonstrates the potential of FAV-NSS for

    accelerating the optimisation, integration and verification of smart ECUs and

    communication controllers in current and future vehicular systems.'
  url: http://arxiv.org/abs/2505.15393v1
  keywords: Hardware in the Loop, Controller Area Network, Intrusion Detection System,
    Machine Learning, Field Programmable Gate Arrays, Quantised Neural Nets
  document: '#### I. INTRODUCTION


    Recent years have seen rapid adoption of intelligent systems in production vehicles
    that improve the safety, reliability, and comfort of users [\[1\]](#page-7-0).
    The advancements are enabled by a number of networked electronic control units
    (ECUs) that run software tasks to observe, monitor, and control different sensors
    and actuators in real-time. Typically, over 100 ECUs are present in modern cars
    [\[2\]](#page-7-1) that exchange information over robust communication protocols
    such as local interconnect network (LIN), FlexRay, controller area network (CAN),
    and Automotive Ethernet. Despite numerous security issues, CAN continues to be
    the most widely used network protocol for critical and non-critical functions
    in modern vehicles.


    Development of automotive functions are typically done in silos and integrated
    into test fixtures for validating their functionality prior to deployment in real
    systems. Hardwarein-loop (HIL) simulation/emulation has been used for realtime
    testing of embedded systems, especially in the automotive and aerospace areas
    [\[3\]](#page-7-2), [\[4\]](#page-7-3). Multiple research has shown that field
    programmable gate arrays (FPGAs) are optimal target platforms for a flexible HIL
    setup. FPGA-based HIL systems allow rapid exploration and validation of design
    choices through real-time emulation and reprogramming, rather than requiring expensive
    rewiring and re-validation with fixed platform-based HIL models. HIL setups are
    also adopted in automotive systems development, primarily for functionallevel
    and system-level verification of new features. With increasing connectivity in
    automotive systems, system and network security verifications are increasingly
    gaining attention in the automotive domain. Unlike functional and system-level
    tests, an HIL system for in-vehicle safety research should be capable of rapidly
    exploring design solutions and verifying the constructed safety methods for myriad
    network conditions and ECU functions. It should provide means for injecting (new)
    attack features and real-time recording and inspection, allowing end-to-end verification
    of security countermeasures and quantify their impacts on key ECU functions. Techniques
    such as quantised neural networks based IDS [\[5\]](#page-7-4) provide unique
    integration strategies for security measures, moving them closer to the network
    controller, as opposed to traditional ECU-coupled and software IDS solutions.
    Validating the integration of these functions (in software/hardware) would require
    extensive modifications in a fixed HIL environment, and hence, impose restrictions
    on investigating novel security schemes, integration strategies and their validation
    in vehicular networks.


    This paper presents a HIL framework for testing and validating network security
    schemes (such as IDSs & IPSs) for CAN-based vehicular systems, with an FPGA as
    the heart of the framework. Our CAN testbed uses an Artix-7 FPGA to emulate multiple
    ECUs that are interconnected with a ''virtual'' CAN bus on the logic, with the
    ability to expand to multi-FPGA setups for scalability. The testbed can be controlled
    and configured through both a GUI-based or an API-based interface, and supports
    injection of attack vectors (including preconfigured ones such as DoS), replay
    of captured sequences (for replicating datasets), programming ECUs, and real-time
    acquisition and monitoring and analysis of CAN bus data and ECU status signals.
    To demonstrate the potential of the platform, we investigate two case studies
    which present different IDS integration pathways – first, as a coupled accelerator
    to the ECU and second, as an integration closer to the CAN controller of the ECU.
    In both cases, we use an identical quantised neural network as the IDS to show
    that the platform uniquely provides the ability to evaluate the difference in
    integration strategies, in terms of detection performance and latency, in addition
    to the standard functional verification and end-to-end validation. The major contributions
    of this paper are as follows:


    - An FPGA-based hardware-in-loop testbed with configurable parameters for accelerating
    validation and testing of CAN IDS/IPS at both the ECU and networked function levels,
    while capturing ECU/function-level performance impacts due to IDS/IPS.

    - Case study demonstrates the adaptability of the HIL platform to two different
    integration strategies for intrusion detection systems: the conventional coupled
    accelerator and the novel IDS-enabled CAN controller. We show that either integration
    can be tested and qualified against a range of attack vectors that can be injected
    under software control (GUI/API) on FAV-NSS.

    - Additionally, the timing characterisation of the integration strategy on FAV-NSS
    shows that IDS integrated close to a CAN controller reduces the detection latency
    by 6.3×, allowing line-rate detection on most CAN networks.


    The rest of this paper is organized as follows. Section [II](#page-1-0) provides
    background and related research works on CAN security and mitigation schemes as
    well as on hardwarein-loop testbed and components; Section [III](#page-2-0) describes
    the system architecture and implementation of the hardware-inthe-loop testbed,
    the software framework and the IDS model used for the case study. Section [IV](#page-4-0)
    outlines the different integration methods for the IDS, with section [V](#page-5-0)
    capturing the observed results from testing across different configured and programmable
    attacks, as well as the latency gains enabled by the different integration strategies.
    Finally, we conclude the paper in section [VI.](#page-6-0)


    #### II. BACKGROUND AND RELATED WORKS


    ## <span id="page-1-0"></span>*A. CAN security threats & mitigations*


    CAN provides numerous advantages over competing protocols such as resistance to
    electromagnetic interference (due to twisted wire transmission), integrated arbitration
    and priorities, and its linear broadcast bus topology leading to lower overall
    cabling weight and cost. However, due to increased connectivity to the external
    systems, some of these built-in properties can be exploited for network intrusion
    attacks, affecting the safety and reliability of the systems interconnected by
    CAN [\[6\]](#page-7-5). Vulnerabilities in the CAN protocol have been thoroughly
    evaluated in the research literature [\[7\]](#page-7-6)–[\[9\]](#page-7-7). The
    broadcast nature allows all ECUs to receive communication


    ![](_page_1_Figure_8.jpeg)


    <span id="page-1-1"></span>Fig. 1. Figure shows a compromised ECU using an active
    DoS attack to block critical message communication. In the figure, the engine
    control ECU fails to receive a message from the body control module due to the
    active DoS injections on the bus.


    from any known/unknown/infected ECU on the network, passively sniff data on the
    network to identify/record unique information or reuse the sensor/actuator commands
    for replay attacks. The built-in arbitration scheme could be exploited for targeted
    attacks or simple flooding and DoS attacks, completely crippling the network.
    In [\[10\]](#page-7-8), the authors were the first to formulate a theoretical
    risk level corresponding to the level of attacks that could be injected into vehicles.
    Further in [\[11\]](#page-7-9), [\[12\]](#page-7-10), the authors showed that
    it was possible to inject information frames into the vehicle control bus to bypass
    *driver* and *system* control to maliciously take over critical functions in the
    vehicle. In [\[13\]](#page-7-11), authors proposed a targeted DoS attack method
    on the CAN bus, which can isolate specific ECUs on the network and prevent them
    from communicating. While many attack vectors could exploit these vulnerabilities
    in CAN, the most widely reported ones in the literature are *DoS*, *fuzzing* and
    *targeted spoofing* attacks [\[8\]](#page-7-12). DoS attacks can be launched on
    the CAN bus through *bus flooding*, exploiting the *error propagation* feature
    or by preventing the *sleepwake* sequence causing the ECUs to stay in sleep mode.
    Bus flooding attack, shown in Fig. [1,](#page-1-1) is the most common DoS attack
    where the malicious ECU floods the bus with highpriority messages, preventing
    all other ECUs from using the bus. Error propagation attacks are more sophisticated,
    where the error handling scheme in CAN is exploited by intentionally causing bit
    stuffing errors, which triggers listening ECUs to send error frames, thus stifling
    the bus. Fuzzing attacks use random payloads to disrupt normal functionality or
    to identify vulnerabilities in a specific ECU by observing their response to the
    payload. Targeted spoofing attacks use specific payloads aimed at one or more
    components, forcing them to perform incorrect operations or generate incorrect
    responses when polled.


    To address these issues, multiple IDS approaches have been proposed in the research
    literature. Although many rule-based and signature-based IDSs were initially suggested,
    deep neural network (DNN)-based IDS solutions have increasingly gained traction,
    as they have shown to be more accurate than rule- and signature-based IDS solutions
    [\[9\]](#page-7-7), [\[14\]](#page-7-13)–[\[16\]](#page-7-14). Optimisations such
    as quantisation and pruning are often employed to make these DNN-based IDSs deployable
    in resource-constrained invehicle environments [\[5\]](#page-7-4).


    ## *B. CAN testbeds*


    CAN testbeds have been proposed and developed to model, test and validate CAN-based
    ECUs. In [\[17\]](#page-7-15), the authors proposed a virtual CAN overlay that
    abstracts the communication interface of the Multiprocessor System-on-Chip (MPSoC)
    and provides programmers with an application programmer interface (API) for interacting
    with the CAN network. The work in [\[18\]](#page-7-16) presents a scheme to establish
    identical time base and message order in a virtual CAN network as the real (physical)
    one. A lightweight CAN virtualisation is proposed in [\[19\]](#page-7-17) for
    virtual controllers to improve the functional and Quality of Service(QoS) issues
    with prior works by reducing the virtualisation overhead by 20%. A novel device-level
    virtualisation is proposed in [\[20\]](#page-7-18) allowing CAN to be deployed
    in Integrated Modular Avionics (IMA) architecture. However, most of the virtualisation
    schemes emulate the CAN layer and are not compatible with external hardware CAN
    instances or sensors. This limited the scalability and adaptability of these simulation-based
    schemes for real-time multi-ECU test setups. Similar to network virtualisation,
    ECU virtualisation has also been explored for developing and optimising ECU architectures
    for performance and safety [\[21\]](#page-7-19), functional validation of automotive
    software applications [\[22\]](#page-7-20), [\[23\]](#page-7-21) among others.
    For large-scale testing, the work in [\[24\]](#page-7-22) showed the use of virtual
    ECUs as abstractions integrated into a network-on-chip environment that is compliant
    with AUTOSAR specifications. Our work takes inspiration from these approaches
    but uses a RISC soft-core processor-based system as the ECU core with CAN controller
    logic as a memory-mapped IP for each ECU; ECUs are subsequently interconnected
    through a virtual CAN bus to model the systems as closely as possible to the real
    world.


    # *C. HIL Test Setups*


    Hardware-in-the-loop simulators/emulators are widely used for embedded system
    performance evaluation systems to accelerate the testing and validation of the
    performance of the system during development or revision phases in an environment
    that closely matches real-world settings. Early reported use of HIL simulators
    was in the development and testing phases of fly-by-wire systems, flight simulation
    [\[25\]](#page-7-23) and missile guidance systems [\[26\]](#page-7-24), and for
    subsystem level verification of components of spacecraft systems [\[27\]](#page-7-25).
    Several R&D efforts in the automotive domain subsequently adopted HIL simulations
    to verify ECU architecture, functionality and for safety testing [\[28\]](#page-7-26),
    [\[29\]](#page-7-27). Extensive software models have been used to recreate system
    dynamics (e.g., of engines) when validating ECU functions in an HIL setup [\[30\]](#page-7-28).
    Such frameworks often tend to be specialised (and thus relatively inflexible),
    requiring different setups to validate specific capabilities – for e.g., setups
    for protocol-level development/changes are inflexible to be adapted for end-to-end
    functional validation of ECU architecture/software components. Additionally, capturing
    low-level details leads to highly complex software models that force the tests
    to be run at a slower speed than real-world deployment conditions. Alternatively,
    some models abstract away low-level details to improve speed of simulation/emulation,
    and in both cases lead to additional development time for functional, safety and
    integration tests at the system level. Field programmable gate arrays (FPGAs)
    have increasingly become a key component of such systems, allowing much simpler
    reconfiguration of the setup for different testing/development scenarios while
    also offering real-time performance as the dedicated hardwarebased setups [\[3\]](#page-7-2),
    [\[31\]](#page-7-29), [\[32\]](#page-7-30). Our work builds on this approach,
    with a focus on enabling rapid prototyping and validation of (hardware-) accelerated
    network security approaches (IDS, IPS, smart controllers, integration strategies
    and others) for in-vehicle networks.


    ## III. SYSTEM ARCHITECTURE


    <span id="page-2-0"></span>In this section, we present the system architecture
    of the HIL framework and the CAN testbed. The high-level overview of the framework
    is shown in Fig. [2.](#page-3-0) The framework is comprised of a software GUI
    front-end which interacts with the CAN testbed that is implemented on an FPGA.
    The software can access pre-prepared test profiles, ECU application object code
    and relevant test cases from a pool of resources which can be used to set up and
    automate the test cases. The interface provides functionality to download the
    test configuration to the testbed, download target code to the ECUs, execute the
    test cases (attacks), perform real-time monitoring of the test and generate data
    dumps for detailed analysis. For this development, we have used AMD''s Artix-7
    XC7A200T FPGA on the Digilent Nexys Video kit. The large FPGA can accommodate
    numerous soft-core processors with their own peripherals and CAN network interface
    (mimicking full-fledged ECUs) as well as dedicated blocks for injecting attacks,
    controlling the test setups and for real-time observation/monitoring. Each ECU
    runs its own application(s) independently and uses the CAN interface to communicate
    with other ECUs on the network. In addition to the software inputs, the dedicated
    I/O on the Nexys board is also mapped as control and status inputs to each of
    the ECUs. We discuss the various components of the testbed in detail in the next
    sections.


    ## *A. Software Subsystem*


    The hardware testbed is controlled and configured primarily through a graphical
    interface we developed using PyQt. Each action in the GUI, partitioned into one
    of the 7 sections on the GUI as seen in Fig. [2,](#page-3-0) uses a set of APIs
    defined in Table [I](#page-3-1) to interface with the hardware testbed via JTAG,
    UART and Ethernet links from the host machine. The GUI primarily relies on click-based
    configuration although the APIs could be wrapped into scripts to automate the
    testing. The interface configuration function uses the UART and JTAG APIs to perform
    the configuration of a series of hardware registers, counters and special function
    blocks in the testbed and to perform serial status monitoring of different ECUs.
    This link is also used to configure the Ethernet run-time debugger link between
    the host and the testbed. The ECU configuration subsystem is used to load different
    applications and system configurations onto a target ECU, primarily through the
    JTAG APIs. These are invoked when loading specific *elf* files to the ECU, to
    reset specific ECUs during a testing session and to periodically poll the status
    of the ECUs for updating on the GUI.


    ![](_page_3_Figure_0.jpeg)


    <span id="page-3-0"></span>Fig. 2. Overview of the architecture of the hardware-in-the-loop
    simulator showing how the different components interact with each other.


    TABLE I INTERFACES AND APIS


    <span id="page-3-1"></span>


    | Link/Function      | API              | Description                                                                                                    |

    |--------------------|------------------|----------------------------------------------------------------------------------------------------------------|

    | UART/\$monitor     | sys log()        | Logs of basic system status, high-level
    er<br>rors, and IDS stats                                              |

    | UART/\$config      | sys ctrl()       | Transmits configuration parameters to
    setup<br>ECU frequency, CAN bus mode, and default<br>simulation settings |

    | UART/\$attack      | att ctrl()       | Transmits specified injection attack
    content<br>or delivers DoS attack and fuzzing attack                      |

    | UART/\$monitor     | monitor()        | commands to the corresponding attack
    core<br>low-resolution log of CAN bus and the IDS<br>ECU signals          |

    | JTAG/\$config      | prog node()      | Download ELF to specific ECU, configure<br>special
    nodes, and expose basic debugging                           |

    | JTAG/\$control     | reset node()     | Apply reset sequence to the specific
    node or<br>all nodes                                                      |

    | JTAG/\$config      | debug config()   | Configure wave capture through bridge<br>node.                                                                 |

    | Ethernet/\$monitor | bus log()        | Monitors CAN bus data at highest resolu<br>tion.                                                               |

    | Ethernet/\$monitor | signal capture() | Captures pre-defined signals such as
    virtual<br>CAN bus levels, node data.                                     |

    | Ethernet/\$monitor | user capture()   | Monitors user-defined byte or bit-level
    data.                                                                  |


    The interface configuration function is used to establish a reliable communication
    interface between the CAN testbed and the configuration/analysis features of the
    software system. It can be used to specify the interface parameters and to optionally
    set up high-speed connectivity between the host and the testbed. Once the connectivity
    is established, the ECU configuration subsystem is used to load different applications
    and system configurations onto a target ECU. The interface can be used to load
    specific *elf* files to the ECU and/or to reset the ECU to a known good state
    if it encounters errors during the testing phase. Once configured and active,
    the status of the ECU and its parameters are periodically polled and updated on
    the GUI when a test is in progress.


    A key feature of our framework is the real-time monitoring of the test setup and
    internal signals from the hardware platform at very high granularity through a
    set of monitoring APIs. The monitoring APIs allow for reading specific status
    signals on demand or automatically reading them at a defined periodicity (default
    300 ms) and logging them for further analysis. The API also allows simple operations
    to be applied to the status signals (such as conditional checks and logical operations)
    while being read from the hardware testbed to flag any anomalies during a testing
    session in near-real-time. Additionally, selected signals (from ECUs) and the
    bus activity are captured in real-time and transferred back to the host for visualising
    system conditions. The bridge node can be configured using the configuration APIs
    to capture a sequence of signals at the highest sampling rate (clock speed) and
    pack them as layer-2 Ethernet packets to be sent to the host. On the host, the
    receiving API will decode the signals from the packet, add them to a wave visualiser
    for real-time monitoring of the ECU states and the CAN network, and logs them
    for offline analysis, if specified.


    Finally, the attack injection and control are handled from the GUI which invokes
    dedicated APIs that communicate with the control node on the testbed. Using our
    framework, raw attack messages from openly available datasets can be replayed
    on the internal bus by the control node to recreate the attack scenario on our
    testbed. The APIs also allow for specific attack vector injections whereby the
    control node injects a targeted attack message on the CAN bus, to model spoofing
    attacks. Large-scale DoS and Fuzzing attacks can also be launched this way, although
    the dedicated node on the hardware can be used to inject them at higher speed
    by specifying a flag in the API call. The APIs can also load long sequences of
    attack vectors and benign messages (in a csv file, for instance) into the test
    environment from the GUI and trigger the logging system for automating long-form
    tests. An extension of the logging API can be additionally used to perform quick
    parsing of the captured CAN data bus to identify known attacks (such as flooding-based
    DoS), tag them using labels, and log them for further analysis. The tagging can
    be used to develop new attack datasets and/or to support/validate online training
    of supervised learning models with the hardware model-in-theloop.


    #### *B. Hardware Subsystem*


    The hardware subsystem (testbed) of our framework implements the functional blocks
    of ECUs, the CAN bus, and specialised control blocks that implement attack injection,
    monitoring, and interfacing with the host PC. For our deployment, we use AMD''s
    Artix-7 XC7A200T (Nexys Video board) as the FPGA platform to implement the testbed.


    *1) CAN Controller and Virtual CAN bus:* To model the CAN network, we modified
    the open-source CAN controller''s host interface by adapting an AXI4 interface
    (for configuration by ECU) and exposing selected internal signals via a register
    interface to the bridge node. At the bus end, we use buffers to connect *tx* and
    *rx* pins to the shared CAN bus (virtual bus) with tristate logic on the *tx*
    lines to control transmission. The


    <span id="page-4-1"></span>TABLE II TEST ECU FUNCTIONS DEPLOYED FOR THE EVALUATION


    | ECU# | Implemented Function                                  |

    |------|-------------------------------------------------------|

    | ECU1 | Engines and brake control unit                        |

    | ECU2 | Airbag actuators and light sensor for auto headlights |

    | ECU3 | Brake sensors and Collision detection sensors         |

    | ECU4 | Controls headlights, tail-lights and brake lights     |


    virtual bus implements the wired-AND logic behaviour of a physical CAN bus on
    twisted-pair connections, thus allowing all CAN protocol specifications such as
    inbuilt arbitration and error flagging on the virtual bus. The bus functions and
    CAN controller functionality were validated by wiring up to offthe-shelf CAN controllers.
    The virtual bus exposes the CAN signals via the Pmod interface on the Nexys board
    for external monitoring (via scope), wiring up to physical transceivers for bus
    expansion, and for scaling the testbed across multiple FPGA boards.


    *2) ECU models:* In the testbed, ECUs are modeled around MicroBlaze soft processor
    and dedicated peripherals including our modified CAN controller. The main functions
    of the 4 ECU system we emulated for our tests are shown in Table [II.](#page-4-1)
    Each ECU receives an independent clock signal from a clock manager that can be
    configured to the required clock speed at startup (from the host PC). At runtime,
    the software tasks on the ECU perform periodic (and event-driven) processing of
    sensor inputs and CAN messages and determines responses to be applied to actuators
    and/or to be sent via the CAN interface. For instance, in Figure [1](#page-1-1)
    when *ECU3* detects a collision from the sensor input or CAN message, *ECU1* read
    a corresponding actuating message on the CAN bus to block the engine and transmission
    unit, and *ECU2* trigger the airbags based on the CAN message to protect the passengers.
    Additionally, an external reset can be applied to a specific ECU using the I/O
    switches on the Nexys platform. The software application for each ECU is implemented
    in bare-metal C and compiled using Vitis-SDK. Additionally, each ECU implements
    a task event counter as a life signal, the value of which is periodically transmitted
    as a CAN message to indicate its status. At the host, any deviation in the rate
    of increment can be detected as an additional load on the ECU during the testing
    phase.


    *3) FINN-IDS core:* To show the feasibility of validating IDS models using the
    framework, we integrated a 5-layer 4 bit quantised multi-layer perceptron (MLP)
    network described in [\[5\]](#page-7-4) as the IDS engine. The model was trained
    and quantised using the brevitas quantisation aware training library [\[33\]](#page-7-31)
    and compiled to hardware using the FINN toolchain from AMD [\[34\]](#page-7-32).
    The FINN flow generates an AXI-stream IP block of the model which is then integrated
    as a coupled accelerator to the ECU (case study-I) and also at the CAN controllers''
    host-interface logic (case study-II). The specific MLP model was chosen as it
    provided state-of-the-art detection accuracy across multiple attack vectors with
    low resource- and energyoverhead for near-line-rate detection on CAN systems.
    Note that the same IDS IP block is used across both case studies to show the ability
    of the flexible platform to support different integration schemes and to perform
    trade-off evaluations in near-real-world conditions.


    *4) External integration & extensions:* While our tests use self-contained ECUs
    with limited external I/O, the testbed can be extended to allow hardware sensors
    and actuators (or other I/O functions) to be connected to specific ECUs for a
    full-fledged HIL environment. Similarly, external standalone ECUs could be integrated
    into the test setup by connecting the virtual CAN bus to a CAN PHY and transceiver
    to support the line voltage on the physical layer. As mentioned before, the hardware
    platform can be extended through the pmod interface for multi-FPGA testbeds. The
    software component can also scale in the above cases to enable real-time monitoring,
    logging and attack injection in the expanded test setup, and can be extended to
    support interfacing to multiple FPGAs independently to monitor all ECUs with relative
    ease.


    # IV. IDS INTEGRATION CASE STUDY


    <span id="page-4-0"></span>We used the Artix-7 XC7A200T on the Nexys Video development
    board as our testbed for the HIL simulator. For our tests, the CAN controllers
    are configured for 500 kbps operation, and the ECUs are clocked using independent
    100 MHz clocks. WaveTrace was used to display the real-time waveform data received
    on the host machine. We evaluate the capabilities by testing two IDS integration
    strategies progressing from an ECU-coupled accelerator to an embedded accelerator
    within the CAN controller''s datapath.


    # *A. Case Study - I: IDS as a coupled accelerator*


    In this case, the IDS model is integrated as a traditional memory-mapped accelerator
    to an ECU, MB 5 in Fig. [3,](#page-5-1) to form an IDS-enabled ECU. The IDS IP
    is attached as an AXI-streaming accelerator to the MicroBlaze processor, which
    is dedicated to executing the IDS task. All other ECUs in the network perform
    a different function as described before. When a new message is available at the
    CAN interface of the IDS-enabled ECU, the message is read and pre-processed by
    a software task to generate a window of 4 messages, which is fed as the input
    feature to the IDS core to check for attack signatures. Once the IDS IP computes
    the result, a task on MB 5 reads the result and applies a Softmax function on
    the 4 output values to arrive at the classification result. To display the result
    on the GUI, the 4-bit computed result is read by the *bridge node* and passed
    to the software API.


    # *B. Case Study - II: Extended CAN controller with IDS*


    The coupled accelerator flow incurs software overheads and latency in moving the
    message from the CAN interface to ECU memory and subsequently to the accelerator
    and back, which could affect the performance of critical tasks on the ECU. We
    attempt to alleviate this latency and overhead by stitching the FINN core directly
    to the CAN controller''s receive interface in case-II. For simplicity, we use
    a CAN receiver-only core, but the same approach could equally be applied with
    a full-fledged CAN controller. A hardware pre-processor logic parses the received
    messages directly off the host interface of the CAN receiver and uses a FIFO as
    a sliding window to accumulate 4 consecutive CAN messages, which is passed as
    the feature


    ![](_page_5_Figure_0.jpeg)


    **Casestudy-1:** IDS integrated as slave accelerator of MicroBlaze ECU


    **Casestudy-2**: IDS as a CAN Controller Extension


    <span id="page-5-1"></span>Fig. 3. Overview of the CAN frames'' datapath through
    the different IDS integrations (with & without MicroBlaze ECU (MB) ) in the HIL
    environment. The left-hand side architecture presents a state-of-the-art coupled
    accelerator approach and the right-hand side architecture presents an integration
    approach as an extension of the CAN controller, with the coloured arrows indicating
    the flow of the CAN frame/features.


    input to the IDS IP over the AXI streaming interface, stitching the IDS function
    at the network interface. Vitis-HLS is used to generate a hardware implementation
    of Softmax using DSP blocks. The resulting predictions are read by the *bridge
    node* as before to report the observations to the software APIs.


    To compute the latency difference between the two approaches, we have instrumented
    the setup with timers which capture elapsed time from the start of a CAN frame
    on the bus to the completion of the Softmax function in both cases. Also, both
    cases use identical IDS IP to deploy our DNN-IDS function, with the final activation
    (Softmax) performed in software (in case 1) or through dedicated hardware (in
    case 2). This allows us to quantify the benefits and/or overheads of each integration
    approach in terms of IDS latency and hardware resource consumption.


    ## V. RESULTS


    <span id="page-5-0"></span>In this section, we present the different attack injection
    capabilities tested through the platform and observe the response of the ECU functions
    in the presence of these attacks. Subsequently, we integrate IDS functionality
    and evaluate the detection performance of the IDS model through the HIL framework.
    We further quantify the detection latency of the IDS with both the integration
    strategies within testbed as discussed previously. Finally, we quantify the resource
    overhead of the testbed and identify the overheads incurred for IDS integration
    in cases I and II respectively.


    *1) Attack simulation on ECUs:* To evaluate the attack injection capabilities,
    we simulate three different attacks, DoS, fuzzing, and spoofing, one each on three
    ECUs in the testbed. We initially observe the normal ECU behaviour under no attack
    conditions, and the expected responses from each ECU in the presence of control
    input(s) are recorded automatically through the setup. Subsequently, the attack
    conditions are


    <span id="page-5-2"></span>TABLE III EXPECTED V/S OBSERVED BEHAVIOURS OF ECUS
    UNDER ATTACK CONDITIONS.


    | Test<br>Case               | Inputs                                               |
    Expected Result                                                                             |
    Under Attack                                                                                                                     |

    |----------------------------|------------------------------------------------------|---------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|

    | Collision<br>signal        | ECU3<br>activates<br>collision<br>detected<br>signal
    | activates<br>ECU2<br>ECU1<br>airbags,<br>disable<br>will<br>control<br>engine<br>(EC)
    & TCU | Under DoS attack, life signal lost,<br>airbag cannot be activated, EC
    and<br>TCU cannot be disabled, IDS re<br>ports threat      |

    | Light<br>control<br>signal | ECU2<br>activates<br>light sensor                    |
    activates<br>ECU4<br>and<br>headlights<br>tail lights                                       |
    Under fuzzing attack, lights acti<br>vated without any light sensor in<br>put,
    IDS reports threat                                |

    | Brake<br>signal            | ECU3 senses<br>brake signal                          |
    ECU1 will per<br>form braking ac<br>tion ECU4 acti<br>vates brake lights                    |
    Under spoofing attack, the brak<br>ing action was unexpectedly termi<br>nated,
    Brake lights not activated,<br>IDS reports threat |


    triggered using the software interface, which injects the attack and automatically
    logs the network data and ECU responses for logging and analysis. The details
    of the tests are presented in Table [III.](#page-5-2) Under all three attack conditions,
    normal ECU functionality cannot be carried out leading to critical functional
    loss such as no airbag deployment, braking action being terminated abruptly, and
    headlights toggling on/off without any user input. All three scenarios can potentially
    threaten the safety of passengers under different conditions. Extended attacks
    were also injected through random injection of attack messages and looped testing
    through the software interface, where a similar loss of functionality was observed
    across the ECUs in the presence of attack messages. With the IDS integrated, the
    attack messages were positively flagged by the IDS for all three attack types,
    and we quantify the detection accuracy of the model using the framework next.


    <span id="page-6-1"></span>TABLE IV CONFUSION MATRIX CAPTURING THE CLASSIFICATION
    RESULTS OF THE QNN-IDS.


    |               | Predicted Values |       |         |           |

    |---------------|------------------|-------|---------|-----------|

    | Attack Values | Benign           | DoS   | Fuzzing | RPM-Spoof |

    | Benign        | 103169           | 5     | 2       | 0         |

    | DoS           | 3                | 23690 | 0       | 0         |

    | Fuzzing       | 23               | 0     | 28065   | 1         |

    | RPM-Spoof     | 0                | 0     | 0       | 25042     |


    TABLE V ACCURACY METRICS OF THE QNN-IDS.


    <span id="page-6-2"></span>


    | Attacks   | Precicion | Recall | F1-Score |

    |-----------|-----------|--------|----------|

    | DoS       | 99.99     | 99.98  | 99.98    |

    | Fuzzing   | 99.99     | 99.91  | 99.95    |

    | RPM-Spoof | 100       | 100    | 100      |


    *2) IDS Accuracy:* The QNN-IDS integrated for our test is a multi-class classification
    model that can detect DoS, fuzzing, and spoofing attacks along with benign messages.
    For our tests, we replayed 180,000 test messages from the openly available CAR
    Hacking dataset [\[8\]](#page-7-12) which consisted of benign messages as well
    as different attack sequences, which were loaded as raw data for the test with
    automated monitoring and exporting enabled. The model achieved a classification
    accuracy of 99.98%, and the detailed confusion matrix of the classification result
    is shown in Table [IV.](#page-6-1) Across the entire test set, the model only
    misclassified *34* messages. Furthermore, the model only misclassified 7 benign
    messages as false positives out of the total 103176 benign messages in the test
    set, which shows a low false alarm rate for the model. The detailed table showing
    accuracy metrics (*Precision*, *Recall* & *F1-score*) for all the attacks is shown
    in Table [V.](#page-6-2) The key capability here is that the model can be tested
    in an integrated environment and the validation can be automated through the framework
    to allow rapid prototyping of the IDS models in a real environment.


    *3) Latency & Resource Utilisation:* We quantify the detection latency for both
    the IDS integration strategies discussed in the previous section. For case study
    I, we observe the total detection time to be *5,056 us*, from the message arriving
    at the CAN interface to the completion of the Softmax activation in the MB 5 IDS-ECU.
    For case study II, we observe more than 6× reduction in the detection latency
    with the classification result available in *794 us* from the message arriving
    at the CAN interface to the completion of the hardware Softmax. The latency computation
    in both the case studies includes the message reading time from the CAN bus up
    to the final results reported back by the *bridge node*. Embedding the IDS closer
    to the controller alleviates software processing overheads (and delays) for preprocessing,
    data movement and Softmax activation, as shown in Fig. [3.](#page-5-1) Offloading
    *preprocessing* (ID, payload extraction + message concatenation) and *post-processing*
    (softmax) computations to hardware and integrating this close to the CAN controller
    removes the software bottleneck, and enables lower detection latency for


    <span id="page-6-3"></span>TABLE VI RESOURCE UTILISATION OF HARDWARE NODES USED
    IN THE TESTBED.


    | Functions    | LUTs         | FFs         | BRAMs        | DSPs  |

    |--------------|--------------|-------------|--------------|-------|

    | ECU1         | 3330         | 2664        | 32           | 0     |

    | ECU2         | 3235         | 2653        | 32           | 0     |

    | ECU3         | 3319         | 2758        | 32           | 0     |

    | ECU4         | 3224         | 2614        | 32           | 0     |

    | Control Node | 4549         | 3557        | 32           | 0     |

    | DoS Node     | 2658         | 2277        | 32           | 0     |

    | Bridge       | 2522         | 2713        | 32           | 0     |

    | Debug        | 736          | 1121        | 0.5          | 0     |

    | Ethernet     | 1079         | 1483        | 0            | 0     |

    | Total (%)    | 24930 (18.5) | 23252 (8.6) | 224.5 (61.5) | 0 (0) |


    TABLE VII IDS RESOURCE UTILISATION FOR BOTH CASE STUDIES.


    <span id="page-6-4"></span>


    | Function         | LUTs (%)   | FFs (%)    | BRAMs (%) | DSPs (%) |

    |------------------|------------|------------|-----------|----------|

    | Case-I: IDS-ECU  | 5990 (4.5) | 6297 (2.4) | 39 (10.6) | 0 (0)    |

    | Case-II: CAN-IDS | 4788 (3.7) | 4773 (1.8) | 23 (6.2)  | 12 (1.6) |


    line-rate IDS implementations. For line rate detection on our 500 kbits/s CAN
    network, we consider the acquisition window of 4 minimal-length CAN data frames
    (296 µs each) with protocol overheads. In this case, the maximum latency for line
    rate detection should be < 1184 µs. From the tests using our HIL setup, we measure
    that IDS coupled to the CAN controller (case II) can perform detection at 794
    µs compared to the 5056 µs incurred by the traditional ECU-coupled IDS scheme.
    For higher speed CAN interfaces, the latency of the IDS IP can be further reduced
    through higher parallelization (unrolling) at the expense of higher energy and
    resource consumption.


    We further quantify the hardware resource utilisation of all the common hardware
    components (ECUs, Control Node, Injection Node, Bridge Node, and the Debug Node)
    which are shared across both integration case studies in Table [VI.](#page-6-3)
    The resource consumption of the different IDS pathways is captured in Table [VII.](#page-6-4)
    The hardware offload consumes less general purpose and memory resources than the
    MicroBlaze IDS-ECU (IDS enabled ECU in the Table [VII\)](#page-6-4), while the
    hardware Softmax incurs DSP blocks to maximize performance. We observe a reduction
    of ≈1,200 LUTs (0.91 %), ≈1,500 FFs (0.59 %) and 16 BRAMs (4.3 %) with the extended
    CAN controller scheme (CAN-coupled IDS in Table [VII\)](#page-6-4), compared to
    the coupled accelerator IDS-ECU method described in most literature. Additionally,
    the overall utilisation of the hardware subsystem (with 4 ECUs and IDS) is less
    than ≈65% (BRAMs) of resources on the Artix-7 FPGA, making it possible to scale
    the hardware subsystem to incorporate more ECUs and faster data interfacing to
    the host, with a single or multi-FPGA environment, making it an ideal solution
    for large-scale HIL validation and testing setup.


    # VI. CONCLUSION


    <span id="page-6-0"></span>In this paper, we presented FAV-NSS, a hardware-in-loop
    test framework that can effectively emulate a multi-ECU setup with the CAN bus
    network protocol on an FPGA device. The software subsystem of FAV-NSS provides
    numerous capabilities for the user through graphical interfaces and APIs that
    enable controlling/automating test cases (injection of DoS, Fuzzing, Spoofing
    or other network attacks) on the CAN bus, real-time logging of test data (network,
    ECU status), and performing analysis and tagging of observed bus data at runtime.
    The hardware subsystem models the ECUs using the MicroBlaze processor with a CAN
    controller as a memorymapped peripheral. A virtual CAN bus connects together the
    CAN controllers and implements the wired-AND logic to model physical layer capabilities
    of the protocol. To show the capabilities of the framework, we investigated multiple
    hardware integration strategies for an IDS solution, aiming to quantify the detection
    performance in terms of accuracy and latency. We used the software framework to
    automate the injection of attack vectors mixed with benign messages using a large
    test set of 180,000 CAN messages. With the framework, the detection accuracy of
    the IDS in both integration strategies – close to the network controller or as
    a coupled accelerator to an ECU – was measured. Additionally, the timing performance
    of both approaches was characterised by the instruments in the framework at runtime,
    with the network controller integration achieving 6.3× reduction in latency compared
    to traditional ECU-coupled IDS accelerator. In the future, we want to further
    enhance the communication method between software and hardware platforms to improve
    the capabilities of the framework specifically for multi-FPGA testbeds and to
    model ECUs using more complex processor architectures.


    #### REFERENCES


    - <span id="page-7-0"></span>[1] J. Nidamanuri, C. Nibhanupudi, R. Assfalg, and
    H. Venkataraman, "A progressive review: Emerging technologies for ADAS driven
    solutions," *IEEE Transactions on Intelligent Vehicles*, vol. 7, no. 2, pp. 326–341,
    2021.

    - <span id="page-7-1"></span>[2] P. Research, "Automotive ECUs." [Online]. Available:
    [https://www.](https://www.precedenceresearch.com/automotive-electronic-control-unit-market)
    [precedenceresearch.com/automotive-electronic-control-unit-market](https://www.precedenceresearch.com/automotive-electronic-control-unit-market)

    - <span id="page-7-2"></span>[3] S. Shreejith, S. A. Fahmy, and M. Lukaseiwycz,
    "Accelerating validation of time-triggered automotive systems on FPGAs," in *Proc.
    International Conference on Field-Programmable Technology (FPT)*. IEEE, 2013,
    pp. 4–11.

    - <span id="page-7-3"></span>[4] L. S. Rao, P. S. Rao, I. Chhabra, and L. S. Kumar,
    "Modeling and compensation of time-delay effects in HILS of aerospace systems,"
    *IETE Technical Review*, vol. 39, no. 2, pp. 375–388, 2022.

    - <span id="page-7-4"></span>[5] S. Khandelwal and S. Shreejith, "Exploring highly
    quantised neural networks for intrusion detection in automotive CAN," in *Proc.
    International Conference on Field-Programmable Logic and Applications (FPL)*.
    IEEE, 2023, pp. 235–241.

    - <span id="page-7-5"></span>[6] M. Bozdal, M. Samie, S. Aslam, and I. Jennions,
    "Evaluation of CAN bus security challenges," *Sensors*, vol. 20, no. 8, p. 2364,
    2020.

    - <span id="page-7-6"></span>[7] S. Khandelwal and S. Shreejith, "A lightweight
    FPGA-based IDS-ECU architecture for automotive CAN," in *2022 International Conference
    on Field-Programmable Technology (ICFPT)*, 2022, pp. 1–9.

    - <span id="page-7-12"></span>[8] H. M. Song, J. Woo, and H. K. Kim, "In-vehicle
    network intrusion detection using deep convolutional neural network," *Vehicular
    Communications*, vol. 21, p. 100198, 2020.

    - <span id="page-7-7"></span>[9] L. Yang, A. Moubayed, and A. Shami, "MTH-IDS:
    A multitiered hybrid intrusion detection system for internet of vehicles," *IEEE
    Internet of Things Journal*, vol. 9, no. 1, pp. 616–632, 2022.

    - <span id="page-7-8"></span>[10] M. Wolf, A. Weimerskirch, and C. Paar, "Security
    in automotive bus systems," in *Proc. Workshop on Embedded Security in Cars*.
    Bochum, 2004, pp. 1–13.

    - <span id="page-7-9"></span>[11] F. Cosimi, F. Tronci, S. Saponara, and P. Gai,
    "Analysis, hardware specification and design of a programmable performance monitoring
    unit (PPMU) for RISC-V ECUs," in *Proc. International Conference on Smart Computing
    (SMARTCOMP)*. IEEE, 2022, pp. 213–218.

    - <span id="page-7-10"></span>[12] T. Hoppe, S. Kiltz, and J. Dittmann, "Security
    threats to automotive CAN networks–practical examples and selected short-term
    countermeasures," in *Proc. International Conference on Computer Safety, Reliability,
    and Security (SAFECOMP)*. Springer, 2008, pp. 235–248.

    - <span id="page-7-11"></span>[13] A. Palanca, E. Evenchick, F. Maggi, and S.
    Zanero, "A stealth, selective, link-layer denial-of-service attack against automotive
    networks," in *Proc. International Conference on Detection of Intrusions and Malware,
    and Vulnerability Assessment (DIMVA)*. Springer, 2017, pp. 185–206.

    - <span id="page-7-13"></span>[14] S. Khandelwal, E. Wadhwa, and S. Shreejith,
    "Deep learning-based embedded intrusion detection system for automotive CAN,"
    in *2022 IEEE 33rd international conference on application-specific systems, architectures
    and processors (ASAP)*. IEEE, 2022, pp. 88–92.

    - [15] Q. Wang, Z. Lu, and G. Qu, "An entropy analysis based intrusion detection
    system for controller area network in vehicles," in *Proc. IEEE International
    System-on-Chip Conference (SOCC)*. IEEE, 2018, pp. 90–95.

    - <span id="page-7-14"></span>[16] M. H. Shahriar, Y. Xiao, P. Moriano Salazar,
    W. Lou, and T. Hou, "CAN-Shield: Signal-based intrusion detection for controller
    area networks," Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States),
    Tech. Rep., 2022.

    - <span id="page-7-15"></span>[17] A. Wasicek, O. Hoftberger, M. Elshuber, H.
    Isakovic, and A. Fleck, ¨ "Virtual CAN lines in an integrated MPSoC architecture,"
    in *Proc. International Symposium on Object/Component/Service-Oriented Real-Time
    Distributed Computing*. IEEE, 2014, pp. 158–165.

    - <span id="page-7-16"></span>[18] R. Obermaisser, "Ordering messages in virtual
    CAN networks," in *Proc. International Conference on Electronics, Circuits and
    Systems*. IEEE, 2005, pp. 1–4.

    - <span id="page-7-17"></span>[19] S.-H. Lee, J.-S. Kim, J.-S. Seok, and H.-W.
    Jin, "Virtualization of industrial real-time networks for containerized controllers,"
    *Sensors*, vol. 19, no. 20, p. 4405, 2019.

    - <span id="page-7-18"></span>[20] J.-S. Kim, S.-H. Lee, and H.-W. Jin, "Fieldbus
    virtualization for integrated modular avionics," in *Proc. ETFA 2011*. IEEE, 2011,
    pp. 1–4.

    - <span id="page-7-19"></span>[21] Y. Niimi, T. Ono, and N. Tsuchiya, "Virtual
    development of engine ECU by modeling technology," SAE Technical Paper, Tech.
    Rep., 2012.

    - <span id="page-7-20"></span>[22] J. Mauss, "Chip simulation used to run automotive
    software on PC," in *Embedded real-time software and systems (ERTS*<sup>2</sup>
    *2014)*, 2014.

    - <span id="page-7-21"></span>[23] S. Bidkar, S. Patil, and P. Shinde, "Virtual
    ECU development for vehicle diagnostics software testing using UDS protocol,"
    in *Proc. Asian Conference on Innovation in Technology (ASIANCON)*. IEEE, 2021,
    pp. 1–6.

    - <span id="page-7-22"></span>[24] M. Urbina and R. Obermaisser, "Multi-core architecture
    for AUTOSAR based on virtual electronic control units," in *Proc. Conference on
    Emerging Technologies & Factory Automation (ETFA)*. IEEE, 2015, pp. 1–5.

    - <span id="page-7-23"></span>[25] A. Dillard, "Real-time operational evaluations
    using advanced flight simulators," in *17th DASC. AIAA/IEEE/SAE. Digital Avionics
    Systems Conference. Proceedings (Cat. No. 98CH36267)*, vol. 1. IEEE, 1998, pp.
    E16–1.

    - <span id="page-7-24"></span>[26] P. Pace, B. Nishimura, W. Morris, and R. Surratt,
    "Effectiveness calculations in captive-carry HIL missile simulator experiments,"
    *IEEE transactions on aerospace and electronic systems*, vol. 34, no. 1, pp. 124–136,
    1998.

    - <span id="page-7-25"></span>[27] J. Leitner, "Space technology transition using
    hardware in the loop simulation," in *Proc. IEEE Aerospace Applications Conference*,
    vol. 2. IEEE, 1996, pp. 303–311.

    - <span id="page-7-26"></span>[28] H. Hanselmann, "Hardware-in-the-loop simulation
    testing and its integration into a CACSD toolset," in *Proc. Joint Conference
    on Control Applications Intelligent Control and Computer Aided Control System
    Design*. IEEE, 1996, pp. 152–156.

    - <span id="page-7-27"></span>[29] F. Puschmann, "Safe testing through power hardware-in-the-loop
    systems," *ATZelectronics worldwide*, vol. 16, no. 7, pp. 50–53, 2021.

    - <span id="page-7-28"></span>[30] C. A. Rabbath, M. Abdoune, J. Belanger, and
    K. Butts, "Simulating hybrid dynamic systems," *IEEE Robotics & Automation Magazine*,
    vol. 9, no. 2, pp. 39–47, 2002.

    - <span id="page-7-29"></span>[31] D. Bullock, B. Johnson, R. B. Wells, M. Kyte,
    and Z. Li, "Hardwarein-the-loop simulation," *Transportation Research Part C:
    Emerging Technologies*, vol. 12, no. 1, pp. 73–89, 2004.

    - <span id="page-7-30"></span>[32] F. Mihalic, M. Trunti ˇ c, and A. Hren, "Hardware-in-the-loop
    simulations: ˇ A historical overview of engineering challenges," *Electronics*,
    vol. 11, no. 15, p. 2462, 2022.

    - <span id="page-7-31"></span>[33] A. Pappalardo, "Xilinx/brevitas," 2023. [Online].
    Available: [https:](https://doi.org/10.5281/zenodo.3333552) [//doi.org/10.5281/zenodo.3333552](https://doi.org/10.5281/zenodo.3333552)

    - <span id="page-7-32"></span>[34] Y. Umuroglu, N. J. Fraser, G. Gambardella,
    M. Blott, P. Leong, M. Jahre, and K. Vissers, "Finn: A framework for fast, scalable
    binarized neural network inference," in *Proc. ACM/SIGDA International Symposium
    on Field-Programmable Gate Arrays*, 2017, pp. 65–74.'
- title: "WISP: Image Segmentation-Based Whitespace Diagnosis for Optimal\n  Rectilinear\
    \ Floorplanning"
  abstract: 'The increasing number of rectilinear floorplans in modern chip designs

    presents significant challenges for traditional macro placers due to the

    additional complexity introduced by blocked corners. Particularly, the widely

    adopted wirelength model Half-Perimeter Wirelength (HPWL) struggles to

    accurately handle rectilinear boundaries, highlighting the need for additional

    objectives tailored to rectilinear floorplan optimization. In this paper, we

    identify the necessity for whitespace diagnosis in rectilinear floorplanning,

    an aspect often overlooked in past research. We introduce WISP, a novel

    framework that analyzes and scores whitespace regions to guide placement

    optimization. WISP leverages image segmentation techniques for whitespace

    parsing, a lightweight probabilistic model to score whitespace regions based on

    macro distribution, a Gaussian Mixture Model (GMM) for whitespace density

    scoring and direction-aware macro relocation to iteratively refine macro

    placement, reduce wasted whitespace, and enhance design quality. The proposed

    diagnostic technique also enables the reclamation of block-level unused area

    and its return to the top level, maximizing overall area utilization. When

    compared against state-of-the-art academia placer DREAMPlace 4.1, our method

    achieves an average improvement of 5.4% in routing wirelength, with a maximum

    of 11.4% across widely-used benchmarks. This yields an average of 41.5% and

    43.7% improvement in Worst Negative Slack (WNS) and Total Negative Slack (TNS),

    respectively. Additionally, WISP recycles an average of 16.2% area at the block

    level, contributing to more efficient top-level area distribution.'
  url: http://arxiv.org/abs/2505.15271v1
  keywords: Rectilinear floorplanning, Whitespace, Macro placement, Image segmentation
  document: '## I. INTRODUCTION


    With the rapid development of semiconductor technology and the exponential growth
    in chip complexity, floorplanning, the first step of back-end design process,
    has emerged as one of the most critical research areas in Electronic Design Automation
    (EDA) [\[1\]](#page-8-0), [\[2\]](#page-8-1). The quality of a floorplan significantly
    influences the overall PPA (Performance, Power, and Area) of a design, making
    floorplanning optimization a vital aspect of improving chip design performance.
    In modern chip design, System on Chip (SoC) has become a mainstream design model
    which relies on numerous internal or external IPs. As SoCs scale in complexity
    and size, hierarchical floorplanning methodologies have emerged to support block
    reuse both within a single chip and across different chips. This approach also
    reduces the computational burden on placers and routers


    <span id="page-0-0"></span>![](_page_0_Figure_9.jpeg)


    <span id="page-0-1"></span>Fig. 1. (a) Illustration of the hierarchical floorplanning
    methodology, showcasing an optimization process from the top-level floorplan to
    the blocklevel rectilinear module floorplan. At the top level, modules are transformed
    from regular rectangles into rectilinear shapes to improve area utilization. The
    block-level floorplanning starts with a suboptimal placement, where inefficient
    usage of space leads to noticeable wasted whitespace. The optimized result highlights
    how effective whitespace analysis and reuse can significantly improve floorplan
    quality and area efficiency. (b) Challenges in current rectilinear floorplanning,
    including increased possibilities in wasted whitespace, the creation of notch-induced
    dead zones, and the limitations of traditional wirelength models such as HPWL
    in capturing the true layout characteristics of rectilinear modules.


    by enabling focused optimization at different hierarchy levels. Specifically,
    top-level floorplanning addresses the global placement of major blocks across
    the chip, while block-level floorplanning focuses on arranging standard cells
    and macros within individual blocks. As illustrated on the left side of Fig. [1\(a\),](#page-0-0)
    to accommodate a wide range of IPs and improve area utilization, top-level floorplanning
    often transforms modules into rectilinear shapes, deviating from traditional rectangular
    shapes [\[3\]](#page-8-2), [\[4\]](#page-8-3).


    With the increasing adoption of rectilinear modules at the top-level floorplanning
    stage, attention naturally shifts to the block-level rectilinear floorplanning,
    which plays a critical role in managing the internal layout of these complexshaped
    modules. However, most existing placers lack specific optimizations for rectilinear
    shapes. As shown in the left part of Fig. [1\(b\),](#page-0-1) state-of-the-art
    placers typically enclose the rectilinear shape within a bounding rectangle and
    treat the non-functional regions, i.e., the areas outside the original


    This work has been submitted to the IEEE for possible publication. Copyright may
    be transferred without notice, after which this version may no longer be accessible.


    <span id="page-1-0"></span>


    | TABLE I<br>A LIST OF RECENT FLOORPLANNING WORK.                      |                          |   |                            |  |  |  |  |  |  |

    |----------------------------------------------------------------------|--------------------------|---|----------------------------|--|--|--|--|--|--|

    | Support Rectilinear?<br>Optimization for Rectilinear<br>Work<br>Year |                          |   |                            |  |  |  |  |  |  |

    | Top-Level floorplanning                                              |                          |   |                            |  |  |  |  |  |  |

    | TOFU [3]<br>✓<br>Whitespace Refinement<br>2023                       |                          |   |                            |  |  |  |  |  |  |

    | JigsawPlanner [4]                                                    | 2024                     |
    ✓ | Jonker-Volgenant Algorithm |  |  |  |  |  |  |

    | ICCAD''24 [5]                                                         | Differentiable
    Mechanism |   |                            |  |  |  |  |  |  |

    | Block-Level floorplanning                                            |                          |   |                            |  |  |  |  |  |  |

    | HiDaP [6]                                                            | 2019                     |
    × | ×                          |  |  |  |  |  |  |

    | RectilinearMP [7]                                                    | 2023                     |
    ✓ | Block region               |  |  |  |  |  |  |

    | Hier-RTLMP [8]                                                       | 2023                     |
    × | ×                          |  |  |  |  |  |  |

    | AutoDMP [9]                                                          | 2023                     |
    × | ×                          |  |  |  |  |  |  |

    | DREAMPlace 4.1 [10]                                                  | 2023                     |
    ✓ | No Specific Optimization   |  |  |  |  |  |  |

    | DATE''24 [11]                                                         | 2024                     |
    × | ×                          |  |  |  |  |  |  |

    | IncreMP [12]                                                         | 2024                     |
    ✓ | No Specific Optimization   |  |  |  |  |  |  |

    | Ours                                                                 | 2025                     |
    ✓ | Whitespace Diagnosis       |  |  |  |  |  |  |


    shape but within the bounding box, as blockage macros. This approach introduces
    significant challenges compared to traditional rectangular layouts due to the
    irregular geometric constraints of rectilinear shapes.


    The first major challenge is the increased likelihood of creating notch areas
    or dead placement regions, leading to inefficient utilization of space and wasted
    area. Secondly, the widely used Half-Perimeter Wirelength (HPWL) model, one of
    the key optimization metrics in traditional placer, becomes less effective in
    this context. Since HPWL calculates wirelength based on the bounding rectangle,
    it often overestimates wirelength and fails to accurately capture the true connectivity
    cost within a rectilinear boundary. In the absence of a wirelength model tailored
    to rectilinear modules, alternative approaches are needed to better address these
    optimization challenges.


    To this end, we argue that block-level rectilinear floorplanning deserves special
    attention, and propose that effective whitespace analysis and utilization is key
    to improving its quality. As illustrated in the right part of Fig. [1\(a\),](#page-0-0)
    the selected block represents a rectilinear module from the top-level floorplan.
    Macro placement within this module creates various types of whitespace, some are
    fully open and suitable for standard cell insertion, while others, especially
    in notch areas, are less practical and wasteful. We propose a method to label
    and optimize these wasted whitespace regions, improving both placement quality
    and area efficiency. Moreover, the reclaimed whitespace can be recycled to the
    top-level floorplan, enabling iterative refinement of the overall chip layout.


    Although previous works [\[3\]](#page-8-2), [\[4\]](#page-8-3), [\[5\]](#page-8-4)
    have explored whitespace optimization at the top-level, where many initially regular
    rectangular blocks are transformed into rectilinear shapes, most do not address
    the increasingly important challenge of rectilinear floorplanning at the block
    level. As the number of block-level rectilinear floorplans continues to grow,
    it becomes critical to develop techniques specifically targeting their optimization.
    As summarized in Table [I,](#page-1-0) many recently proposed macro placers or
    floorplan optimization techniques struggle to even parse design files with rectilinear
    layouts. Exceptions include DREAMPlace 4.1 [\[10\]](#page-8-9), IncreMP [\[12\]](#page-8-11),
    and RectilinearMP [\[7\]](#page-8-6). However, the first two are not tailored
    for placement optimization in rectilinear floorplans, often resulting in suboptimal
    or unfeasible placement outcomes. RectilinearMP [\[7\]](#page-8-6) also provides
    optimization by using block regions to fill rectilinear spaces, transforming the
    floorplan into a rectangular shape and then applying simulated annealing for wirelength
    refinement. However, this approach still uses the HPWL model as the main optimization
    target, which does not address the critical aspect of rectilinear floorplan and
    whitespace analysis, leaving room for further improvement.


    In summary, there is a growing demand from top-level floorplanning perspectives
    to generate more rectilinear floorplans at the block level. To address this, this
    paper proposes a novel methodology named WISP that begins with analyzing whitespace
    regions using an image segmentation-inspired approach, treating the layout as
    an image based on initial placement. The whitespace is then scored and differentiated,
    enabling iterative refinement of macro placement until an optimal overall whitespace
    score is achieved. WISP is lightweight yet effective, fully adaptive, and specifically
    tailored for rectilinear floorplanning. The key contributions are summarized below.


    - 1) Agile Image Segmentation-based Floorplan Whitespace Parsing and Scoring.
    We introduce a novel computer vision-inspired approach that leverages image segmentation
    to replace traditional algorithmic methods for whitespace diagnosis and parsing,
    significantly reducing computational complexity and improving portability. A Gaussian
    Mixture Model (GMM) is then leveraged to score the whitespace, enabling detailed
    analysis of whitespace distribution and characteristics across the floorplan [Sections
    [III-A](#page-3-0) and [III-B\]](#page-3-1).

    - 2) Direction-Aware Macro Placement Refinement. Guided by the whitespace score,
    we propose a directionaware simulated annealing (SA) technique that adaptively
    explores macro movement for faster and more effective macro placement refinement
    technique [Section [III-C\]](#page-4-0).

    - 3) Quality Improvement and Potential Area Recycling. The proposed WISP flow
    consistently outperforms baseline placers in routing wirelength, Worst Negative
    Slack (WNS), Total Negative Slack (TNS), and power, within acceptable runtime.
    Leveraging the whitespace density scores, we introduce an area recycling strategy
    that reclaims low-score whitespace and feeds them back to the top-level floorplan,
    enabling further chip area optimization. This approach achieves an average of
    16.2% area reduction over state-of-the-art mixed-size placers, while delivering
    superior timing performance [Section [IV\]](#page-5-0).


    ## II. PRELIMINARIES


    In this section, we first discuss the current status of toplevel and block-level
    rectilinear floorplanning methodologies. We then introduce a new term, *wasted
    whitespace*, specifically for rectilinear floorplanning. Finally, we compare traditional
    whitespace extraction algorithms with our proposed image segmentation-based whitespace
    diagnosis approach.


    ## *A. Top-level Floorplanning*


    <span id="page-2-1"></span><span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)


    Fig. 2. (a) Original top-level floorplan with whitespace as discussed in [\[4\]](#page-8-3).
    (b) Top-level floorplan with modules converted into rectilinear blocks.


    <span id="page-2-3"></span>![](_page_2_Figure_2.jpeg)


    <span id="page-2-2"></span>Fig. 3. (a) *Wasted whitespace* regions can lead to
    long routing. (b) Impact of *wasted whitespace* in rectilinear floorplanning.


    Top-level floorplanning is an initial and essential step for the whole physical
    design phase in a large-scale SoC. Traditional methods typically approximate modules
    as rectangles, as shown in Fig. [2\(a\).](#page-2-0) While this simplification
    eases algorithmic implementation, it is no longer essential in modern designs
    and often limits placement flexibility—resulting in inefficient area utilization
    due to excessive whitespace or unavoidable overlaps. To address these limitations,
    recent efforts such as TOFU [\[3\]](#page-8-2), JigsawPlanner [\[4\]](#page-8-3),
    ICCAD''24 [\[5\]](#page-8-4), and FloorSet [\[13\]](#page-8-12) from Intel Labs
    have all introduced toplevel floorplanning techniques that support rectilinear
    modules. These approaches enable more flexible and accurate modeling of complex
    module shapes, thereby improving overall area efficiency. As illustrated in Fig.
    [2\(b\),](#page-2-1) converting modules into rectilinear blocks allows their irregular
    outlines to better conform to available whitespace, leading to a notable reduction
    in top-level floorplan area. These studies further underscore the importance of
    optimizing area utilization within rectilinear floorplans while preserving overall
    design quality.


    ## *B. Wasted Whitespace in Rectilinear Floorplan*


    Given a layout, aside from the areas occupied by macros and cell instances, the
    remaining spaces are defined as *whitespace regions*. As illustrated in Fig. [3,](#page-2-2)
    certain whitespace regions located in corner positions are relatively enclosed
    and blocked, making them less suitable for accommodating macros or cells. Therefore,
    within the *whitespace regions* in the floorplan, we define *wasted whitespace*
    as small place and route (P&R) regions that fall into a specific area threshold
    range (200 to 20000 pixels in our case), have relatively enclosed shapes, exhibit
    0% utilization, and can potentially cause routability issues. As shown in the
    figure, *wasted whitespace* contributes to increased wirelength, and minimizing
    it can significantly optimize wirelength [\[12\]](#page-8-11). For rectilinear
    floorplans like the L-shape example in Fig. [3\(a\),](#page-2-3) numerous wires
    might cross the notch regions, which are often prone to DRC violations and routing
    congestion. Compared to rectangular floorplans, rectilinear designs with more
    notches are more prone to gener-


    <span id="page-2-4"></span>![](_page_2_Figure_7.jpeg)


    <span id="page-2-5"></span>Fig. 4. Comparison between traditional algorithms and
    our proposed Image Segmentation-inspired method for whitespace analysis.


    ![](_page_2_Figure_9.jpeg)


    <span id="page-2-6"></span>Fig. 5. The overall flow of the proposed WISP methodology.


    ating *wasted whitespace*, which in turn introduces routing challenges. Targeting
    rectilinear floorplan-specific optimizations through whitespace analysis can therefore
    offer significant benefits in improving placement and routing efficiency.


    ## *C. Traditional Algorithm vs. Image Segmentation-Inspired Parsing*


    In design practice, designers typically rely on tools to analyze spatial relationships
    among instances and place them closer to minimize notch regions. As shown in Fig.
    [4\(a\),](#page-2-4) traditional algorithms, such as the classic Scanline Algorithm
    and Triangulation, depend on parsing circuit files (e.g., .def, .lef, and .lib)
    to obtain information. Due to the unique databases of different EDA tools, adapting
    such methods across tools and design stages is challenging. Moreover, analyzing
    the relative positions of all macro and cell instances requires multiple iterations,
    resulting in high time complexity. Inspired by image segmentation in computer
    vision, we reformulate whitespace analysis as an image processing task, using
    only a layout image to analyze spatial relationships, as shown in Fig. [4\(b\).](#page-2-5)
    This approach focuses on mask-level analysis, reducing timing complexity and offering
    portability across toolchains.


    ## III. PROPOSED METHODOLOGY


    The overall flow, shown in Fig. [5,](#page-2-6) begins with an initial floorplan
    that includes pre-placed macros and cells using any placers. First, we use an
    Image Segmentation-based floorplan parsing method to diagnose, parse and label
    the *wasted whitespace* in the current floorplan. The darker purple-shaded areas
    in the top figure of Fig. [5](#page-2-6) represent the identified *wasted whitespace*.
    Then, we further leverage a Gaussian Mixture Model (GMM)-based scoring mechanism
    to derive a global scoring map of whitespace for downstream placement optimization.
    Next, a direction-aware macro placement strategy is proposed to refine the macro
    location. We select the most suitable direction based on score density for movement
    at each simulated annealing (SA) iteration step which ensures a more effective
    optimization. The three steps are sequentially and iteratively performed until
    the cost function convergence. This flow results in optimized macro placements,
    followed by a re-execution of global placement for standard cells to further enhance
    the final floorplan quality. Finally, an area optimization strategy will be implemented
    to achieve the area recycling that introduced in Section [IV-D.](#page-7-0) The
    following subsections explain detailed techniques used in each stage.


    ## <span id="page-3-0"></span>*A. Image Segmentation-based Floorplan Parsing*


    The complete algorithm for Image Segmentation-based floorplan parsing is listed
    in Algorithm [1.](#page-3-2) The input floorplan image is mapped from design layout
    and scaled to a fix-sized canvas where the longest side is 800 pixels and the
    other side is resized based on the design aspect ratio (maximum size: 800×800
    pixels), which allows for a trade-off between stable processing runtime and detailed
    mask extraction for any designs. The process begins with the edge detection on
    the floorplan image using the Canny edge detection algorithm [\[14\]](#page-8-13),
    which calculates the edge angles and labels the rightangled regions, facilitating
    the detection of key structural features. Based on the detected edges, three masks
    are generated: mask cell for densely placed cell regions, mask macro for macros,
    and mask whitespace for whitespace areas. These steps, corresponding to lines
    3-5 in Algorithm [1,](#page-3-2) establish the initial segmentation of the floorplan.
    Subsequently, since cell placement is dense yet contains gaps, these gaps are
    necessary and cannot be repurposed. To better extract cell mask, we introduce
    a morphological dilation algorithm to fill these gaps,


    #### <span id="page-3-2"></span>Algorithm 1: Image Segmentation-based Floorplan
    Parsing Algorithm Input: Input floorplan image I Output: I with parsed *wasted
    whitespace* regions <sup>1</sup> Convert I to HSV space <sup>2</sup> Generate
    masks by edge segmentation: <sup>3</sup> mask cell ← captures cell regions <sup>4</sup>
    mask macro ← captures macro regions <sup>5</sup> mask whitespace ← captures whitespace
    regions <sup>6</sup> ▷ Dilate cell region to eliminate noise <sup>7</sup> mask
    celldilated ← cv.dilate(mask cell, kernelc, iter = 3) <sup>8</sup> foreach *pixel*
    (x, y) *in* mask whitespace do <sup>9</sup> if *pixel* (x, y) *in both* mask whitespace
    *and* mask celldilated then <sup>10</sup> remove (x, y) as whitespace in mask
    whitespace <sup>11</sup> Update mask whitespace <sup>12</sup> ▷ Dilate macro region
    to identify closed region <sup>13</sup> macro maskdilated ← cv.dilate(mask macro,
    kernelm, iter = 3) <sup>14</sup> ▷ Filter whitespace contours by area thresholds
    <sup>15</sup> foreach *whitespace* contour *in* mask whitespace do <sup>16</sup>
    if *overlap exists with* macro maskdilated then <sup>17</sup> Calculate contour
    area and skip if not in area threshold <sup>18</sup> Label overlapping region
    as part of wasted whitespace mask <sup>19</sup> Record *wasted whitespace* pixel
    locations


    <sup>20</sup> Create overall wasted whitespace mask


    <sup>21</sup> return I, wasted whitespace mask


    as shown in Algorithm [1](#page-3-2) line 7. The dilation operation can be expressed
    mathematically as Equation [1](#page-3-3) and [2:](#page-3-4)


    <span id="page-3-3"></span>

    $$I\_{\text{dilated}} = I \oplus B = \left\{ (x, y) \mid (B)\_{(x, y)} \cap I
    \neq \emptyset \right\} \qquad (1)$$


    <span id="page-3-4"></span>

    $$I = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 0 & 0
    & 1 & 0 \end{bmatrix} \\ B = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \\ I
    \oplus B = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \end{bmatrix} \\ \tag{2}$$


    where ⊕ is dilation operator, I is the binary image matrix converted from HSV
    (Hue, Saturation, Value) image of the floorplan, B is the dilation kernel matrix
    (a 2×2 square), and Idilated is the result from the dilation operation. This operation
    ensures that any small gaps within dense regions of the floorplan are filled,
    ensuring a more precise and complete extraction of the cell mask.


    To identify the enclosed regions around macros within the whitespace, morphological
    dilation is employed again to extend the boundaries around macros with the kernel
    matrix (a 2×2 square), as described in Algorithm [1](#page-3-2) line 13. This
    action effectively pinpoints potential *wasted whitespace* regions, and the regions
    are filtered based on an area threshold (2000 to 20000 pixels in this work), as
    outlined in Algorithm [1](#page-3-2) line 15- 20. This threshold value is determined
    relative to the design''s area. Once the filtering is done, the remaining regions
    are assigned labels, resulting in a parsed floorplan with clearly marked *wasted
    whitespace* regions.


    ## <span id="page-3-1"></span>*B. Whitespace Density Scoring*


    After Image Segmentation-based whitespace parsing, the *wasted whitespace* regions
    are extracted and labeled. However, throughout the entire floorplan, the influence
    of other whitespace along with the regions with placed cells is nonuniform. To
    address this, we further analyze the entire regions excluding macros and assign
    a score to each pixel to generate a heatmap that visualizes the intensity of the
    whitespace.


    The impact of whitespace is closely related to the spatial distribution of macros,
    where excessive gaps between adjacent macros degrade placement quality by reducing
    area efficiency and increasing wirelength [\[12\]](#page-8-11). In addition, macros
    placed at the center of floorplan can divide the standard cells connected by the
    same net into different sub-regions, resulting in longer wirelength and more inter-region
    traversals. Considering these sub-regions caused by macros, it is essential to
    analyze the whitespace between macros and their impact on the surrounding regions.
    To address these challenges, it is crucial to capture the spatial impact of each
    macro which diminishes with distance. We model the influence of each macro along
    both horizontal and vertical directions using Gaussian functions, and introduce
    a 2-dimensional Gaussian Mixture Model (GMM) for all macros, as defined in Equation
    [3](#page-4-1) and illustrated in Fig. [6\(a\):](#page-4-2)


    <span id="page-4-1"></span>

    $$P(s(x,y)\mid\theta) = \sum\_{k=1}^{K} \alpha\_k \cdot \mathcal{N}(s(x,y)\mid
    \mu\_k, \Sigma\_k) \qquad (3)$$


    where s(x, y) is the pixel in 2-dimensional floorplan space, and N (s | µ<sup>k</sup>
    , Σk) denotes the Gaussian distribution of the k th macro with parameter (µk,
    Σk) and weight αk. We assume the impact of each macro follows a Gaussian distribution
    parameterized by (µk, Σk), as defined in Equation [4:](#page-4-3)


    <span id="page-4-3"></span>

    $$

    \mu\_k = \begin{bmatrix} x\_k \\ y\_k \end{bmatrix}, \quad \Sigma\_k = \begin{bmatrix}
    \sigma\_x^2 \propto w & 0 \\ 0 & \sigma\_y^2 \propto h \end{bmatrix} \tag{4}

    $$


    the Gaussian probability density function is calculated as:


    $$\mathcal{N}(s \mid \mu\_k, \Sigma\_k) = \frac{1}{2\pi\sigma\_x\sigma\_y} e^{-\frac{1}{2}\left[\left(\frac{x
    - x\_k}{\sigma\_x}\right)^2 + \left(\frac{y - y\_k}{\sigma\_y}\right)^2\right]}
    \qquad (5)$$


    where µ<sup>k</sup> represents the mean of the Gaussian distribution, with x<sup>k</sup>
    and y<sup>k</sup> denoting the x and y coordinates of the macro, respectively.
    σ 2 x , σ 2 y are the variances in x and y dimensions, and they are proportional
    to the width (denoted as w) and height (denoted as h) of the macro, respectively.
    The weight α<sup>k</sup> for the Gaussian distribution of the k th macro at pixel
    (x, y) is calculated in Equation [6:](#page-4-4)


    <span id="page-4-4"></span>

    $$\alpha\_k = \frac{D((x, y), M\_k)}{\sum\_{i=1}^{K} D((x, y), M\_i)} \tag{6}$$


    where M<sup>k</sup> = (xk, yk) represents the center coordinates of the k th macro,
    and D((x, y), Mi) = p (x − xi) <sup>2</sup> + (y − yi) 2 denotes the Euclidean
    distance from the pixel (x, y) to the center of the i th macro.


    Moreover, we also take the labeled *wasted whitespace* into consideration. The
    pixel collection in the *wasted whitespace* region are designated as set A. Then,
    we can derive the score formula for the whole floorplan outside of the macros,
    defined in Equation [7,](#page-4-5) where γ is a hyper-parameter for the weight


    <span id="page-4-2"></span>![](_page_4_Figure_11.jpeg)


    <span id="page-4-6"></span>Fig. 6. (a) Illustration of scoring method on bp\_fe
    with parsed *wasted whitespace* (blue region). The right part illustrates the
    impact of a macro on the surrounding region. (b) The lighter-colored regions represent
    higher scores that require optimization.


    ![](_page_4_Figure_13.jpeg)


    <span id="page-4-7"></span>Fig. 7. Direction determination in macro location optimization.
    By calculating the scoring density in field of vision (FOV), determine the moving
    direction. of pixel point in *wasted whitespace*, we set it as 0.8 in our process
    and can adjust it based on the impact intensity.


    <span id="page-4-5"></span>

    $$P((x,y)\mid\theta) = \begin{cases} \sum\_{k=1}^{K} \alpha\_k \mathcal{N}\left((x,y)\mid
    \theta\_k\right) & (x,y)\notin A\\ (1+\gamma) \times \sum\_{k=1}^{K} \alpha\_k
    \mathcal{N}\left((x,y)\mid \theta\_k\right) & (x,y)\in A \end{cases} (7)$$


    By calculating the value of each whitespace pixel using Equation [7,](#page-4-5)
    we obtain the whitespace scoring for the whole floorplan. Taking the design bp\_fe
    as an example, its extracted *wasted whitespace* and the score calculation of
    each macro is shown in Fig. [6\(a\).](#page-4-2) The specific whitespace score
    of heat map is shown in Fig. [6\(b\),](#page-4-6) where lighter-colored regions
    represent higher scores. It can be observed that by utilizing Gaussianbased Whitespace
    Scoring, the spatial relationships between a macro and its surrounding macros
    are effectively extracted. As illustrated in Fig. [6\(b\),](#page-4-6) the region
    between two closely placed macros is highlighted by a lighter color, indicating
    a higher whitespace score and thus a higher priority for optimization. In contrast,
    darker-colored regions correspond to areas between macros that are farther apart,
    reflecting a lower interaction intensity and less need for adjustment. This will
    guide the subsequent optimization of macro placement, ensuring that whitespace
    is utilized more efficiently while minimizing disruptions to connectivity.


    ## <span id="page-4-0"></span>*C. Direction-aware Macro Location Refinement*


    In this step, macro placement optimization is guided using the whitespace scoring
    heat map derived from the scoring calculations described earlier. The goal is
    to guide the macro movement toward regions with higher score density than its
    current position, effectively optimizing its placement to reduce *wasted whitespace*
    and improve overall layout quality.


    To achieve this, we propose direction-aware macro location refinement strategy,
    which is illustrated in Fig. [7.](#page-4-7) Assuming *B1* and *B2* are macros
    to be optimized. To guide macro movement using the whitespace scoring heat map,
    we evaluate the whitespace score density within its Field of View (FOV)


    <span id="page-5-3"></span>


    | Design Name     | Std Cells Count     | Macros Count | Macro Type          |
    # of IOs |  |  |

    |-----------------|---------------------|--------------|---------------------|----------|--|--|

    | TinyRocket      | 27217               | 2            | 1                   |
    269      |  |  |

    | bp_be           | 59882               | 10           | 3                   |
    3029     |  |  |

    | bp_fe           | 29993               | 11           | 3                   |
    2511     |  |  |

    | black parrot    | 427501              | 24           | 5                   |
    1198     |  |  |

    | bp_multi        | 209086              | 26           | 6                   |
    1453     |  |  |

    | swerv_wrapper   | 99750               | 28           | 3                   |
    1416     |  |  |

    | ariane133       | 165953              | 133          | 1                   |
    495      |  |  |

    | ariane136       | 166200              | 136          | 1                   |
    495      |  |  |

    | bp_quad         | 1238636             | 220          | 6                   |
    135      |  |  |

    |                 | Rectilinear Num = 1 |              | Rectilinear Num = 2 |          |  |  |

    | Floorplan Shape |                     |              |                     |          |  |  |


    TABLE II BENCHMARK STATISTICS.


    in four cardinal directions. The calculation of FOV scoring density is defined
    in Equation [8.](#page-5-1)


    <span id="page-5-1"></span>

    $$FOV\ density = \frac{\sum\_{i=1}^{N\_{FOV}} s\_i}{N\_{FOV}} \tag{8}$$


    Here, s<sup>i</sup> denotes the heat map score value at pixel i, and NF OV is
    the number of pixels within the FOV. This metric reflects the concentration of
    whitespace inefficiency in each direction.


    The order of macro movement is based on the sum of score densities value in four
    FOVs, with the macro having the larger average chosen for prior movement. If another
    macro exists within the FOV in a given direction, moving in that direction is
    avoided to prevent overlap and increased congestion. Among the remaining feasible
    directions, we rank the score densities and select the direction with the highest
    density for the macro''s movement. The process is iteratively applied to all macros.


    We then apply Simulated Annealing (SA) to refine macro positions based on a cost
    function that balances wirelength and whitespace distribution defined in Equation
    [9.](#page-5-2) This ensures a balanced approach to improve routing efficiency
    while minimizing *wasted whitespace*.


    <span id="page-5-2"></span>

    $$\text{min } cost = \alpha \times Wirelength + \beta \times Whitespace\ Score\
    (9)$$


    Here α and β are hyperparameters (set as α = 0.05 and β = 0.95 in our work) and
    can be adjusted according to the design. We adopt SA as our optimization strategy
    for macro placement due to its ability to perform local search with probabilistic
    exploration. Unlike global optimization methods, SA allows for localized adjustments
    to macro positions, suitable for adjusting macro positions based on surrounding
    whitespace distribution. During the optimization, we set the macro movement step
    size to 10 pixels, allowing for effective yet controlled adjustments to macro
    positions at each iteration.


    ## IV. EXPERIMENT RESULTS


    ## <span id="page-5-0"></span>*A. Experiment Setup*


    Evaluation Flow. The proposed methodology is implemented in python and C++, leveraging
    OpenCV library and choosing DREAMPlace 4.1 [\[10\]](#page-8-9) as the baseline
    framework. The flow runs on Intel Core i7 11700 CPU with 128GB of memory. The
    proposed method is compared against DREAMPlace 4.1 [\[10\]](#page-8-9), a state-of-the-art
    open-source mixed-size placer capable of handling rectilinear floorplans. We could
    not compare with


    ![](_page_5_Figure_11.jpeg)


    <span id="page-5-4"></span>Fig. 8. Hyperparameter selection of simulated annealing
    cost function.


    RectilinearMP [\[7\]](#page-8-6) and IncreMP [\[12\]](#page-8-11) due to their
    lack of open-source availability. Additionally, the built-in floorplanner in OpenROAD
    does not support rectilinear floorplanning, making direct comparison infeasible.
    For a fair comparison, the subsequent placement of standard cells and routing
    are all performed by the state-of-the-art commercial P&R tool. All metrics are
    collected after post-routing optimization stage.


    Comparison Metrics. In this study, to evaluate the improvement in routability
    and timing, we use post-route PPA metrics, including routed wirelength (rWL),
    timing (WNS and TNS), power. Additionally, in our proposed area recycling part,
    we record the original area and optimized area too. Runtime is also recorded to
    evaluate the framework''s efficiency.


    Benchmark Selection. We select nine diverse benchmark designs from recently published
    work or new test suites for floorplan evaluation [\[15\]](#page-8-14), [\[16\]](#page-8-15).
    The synthesized netlist is obtained by running Yosys [\[17\]](#page-8-16) using
    the open-source NanGate 45nm technology node [\[18\]](#page-8-17). Since no publicly
    available rectilinear floorplans exist for benchmarking, we created in-house rectilinear
    floorplans. These were designed to maintain reasonable total area utilization,
    ensuring that baseline tools could perform initial placement without giving up,
    and highlight issues such as *wasted whitespace* in baseline tool placement results.
    Table [II](#page-5-3) summarizes the benchmark statistics. Floorplans with one
    missing corner are labeled rectilinear number of 1, and with two missing corners
    are labeled rectilinear number of 2. These setups cover challenging situations
    where traditional placers might struggle to achieve optimal macro placement outcomes.
    Hyperparameter Selection. We determine the default values of (α, β) in our SA
    cost function through a sweep study. We vary (α, β) pairs such that α + β = 1,
    run the full placement flow, and record the final cost after macro placement.
    The cost is normalized to the result from our default setting (α = 0.05, β = 0.95),
    as shown in Fig. [8](#page-5-4) (on design bp\_be). This setting consistently
    yields competitive overall placement quality and is adopted for all designs.


    ## *B. Floorplan Optimization Assessment*


    Tables [III](#page-6-0) and [IV](#page-6-1) summarize the routing wirelength (rWL),
    whitespace score, timing, and power results for designs with rectilinear numbers
    of 1 and 2, respectively. The results are obtained using DREAMPlace 4.1 [\[10\]](#page-8-9)
    and our proposed method. For each design, the best result in each metric is highlighted
    in bold. Overall, the results demonstrate that our proposed method outperforms
    the DREAMPlace 4.1 in both rWL and timing. For designs with a rectilinear number
    of 1, the proposed method achieves an average improvement of


    <span id="page-6-0"></span>TABLE III COMPARISON OF ROUTING WIRELENGTH (RWL), WHITESPACE
    SCORE, TIMING AND POWER RESULTS OBTAINED AFTER ROUTING FOR 1 NOTCH CORNER (RECT.
    NUM = 1).


    |           | Design        | DREAMPlace 4.1 [10] |       |          |          |            |
    WISP (Proposed Method) |       |          |          |            |

    |-----------|---------------|---------------------|-------|----------|----------|------------|------------------------|-------|----------|----------|------------|

    | Rect. Num | Name          | rWL (um)            | Score | WNS (ns) | TNS (ns)
    | Power (mW) | rWL (um)               | Score | WNS (ns) | TNS (ns) | Power (mW)
    |

    | num = 1   | TinyRocket    | 453557              | 174.4 | -0.032   | -0.05    |
    113.3      | 444502                 | 144.9 | 0.003    | 0.00     | 112.7      |

    | num = 1   | bp_be         | 2547806             | 304.1 | -0.587   | -214.60  |
    438.4      | 2287066                | 287.5 | -0.581   | -183.98  | 432.1      |

    | num = 1   | bp_fe         | 1515994             | 266.3 | -0.585   | -12.70   |
    267.6      | 1430214                | 241.2 | -0.492   | -10.95   | 265.4      |

    | num = 1   | black parrot  | 6068799             | 281.4 | -0.068   | -18.47   |
    339.9      | 6035678                | 273.4 | -0.031   | -1.28    | 335.7      |

    | num = 1   | bp_multi      | 3329946             | 265.6 | -0.303   | -666.51  |
    378.9      | 3292197                | 261.3 | -0.281   | -585.64  | 377.8      |

    | num = 1   | swerv_wrapper | 3958292             | 179.6 | -0.267   | -401.99  |
    1549.9     | 3691505                | 152.9 | -0.110   | -394.50  | 1520.2     |

    | num = 1   | ariane133 1   | 4791097             | 341.3 | 0.008    | 0.00     |
    266.1      | 4639719                | 330.8 | 0.006    | 0.00     | 264.4      |

    | num = 1   | ariane136     | 4754798             | 377.7 | -0.081   | -81.11   |
    970.5      | 4528625                | 345.7 | -0.081   | -72.12   | 988.5      |

    | num = 1   | bp_quad       | 28510588            | 574.1 | -0.008   | -2.34    |
    6166.9     | 28342822               | 545.9 | -0.007   | -2.28    | 6187.1     |

    | -         | Normalized    | 1.041               | 1.085 | 1.486    | 1.403    |
    1.005      | 1.000                  | 1.000 | 1.000    | 1.000    | 1.000      |


    <sup>1</sup> Use post-placement results because the placement results from DREAMPlace
    4.1 fail to complete routing in commercial tool.


    TABLE IV


    <span id="page-6-1"></span>COMPARISON OF ROUTING WIRELENGTH (RWL), WHITESPACE
    SCORE, TIMING AND POWER RESULTS OBTAINED AFTER ROUTING FOR 2 NOTCH CORNERS (RECT.
    NUM = 2).


    |           | Design        | DREAMPlace 4.1 [10] |       |          |          |            |
    WISP (Proposed Method) |       |          |          |            |

    |-----------|---------------|---------------------|-------|----------|----------|------------|------------------------|-------|----------|----------|------------|

    | Rect. Num | Name          | rWL (um)            | Score | WNS (ns) | TNS (ns)
    | Power (mW) | rWL (um)               | Score | WNS (ns) | TNS (ns) | Power (mW)
    |

    | num = 2   | TinyRocket    | 506270              | 167.8 | -0.084   | -36.62   |
    273.3      | 478431                 | 159.7 | -0.078   | -31.87   | 273.3      |

    | num = 2   | bp_be         | 3442510             | 332.3 | -0.632   | -192.36  |
    476.12     | 2732402                | 303.5 | -0.628   | -178.48  | 440.9      |

    | num = 2   | bp_fe         | 2122717             | 269.7 | -0.405   | -10.01   |
    587.3      | 1871999                | 231.8 | -0.061   | -1.45    | 574.3      |

    | num = 2   | black parrot  | 6870222             | 293.3 | -0.442   | -2639.20
    | 381.3      | 6632289                | 268.6 | -0.451   | -2558.30 | 380.9      |

    | num = 2   | bp_multi      | 3506285             | 278.3 | -0.349   | -875.98  |
    382.3      | 3422638                | 264.5 | -0.343   | -832.99  | 383.1      |

    | num = 2   | swerv_wrapper | 3728282             | 157.5 | -0.336   | -733.36  |
    440.8      | 3729285                | 133.8 | -0.328   | -614.83  | 440.8      |

    | num = 2   | ariane133     | 4764971             | 355.1 | -0.088   | -79.68   |
    1006.6     | 4590018                | 319.7 | -0.088   | -63.62   | 1045.8     |

    | num = 2   | ariane136     | 4481675             | 386.9 | -0.124   | -160.91  |
    991.9      | 4315827                | 366.1 | -0.073   | -62.41   | 982.3      |

    | num = 2   | bp_quad       | 29878305            | 595.8 | -0.009   | -3.37    |
    7177.2     | 29553113               | 567.8 | -0.007   | -3.37    | 7174.2     |

    | -         | Normalized    | 1.066               | 1.094 | 1.343    | 1.470    |
    1.008      | 1.000                  | 1.000 | 1.000    | 1.000    | 1.000      |


    <span id="page-6-3"></span>![](_page_6_Figure_6.jpeg)


    <span id="page-6-4"></span><span id="page-6-2"></span>Fig. 9. Layouts comparison
    of multiple benchmark designs with rectilinear floorplan (Rectilinear Num = 1
    and Num = 2) between DREAMPlace 4.1 [\[10\]](#page-8-9) and our proposed method.
    A red line crossing the grey box highlights areas that can be further reduced
    at the block level.


    4.1% in rWL, 48.6% in WNS, 40.3% in TNS, and 0.5% in power. For rectilinear number
    of 2, the average improvements increase to 6.6% in rWL, 34.3% in WNS, 47.0% in
    TNS, and 0.8% in power. These results indicate that the whitespace score reduction
    targeted by our method effectively translates into design performance gains in
    the majority of test cases. <span id="page-6-6"></span><span id="page-6-5"></span>To
    further visualize the impact, Fig. [9](#page-6-2) shows the placement results
    for various benchmark designs. As shown in Fig. [9\(a\),](#page-6-3) ariane133
    demonstrates a significantly improved placement result. Compared to DREAMPlace
    4.1, the *wasted whitespace* is effectively reduced through strategic macro position
    adjustments, highlighting the effectiveness of our algorithm in


    ![](_page_7_Figure_0.jpeg)


    <span id="page-7-1"></span>Fig. 10. Runtime breakdown of the whole flow for all
    designs.


    TABLE V BLOCK-LEVEL AREA REDUCTION VIA WHITESPACE ANALYSIS.


    <span id="page-7-3"></span>


    |                     |            | Area<br>(um2<br>) | ∆Area | rWL<br>(um) |
    Power<br>(mW) | WNS<br>(ns) | TNS<br>(ns) |

    |---------------------|------------|-------------------|-------|-------------|---------------|-------------|-------------|

    | Tiny Rocket         | DMP4.1[10] | 134619            | -     | 506270      |
    273           | -0.084      | -36.62      |

    | Recti Num = 2       | WISP       | 104637            | 22.3% | 471600      |
    262           | -0.083      | -30.05      |

    | bp_be               | DMP4.1[10] | 591436            | -     | 2547806     |
    438           | -0.590      | -214.59     |

    | Recti Num = 1       | WISP       | 374434            | 36.7% | 2387468     |
    432           | -0.580      | -166.59     |

    | bp_fe               | DMP4.1[10] | 606328            | -     | 2122716     |
    587           | -0.405      | -10.01      |

    | Recti Num = 2       | WISP       | 492964            | 18.7% | 1662803     |
    549           | -0.038      | -1.05       |

    | black_parrot        | DMP4.1[10] | 1854920           | -     | 6870222     |
    381           | -0.442      | -2639.20    |

    | Recti Num = 2       | WISP       | 1680956           | 9.4%  | 6625714     |
    321           | -0.406      | -2102.50    |

    | bp_multi            | DMP4.1[10] | 1389160           | -     | 3506285     |
    379           | -0.303      | -666.51     |

    | Recti Num = 1       | WISP       | 1174320           | 15.5% | 3251848     |
    379           | -0.298      | -584.12     |

    | swerv_wrapper       | DMP4.1[10] | 1130514           | -     | 3958292     |
    1549          | -0.267      | -401.99     |

    | Recti Num = 1       | WISP       | 976964            | 13.6% | 3654314     |
    1482          | -0.243      | -380.42     |

    | ariane133           | DMP4.1[10] | 1648020           | -     | 4764971     |
    1006          | -0.088      | -79.68      |

    | Recti Num = 2       | WISP       | 1230460           | 25.3% | 4613951     |
    975           | -0.087      | -69.51      |

    | ariane136           | DMP4.1[10] | 1566414           | -     | 4754798     |
    970           | -0.081      | -81.11      |

    | Recti Num = 1       | WISP       | 1534407           | 2.4%  | 4600331     |
    899           | -0.092      | -84.55      |

    | bp_quad             | DMP4.1[10] | 8817303           | -     | 28510588    |
    6166          | -0.008      | -2.34       |

    | Recti Num = 1       | WISP       | 8639603           | 2.0%  | 28243888    |
    7138          | -0.007      | -1.53       |

    | Average Improvement |            | 16.2% ↑           | -     | 6.3% ↑      |
    3.0% ↑        | 13.9% ↑     | 23.9% ↑     |


    handling macro-rich designs. A maximum rWL improvement of 11.4% has been achieved
    in design bp\_be. The placement outcome for this design including rectilinear
    number of 1 and 2 are shown in Fig. [9\(c\)\(](#page-6-4)2) and [9\(d\)\(](#page-6-5)2)
    respectively, where it is compared against DREAMPlace 4.1 [\[10\]](#page-8-9)
    (Fig. [9\(c\)\(](#page-6-4)1) and Fig. [9\(d\)\(](#page-6-5)1)). It demonstrates
    that our method achieves a final placement with no *wasted whitespace*, significantly
    improving block-level area utilization. Unusable gaps between macros are minimized,
    and macros are placed closer to each other and the boundary, resembling the strategy
    of an experienced engineer. This not only optimizes whitespace usage but also
    leaves more open area for subsequent placement and routing stages.


    The overall timing improvement comes from the reduction of *wasted whitespace*
    and routing wirelength (rWL). This reduction minimizes the occurrence of long
    and zigzag wires, which are commonly seen in notch areas and *wasted whitespace*
    regions, thereby enhancing routability and overall timing performance of the designs.


    ## *C. Runtime Results*


    Fig. [10](#page-7-1) provides a breakdown of the runtime for the placement and
    analysis stages in the proposed approach for all designs. On average, for the
    nine designs, the Image Segmentation-based floorplan parsing accounts for 13.81%
    of


    ![](_page_7_Figure_8.jpeg)


    <span id="page-7-2"></span>Fig. 11. The illustration of an area recycling iteration
    from block-level rectilinear module floorplan to a top-level floorplan.


    the total runtime, while the GMM-based scoring contributes 33.21%. This highlights
    the relatively low complexity of analyzing, segmenting regions, and scoring whitespace
    pixels within the layout. In contrast, the pre-placement and SA stages dominate
    the runtime, collectively accounting for 52.94%. These results underscore the
    computational efficiency of the parsing and scoring stages, which remain largely
    unaffected by design scale. However, the placement phase runtime increases significantly
    with design complexity, making it the primary contributor to overall runtime.


    ## <span id="page-7-0"></span>*D. Area Recycling —— Reclaiming Wasted Whitespace
    from Block Level via Whitespace Analysis*


    The interaction between top-level and block-level owners often involves iterative
    negotiations to exchange area effectively. While recent top-level floorplanning
    works suggest achieving more rectilinear block-level layouts [\[3\]](#page-8-2),
    [\[4\]](#page-8-3), [\[5\]](#page-8-4), our whitespace analysis can identify block-level
    potential *wasted whitespace* with lower whitespace scores. As illustrated in
    Fig. [11,](#page-7-2) our analysis provides a systematic approach to reclaim these
    underutilized regions confidently and return them to the top-level, thereby maximizing
    overall chip area utilization. The proposed methodology enhances the baseline
    placer by identifying and reclaiming additional area without compromising performance
    or power. These analysis results for all designs are summarized in Table [V,](#page-7-3)
    with the area reduction of 16.2%, improvements of 6.3% in rWL, 13.9% in WNS and
    23.9% in TNS, which reveal the effectiveness of our method.


    This capability is also illustrated in Fig. [9,](#page-6-2) where we analyze the
    designs black\_parrot and ariane133 with a rectilinear number of 2, as well as
    bp\_be with rectilinear numbers of 1 and 2. In Fig. [9\(a\)\(](#page-6-3)3), [9\(b\)\(](#page-6-6)3),
    [9\(c\)\(](#page-6-4)3), and [9\(d\)\(](#page-6-5)3), lowscore regions are highlighted
    in the upper portions of the layouts, indicating substantial wasted whitespace.
    By removing these regions and rerunning the global placement, we obtain optimized
    layouts shown in Fig. [9\(a\)\(](#page-6-3)4), [9\(b\)\(](#page-6-6)4), [9\(c\)\(](#page-6-4)4),
    and [9\(d\)\(](#page-6-5)4), which demonstrate significant area reduction and
    improved timing. These results validate the effectiveness of our approach in reclaiming
    *wasted whitespace* while enhancing rectilinear floorplan performance. The recycled
    area can be returned to the top-level floorplanner for further redistribution
    and overall layout refinement.


    ## V. CONCLUSIONS


    In this paper, we highlight the importance of performing whitespace analysis in
    rectilinear floorplanning and propose a novel framework WISP that extracts *wasted
    whitespace* and scores the overall whitespace distribution to guide the macro
    location optimization. The extracted whitespace information is seamlessly incorporated
    into the loss function to improve subsequent macro placement steps. Through extensive
    experiments on diverse benchmarks with different rectilinear floorplan shapes,
    we demonstrate that the proposed methodology significantly outperforms the leading
    baseline placer in routing wirelength (rWL) by 5.4%, and in timing, with improvements
    of 41.5% in WNS and 43.7% in TNS. Additionally, WISP offers opportunity to recycle
    16.2% wasted area on average from the rectilinear blocks while maintaining or
    improving key design metrics. As future work, we plan to open-source this methodology,
    aiming to integrate it as a key feature in other open-source macro or mixed-size
    placers.


    ## REFERENCES


    - <span id="page-8-0"></span>[1] A. B. Kahng, S. Kang, S. Kundu, K. Min, S. Park,
    and B. Pramanik, "PPA-Relevant Clustering-Driven Placement for Large-Scale VLSI
    Designs," in *24rd ACM/IEEE Design Automation Conference*, 2024.

    - <span id="page-8-1"></span>[2] J. Im and S. Kang, "SkyPlace: A New Mixed-size
    Placement Framework using Modularity-based Clustering and SDP Relaxation," in
    *ACM/IEEE Design Automation Conference*, 2024.

    - <span id="page-8-2"></span>[3] S. Kai, C.-W. Pui, F. Wang, S. Jiang, B. Wang,
    Y. Huang, and J. Hao, "TOFU: A Two-Step Floorplan Refinement Framework for Whitespace
    Reduction," in *2023 Design, Automation and Test in Europe Conference and Exhibition
    (DATE)*, 2023, pp. 1–5.

    - <span id="page-8-3"></span>[4] X. Du, R. Zhong, Z. Tang, S. Xu, S. Kai, J. Hao,
    M. Yuan, and J. Yan, "JigsawPlanner: Jigsaw-like Floorplanner for Eliminating
    Whitespace and Overlap among Complex Rectilinear Modules," in *2024 IEEE/ACM International
    Conference on Computer Aided Design (ICCAD)*, 2024.

    - <span id="page-8-4"></span>[5] Y.-Y. Chen, Y.-C. Lin, T.-H. Hsu, I. H.-R. Jiang,
    T.-C. Chen, T.-C. Chen, and H.-Y. Chang, "Modern Fixed-Outline Floorplanning with
    Rectilinear Soft Modules," in *2024 IEEE/ACM International Conference on Computer
    Aided Design (ICCAD)*, 2024.

    - <span id="page-8-5"></span>[6] A. Vidal-Obiols, J. Cortadella, J. Petit, M.
    Galceran-Oms, and F. Martorell, "RTL-Aware Dataflow-Driven Macro Placement," in
    *2019 Design, Automation and Test in Europe Conference and Exhibition (DATE)*,
    2019, pp. 186–191.

    - <span id="page-8-6"></span>[7] T. P. Le, H. T. Nguyen, S. Baek, T. Kim, J. Lee,
    S. Kim, H. Kim, M. Jung, D. Kim, S. Lee, and D. Choi, "Toward Reinforcement Learning-based
    Rectilinear Macro Placement Under Human Constraints," *arXiv preprint arXiv:2311.03383*,
    2023.

    - <span id="page-8-7"></span>[8] A. B. Kahng, R. Varadarajan, and Z. Wang, "Hier-RTLMP:
    A hierarchical automatic macro placer for large-scale complex IP blocks," *IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems*, 2023.

    - <span id="page-8-8"></span>[9] A. Agnesina, P. Rajvanshi, T. Yang, G. Pradipta,
    A. Jiao, B. Keller, B. Khailany, and H. Ren, "AutoDMP: Automated DREAMPlace-based
    Macro Placement," in *Proceedings of the 2023 International Symposium on Physical
    Design*, ser. ISPD ''23, 2023, p. 149–157.

    - <span id="page-8-9"></span>[10] Y. Chen, Z. Wen, Y. Liang, and Y. Lin, "Stronger
    Mixed-Size Placement Backbone Considering Second-Order Information," in *2023
    IEEE/ACM International Conference on Computer Aided Design (ICCAD)*, 2023, pp.
    1–9.

    - <span id="page-8-10"></span>[11] X. Zhao, T. Wang, R. Jiao, and X. Guo, "Standard
    cells do matter: Uncovering hidden connections for high-quality macro placement,"
    in *2024 Design, Automation and Test in Europe Conference (DATE)*, 2024, pp. 1–6.

    - <span id="page-8-11"></span>[12] Y. Pu, T. Chen, Z. He, C. Bai, H. Zheng, Y.
    Lin, and B. Yu, "IncreMacro: Incremental Macro Placement Refinement," in *Proceedings
    of the 2024 International Symposium on Physical Design*, ser. ISPD ''24. New York,
    NY, USA: Association for Computing Machinery, 2024, p. 169–176. [Online]. Available:
    <https://doi.org/10.1145/3626184.3633321>

    - <span id="page-8-12"></span>[13] U. Mallappa, H. Mostafa, M. Galkin, M. Phielipp,
    and S. Majumdar, "FloorSet - a VLSI Floorplanning Dataset with Design Constraints
    of Real-World SoCs," in *2024 IEEE/ACM International Conference on Computer Aided
    Design (ICCAD)*, 2024.

    - <span id="page-8-13"></span>[14] J. Canny, "A computational approach to edge
    detection," *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. PAMI-8, no. 6, pp. 679–698, 1986.

    - <span id="page-8-14"></span>[15] "Tilos macro placement benchmark," [https://github.com/](https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Testcases)
    [TILOS-AI-Institute/MacroPlacement/tree/main/Testcases,](https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Testcases)
    2022.

    - <span id="page-8-15"></span>[16] "Openroad-flow-script," [https://github.com/The-OpenROAD-Project/](https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts)
    [OpenROAD-flow-scripts,](https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts)
    2023.

    - <span id="page-8-16"></span>[17] C. Wolf, "Yosys open synthesis suite," [https://github.com/YosysHQ/](https://github.com/YosysHQ/yosys)
    [yosys,](https://github.com/YosysHQ/yosys) 2016.

    - <span id="page-8-17"></span>[18] "Nangate 45nm library," [http://www.nangate.com/,](http://www.nangate.com/)
    2006.'
- title: "RISC-Q: A Generator for Real-Time Quantum Control System-on-Chips\n  Compatible\
    \ with RISC-V"
  abstract: "Quantum computing imposes stringent requirements for the precise control\
    \ of\nlarge-scale qubit systems, including, for example, microsecond-latency feedback\n\
    and nanosecond-precision timing of gigahertz signals -- demands that far exceed\n\
    the capabilities of conventional real-time systems. The rapidly evolving and\n\
    highly diverse nature of quantum control necessitates the development of\nspecialized\
    \ hardware accelerators. While a few custom real-time systems have\nbeen developed\
    \ to meet the tight timing constraints of specific quantum\nplatforms, they face\
    \ major challenges in scaling and adapting to increasingly\ncomplex control demands\
    \ -- largely due to fragmented toolchains and limited\nsupport for design automation.\n\
    \  To address these limitations, we present RISC-Q -- an open-source flexible\n\
    generator for Quantum Control System-on-Chip (QCSoC) designs, featuring a\nprogramming\
    \ interface compatible with the RISC-V ecosystem. Developed using\nSpinalHDL,\
    \ RISC-Q enables efficient automation of highly parameterized and\nmodular QCSoC\
    \ architectures, supporting agile and iterative development to meet\nthe evolving\
    \ demands of quantum control. We demonstrate that RISC-Q can\nreplicate the performance\
    \ of existing QCSoCs with significantly reduced\ndevelopment effort, facilitating\
    \ efficient exploration of the hardware-software\nco-design space for rapid prototyping\
    \ and customization."
  url: http://arxiv.org/abs/2505.14902v1
  keywords: ''
  document: '## I. INTRODUCTION


    The transformative advances of quantum computing will only be realized when systems
    can scale reliably and efficiently. In the past decade, quantum hardware has seen
    rapid progress across multiple implementation technologies. Although today''s
    quantum computers have not yet reached practical utility, their rapid scaling
    is already intensifying demands on the quantum control stack. Quantum control
    systems, originally designed for proof-of-concept demonstrations in meticulously
    controlled lab settings, must now evolve to meet industrial demands—delivering
    unprecedented levels of precision, timing accuracy, and scalable operation. This
    transformation positions them not merely as supporting hardware, but as the critical
    infrastructure enabling practical, highperformance quantum computing.


    The fundamental control requirements for quantum systems already exceed the capabilities
    of conventional real-time systems, necessitating customized Quantum Control System-on-Chip
    (QCSoC) solutions. Superconducting qubits, for example, demand gigahertz signal
    control with nanosecond timing precision and closed-loop feedback with microsecond
    latency. While atomic systems (such as trapped ions and neutral atoms) have somewhat
    relaxed timing constraints, they introduce other demanding real-time control challenges,
    such as, precise ion


    <span id="page-0-0"></span><sup>1</sup><https://github.com/Wu-Quantum-Application-System-Group/RISC-Q>


    shuttling, dynamic optical tweezer control, atom loading/rearrangement, and high-speed
    image processing for state readout - all requiring specialized real-time hardware
    solutions.


    As quantum hardware evolves, increasingly sophisticated control requirements continue
    to emerge as active research frontiers. A prime example is quantum error correction
    (QEC) [\[1\]](#page-8-0), where the demanding need to complete full syndrome measurement,
    decoding, and correction cycles within the coherence time remains a crucial challenge.
    Mid-circuit measurement is another emerging control requirement enabling advanced
    capabilities like real-time feedback (e.g., [\[2\]](#page-8-1)) and dynamic circuit
    execution (e.g., [\[3\]](#page-8-2), [\[4\]](#page-8-3)). Scalable quantum control
    would also require resource-efficient architectures, distributed QCSoC coordination,
    and tight GPU integration to support advanced quantum-classical processing.


    While existing solutions—including open-source projects (e.g., [\[5\]](#page-8-4),
    [\[6\]](#page-8-5), [\[7\]](#page-8-6), [\[8\]](#page-8-7)), closed-source projects
    (e.g., [\[9\]](#page-8-8), [\[10\]](#page-8-9), [\[11\]](#page-8-10)), and commercial
    systems (e.g., Zurich Instruments, Quantum Machines, Qblox)—can address basic
    control requirements for small-scale quantum systems, they face significant limitations
    in supporting the fast-evolving and scaling control needs due to two systemic
    issues: (1) *fragmented toolchains* where hardware-specific control architecture
    and software APIs create vendor lock-in and impede cross-platform development;
    and (2) *limited design automation* that forces researchers to manually implement
    the low-level control architecture and optimize its design parameters rather than
    leveraging high-level design tools.


    Inspired by the success of agile development in hardware accelerators for machine
    learning and specialized computing kernels [\[12\]](#page-8-11), [\[13\]](#page-8-12),
    we introduce RISC-Q—the first opensource generator for QCSoC designs, featuring
    a programming interface compatible with the RISC-V ecosystem. We anticipate that
    extensive design exploration will be required to identify optimal control architectures
    for heterogeneous quantum platforms. RISC-Q facilitates this process by enabling
    efficient automation of highly parameterized and modular QCSoC architectures,
    supporting agile and iterative development to explore the hardware–software co-design
    space and accelerate rapid prototyping and customization. Moreover, by providing
    a unified and extensible platform, RISC-Q fosters interoperability and collaborative
    community efforts and promote shared innovation in quantum control system design.


    The development of RISC-Q begins with a general modeling of quantum control systems
    across various implementation technologies. While these systems differ in their
    specifics, they can all be modeled using classical digital components for logic
    control and analog RF signal components for interacting with the physical quantum
    systems. Fig. [1](#page-2-0) refers to a more detailed modeling of QCSoCs developed
    in Section [II](#page-1-0) based on both current control requirements and future
    scalability demands.


    RISC-Q is a modular and flexible QCSoC generator implemented in SpinalHDL [\[14\]](#page-8-13).
    It enables fine-grained parameterization, interface abstraction, and flexible
    peripheral integration to maximize component reuse and adaptability. Additionally,
    RISC-Q integrates RISC-V ISA-compatible controllers, leveraging the broader RISC-V
    software ecosystem for enhanced scalability and interoperability.


    We present a fully functional QCSoC for superconducting quantum computers, designed
    with RISC-Q and deployed on a Xilinx ZCU216 RFSoC board. Our QCSoC achieves performance
    on par with the state-of-the-art systems like QICK [\[5\]](#page-8-4) and QubiC
    [\[6\]](#page-8-5) in terms of frequency, DAC/ADC channels, and resource utilization—while
    significantly reducing both development effort and codebase size (by ∼75%). As
    a representative case study, we focus on RISC-Q''s implementation of the measurement
    step to illustrate how its flexible parameterization and integration capabilities
    enable efficient design space exploration and rapid prototyping. Finally, we demonstrate
    how RISC-Q supports the co-design of conditional gate implementations triggered
    by mid-circuit measurements, as well as the direct on-chip execution of calibration
    protocols to minimize communication overhead and enable faster feedback.


    In brief, this work makes the following contributions:


    - 1) We present RISC-Q, an open-source modular generator for QCSoCs, featuring
    a RISC-V–compatible programming interface and support for flexible parameterization
    and peripheral integration (Section [III\)](#page-3-0).

    - 2) We develop a fully functional QCSoC for superconducting qubit control with
    RISC-Q, achieving SOTA performance with significantly reduced development effort
    and enhanced flexibility for design space exploration (Section [IV,](#page-4-0)[V\)](#page-7-0).

    - 3) We demonstrate that RISC-Q facilitates implementation codesign for conditional
    branching and enables direct on-chip execution of calibration protocols (Section
    [V\)](#page-7-0).


    ## II. MODELING QUANTUM CONTROL SYSTEMS


    <span id="page-1-0"></span>Physical Requirements: Several physical platforms are
    widely used in quantum computing, including, e.g., superconducting circuits, neutral
    atoms, and trapped ions. Despite differences in the underlying physics, these
    systems are typically controlled using radio-frequency (RF) analog signals. Unlike
    classical systems, quantum systems impose stringent real-time requirements on
    these signals–—demanding high frequency, precise phase coherence, and ultra-low
    latency—–due to their time-sensitive and fragile nature.


    For instance, superconducting qubits, among the most demanding platforms, require
    control signals in the 3–6 GHz range. The phase of these signals determines the
    rotation axis of quantum gates, making phase precision essential for highfidelity
    operations [\[15\]](#page-8-14). Maintaining phase coherence at these frequencies
    is particularly challenging. Furthermore, because of the short coherence times
    of superconducting qubits, measurement decoding and feedback control must be completed
    within latencies of microseconds [\[16\]](#page-8-15), [\[17\]](#page-8-16). Prior
    research [\[5\]](#page-8-4), [\[6\]](#page-8-5), [\[7\]](#page-8-6) has demonstrated
    that high-performance digital control systems can generate analog signals that
    meet the demanding requirements of quantum systems.


    Architecture of Quantum Control Systems: Fig. [1](#page-2-0) illustrates a representative
    quantum control system architecture. The system components are categorized based
    on their functions, including RF signal processors, controllers, custom accelerators,
    and communication modules, illustrated as follows.


    ## *A. RF Signal Processor*


    Quantum systems are driven by RF signals, which require dedicated modules for
    generation and decoding.


    *1) RF Signal Generator:* The RF signal generator produces the signals that drive
    quantum systems. In digital control systems, these signals are generated by outputting
    discrete samples to a DAC (Digital-to-Analog Converter), which transforms them
    into analog signals.


    The RF signal generator is the most real-time critical component. The phase ϕ<sup>k</sup>
    of a driving signal directly affects the resulting quantum gate. With carrier
    frequencies ω<sup>k</sup> in the MHz-GHz range, even a single-cycle timing error
    can result in significant phase deviation. The requirements of the generator vary
    across platforms. For superconducting qubits, the high carrier frequencies (several
    GHz) demand extremely precise timing. Neutral atom and trapped ion systems typically
    demand multi-tone signals at lower frequencies but with greater spectral complexity
    [\[18\]](#page-8-17).


    *2) RF Signal Decoder:* In superconducting systems, qubit measurement involves
    sending a readout RF signal and extracting the phase information of the feedback
    RF signal. Since measurement outcomes are required for real-time feedback in tasks
    such as quantum error correction and dynamic quantum circuits, the latency of
    the decoding process is critical. Additionally, noise in the readout signal introduces
    a probability of incorrect results, making decoding accuracy also important.


    The standard procedure involves demodulating the I/Q components of the feedback
    signal followed by low-pass filtering to extract the phase [\[15\]](#page-8-14).
    Machine learning methods have also been explored to improve performance [\[19\]](#page-8-18),
    [\[20\]](#page-8-19).


    ## *B. Controller*


    The controller provides a programmable interface to the quantum control system.
    It must support diverse signal sequences and feedback logic required by different
    experimental protocols. General-purpose CPUs are often used to enable flexible,
    software-defined control without requiring hardware redesign, making it a natural
    choice for the controller.


    A key requirement is a stable and flexible programming interface, built on a well-tested
    instruction set architecture (ISA). This ensures a mature software ecosystem and
    allows control software to remain reusable across iterative hardware revisions
    and collaborative development cycles.


    In addition to software flexibility, the controller must also integrate heterogeneous
    components, including RF processors and custom accelerators. It must also meet
    strict real-time constraints, including nanosecond-accurate timing for phasesensitive
    operations, clock rates in hundreds of megahertz to


    ![](_page_2_Figure_1.jpeg)


    <span id="page-2-0"></span>Fig. 1. A Schematic Overview of the RISC-Q hardware
    architecture for Quantum Control System-on-Chip (QCSoC). The RFSP program refers
    to programs that process Radio-Frequency Signal Processing tasks.


    match RF signal processing requirements, and microsecondlevel feedback latency
    to support real-time protocols like quantum error correction.


    # *C. Custom Accelerator*


    Although general-purpose controllers offer programmability, they can struggle
    with computationally intensive tasks under real-time constraints. To address this,
    custom accelerators are incorporated to handle the demanding computations required
    in quantum control protocols.


    *1) QEC Decoder:* Quantum systems are inherently sensitive to environmental noise
    and will rely on QEC for reliability. QEC protocols rely on the decoding process
    that analyzes measurement outcomes to identify and correct errors in real-time.
    Practical decoding implementations typically involve algorithms such as minimum-weight
    perfect matching, belief propagation, or neural-network-based approaches. However,
    these algorithms can be computationally intensive, often breaking the real-time
    constraints imposed by quantum systems. Consequently, specialized hardware acceleration
    becomes essential to meet latency demands and ensure timely error correction [\[16\]](#page-8-15),
    [\[17\]](#page-8-16), [\[21\]](#page-8-20).


    *2) Image Processor:* In neutral atom and trapped ion systems, optical sensors
    are used to determine qubit positions and measurement outcomes. These sensors
    produce raw images, which must be processed to extract relevant information. Because
    these results are used in time-sensitive feedback operations, such as QEC or atom
    rearrangement, low-latency image processing is crucial. Dedicated image processors
    can reduce processing latency significantly [\[22\]](#page-8-21).


    *3) Optical Tweezer Controller:* Neutral atom systems use optical tweezers controlled
    by RF-driven Acousto-Optic Deflectors (AODs) to position qubits. The optical tweezer
    controller translates positional information into RF control parameters [\[18\]](#page-8-17).


    *4) Rearrangement Scheduler:* During the initialization of neutral atom systems,
    atoms are probabilistically loaded into optical traps, often resulting in partially
    filled arrays. Rearrangement is required to move atoms and fill the empty sites,
    achieving the desired qubit layout.


    Conventional approaches involves sending an image of the array to a PC, computing
    the movement schedule, and sending it back to the control system. This round-trip
    communication becomes a latency bottleneck. Recent efforts have suggested hardware-accelerated
    rearrangement scheduling directly within the control system, eliminating the need
    for time-consuming data transfers [\[22\]](#page-8-21), [\[23\]](#page-8-22).


    ## *D. Communication Module*


    Practical quantum computing involves controlling thousands to millions of qubits,
    which necessitates a distributed architecture [\[24\]](#page-8-23). Each node
    in the system requires a communication module to exchange data and synchronize
    operations.


    Communication tasks include sharing measurement outcomes, delivering gate instructions,
    and synchronizing clocks across controllers. For instance, QEC protocols require
    realtime sharing of measurement results to compute error syndromes and apply corrections
    [\[17\]](#page-8-16). Achieving high fidelity thus requires extremely low-latency
    communication.


    Clock synchronization is also crucial: phase coherence between controllers depends
    on precise alignment in time. Therefore, the communication module must support
    highprecision synchronization protocols.


    # *E. Extensive Design Space for Exploration*


    Existing open-source quantum control systems already demonstrate significant diversity
    in their architectural and implementation choices. For example, QubiC [\[5\]](#page-8-4)
    adopts a distributed architecture where each qubit is controlled by a dedicated
    processor, simplifying the control logic per processor and reducing hardware complexity.
    In contrast, QICK [\[6\]](#page-8-5) uses a centralized architecture in which
    a single processor manages multiple qubits, reducing software complexity and simplifying
    coordination across channels.


    These systems also differ in instruction encoding and control strategies. QubiC
    uses a single 128-bit instruction to configure all parameters of an RF signal
    generator in parallel, achieving low control latency. QICK, on the other hand,
    configures each parameter individually using 64-bit instructions, offering greater
    programming flexibility. As a result of these differing approaches, their operating
    frequencies also vary: QubiC runs at 500 MHz, while QICK operates at 384 MHz.


    The combination of architectural, implementation, and parameters forms a vast
    design space for quantum control systems. Efficient exploration of this space
    is critical to meeting the stringent real-time constraints of quantum experiments
    while remaining within available hardware resources.


    Consider quantum error correction as an illustrative example. Architecturally,
    a distributed control protocol is required to collect measurement results and
    perform syndrome decoding across a large number of qubits. At the implementation
    level, various decoding algorithms are available, each with different performance
    characteristics. For instance, surface codes can be decoded using union-find algorithms
    [\[25\]](#page-8-24), minimum-weight perfect matching [\[26\]](#page-8-25), [\[21\]](#page-8-20),
    tensor network contraction [\[27\]](#page-8-26), or machine learning-based methods
    [\[28\]](#page-8-27), [\[29\]](#page-9-0), [\[30\]](#page-9-1).


    While open-source decoders such as those in [\[21\]](#page-8-20) exist, integrating
    them into control systems still requires substantial effort. The integration must
    consider how the decoder is triggered and controlled—whether through memory-mapped
    I/O, custom instructions or dedicated controllers—balancing the trade-off between
    complexity and real-time performance. Parameter tuning further expands the design
    space, as different code sizes and QEC protocols introduce varying timing and
    resource constraints.


    To effectively navigate this complex design space, an efficient and flexible design
    tool is essential—one that supports architectural customization, modular implementation,
    and parameter-driven optimization. RISC-Q is developed to meet this need, enabling
    agile and iterative exploration of the quantum controller design landscape.


    ## III. DESIGN AND IMPLEMENTATION OF RISC-Q


    <span id="page-3-0"></span>RISC-Q is a modular and flexible generator of quantum
    control system-on-chip designs that are compatible with the RISC-V ISA. It is
    developed using SpinalHDL [\[14\]](#page-8-13), a high-level hardware description
    language (HDL), which supports extensive parameterization and modular hardware
    design through meta-programming in Scala. By providing high-level abstractions
    and interfaces for both software and hardware, RISC-Q decouples their design process,
    enhances reusability across components, and facilitates collaborative development
    of quantum control systems.


    ## *A. Programming Interface*


    Quantum algorithms are typically implemented as quantum programs that describe
    quantum circuits, which are sequences of quantum gates. Since quantum systems
    are driven by RF signals, each gate is physically realized through a corresponding
    sequence of RF signals. These sequences are compiled into RF signal processing
    programs, which configure the RF signal processors in real time, based on predefined
    inputs and measurement-based feedback.


    An illustrative example of RF signal processing programs can be obtained by replacing
    the gates and measurements in quantum programs written in OpenQASM 3 [\[31\]](#page-9-2)
    with external RF instructions that directly control the underlying RF signal hardware.
    These programs are responsible not only for executing quantum circuits but also
    for essential control tasks such as QEC and calibration—both of which require
    branching, arithmetic, and memory operations.


    Many existing quantum control systems employ custom instruction sets to program
    RF signal processing [\[7\]](#page-8-6), [\[6\]](#page-8-5), [\[5\]](#page-8-4).
    While effective for particular hardware platforms, these domain-specific instruction
    sets create fragmented software ecosystems that require dedicated compilers, drivers,
    and libraries for each system.


    Despite differences in quantum-specific operations, a large portion of classical
    instructions—such as arithmetic, branching, and memory access—remains common across
    platforms. This motivates the adoption of a shared base ISA. The open and extensible
    nature of RISC-V makes it an ideal foundation for quantum controller design.


    RISC-Q generates controllers that are compatible with the RISC-V ISA, enabling
    reuse of the broader RISC-V software ecosystem, including compilers, libraries,
    and development tools. The resulting controllers expose a familiar bare-metal
    programming interface. Quantum-specific peripherals such as RF signal processors
    and accelerators are controlled via memory-mapped I/O or custom instructions,
    and their interfaces are abstracted through reusable C libraries and drivers.


    As a result, RF signal processing programs can be written in high-level languages
    such as C, C++, or Rust, compiled to RISC-V instructions using standard toolchains,
    and executed directly on the RISC-V-compatible controller generated by RISC-Q.
    This compatibility enables reuse of application code across different hardware
    implementations and quantum experiments, and lays the foundation for future operating
    systems for quantum hardware. With open-source toolchains and modern development
    workflows, RISC-Q improves reusability and extensibility in low-level quantum
    control software.


    ## *B. Integration*


    While some components are common across quantum systems, different physical platforms
    often impose unique hardware requirements. RF signal generators for trapped ions
    or neutral atoms may require more frequency components than those used for superconducting
    qubits, while the latter demand higher carrier frequencies. Controllers must therefore
    support flexible integration of diverse peripherals including the RF signal processors
    and custom accelerators in Fig. [1.](#page-2-0)


    Inspired by VexiiRiscv [\[32\]](#page-9-3), RISC-Q facilitates flexible peripheral
    integration. It supports two integration methods: Memory-Mapped I/O (MMIO) and
    custom instructions.


    MMIO is a widely used method where peripheral ports are mapped to specific memory
    addresses. For example, to set the frequency of a RF signal generator, a value
    is written to a designated address via the RISC-V sw instruction. Similarly, decoder
    outputs can be read with the lw instruction. Built on the SpinalHDL''s TileLink
    library, RISC-Q allows peripherals to be memory-mapped with minimal configuration
    overhead.


    However, MMIO can introduce latency and timing jitter, which may violate the strict
    real-time constraints of quantum control. To address this, custom instructions
    offer a faster alternative. For example, QubiC [\[5\]](#page-8-4) implements a
    dedicated instruction to configure all RF signal generator parameters, reducing
    control latency by several cycles.


    To support a broad range of hardware needs, custom instructions must be implemented
    in a modular and extensible way. This is particularly challenging for pipelined
    processors, where handling an instruction spans multiple pipeline stages. In such
    architectures, adding or modifying instructions traditionally requires editing
    multiple files across decode, execution, and writeback stages, making the process
    error-prone and difficult to maintain.


    RISC-Q addresses this challenge by adopting the pluginbased framework of VexiiRiscv
    and leveraging SpinalHDL''s pipeline API, which provides fine-grained access to
    hardware signals at each stage of the pipeline. This allows custom instructions
    to be implemented modularly in isolated files with parameterized pipeline stages,
    without modifying the core processor codebase.


    This design promotes collaborative development, simplifies maintenance, and enables
    the reuse of hardware features across different projects, making RISC-Q a robust
    and extensible platform for integrating quantum-specific peripherals.


    ## *C. Parameterization*


    In addition to varying peripheral types, the internal implementation of each peripheral
    often differs across quantum experiments. RISC-Q supports fine-grained parameterization
    and interface abstraction to maximize reuse and adaptability of components.


    For example, both RF signal generators and decoders require high-frequency carrier
    generators, but their throughput and latency requirements may differ. A parameterized
    carrier generator can be tailored to these differing constraints without duplicating
    design effort.


    SpinalHDL offers rich support for parameterization, including component interfaces,
    subcomponent implementations. This enables developers to adapt a single component
    design across a wide range of use cases with minimal modification.


    Fig. [2](#page-4-1) illustrates the software and hardware stack of RISC-Q. With
    its compatibility with open toolchains, flexible peripheral integration, and rich
    parameterization support, RISC-Q provides a scalable and reusable foundation for
    quantum control system design.


    ## IV. CASE STUDY AND ILLUSTRATION


    <span id="page-4-0"></span>RISC-Q is designed to generate quantum control systemon-chip
    targeting a wide range of quantum platforms, with support for both FPGA and ASIC
    implementations. As an initial demonstration, we focus on a prototype tailored
    for superconducting circuits. These systems pose some of the most stringent real-time
    requirements among quantum platforms, yet typically require fewer domain-specific
    accelerators. This makes them an ideal starting point for validating the timing
    and integration capabilities of RISC-Q.


    Our prototype is implemented on Xilinx RFSoCs, a class of devices widely adopted
    in modern quantum control systems [\[7\]](#page-8-6), [\[6\]](#page-8-5), [\[5\]](#page-8-4).
    RFSoCs integrate a multi-core ARM Processing System (PS) with a high-performance
    FPGA known as the Programmable Logic (PL), and include up to 16 RF DACs and ADCs.
    Our prototype QCSoC is synthesized for the PL of the Xilinx ZCU216 RFSoC evaluation
    board, which


    ![](_page_4_Figure_11.jpeg)


    <span id="page-4-1"></span>Fig. 2. Software and hardware stack of RISC-Q. Blue
    boxes indicate components implemented within the RISC-Q framework.


    offers sufficient RF I/O and logic resources for complex quantum experiments.
    Runtime communication with the RISC-Q QCSoC is coordinated by the ARM processor
    using the AXI interconnect. We employ the PYNQ [\[33\]](#page-9-4) library to
    facilitate interaction between the host software running on the PS and the programmable
    hardware in the PL. This setup enables high-level Python-based control over the
    low-level QCSoC, simplifying development and debugging.


    While we implemented a fully functioning QCSoC for superconducting qubits matching
    the performance of QubiC [\[5\]](#page-8-4) and QICK [\[6\]](#page-8-5) (see Section
    [V\)](#page-7-0), we will describe how RISC-Q implements the measurement logic
    for superconducting qubits as a representative example for illustration purpose.


    ## *A. The Measurement Process*


    The dataflow for the qubit measurement logic is shown in Fig. [3.](#page-6-0)
    To measure a superconducting qubit, a readout pulse of the form E(t)A cos(ωt +
    ϕ) is sent to the readout coupler. The returning signal carries information about
    the qubit state, which is encoded in the phase ϕ ′ of the reflected waveform.


    Accurate and low-latency extraction of ϕ ′ is critical for feedback control protocols.
    This requires a tightly orchestrated sequence of signal generation and decoding—all
    of which must be executed with deterministic timing in hardware.


    The measurement sequence begins with the control CPU specifying the measurement
    start time t<sup>0</sup> and duration T, and configuring the RF signal generator
    with the desired parameters. While the signal generator is fully pipelined, each
    parameter incurs a different latency before taking effect at the output. Therefore,
    the parameters must be timed precisely to synchronize their effect. To handle
    this, we employ timed FIFOs that buffer parameters and release them according
    to t<sup>0</sup> and their latencies. This mechanism decouples the timing constraints
    of the signal generator from the timing of CPU instructions. Such decoupling also
    enables a single CPU to coordinate multiple qubit channels, as demonstrated by
    QICK [\[6\]](#page-8-5). For readout, the RF signal decoder must be configured
    with the appropriate carrier frequency and phase. During calibration routines,
    the raw readout signal may also be captured into a readout buffer to assist with
    parameter tuning of the decoder logic.


    ## *B. Parameterization*


    The measurement and control logic described above involves a wide range of configurable
    parameters, which are selected based on the requirements of a specific experiment.
    RISC-Q enables fine-grained parameterization of components to ensure efficient
    reuse, scalability, and performance tuning. Below, we describe several representative
    parameters:


    *1) Qubits per CPU:* Thanks to the decoupling provided by timed FIFOs, signal
    generation can be scheduled in advance, enabling a single CPU to control multiple
    qubits. This reduces hardware resource usage and simplifies software development.
    However, the number of qubits a single CPU can support depends on the specific
    experimental protocol, as all timingsensitive signals must be pre-scheduled relative
    to a known start time. RISC-Q provides a parameter to decide the number of qubits
    assigned to each CPU.


    *2) Samples per Cycle of Carrier Generator:* As shown in Fig. [3,](#page-6-0)
    there are two carrier generators—one each in the RF signal generator and decoder.
    These modules must operate at different data rates due to the different sample
    rates of RF interfaces. For instance, with the CPU and RF signal processors running
    at 500 MHz, and the DAC and ADC operating at 8 GHz and 2 GHz respectively, the
    system must feed 16 samples per cycle to the DAC and receive 4 samples per cycle
    from the ADC. Consequently, the carrier generators must produce 16 and 4 samples
    per cycle, respectively. RISC-Q provides a parameterized carrier generator that
    supports varying sample rates, enabling reuse across both signal paths.


    *3) Envelope Memory:* The envelope generator must provide high-throughput amplitude
    envelopes, supplying 16 samples per cycle for mixing with the carrier signal.
    A common implementation stores precomputed envelopes in memory and reads multiple
    samples per cycle. However, storing many long envelopes—especially for systems
    controlling multiple qubits—can consume significant memory resources and become
    a limiting factor. To address this, RISC-Q allows configurable allocation of envelope
    memory for each signal generator, enabling designers to balance memory usage according
    to experiment requirements.


    *4) Readout Buffer:* During calibration, raw readout waveforms are often recorded
    to determine optimal decoding parameters. A readout buffer captures this data,
    but at the cost of substantial memory consumption, which can compete with other
    modules such as envelope generators. RISC-Q allows the inclusion or exclusion
    of readout buffers via parameters, so memory resources can be reallocated after
    calibration when they are no longer needed.


    *5) Implementation of Trigonometric Functions:* Each carrier generator contains
    a module that computes trigonometric functions (e.g., sine and cosine). Different
    implementations offer trade-offs between latency and resource usage. For example,
    lookup-table-based methods provide low latency but consume more memory, while
    CORDIC algorithms reduce memory usage at the cost of higher latency. RISC-Q supports
    parameterized selection of the trigonometric function implementation. Moreover,
    the pipeline structure of the RF signal generator is automatically adjusted based
    on the latency of the chosen implementation, using high-level parameterization
    by SpinalHDL. This flexibility allows developers to optimize for either timing
    or resource constraints based on the need.


    ## *C. Integration*


    RISC-Q uses SpinalHDL-based automation to simplify integrating peripherals and
    on-chip memory, improving both internal communication and quantum system interfacing.


    *1) Memory Integration:* Many components in the quantum control system require
    dedicated memory blocks. For example, as shown in Fig. [3,](#page-6-0) the RISC-V
    CPU uses RAM for program and data storage, the RF signal generator uses memory
    to store envelopes, and the RF signal decoder needs a buffer for capturing raw
    measurement data for offline analysis. These memories are typically implemented
    using the Block RAMs (BRAMs) available on the RFSoC.


    In addition to access by their associated components, these memories must also
    be accessible from the ARM processor to support runtime configuration and data
    retrieval. For instance, the RF signal processing program must be written to CPU
    RAM, envelope data loaded into the signal generator, and measurement results retrieved
    from the readout buffer.


    Because memory configurations often vary depending on experimental requirements,
    managing this integration manually can be error-prone and time-consuming. RISC-Q
    addresses this using the TileLink bus protocol [\[34\]](#page-9-5), a widely adopted
    standard in the RISC-V ecosystem. As illustrated in Fig. [4,](#page-6-1) memory
    blocks are connected to a TileLink interconnect that interfaces with the ARM-side
    AXI bus.


    This integration is facilitated with the tilelink.fabric library in SpinalHDL,
    a parameter-negotiation framework similar to Diplomacy [\[35\]](#page-9-6). The
    library automatically generates necessary TileLink components—including arbiters,
    decoders, width adapters, and buffers—based on the parameters of connected nodes.
    Attaching a new memory to a specific address can be done in just a few lines of
    code. This infrastructure enables seamless and flexible memory integration for
    peripherals with externally accessible memories.


    *2) MMIO:* To configure signal parameters and retrieve measurement results, the
    CPU must access peripheral ports such as parameter FIFOs connected to the signal
    generator and readout interfaces in the decoder (Fig. [3\)](#page-6-0). A common
    method for this is MMIO, which maps peripheral registers into the CPU''s address
    space, enabling access via standard load/store instructions. However, manually
    constructing MMIO buses for a large number of ports—especially across multiple
    iterations of hardware customization—can be tedious and error-prone. RISC-Q provides
    an automated solution based on SpinalHDL''s BusSlaveFactory library. Developers
    use a factory class to register (port, address) pairs for each instantiated peripheral.
    The library generates the hardware logic required to route read/write operations
    based on the TileLink protocol.


    ![](_page_6_Figure_0.jpeg)


    <span id="page-6-0"></span>Fig. 3. An example dataflow of superconducting qubit
    measurement in RISC-Q. RefTime is a 32-bit reference clock increased by 1 in every
    cycle for precise phase and timing.


    ![](_page_6_Figure_2.jpeg)


    <span id="page-6-1"></span>Fig. 4. Integration of memories and peripherals in
    RISC-Q. Red boxes indicate memories connected via the TileLink bus. Orange boxes
    represent peripherals integrated through MMIO and custom instructions.


    Combined with the flexible memory integration, this MMIO infrastructure enables
    rapid prototyping of experiment-specific peripherals with minimal effort.


    *3) Custom Instruction:* While MMIO provides a flexible and general approach for
    peripheral access, it may introduce latency and limit throughput—especially when
    multiple parameters must be written to initiate a single operation. To optimize
    for real-time performance, QubiC adopts a 128-bit custom instruction that encodes
    all pulse parameters [\[24\]](#page-8-23).


    To support this kind of extensibility, RISC-Q uses the VexiiRiscv processor framework,
    which adopts a plugin-based architecture inspired by VexRiscv [\[36\]](#page-9-7).
    In this model, custom instructions can be implemented as independent plugins with
    access to internal processor signals, eliminating the need to modify core pipeline
    modules such as decode and execute stages. Following this approach, RISC-Q implements
    a custom 128-bit instruction that configures all required parameters to launch
    an RF signal with a single instruction. The instruction format is illustrated
    in Fig. [5.](#page-6-2)


    | id<br>frequency   amplitude   phase   envelope address   duration | 127 - -
    | 112 111 | 96 96 95 | 80 79 |  | 64 63 | 48 47 |  | 43 42 | 15 14 |  |

    |-------------------------------------------------------------------|---------|---------|----------|-------|--|-------|-------|--|-------|-------|--|

    |                                                                   |         |         |          |       |  |       |       |  |       |       |  |


    <span id="page-6-2"></span>Fig. 5. Custom instruction for RF signal generation.
    id specifies the RF signal generator to be used.


    Moreover, gate-level instruction sets—such as those used in [\[9\]](#page-8-8),
    [\[37\]](#page-9-8)—can be realized by mapping gate operations to specific signal
    parameters using this extensible instruction interface.


    # *D. Programming Interface*


    Since the RISC-Q controller adheres to the RISC-V ISA, it can be programmed in
    any language that compiles to RISC-V. In our prototype, we use C to implement
    the control software. MMIO-based parameter access is performed by writing to specific
    memory addresses. For example, setting the phase of the 7th RF signal generator
    to π/2 is written as:


    \*( v o l a t i l e i n t 3 2 \_ t \*)SG\_PHASE\_ADDR( 7 ) = PHASE\_PI ( 0 . 5
    ) ;


    Here, SG\_PHASE\_ADDR computes the address of the phase register from the index
    7, and PHASE\_PI computes the fixedpoint representation of the phase. Similarly,
    measurement results can be read from the 7th decoder via:


    ```

    i n t r e s u l t = *( v o l a t i l e i n t 3 2 _ t *)RD_RES_ADDR ( 7 ) ;

    ```

    This bare-metal programming model allows quantum control software to be written
    in widely used high-level languages such as C, C++, and Rust. With the help of
    the abstraction capabilities of these languages and the RISC-V toolchain, developers
    can build reusable quantum control libraries that are portable across hardware
    platforms, enabling collaborative and modular software development.


    This compatibility lays the groundwork for implementing complex quantum control
    protocols—such as calibration routines and QEC—directly on the controller, avoiding
    the latency of round-trip communication with the processing unit. It opens the
    path toward developing more advanced software stacks for quantum control, ultimately
    enabling the emergence of quantum operating systems.


    In summary, the implementation of RISC-Q showcases how design automation, modular
    architecture, and compatibility with industry-standard toolchains can streamline
    the development of quantum control SoCs. By leveraging SpinalHDL for peripheral
    parameterization and integration, TileLink for flexible interconnects, and RISC-V
    for a standardized programming interface, RISC-Q enables efficient, scalable,
    and extensible control system designs. This foundation supports rapid prototyping,
    facilitates hardware–software co-design, and provides a versatile platform for
    exploring a wide range of quantum control applications.


    ## V. EVALUATION


    <span id="page-7-0"></span>This section evaluates the prototype QCSoC generated
    by RISC-Q in comparison with the state-of-the-art open-source control systems
    for superconducting qubits. We assess its hardware performance and resource efficiency.
    We also demonstrate RISC-Q''s new capabilities through two case studies: the hardware–software
    co-design of conditional gates and the implementation of an on-chip calibration
    protocol.


    ## *A. Performance and Resource Evaluation*


    Our fully functional QCSoC reproduces the core functionalities and performance
    of leading open-source control systems for superconducting circuits, including
    QICK and QubiC, with significantly reduced development effort through high-level
    design automation.


    *1) Development Effort:* To provide an intuitive comparison, we quantify the hardware
    design complexity in terms of lines of HDL code. The current implementation of
    RISC-Q consists of 11,549 lines of HDL code, compared to 172,294 lines in QICK
    and 45,835 lines in QubiC.


    While the codebases of QICK and QubiC include legacy modules and unused components
    that may inflate their line counts, RISC-Q still exhibits a substantial reduction
    in code volume. More importantly, its modular, extensible architecture enables
    adding new peripherals without altering existing components, improving maintainability
    and scalability.


    *2) Performance:* While signal generation delay and feedback latency are critical
    performance metrics for quantum control, fair benchmarking remains difficult due
    to differing architectures and the absence of standard evaluation tools. Therefore,
    we compare core system characteristics such as clock frequency and the number
    of supported RF channels.


    Our RISC-Q generated prototype operates at 500 MHz, controlling 16 DACs and 8
    ADCs. QubiC supports the same number of channels at the same frequency, whereas
    QICK operates at 384 MHz with 7 DACs and 2 ADCs.


    *3) Hardware Resource Usage:* We investigate the FPGA resource (e.g., Look-Up
    Table (LUT), Flip-Flop (FF)) utilization across all systems in Table [I.](#page-7-1)
    RISC-Q generated QCSoC has a slightly more but comparable resource consumption
    than QubiC, both of which are much more favorable than QICK.


    <span id="page-7-1"></span>TABLE I FPGA RESOURCE UTILIZATION AND PER-DAC-CHANNEL
    BREAKDOWN FOR QICK, QUBIC, AND RISC-Q FOR FAIR COMPARISON


    |            | LUT    | FF     | DACs | LUT/DAC | FF/DAC |

    |------------|--------|--------|------|---------|--------|

    | QICK [6]   | 106935 | 175769 | 7    | 15276   | 25110  |

    | QubiC [24] | 60996  | 119590 | 16   | 3812    | 7474   |

    | RISC-Q     | 74551  | 172925 | 16   | 4659    | 10808  |


    ## *B. Implementation Co-Design of Conditional Gates*


    Conditional gates are a common construct in dynamic quantum circuits, where gate
    execution depends on the result of prior mid-circuit measurements. A simple example
    is fast reset, where a qubit is reset to |0⟩ by measuring it and conditionally
    applying an X gate if the outcome is 1. With RISC-Q,


    Branch-based implementations, used in systems like QICK and QubiC, can introduce
    latency on a pipelined processor due to pipeline stalls. While branch prediction
    is effective in classical computing, it is significantly less effective in quantum
    control due to the inherent randomness of measurement results.


    To address this, RISC-Q also supports a branchless alternative that reduces latency
    by avoiding control flow changes. This is achieved by preloading both candidate
    gates into two parallel sets of timed FIFOs that feed the RF signal generator.
    A control register then selects the active path based on the measurement result,
    which is written via MMIO:


    ```

    i n t r e s u l t = *( v o l a t i l e i n t 3 2 _ t *)RD_RES_ADDR ( 7 ) ;

    *( v o l a t i l e i n t 3 2 _ t *)MULTIPLEX_REG_ADDR( 7 ) = r e s u l t ;

    ```

    This approach avoids branching entirely, reducing latency at the potential cost
    of throughput due to the increased complexity of preloading.


    RISC-Q''s modular architecture and flexible software interface allow developers
    to easily switch between branch-based and branchless implementations depending
    on the experimental requirements for latency and throughput.


    ## *C. On-chip Calibration*


    Quantum systems are extremely sensitive to both environmental and internal fluctuations,
    requiring frequent recalibration of control parameters such as RF signal frequency
    and amplitude. Recent work [\[38\]](#page-9-9), [\[39\]](#page-9-10) has demonstrated
    that lightweight closed-loop protocols can effectively maintain calibration during
    extended experiments. Although computationally efficient, these protocols often
    rely on host-controller communication for calibration, which can become a bottleneck
    in fast feedback loops.


    On-chip calibration addresses this issue by eliminating hostcontroller data transfer,
    enabling faster and more responsive correction. However, implementing calibration
    protocols directly on existing control systems is challenging, as they require
    programming in custom low-level assembly languages.


    To demonstrate the programmability of RISC-Q, we implement the amplitude calibration
    protocol from [\[39\]](#page-9-10) directly on the controller using the C programming
    language. The routine is implemented as a C function and can be easily integrated
    into RF signal processing programs, allowing online calibration to run between
    shots in high-repetition experiments. This reduces manual intervention and helps
    maintain high gate fidelity throughout the experimental process.


    ## VI. CONCLUSION


    We present RISC-Q, an open-source and flexible generator for QCSoCs with RISC-V
    compatibility. RISC-Q enables the reproduction of a fully functional QCSoC for
    superconducting qubit systems with significantly reduced development effort, while
    matching the state-of-the-art performance. Its support for fine-grained parameterization
    and flexible peripheral integration facilitates efficient design space exploration
    and rapid prototyping. We believe RISC-Q could significantly enhance productivity
    in the development of quantum control, microarchitecture, and low-level operating
    systems, which are crucial infrastructures for enabling practical, scalable, and
    highperformance quantum computing.


    RISC-Q has the potential to benefit the entire quantum computing community—whether
    you''re an experimental physicist, an application/protocol designer, a quantum
    control developer, or a quantum system researcher. By providing a unified and
    extensible platform, we envision RISC-Q fostering collaborative efforts and driving
    shared innovation in quantum (control) system design and implementation.


    ## ACKNOWLEDGMENT


    We are deeply grateful to David Schuster for introducing open-source quantum control
    systems, which served as the inspiration for this entire project. We sincerely
    thank the QubiC team—especially Gang Huang and Yilun Xu—for their invaluable assistance
    in testing the RISC-Q generated prototype, Will Oliver''s group for generously
    sharing their control hardware, and the QICK team for their help in understanding
    their codebase. We are also grateful for the insightful discussions with Hanrui
    Wang, Margaret Martonosi, Fred Chong, Jens Palsberg, Swamit Tannu, Adam Chlipala,
    Mark Horowitz, Priyanka Raina, Jason Cong, and Lin Zhong throughout the various
    stages of RISC-Q''s development, which helped shape its current form.


    This project is partially supported by Air Force Office of Scientific Research
    under award number FA9550-21-1-0209, the U.S. National Science Foundation grant
    CCF-1942837 (CAREER), CCF-2330974, NQVL-2435244, and a Sloan Research Fellowship.


    ## REFERENCES


    - <span id="page-8-0"></span>[1] B. M. Terhal, "Quantum error correction for quantum
    memories," *Rev. Mod. Phys.*, vol. 87, pp. 307–346, Apr 2015. [Online]. Available:
    <https://link.aps.org/doi/10.1103/RevModPhys.87.307>

    - <span id="page-8-1"></span>[2] A. Vepsäläinen, R. Winik, A. H. Karamlou, J.
    Braumüller, A. D. Paolo, Y. Sung, B. Kannan, M. Kjaergaard, D. K. Kim, A. J. Melville,
    B. M. Niedzielski, J. L. Yoder, S. Gustavsson, and W. D. Oliver, "Improving qubit
    coherence using closed-loop feedback," *Nature Communications*, vol. 13, no. 1,
    p. 1932, 2022. [Online]. Available: <https://doi.org/10.1038/s41467-022-29287-4>

    - <span id="page-8-2"></span>[3] A. Carrera Vazquez, C. Tornow, D. Ristè, S. Woerner,
    M. Takita, and D. J. Egger, "Combining quantum processors with real-time classical
    communication," *Nature*, vol. 636, no. 8041, pp. 75–79, 2024. [Online]. Available:<https://doi.org/10.1038/s41586-024-08178-2>

    - <span id="page-8-3"></span>[4] E. Bäumer, V. Tripathi, D. S. Wang, P. Rall,
    E. H. Chen, S. Majumder, A. Seif, and Z. K. Minev, "Efficient long-range entanglement
    using dynamic circuits," *PRX Quantum*, vol. 5, p. 030339, Aug 2024. [Online].
    Available:<https://link.aps.org/doi/10.1103/PRXQuantum.5.030339>

    - <span id="page-8-4"></span>[5] Y. Xu, G. Huang, N. Fruitwala, A. Rajagopala,
    R. K. Naik, K. Nowrouzi, D. I. Santiago, and I. Siddiqi, "QubiC 2.0: An extensible
    open-source qubit control system capable of mid-circuit measurement and feedforward,"
    *arXiv preprint arXiv:2309.10333*, 2023.

    - <span id="page-8-5"></span>[6] L. Stefanazzi, K. Treptow, N. Wilcer, C. Stoughton,
    C. Bradford, S. Uemura, S. Zorzetti, S. Montella, G. Cancelo, S. Sussman, A. Houck,
    S. Saxena, H. Arnaldi, A. Agrawal, H. Zhang, C. Ding, and D. I. Schuster, "The
    QICK (Quantum Instrumentation Control Kit): Readout and control for qubits and
    detectors," *Review of Scientific Instruments*, 2022.

    - <span id="page-8-6"></span>[7] G. Kasprowicz, P. Kulik, M. Gaska, T. Przywozki,
    K. Pozniak, J. Jarosinski, J. W. Britton, T. Harty, C. Balance, W. Zhang *et al.*,
    "ARTIQ and Sinara: Open software and hardware stacks for quantum physics," in
    *Quantum 2.0*. Optica Publishing Group, 2020, pp. QTu8B–14.

    - <span id="page-8-7"></span>[8] P. Kulik, M. Sowinski, G. Kasprowicz, D. Allcock,
    C. Ballance, S. Bour- ´ deauducq, J. Britton, M. G ˛aska, T. Harty, J. Jarosinski
    ´ *et al.*, "Latest developments in the Sinara open hardware ecosystem," in *2022
    IEEE International Conference on Quantum Computing and Engineering (QCE)*. IEEE,
    2022, pp. 799–802.

    - <span id="page-8-8"></span>[9] X. Fu, L. Riesebos, M. Rol, J. Van Straten, J.
    Van Someren, N. Khammassi, I. Ashraf, R. Vermeulen, V. Newsum, K. Loh *et al.*,
    "eQASM: An executable quantum instruction set architecture," in *2019 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*. IEEE, 2019, pp. 224–237.

    - <span id="page-8-9"></span>[10] R. Gebauer, N. Karcher, M. Güler, and O. Sander,
    "QiCells: A modular rfsoc-based approach to interface superconducting quantum
    bits," *ACM transactions on reconfigurable technology and systems*, vol. 16, no.
    2, pp. 1–23, 2023.

    - <span id="page-8-10"></span>[11] X. Guo, K. Qin, and M. Schulz, "HiSEP-Q: A
    highly scalable and efficient quantum control processor for superconducting qubits,"
    in *2023 IEEE 41st International Conference on Computer Design (ICCD)*. IEEE,
    2023, pp. 86–93.

    - <span id="page-8-11"></span>[12] H. Genc, S. Kim, A. Amid, A. Haj-Ali, V. Iyer,
    P. Prakash, J. Zhao, D. Grubb, H. Liew, H. Mao *et al.*, "Gemmini: Enabling systematic
    deeplearning architecture evaluation via full-stack integration," in *2021 58th
    ACM/IEEE Design Automation Conference (DAC)*. IEEE, 2021, pp. 769–774.

    - <span id="page-8-12"></span>[13] K. Koul, J. Melchert, K. Sreedhar, L. Truong,
    G. Nyengele, K. Zhang, Q. Liu, J. Setter, P.-H. Chen, Y. Mei, M. Strange, R. Daly,
    C. Donovick, A. Carsello, T. Kong, K. Feng, D. Huff, A. Nayak, R. Setaluri, J.
    Thomas, N. Bhagdikar, D. Durst, Z. Myers, N. Tsiskaridze, S. Richardson, R. Bahr,
    K. Fatahalian, P. Hanrahan, C. Barrett, M. Horowitz, C. Torng, F. Kjolstad, and
    P. Raina, "AHA: An agile approach to the design of coarse-grained reconfigurable
    accelerators and compilers," *ACM Trans. Embed. Comput. Syst.*, vol. 22, no. 2,
    Jan. 2023.

    - <span id="page-8-13"></span>[14] "SpinalHDL," [https://github.com/SpinalHDL/SpinalHDL,](https://github.com/SpinalHDL/SpinalHDL)
    2025, [Accessed April-20-2025].

    - <span id="page-8-14"></span>[15] P. Krantz, M. Kjaergaard, F. Yan, T. P. Orlando,
    S. Gustavsson, and W. D. Oliver, "A quantum engineer''s guide to superconducting
    qubits," *Applied physics reviews*, vol. 6, no. 2, 2019.

    - <span id="page-8-15"></span>[16] G. Q. AI *et al.*, "Quantum error correction
    below the surface code threshold," *Nature*, vol. 638, no. 8052, p. 920, 2024.

    - <span id="page-8-16"></span>[17] L. Caune, L. Skoric, N. S. Blunt, A. Ruban,
    J. McDaniel, J. A. Valery, A. D. Patterson, A. V. Gramolin, J. Majaniemi, K. M.
    Barnes *et al.*, "Demonstrating real-time and low-latency quantum error correction
    with superconducting qubits," *arXiv preprint arXiv:2410.05202*, 2024.

    - <span id="page-8-17"></span>[18] A. W. Young, W. J. Eckner, W. R. Milner, D.
    Kedar, M. A. Norcia, E. Oelker, N. Schine, J. Ye, and A. M. Kaufman, "Half-minute-scale
    atomic coherence and high relative stability in a tweezer clock," *Nature*, vol.
    588, no. 7838, pp. 408–413, 2020.

    - <span id="page-8-18"></span>[19] S. A. Khan, R. Kaufman, B. Mesits, M. Hatridge,
    and H. E. Türeci, "Practical trainable temporal postprocessor for multistate quantum
    measurement," *PRX Quantum*, vol. 5, no. 2, p. 020364, 2024.

    - <span id="page-8-19"></span>[20] N. R. Vora, Y. Xu, A. Hasim, N. Fruitwala,
    N. Nguyen, H. Liao, J. Balewski, A. Rajagopala, K. Nowrouzi, Q. Ji *et al.*, "QubiCML:
    MLpowered real-time quantum state discrimination enabling mid-circuit measurements,"
    in *2024 IEEE International Conference on Quantum Computing and Engineering (QCE)*,
    vol. 2. IEEE, 2024, pp. 414–415.

    - <span id="page-8-20"></span>[21] Y. Wu, N. Liyanage, and L. Zhong, "Micro Blossom:
    Accelerated minimum-weight perfect matching decoding for quantum error correction,"
    *arXiv preprint arXiv:2502.14787*, 2025.

    - <span id="page-8-21"></span>[22] S. Wang, W. Zhang, T. Zhang, S. Mei, Y. Wang,
    J. Hu, and W. Chen, "Accelerating the assembly of defect-free atomic arrays with
    maximum parallelisms," *Physical Review Applied*, vol. 19, no. 5, p. 054032, 2023.

    - <span id="page-8-22"></span>[23] X. Guo, J. Winklmann, D. Stober, A. Elsharkawy,
    and M. Schulz, "Design of an FPGA-based neutral atom rearrangement accelerator
    for quantum computing," *arXiv preprint arXiv:2411.12401*, 2024.

    - <span id="page-8-23"></span>[24] N. Fruitwala, G. Huang, Y. Xu, A. Rajagopala,
    A. Hashim, R. K. Naik, K. Nowrouzi, D. I. Santiago, and I. Siddiqi, "Distributed
    architecture for FPGA-based superconducting qubit control," *arXiv preprint arXiv:2404.15260*,
    2024.

    - <span id="page-8-24"></span>[25] N. Liyanage, Y. Wu, A. Deters, and L. Zhong,
    "Scalable quantum error correction for surface codes using FPGA," in *2023 IEEE
    International Conference on Quantum Computing and Engineering (QCE)*, vol. 1.
    IEEE, 2023, pp. 916–927.

    - <span id="page-8-25"></span>[26] Y. Wu and L. Zhong, "Fusion Blossom: Fast MWPM
    decoders for QEC," in *2023 IEEE International Conference on Quantum Computing
    and Engineering (QCE)*, vol. 1. IEEE, 2023, pp. 928–938.

    - <span id="page-8-26"></span>[27] C. T. Chubb, "General tensor network decoding
    of 2D Pauli codes," *arXiv preprint arXiv:2101.04125*, 2021.

    - <span id="page-8-27"></span>[28] N. P. Breuckmann and X. Ni, "Scalable neural
    network decoders for higher dimensional quantum codes," *Quantum*, vol. 2, p.
    68, 2018.

    - <span id="page-9-1"></span><span id="page-9-0"></span>[30] H. Wang, P. Liu,
    K. Shao, D. Li, J. Gu, D. Z. Pan, Y. Ding, and S. Han, "Transformer-QEC: Quantum
    error correction code decoding with transferable transformers," *arXiv preprint
    arXiv:2311.16082*, 2023.

    - <span id="page-9-2"></span>[31] A. Cross, A. Javadi-Abhari, T. Alexander, N.
    De Beaudrap, L. S. Bishop, S. Heidel, C. A. Ryan, P. Sivarajah, J. Smolin, J.
    M. Gambetta *et al.*, "OpenQASM 3: A broader and deeper quantum assembly language,"
    *ACM Transactions on Quantum Computing*, vol. 3, no. 3, pp. 1–50, 2022.

    - <span id="page-9-3"></span>[32] "VexiiRiscv," [https://github.com/SpinalHDL/VexiiRiscv,](https://github.com/SpinalHDL/VexiiRiscv)
    2025, [Accessed April-20-2025].

    - <span id="page-9-4"></span>[33] "PYNQ," [https://www.pynq.io/,](https://www.pynq.io/)
    2025, [Accessed April-20-2025].

    - <span id="page-9-5"></span>[34] H. M. Cook, *Productive design of extensible
    on-chip memory hierarchies*. University of California, Berkeley, 2016.

    - <span id="page-9-6"></span>[35] H. Cook, W. Terpstra, and Y. Lee, "Diplomatic
    design patterns: A


    TileLink case study," in *1st Workshop on Computer Architecture Research with
    RISC-V*, vol. 23, 2017.


    - <span id="page-9-7"></span>[36] "VexRiscv," [https://github.com/SpinalHDL/VexRiscv,](https://github.com/SpinalHDL/VexRiscv)
    2025, [Accessed April-20-2025].

    - <span id="page-9-8"></span>[37] A. Butko, G. Michelogiannakis, S. Williams,
    C. Iancu, D. Donofrio, J. Shalf, J. Carter, and I. Siddiqi, "Understanding quantum
    control processor capabilities and limitations through circuit characterization,"
    in *2020 International Conference on Rebooting Computing (ICRC)*. IEEE, 2020,
    pp. 66–75.

    - <span id="page-9-9"></span>[38] A. Vepsäläinen, R. Winik, A. H. Karamlou, J.
    Braumüller, A. D. Paolo, Y. Sung, B. Kannan, M. Kjaergaard, D. K. Kim, A. J. Melville
    *et al.*, "Improving qubit coherence using closed-loop feedback," *Nature Communications*,
    vol. 13, no. 1, p. 1932, 2022.

    - <span id="page-9-10"></span>[39] W. Gilbert, T. Tanttu, W. H. Lim, M. Feng,
    J. Y. Huang, J. D. Cifuentes, S. Serrano, P. Y. Mai, R. C. Leon, C. C. Escott
    *et al.*, "On-demand electrical control of spin qubits," *Nature Nanotechnology*,
    vol. 18, no. 2, pp. 131–136, 2023.'
- title: "Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM\n  Virtual\
    \ Platforms"
  abstract: 'In today''s technology-driven world, early-stage software development
    and

    testing are crucial. Virtual Platforms (VPs) have become indispensable tools

    for this purpose as they serve as a platform to execute and debug the

    unmodified target software at an early design stage. With the increasing

    complexity of software, especially in areas like Artificial Intelligence (AI)

    applications, VPs need to provide high simulation speed to ensure the target

    software executes within a reasonable time. Hybrid simulation, which combines

    virtual models with real hardware, can improve the performance of VPs. This

    paper introduces a novel approach for integrating real Peripheral Component

    Interconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI

    devices enable high performance, easy integration, and allow introspection for

    analysis and optimization. To illustrate the practical application of our

    approach, we present a case study where we integrate Google Coral''s Edge Tensor

    Processing Unit (TPU) into an ARM-based VP. The integration allows efficient

    execution of AI workloads, accelerating simulation speeds by up to 480x while

    eliminating the need for complex virtual device models. Beyond accelerating

    AI-workload execution, our framework enables driver development, regression

    testing across architectures, and device communication analysis. Our findings

    demonstrate that embedding PCI devices into SystemC simulations significantly

    enhances'
  url: http://arxiv.org/abs/2505.15590v1
  keywords: ''
  document: '# Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM
    Virtual Platforms


    Nils Bosbach<sup>1</sup> [,](https://orcid.org/0000-0002-2284-949X) Rebecca Pelke<sup>1</sup>
    [,](https://orcid.org/0000-0001-5156-7072) Niko Zurstraßen<sup>1</sup> [,](https://orcid.org/0000-0003-3434-2271)
    Jan Henrik Weinstock<sup>2</sup> [,](https://orcid.org/0009-0008-0902-7652) Lukas
    Jünger<sup>2</sup> [,](https://orcid.org/0000-0001-9149-1690) and Rainer Leupers[1](https://orcid.org/0000-0002-6735-3033)


    > <sup>1</sup> RWTH Aachen University, Germany <sup>2</sup> MachineWare GmbH,
    Germany


    Abstract. In today''s technology-driven world, early-stage software development
    and testing are crucial. Virtual Platforms (VPs) have become indispensable tools
    for this purpose as they serve as a platform to execute and debug the unmodified
    target software at an early design stage. With the increasing complexity of software,
    especially in areas like Artificial Intelligence (AI) applications, VPs need to
    provide high simulation speed to ensure the target software executes within a
    reasonable time. Hybrid simulation, which combines virtual models with real hardware,
    can improve the performance of VPs.


    This paper introduces a novel approach for integrating real Peripheral Component
    Interconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI devices
    enable high performance, easy integration, and allow introspection for analysis
    and optimization.


    To illustrate the practical application of our approach, we present a case study
    where we integrate Google Coral''s Edge Tensor Processing Unit (TPU) into an ARM-based
    VP. The integration allows efficient execution of AI workloads, accelerating simulation
    speeds by up to 480 × while eliminating the need for complex virtual device models.
    Beyond accelerating AI-workload execution, our framework enables driver development,
    regression testing across architectures, and device communication analysis. Our
    findings demonstrate that embedding PCI devices into SystemC simulations significantly
    enhances their capabilities, paving the way for more effective virtual prototyping.


    Keywords: SystemC · TLM · VFIO · TPU · Virtual Platform


    # 1 Introduction


    In an era where the complexity of both systems and Software (SW) continues to
    grow exponentially, the need for early-stage software development and testing
    is paramount. Virtual Platforms (VPs) are indispensable tools for architecture
    exploration, Hardware (HW)/SW co-design, and design verification and testing [\[32\]](#page-13-0).
    However, their effectiveness heavily depends on their simulation speed.


    One factor that contributes to the complexity of VPs is the incorporation of compute-intensive
    machine-learning applications in the target SW. Predictions forecast that the
    global embedded Artificial Intelligence (AI) market size will


    <span id="page-1-1"></span>![](_page_1_Figure_1.jpeg)


    Fig. 1: Application scenarios of the PCI(e) integration.


    grow from USD 8.79 billion in 2023 to USD 21.93 billion in 2030, which highlights
    the relevance of this type of workload [\[12\]](#page-12-0). A solution that enables
    fast execution of AI workloads in VPs is hybrid simulation. Thereby, virtual models
    and real HW devices are combined. Since machine learning accelerators are often
    external devices that communicate with a host machine via interfaces like Peripheral
    Component Interconnect (PCI), they are well-suited for hybrid simulation.


    In this paper, we present a way of integrating real PCI(e)[3](#page-1-0) devices
    into a SystemC-Transaction-Level-Modeling (TLM)-based VP. SystemC [\[16\]](#page-13-1)
    is the industry standard for system-level simulation. It is extended by TLM, which
    enables communication between models without simulating protocol details. Standardized
    interfaces allow model exchange and reuse even between vendors.


    In the past, previous simulators have already demonstrated the advantages of integrating
    PCI(e) devices. A popular example is QEMU [\[4\]](#page-12-1). QEMU is a versatile
    simulator that is capable of executing SW compiled for various architectures on
    several host architectures. It has a PCI(e) pass-through feature that allows to
    pass PCI(e) devices to the simulator. However, the drawback of QEMU compared to
    a SystemC-TLM-based simulation is the absence of standardized interfaces. This
    lack of standardization makes it challenging to adapt simulations for specialized
    use cases, integrate additional models, or reuse existing ones. Furthermore, QEMU
    is neither timed nor deterministic, which are essential properties for reliable
    analysis.


    [Figure 1](#page-1-1) shows a general overview of a virtual PCI (vPCI) device
    and its integration into a VP. Our approach can be used in the following scenarios:


    - 1 Hybrid Simulation: Our VP integration can be used to embed a real device into
    a virtual system. It eliminates the need to develop a virtual model of the PCI(e)
    device, which speeds up the development process. The target SW can access both,
    virtual models and the HW PCI(e) device.

    - 2 Driver Development: Device drivers for different CPU architectures can be
    developed on one machine. Additionally, a single machine can run multiple VPs
    to simulate different architectures. This simplifies regression testing since
    the PCI(e) card does not need to be connected to different machines.

    - 3 Device-Communication Analysis: The accesses from the virtual CPU (vCPU) to
    the PCI(e) device can be traced and analyzed. Furthermore, the interrupt behavior
    of the PCI(e) device can be evaluated.


    In a case study in [Section 5,](#page-8-0) we showcase how our approach can integrate
    Google Coral''s Edge Tensor Processing Unit (TPU) into an ARM-based VP.


    <span id="page-1-0"></span><sup>3</sup> The acronym PCI(e) is used in the following
    to refer to both PCI and PCIe devices


    # 2 Background


    This chapter explains the background information that is needed to follow our
    approach. A short overview of the development of PCI(e) and its working principles
    is given in [Section 2.1.](#page-2-0) In [Section 2.2,](#page-2-1) Linux''s Virtual
    Function I/O (VFIO) driver is presented. The SystemC-based Virtual Components
    Modeling Library (VCML) is presented in [Section 2.3,](#page-3-0) which is our
    used simulation environment.


    ## <span id="page-2-0"></span>2.1 Peripheral Component Interconnect (PCI)


    The PCI bus specification 1.0 was published by Intel in 1992 as a successor of
    the Industry Standard Architecture bus. Over the years, multiple revisions have
    been published to meet the demands of faster and more complex HW.


    Nowadays, PCI Express (PCIe) is the standard for extension cards that can be connected
    to a motherboard, like graphics cards, network cards, or storage devices, such
    as Solid State Drives (SSDs). Additionally, PCI is used to connect on-chip devices.


    To connect a PCI(e) card to the system bus, a PCI host bridge (PCI host) is required.
    A card can contain multiple endpoints, so-called functions, which can be accessed
    by individual drivers. Each function has a configuration memory space with a standardized
    layout, which can be accessed by the CPU.


    The configuration space contains six Base Address Registers (BARs) (BAR0- BAR5).
    Those registers contain information on additional memory regions the device has.
    During bus enumeration, the Operating System (OS) needs to program those registers
    to map the available memory region to either the memory or Input/Output (I/O)
    space. [\[2\]](#page-12-2)


    Most PCI(e) devices have a Direct Memory Access (DMA) engine to directly access
    the host memory without CPU interaction. In a typical procedure, the CPU places
    data in memory that can be directly accessed by the PCI(e) card. The CPU then
    configures the card via the mapped memory areas.


    To notify the CPU, interrupts are available. PCI supports four interrupt lines
    that are shared between all devices [\[26\]](#page-13-2). Interrupt sharing can
    lead to reduced performance because the OS needs to call all handlers associated
    with an interrupt every time an interrupt is signaled [\[25\]](#page-13-3). Since
    PCI 2.3, Message Signalled Interrupts (MSIs) can be used if supported by the device
    [\[27\]](#page-13-4). Using MSI, the OS programs an address to which the device
    can write to trigger an interrupt. In PCI 3.0, MSI-X was added which allows more
    interrupts per device and independent configuration. For PCIe devices, the support
    of either MSI or MSI-X is mandatory.


    #### <span id="page-2-1"></span>2.2 Virtual Function I/O (VFIO)


    VFIO is a Linux user-level driver framework originally developed by Cisco in 2010
    [\[33\]](#page-13-5). It has been part of the Linux kernel since version 3.6.0,
    which was released in 2012 [\[22\]](#page-13-6). It is a device driver to get
    raw access to PCI(e) devices from a user-space process. Accesses to the configuration
    space are virtualized.


    <span id="page-3-1"></span>![](_page_3_Figure_1.jpeg)


    Fig. 2: IOMMU working principle.


    The Memory-Mapped Input/Output (MMIO) regions of the PCI(e) device can directly
    be mapped into the address space of a process.


    To reroute DMA accesses of the device to a memory region of the user-space process,
    an Input-Output Memory Management Unit (IOMMU) of the host system is needed. The
    IOMMU translates addresses on DMA accesses as visualized in [Figure 2.](#page-3-1)
    Similar to a Memory Management Unit (MMU) that translates Virtual Addresses (VAs)
    into Physical Addresses (PAs) on CPU memory accesses, an IOMMU can translate IO
    Virtual Addresses (IOVAs) into PAs on DMA accesses of the PCI(e) device. During
    normal operation of a PCI(e) device, the IOMMU is usually disabled and the device
    directly uses physical addresses. VFIO requires an activated IOMMU. When the VFIO
    driver is activated for a device, a user-space process can use a VFIO call to
    map a memory region from its virtual address space into the I/O virtual address
    space of the PCI(e) device A . VFIO translates the VAs of the memory region into
    PAs using the process''s page tables. The IOMMU is then configured to perform
    the same mapping as the MMU for the memory region B . Once the mapping has been
    set up, the user-space process can pass VAs to the PCI(e) device C . The device
    will be able to access the regions using its DMA because the IOMMU translates
    the IOVAs into the corresponding PAs. Different IOMMU implementations exist such
    as AMD-Vi [\[1\]](#page-12-3) from AMD, Virtualization Technology for Directed
    I/O (VT-d) [\[17\]](#page-13-7) from Intel, or ARM''s System Memory Management
    Unit (SMMU) [\[3\]](#page-12-4).


    VFIO is the ideal driver to connect a PCI(e) device to a Virtual Machine (VM)
    or VP. Since it is a device-independent driver that allows raw access, it can
    be used to expose a PCI(e) to a simulator or virtual environment and leave the
    device-specific configuration to the virtual system.


    #### <span id="page-3-0"></span>2.3 Virtual Components Modeling Library (VCML)


    VCML [\[24\]](#page-13-8) is an open-source modeling library that is built on
    top of SystemC. It adds commonly used building blocks like specialized TLM sockets,
    I/O peripherals, and registers. Moreover, the library contains ready-to-use models
    of devices such as interrupt controllers or Ethernet devices.


    A feature that is used in this work is the protocol implementation. SystemC''s
    TLM extensions allow us to model communication between HW blocks abstractly [\[16\]](#page-13-1).
    Communication always takes place between an initiator socket and a target socket.
    The sockets are objects that can be placed in modules. An initiator socket can
    only communicate with the target socket to which it was bound during the simulation
    setup. The communication itself is called a transaction. A transaction is always
    started by the initiator and processed by the corresponding target. TLM supports
    two different abstraction levels to model communication, blocking and non-blocking
    transactions. They differ in the number of phases that are simulated to send and
    process a transaction. In VCML, the more abstract TLM blocking transport is used
    because the library targets instruction-accurate modeling where the details of
    TLM''s non-blocking transactions, which are used for cycle-accurate modeling,
    are not needed.


    Based on SystemC''s generic TLM sockets, VCML provides implementations for commonly
    used communication protocols, such as Universal Asynchronous Receiver/Transmitter
    (UART), Serial Peripheral Interface (SPI), Controller Area Network (CAN), and
    Ethernet. The protocols provide TLM sockets and TLM payload implementations. Particularly
    relevant for this work is the PCI protocol, which can be used to connect a virtual
    PCI device to a virtual PCI host controller. Based on TLM, it provides PCI initiator
    and target sockets. The PCI device has a PCI target socket that is connected to
    a PCI initiator socket of the PCI host controller. The controller can send PCI
    transactions to the target to access its memory regions. To signal an interrupt
    to the PCI host, the device uses the backward path of the PCI target socket.


    Multiple commercial VPs from MachineWare [\[7,](#page-12-5)[8,](#page-12-6)[9\]](#page-12-7),
    and open-source VPs [\[18\]](#page-13-9) exist that are based on VCML. In the
    case study in [Section 5,](#page-8-0) we use an opensource ARM-based VP to showcase
    our integration. However, as our model is based on SystemC, it can also be integrated
    into VPs that do not use VCML.


    # 3 Related Work


    The integration of PCI(e) devices into other environments like VMs or simulators
    is not a new concept. One of the most well-known simulators supporting VFIO is
    QEMU [\[4\]](#page-12-1), which introduced basic VFIO support in version 1.3 back
    in 2012 [\[5\]](#page-12-8). Since QEMU was primarily designed for the execution
    of software compiled for different architectures, it quickly meets its limits
    when it should be used as a VP. [Table 1](#page-4-0) outlines the key differences
    between our implementation and QEMU.


    Since QEMU is not based on SystemC, it is not as modular as SystemCbased simulations.
    In a SystemC simulation, models are self-contained blocks that communicate via
    standardized communication points, e.g., TLM sockets. The standardization enables
    the model exchange, even between different vendors. Furthermore, tools that rely
    on the use of SystemC''s interfaces can be used [\[6\]](#page-12-9).


    Another drawback of QEMU compared to SystemC is the lack of a virtual time concept.
    In the standard operation mode, QEMU uses the wall-clock time


    Table 1: Comparison between different simulators.


    <span id="page-4-0"></span>


    |           | VFIO | SystemC | Stand<br>ardized | Timed | Determi<br>nistic |
    Tracing |

    |-----------|------|---------|------------------|-------|-------------------|---------|

    | QEMU      |      |         |                  |       |                   |         |

    | This Work |      |         |                  |       |                   |         |


    <span id="page-5-0"></span>![](_page_5_Figure_1.jpeg)


    Fig. 3: Memory access implementation.


    as reference. This can lead to non-deterministic behavior. SystemC, on the other
    hand, has a concept of a virtual time that is kept track of by the SystemC kernel.
    In addition, SystemC simulations are deterministic, meaning that multiple runs
    of the same simulation with the same inputs will produce exactly the same output.
    This is very important for the reproducibility of bugs when VPs are used for testing.


    # 4 Approach


    In this work, we present a vPCI device that uses VFIO to integrate a physical
    PCI(e) card that is connected to the host into a VP. The integration is based
    on VCML [\[24\]](#page-13-8) to benefit from the TLM-2.0-based PCI protocol.


    In a case study in [Section 5,](#page-8-0) we will show the integration of the
    vPCI device into a VP in more detail. For now, we focus on the vPCI model itself.
    Three key features must be supported by the model:


    - 1. Memory Access: The vCPU model needs to be able to access the physical memory
    of the PCI(e) device.

    - 2. DMA: PCI(e) cards can directly access memory using DMA. When such access
    happens, the virtual memory model needs to be accessed.

    - 3. Interrupt Handling: PCI cards can send interrupts. When the physical card
    sends an interrupt, the interrupt needs to be injected into the VP.


    ## <span id="page-5-1"></span>4.1 Memory Access


    PCI(e) devices contain memory that the CPU can read from and write to. As described
    in [Section 2.1,](#page-2-0) a PCI(e) device at least contains a configuration
    memory, that contains device and vendor information. The memory access from the
    CPU to the PCI(e) device is depicted in [Figure 3.](#page-5-0) The memory regions
    of the device are mapped into the address space of the CPU. If the CPU has separate
    address spaces for MMIO and I/O (e.g., for x86 CPUs), a PCI(e) card can have memory
    regions in both address spaces. For this reason, the PCI host bridge has three
    different TLM target sockets to allow different address spaces for the configuration,
    MMIO, and I/O spaces. In [Figure 3,](#page-5-0) we assume a single address space.
    The CPU read and write transactions to the memory-mapped regions of the PCI device
    are received by the PCI host bridge. The bridge knows the mappings of the different
    devices and translates the TLM transaction into a PCI transaction using VCML''s
    PCI TLM protocol.


    <span id="page-6-0"></span>![](_page_6_Figure_1.jpeg)


    Fig. 4: DMA access configuration.


    To process the request, the vPCI device needs to access the HW device. For this
    access, two possibilities exist. If the address belongs to the configuration space,
    Linux''s pread or pwrite syscalls need to be used. The file descriptor that needs
    to be passed as a parameter is the one of the device''s VFIO group. For accesses
    to regions of the MMIO and I/O spaces, the memory region of the device can be
    mapped into the VP''s virtual address space. This allows access without further
    VFIO involvement. The configuration space cannot be directly mapped because some
    fields are virtualized and therefore need the involvement of VFIO.


    #### 4.2 Direct Memory Access (DMA)


    Most PCI(e) devices use DMA to directly access memory. The SW running on the CPU
    configures the PCI(e) device by communicating PAs that contain data to be processed.
    This is also done by the SW running on the vCPU. Since a VP is a user-space process
    running on a host machine, PAs from the target SW''s point of view are not real
    physical addresses of the host machine. When the target SW on the VP configures
    the PCI(e) device, an address translation is needed.


    The performed address translations are shown in [Figure 4a.](#page-6-0) When the
    SW running on the vCPU communicates addresses to the vPCI device, those addresses
    are within the physical address space of the VP. They are referring to the Random-Access
    Memory (RAM) model of the VP which is somewhere placed in the actual RAM of the
    host machine. The RAM model of the VP can translate a PA of the VP into a VA of
    the VP process on the host machine. This VA can be translated into a host PA by
    the host''s Linux kernel. The IOMMU of the host is configured to perform the same
    mapping. Once the mapping has been configured, the vCPU can communicate VP PAs
    to the PCI(e) device and the IOMMU maps them to the corresponding host PAs before
    a DMA access.


    [Figure 4b](#page-6-0) shows the configuration of host''s IOMMU in more detail.
    The user of the VP needs to specify the physical VP address range the PCI(e) should
    be able to access using DMA. This is usually the address range of the VP''s RAM.


    <span id="page-7-0"></span>8 N. Bosbach et al.


    ![](_page_7_Figure_1.jpeg)


    Fig. 5: Interrupt forwarding implementation.


    During the VP setup, the vPCI device requests a Direct Memory Interface (DMI)
    pointer (host VA) to the specified region using the DMI TLM socket of the connected
    PCI host bridge i . The vPCI device configures the host''s IOMMU by passing the
    received DMI pointer (host VA) and the corresponding VP''s PA range to a VFIO
    system call ii . After the IOMMU has been configured, the vCPU can communicate
    VP PAs to the PCI device and the IOMMU performs the mapping to host PAs iii .


    ## 4.3 Interrupt Handling


    When the physical PCI(e) card sends an interrupt, the interrupt is processed by
    the VFIO driver. The interrupt implementation is visualized in [Figure 5.](#page-7-0)
    [Figure 5a](#page-7-0) shows the forwarding of legacy interrupts. When the PCI(e)
    device sends a legacy interrupt, the vPCI device reads the Interrupt Request (IRQ)
    pin number (A, B, C, or D) from the configuration region. It then sends an IRQ
    containing the pin number to the PCI host using the backward path of the vPCI''s
    PCI TLM socket. The PCI host bridge signals the IRQ to the connected interrupt
    controller using VCML''s General Purpose Input/Output (GPIO) protocol.


    In case the interrupt is an MSI(-X), the vPCI device needs to perform a write
    operation to the corresponding MSI(-X) region (see [Figure 5b\)](#page-7-0). This
    region is usually part of the MSI(-X)-capable interrupt controller, such as the
    GICv2m extension. The destination address of the write transaction is configured
    by the OS running on the vCPU. For MSIs, the target address is written in a dedicated
    field in the configuration space of the device. This field is virtualized by VFIO
    meaning a read to this field using VFIO''s Application Programming Interface (API)
    does not return the value of the device. Thereby, different MSI addresses for
    the vPCI(e) and real PCI(e) devices are possible.


    In the case of a MSI-X, the destination addresses are written in a table that
    is placed in the MMIO region of the device by the OS. After the vPCI device has
    read the destination address of the MSI(-X), a transaction is sent through the
    DMA TLM socket of the PCI host bridge. The MSI(-X) capable interrupt controller
    then signals an IRQ.


    <span id="page-8-2"></span>![](_page_8_Figure_1.jpeg)


    Fig. 6: VP setup with connected VFIO device.


    # <span id="page-8-0"></span>5 Case Study


    To showcase the effectiveness and working principle of our vPCI model, we present
    the integration of Google Coral''s PCIe Edge TPU into the ARMv8 Virtual Platform
    (AVP64) [\[18,](#page-13-9)[19\]](#page-13-10). We compare the runtime performance
    of an AI-workload execution on the VP with and without TPU offloading. The workloads
    are based on TensorFlow Lite (TFLite), which is designed for embedded and edge
    devices [?]. [Table 2](#page-8-1) shows the used benchmarks and parameters.


    To connect the vPCI device to AVP64, we have to extend the VP by a PCI host bridge
    and a Generic Interrupt Controller (GIC)v2m, which adds MSI(-X) capabilities to
    the GIC 400. Both peripherals are taken from VCML [\[24\]](#page-13-8). The full
    VP setup is shown in [Figure 6.](#page-8-2) The vPCI device is connected to the
    PCI Host using VCML''s TLM-based PCI sockets. The PCI Host is connected to the
    system bus, which allows configuration from the CPU and access to the RAM. We
    use Buildroot [\[10\]](#page-12-10) to create a Linux image that contains the
    driver for the TPU, Python, and the required packages including TFLite.


    <span id="page-8-1"></span>


    |                                 | Neural Network            |           | Input
    Size Model Size Dataset |            |  |

    |---------------------------------|---------------------------|-----------|-------------------------------|------------|--|

    | Classification                  | EfficientNet L [30]       | 300×300×3 | 12.8
    MB                       |            |  |

    |                                 | EfficientNet M [30]       | 240×240×3 | 8.7
    MB                        | [28]       |  |

    |                                 | EfficientNet S [30]       | 224×224×3 | 6.8
    MB                        |            |  |

    |                                 | MobileNet V1 [15]         | 224×224×3 | 4.5
    MB                        | ILSVRC2012 |  |

    |                                 | MobileNet V2 [29]         | 224×224×3 | 4.1
    MB                        |            |  |

    |                                 | MobileNet V3 [14]         | 224×224×3 | 4.9
    MB                        |            |  |

    |                                 | ResNet-50 [13]            | 224×224×3 | 25.0
    MB                       |            |  |

    | Detection                       | EfficientDet-Lite 0 [31]  | 320×320×3 | 5.7
    MB                        |            |  |

    |                                 | EfficientDet-Lite 1 [31]  | 384×384×3 | 7.6
    MB                        |            |  |

    |                                 | EfficientDet-Lite 2 [31]  | 448×448×3 | 10.2
    MB                       | [21]       |  |

    |                                 | EfficientDet-Lite 3 [31]  | 512×512×3 | 14.4
    MB                       |            |  |

    |                                 | EfficientDet-Lite 3x [31] | 640×640×3 | 20.6
    MB                       | COCO       |  |

    |                                 | SSD MobileNet V2 [11]     | 300×300×3 | 6.7
    MB                        |            |  |

    | SSD/FPN MobileNet V1 [15,20,23] |                           | 640×640×3 | 7.0
    MB                        |            |  |


    Table 2: Benchmark parameters (batch size = 1).


    <span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)


    Fig. 7: Inference results.


    ## 5.1 Speedup Analysis


    To analyze the impact of offloading workload to the TPU, all benchmarks are executed
    in two configurations. At first, they are compiled for CPU-only execution. Then,
    they are compiled using the Edge TPU compiler to offload computation to the TPU.
    The executed workloads perform image classification and detection tasks using
    different Neural Networks (NNs).


    [Figures 7a](#page-9-0) and [7c](#page-9-0) show the wall-clock time needed to
    run the classification and detection NNs using the TPU from the VP with our hybrid
    approach. The shown values are median values of 100 consecutive runs. Due to the
    complexity of the tasks, the detection workloads take longer to execute than the
    classification ones. [Figures 7b](#page-9-0) and [7d](#page-9-0) present the speedups
    that result from the TPU offloading compared to CPU-only execution. The results
    are obtained by dividing the wallclock time needed for the CPU-only execution,
    tCP U , by the wall-clock time needed for inference with TPU offloading, tT P
    U . In general, speedups between approximately 10 × and 480 × are reached.


    <span id="page-10-0"></span>![](_page_10_Figure_0.jpeg)


    (b) Interrupt counts.


    Fig. 8: TPU execution statistics.


    #### 5.2 Communication Analysis


    A benefit of using VPs for SW development compared to real HW is the simplicity
    of tracing data collection. Since the VP offers insights into the models and their
    communication, complex debuggers or probes typically required for HW analysis
    are no longer needed. For SystemC-based simulation, external tools are available
    that make use of the standardized interfaces [\[6\]](#page-12-9).


    In this paper, we use the tracing capabilities of VCML to log TLM transactions
    from the vCPU to the vPCI device, and MSI(-X) sent from the vPCI device. In [Figure
    8a,](#page-10-0) the number of bytes that are exchanged between the vCPU and the
    vPCI device are shown. The TPU features a configuration and an MMIO region. Since
    the configuration region is mainly used during driver initialization, it is not
    accessed during workload execution. In general, accesses to the MMIO region are
    limited because it is only used for device configuration. Data such as NN models
    or inputs are accessed directly by the TPU using its DMA. These accesses cannot


    be traced because they are bypassing the VP (see [Section 4.1\)](#page-5-1). However,
    analyzing the vCPU-to-vPCI communication provides valuable insights, particularly
    for debugging or profiling.


    Further useful information can be obtained from the interrupt behavior. The TPU
    can send 13 different MSI-Xs. For the executed workloads, only two of them, IRQ0
    and IRQ4 (sc-host 0 IRQ), are used. IRQ 0 (instruction queue) is signaled when
    the instruction queue has been executed by the TPU. IRQ 4 (sc-host 0) is signaled
    once the execution has been completed. [Figure 8b](#page-10-0) shows the number
    of sent IRQs for the different workloads. For all workloads, the sc-host 0 IRQ
    has been signaled once at the end of the execution. Depending on the complexity
    of the workload, the instruction-queue IRQ has been signaled multiple times. Details
    about the IRQ behavior, especially in combination with other tracing data like
    read/write accesses can help developers during SW or driver development. They
    provide valuable insights and can be used for automated testing.


    # 6 Conclusion and Future Work


    In this paper, we present an approach to integrate PCI(e) devices into a SystemC-TLM-based
    VP. PCI(e) is today''s standard for extension cards that can be connected to a
    general-purpose PC. VPs allow SW development for Instruction-Set Architectures
    (ISAs) other than the host machine. This enables early SW development even when
    the HW is not available or still under design. Our approach enables forwarding
    the access to a PCI(e) device that is connected to the host machine to a SystemC-based
    VP running on the host. This comes with multiple benefits such as increased performance,
    the superfluity of the creation of a virtual model, and simple regression testing.


    The usage of SystemC TLM-2.0 as the basis of our model and Linux''s VFIO driver
    for accessing the PCI(e) devices guarantees portability to different VPs and PCI(e)
    devices. Tracing data reveals the vCPU-vPCI communication and the signaled IRQs.
    This information can help developers during driver or SW development.


    In a case study, we showed how our vPCI model can be used to embed Google Coral''s
    Edge TPU into an ARM-based VP. We presented how the model is integrated, accessed
    by the vCPU, and how it signals interrupts and uses the host''s IOMMU to get direct
    access to the VP''s RAM. Performance results demonstrate that our integration
    can significantly accelerate a VP by offloading AI tasks to the TPU. This enhancement
    improves SW-design productivity by accelerating simulation speeds by up to two
    orders of magnitude.


    While this paper lays the foundation for integrating PCI(e) devices into SystemC-based
    VPs, there are several avenues for future research and development. Due to the
    large number of available PCI(e) devices, our approach opens the doors for several
    scenarios. In addition to AI accelerators, other devices such as Ethernet or graphics
    cards can be used to replace virtual models. This reduces the modeling effort
    and increases the simulation speed.


    # References


    - <span id="page-12-3"></span>1. AMD Inc.: AMD I/O Virtualization Technology (IOMMU)
    Specification, 48882 (Oct 2023)

    - <span id="page-12-2"></span>2. Anderson, D., Shanley, T.: PCI system architecture.
    PC system architecture series, Addison-Wesley, Boston, Mass. Munich, 4. ed., 15.
    print edn. (1999)

    - <span id="page-12-4"></span>3. ARM: Arm System Memory Management Unit Architecture
    Specification

    - <span id="page-12-1"></span>4. Bellard, F.: QEMU, a fast and portable dynamic
    translator. In: USENIX annual technical conference, FREENIX Track. vol. 41, pp.
    10–5555. California, USA (2005), issue: 46

    - <span id="page-12-8"></span>5. Bellard, F.: ChangeLog/1.3 - QEMU (Dec 2012),
    [https://wiki.qemu.org/](https://wiki.qemu.org/ChangeLog/1.3) [ChangeLog/1.3](https://wiki.qemu.org/ChangeLog/1.3)

    - <span id="page-12-9"></span>6. Bosbach, N., Jünger, L., Joseph, J.M., Leupers,
    R.: NISTT: A Non-Intrusive SystemC-TLM 2.0 Tracing Tool. In: 2022 IFIP/IEEE 30th
    International Conference on Very Large Scale Integration (VLSI-SoC). pp. 1–6 (Oct
    2022). [https:](https://doi.org/10.1109/VLSI-SoC54400.2022.9939578) [//doi.org/10.1109/VLSI-SoC54400.2022.9939578,](https://doi.org/10.1109/VLSI-SoC54400.2022.9939578)
    iSSN: 2324-8440

    - <span id="page-12-5"></span>7. Bosbach, N., Pelke, R., Zurstraßen, N., Junger,
    L., Weinstock, J.H., Leupers, R.: Work-in-Progress: A Generic Non-Intrusive Parallelization
    Approach for SystemC TLM-2.0-based Virtual Platforms. In: Proceedings of the 2023
    International Conference on Hardware/Software Codesign and System Synthesis. pp.
    42–43. ACM, Hamburg Germany (Sep 2023). [https://doi.org/10.1145/3607888.3608596,](https://doi.org/10.1145/3607888.3608596)
    <https://doi.org/10.1145/3607888.3608596>

    - <span id="page-12-6"></span>8. Bosbach, N., Pelke, R., Zurstraßen, N., Weinstock,
    J.H., Jünger, L., Leupers, R.: High-Performance ARM-on-ARM Virtualization for
    Multicore SystemC-TLM-Based Virtual Platforms (May 2025). [https://doi.org/10.48550/arXiv.2505.12987,](https://doi.org/10.48550/arXiv.2505.12987)
    [http://arxiv.org/abs/2505.12987,](http://arxiv.org/abs/2505.12987) arXiv:2505.12987
    [cs]

    - <span id="page-12-7"></span>9. Bosbach, N., Zurstraßen, N., Pelke, R., Jünger,
    L., Weinstock, J.H., Leupers, R.: Towards High-Performance Virtual Platforms:
    A Parallelization Strategy for SystemC TLM-2.0 CPU Models. In: Proceedings of
    the 61st ACM/IEEE Design Automation Conference. pp. 1–6. DAC ''24, Association
    for Computing Machinery, New York, NY, USA (Nov 2024). [https://doi.org/10.1145/3649329.3658257,](https://doi.org/10.1145/3649329.3658257)
    <https://dl.acm.org/doi/10.1145/3649329.3658257>

    - <span id="page-12-10"></span>10. buildroot.org: buildroot.org / buildroot ·
    GitLab (Apr 2024), [https://gitlab.com/](https://gitlab.com/buildroot.org/buildroot)
    [buildroot.org/buildroot](https://gitlab.com/buildroot.org/buildroot)

    - <span id="page-12-13"></span>11. Chiu, Y.C., Tsai, C.Y., Ruan, M.D., Shen, G.Y.,
    Lee, T.T.: Mobilenet-SSDv2: An Improved Object Detection Model for Embedded Systems.
    In: 2020 International Conference on System Science and Engineering (ICSSE). pp.
    1–5 (Aug 2020). [https:](https://doi.org/10.1109/ICSSE50014.2020.9219319) [//doi.org/10.1109/ICSSE50014.2020.9219319,](https://doi.org/10.1109/ICSSE50014.2020.9219319)
    iSSN: 2325-0925

    - <span id="page-12-0"></span>12. Grand View Research: Embedded AI Market Size,
    Share & Trends Analysis Report By Offering (Hardware, Software), By Data Type
    (Sensor Data, Image And Video Data), By Vertical (BSFI, Retail), By Region, And
    Segment Forecasts, 2024 - 2030. Tech. Rep. GVR-4-68040-438-9, [https://www.grandviewresearch.](https://www.grandviewresearch.com/industry-analysis/embedded-ai-market-report)
    [com/industry-analysis/embedded-ai-market-report](https://www.grandviewresearch.com/industry-analysis/embedded-ai-market-report)

    - <span id="page-12-12"></span>13. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual
    Learning for Image Recognition. In: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR) (Jun 2016)

    - <span id="page-12-11"></span>14. Howard, A., Sandler, M., Chu, G., Chen, L.C.,
    Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q.V., Adam,
    H.: Searching for MobileNetV3. pp. 1314–1324 (2019), [https://openaccess.thecvf.com/content\\_ICCV\\_2019/html/](https://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html)
    [Howard\\_Searching\\_for\\_MobileNetV3\\_ICCV\\_2019\\_paper.html](https://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html)

    - 14 N. Bosbach et al.

    - <span id="page-13-13"></span>15. Howard, A.G., Zhu, M., Chen, B., Kalenichenko,
    D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: MobileNets: Efficient Convolutional
    Neural Networks for Mobile Vision Applications. CoRR (2017). [https://doi.org/10.48550/arXiv.1704.](https://doi.org/10.48550/arXiv.1704.04861)
    [04861,](https://doi.org/10.48550/arXiv.1704.04861) [http://arxiv.org/abs/1704.04861,](http://arxiv.org/abs/1704.04861)
    arXiv: 1704.04861

    - <span id="page-13-1"></span>16. IEEE Standards Association and others: IEEE
    Standard for Standard SystemC Language Reference Manual. IEEE Std 1666-2023 (Revision
    of IEEE Std 1666- 2011) (Sep 2023).<https://doi.org/10.1109/IEEESTD.2023.10246125>

    - <span id="page-13-7"></span>17. Intel: Intel Virtualization Technology for Directed
    I/O (4.0) (Jun 2022)

    - <span id="page-13-9"></span>18. Jünger, L.: An ARMv8 Virtual Platform (AVP64)
    (May 2023), [https://github.](https://github.com/aut0/avp64) [com/aut0/avp64,](https://github.com/aut0/avp64)
    original-date: 2020-04-09T15:34:12Z

    - <span id="page-13-10"></span>19. Jünger, L., Weinstock, J.H., Leupers, R., Ascheid,
    G.: Fast SystemC Processor Models with Unicorn. In: Proceedings of the Rapid Simulation
    and Performance Evaluation: Methods and Tools (2019).<https://doi.org/10.1145/3300189.3300191>

    - <span id="page-13-17"></span>20. Lin, T.Y., Dollar, P., Girshick, R., He, K.,
    Hariharan, B., Belongie, S.: Feature Pyramid Networks for Object Detection. pp.
    2117–2125 (2017), [https://openaccess.thecvf.com/content\\_cvpr\\_2017/html/Lin\\_Feature\\_](https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html)
    [Pyramid\\_Networks\\_CVPR\\_2017\\_paper.html](https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html)

    - <span id="page-13-16"></span>21. Lin, T.Y., Maire, M., Belongie, S., Hays, J.,
    Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft COCO: Common Objects
    in Context. In: Computer Vision – ECCV 2014. Springer International Publishing,
    Cham (2014). [https://doi.org/](https://doi.org/10.1007/978-3-319-10602-1_48)
    [10.1007/978-3-319-10602-1\\_48](https://doi.org/10.1007/978-3-319-10602-1_48)

    - <span id="page-13-6"></span>22. Linus Torvalds: Linux-Kernel Archive: Linux
    3.6 (Sep 2012), [https://lkml.iu.edu/](https://lkml.iu.edu/hypermail/linux/kernel/1209.3/04237.html)
    [hypermail/linux/kernel/1209.3/04237.html](https://lkml.iu.edu/hypermail/linux/kernel/1209.3/04237.html)

    - <span id="page-13-18"></span>23. Liu, W., Anguelov, D., Erhan, D., Szegedy,
    C., Reed, S., Fu, C.Y., Berg, A.C.: SSD: Single Shot MultiBox Detector. In: Leibe,
    B., Matas, J., Sebe, N., Welling, M. (eds.) Computer Vision – ECCV 2016. pp. 21–37.
    Springer International Publishing, Cham (2016). [https://doi.org/10.1007/978-3-319-46448-0\\_2](https://doi.org/10.1007/978-3-319-46448-0_2)

    - <span id="page-13-8"></span>24. MachineWare: machineware-gmbh/vcml (Mar 2024),
    [https://github.com/](https://github.com/machineware-gmbh/vcml) [machineware-gmbh/vcml,](https://github.com/machineware-gmbh/vcml)
    original-date: 2018-01-22T10:24:21Z

    - <span id="page-13-3"></span>25. Nguyen, T.L., Silbermann, M., Wilcox, M.: 4.
    The MSI Driver Guide HOWTO — The Linux Kernel documentation,<https://docs.kernel.org/PCI/msi-howto.html>

    - <span id="page-13-2"></span>26. PCI SIG: PCI Local Bus Specification Revision
    2.2 (Dec 1998)

    - <span id="page-13-4"></span>27. PCI SIG: PCI Express Base Specification Revision
    1.0a (Apr 2003)

    - <span id="page-13-12"></span>28. Russakovsky, O., Deng, J., Su, H., Krause,
    J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.,
    Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. Int
    J Comput Vis 115(3), 211–252 (Dec 2015). <https://doi.org/10.1007/s11263-015-0816-y>

    - <span id="page-13-14"></span>29. Sandler, M., Howard, A., Zhu, M., Zhmoginov,
    A., Chen, L.C.: MobileNetV2: Inverted Residuals and Linear Bottlenecks. pp. 4510–4520
    (2018), [https://openaccess.thecvf.com/content\\_cvpr\\_2018/html/Sandler\\_](https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html)
    [MobileNetV2\\_Inverted\\_Residuals\\_CVPR\\_2018\\_paper.html](https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html)

    - <span id="page-13-11"></span>30. Tan, M., Le, Q.: EfficientNet: Rethinking Model
    Scaling for Convolutional Neural Networks. In: Proceedings of the 36th International
    Conference on Machine Learning. Proceedings of Machine Learning Research, vol.
    97. PMLR (2019), <https://proceedings.mlr.press/v97/tan19a.html>

    - <span id="page-13-15"></span>31. Tan, M., Pang, R., Le, Q.V.: EfficientDet:
    Scalable and Efficient Object Detection (Jul 2020). [https://doi.org/10.48550/arXiv.1911.09070,](https://doi.org/10.48550/arXiv.1911.09070)
    arXiv:1911.09070 [cs, eess]

    - <span id="page-13-0"></span>32. Vinco, S., Bertacco, V., Chatterjee, D., Fummi,
    F.: SAGA: SystemC acceleration on GPU architectures. In: DAC (2012).<https://doi.org/10.1145/2228360.2228382>

    - <span id="page-13-5"></span>33. Williamson, A.: VFIO: A user''s perspective
    (Nov 2012), [https://docs.huihoo.com/](https://docs.huihoo.com/kvm/kvm-forum/2012/2012-forum-VFIO.pdf)
    [kvm/kvm-forum/2012/2012-forum-VFIO.pdf](https://docs.huihoo.com/kvm/kvm-forum/2012/2012-forum-VFIO.pdf)'
- title: "CRYPTONITE: Scalable Accelerator Design for Cryptographic Primitives and\n\
    \  Algorithms"
  abstract: "Cryptographic primitives, consisting of repetitive operations with different\n\
    inputs, are typically implemented using straight-line C code due to traditional\n\
    execution on CPUs. Computing these primitives is necessary for secure\ncommunication;\
    \ thus, dedicated hardware accelerators are required in resource\nand latency-constrained\
    \ environments. High-Level Synthesis (HLS) generates\nhardware from high-level\
    \ implementations in languages like C, enabling the\nrapid prototyping and evaluation\
    \ of designs, leading to its prominent use in\ndeveloping dedicated hardware accelerators.\
    \ However, directly synthesizing the\nstraight-line C implementations of cryptographic\
    \ primitives can lead to large\nhardware designs with excessive resource usage\
    \ or suboptimal performance.\n  We introduce Cryptonite, a tool that automatically\
    \ generates efficient,\nsynthesizable, and correct-by-design hardware accelerators\
    \ for cryptographic\nprimitives directly from straight-line C code. Cryptonite\
    \ first identifies\nhigh-level hardware constructs through verified rewriting,\
    \ emphasizing resource\nreuse. The second stage automatically explores latency-oriented\
    \ implementations\nof the compact design. This enables the flexible scaling of\
    \ a particular\naccelerator to meet the hardware requirements. We demonstrate\
    \ Cryptonite's\neffectiveness using implementations from the Fiat Cryptography\
    \ project, a\nlibrary of verified and auto-generated cryptographic primitives\
    \ for\nelliptic-curve cryptography. Our results show that Cryptonite achieves\
    \ scalable\ndesigns with up to 88.88\\% reduced resource usage and a 54.31\\%\
    \ improvement in\nlatency compared to naively synthesized designs."
  url: http://arxiv.org/abs/2505.14657v1
  keywords: High-level synthesis, Cryptography, Hardware acceleration, Cryptonite,
    & Fiat Cryptography Project
  document: '## I. INTRODUCTION


    Modern day computer security heavily relies on cryptography for authentication,
    encryption, and key exchange protocols. The implementation of cryptographic primitives,
    the foundational building blocks of cryptographic protocols, must be both correct
    and efficient to safeguard security and privacy. Improving the efficiency of computing
    such primitives is imperative, as it directly impacts the performance and scalability
    of security systems. Hardware acceleration has become a widely adopted methodology
    to meet this need, alleviating computational bottlenecks, enabling real-time processing,
    and reducing the risk of system vulnerabilities.


    Ongoing efforts have prioritized manually accelerating cryptographic algorithms
    such as Number Theoretic Transforms (NTT) [\[1\]](#page-7-0) and Polynomial Multiplications
    [\[2\]](#page-7-1), with an emphasis on creating efficient data-flow architectures
    [\[3\]](#page-7-2). Implementing such architectures is challenging, time-consuming,
    and requires expert knowledge. Therefore, there is a need to bridge the gap by
    automating the generation of such architectures.


    Cryptographic primitives, usually written in C [\[4\]](#page-7-3), contain repetitive
    computations to encrypt, decrypt, or sign using a particular scheme [\[5\]](#page-7-4)
    with different inputs, making them ideal candidates for hardware acceleration.
    Being written in C with no high-level constructs, they can easily be synthesized
    using High-Level Synthesis (HLS). These straight-line codes lack branch statements,
    loops, and array constructs, leading to low-latency execution with excessive resource
    usage. While resource constraints can be applied using pragmas such as #pragma
    HLS allocation, this usually results in complex control logic and bad timing [\[6\]](#page-7-5),
    [\[7\]](#page-7-6). Therefore, cryptographic primitives need to be rewritten for
    synthesis by HLS to generate high-performing, resource-efficient accelerators.


    Cryptographic primitives are often deployed in resourceconstrained environments
    [\[8\]](#page-7-7), [\[9\]](#page-7-8), making it essential for synthesized designs
    to be scalable. This scalability enables the exploration of trade-offs between
    resource usage and latency. Real-world applications demand parallel execution
    of multiple primitives and co-optimization across them. Tuning the performance
    of each primitive is essential to achieve overall resource efficiency and performance
    optimization.


    For a given C program, the Vitis HLS compiler emits a single design. This design
    changes with different code structures, even if they are functionally equivalent.
    Exploring the resource-latency trade-off of cryptographic accelerators directly
    generated by HLS is challenging, as the straight-line C programs have no explicitly
    parameterizable constructs. Despite this, repeated patterns in straight-line C-code
    can be rolled into loop constructs, enabling design space exploration (DSE). This
    work tackles this challenge with automated, verified source-code rewriting, enabling
    architectural DSE. Users then choose from the Pareto-optimal designs to best meet
    hardware requirements.


    We target straight-line C programs from the Fiat Cryptography project [\[10\]](#page-7-9),
    a library of verified and auto-generated cryptographic primitives for elliptic-curve
    cryptography. Using two implementation baselines, one with the original straight-line
    code from the Fiat Project and the other with loop constructs, we reduced resource
    utilization and comparable latency to the baselines.


    This work, Cryptonite, has three main contributions:


    - Verified source-to-source HLS transformations, generating array and loop constructs
    from straight-line code using equivalence graphs (e-graphs).

    - Automatic Design Space Exploration over looped code, resulting in a range of
    previously unattainable points with varying resource-latency profiles.

    - Exploring the co-optimization of multiple primitives with parallel execution
    for scalable accelerator design.


    The rest of the paper is organized as follows. [Section II](#page-0-0) frames
    and motivates this work. [Section III](#page-2-0) provides an overview of the
    Cryptonite<sup>1</sup> tool. [Section IV](#page-4-0) presents our experimental
    results, and [Section V](#page-6-0) summarizes and addresses related work. Finally,
    [Section VI](#page-7-10) concludes the paper.


    ## II. MOTIVATION


    <span id="page-0-0"></span>The advent of hardware accelerators has optimized computeintensive
    tasks such as Machine Learning (ML) algorithms by leveraging structured code with
    high data reuse, minimal dependencies, and predictable memory access patterns,
    enabling efficient parallelization based on workload demands. Cryptographic primitives,
    on the other hand, have different memory access patterns from ML code that are
    also more irregular. These are often straight-line programs that prioritize runtime
    efficiency rather than decipherable code structure. [Fig. 1](#page-1-0) illustrates
    an example of a straight-line program (❶) with a dependency structure. Note that


    <sup>1</sup>This tool has been open-sourced at [https://github.com/KarthikeyaSharma16/](https://github.com/KarthikeyaSharma16/Cryptonite)
    [Cryptonite](https://github.com/KarthikeyaSharma16/Cryptonite)


    <span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)


    \***The code transformations have been modified for clarity and simplicity.**


    Fig. 1: Cryptonite workflow.


    the displayed code is a simplified version of the original code. In this code,
    no clear patterns emerge, and redundant computations are repeated across different
    code segments. The non-trivial data dependencies due to complex arithmetic, irregular
    array access patterns, and the absence of loops result in an inefficient algorithm-to-hardware
    mapping. For instance, if a straight-line code contains 100 independent assignment
    operations involving at least one 128-bit multiplication, DSP resources are statically
    allocated for every multiplication. This results in poor resource reuse and limits
    scalability for larger cryptographic workloads. To quantify the inefficiencies
    on hardware utilization and scalability, we evaluate the performance of this workload
    using a metric called *Normalized Performance Index (NPI)*, which is defined in
    [Eq. 1:](#page-1-1)


    <span id="page-1-1"></span>

    $$NPI = w\_1 \frac{l - l\_{\rm min}}{l\_{\rm max} - l\_{\rm min}} + w\_2 \frac{r
    - r\_{\rm min}}{r\_{\rm max} - r\_{\rm min}} \tag{1}$$


    Where l is the latency of a design and r is the percentage of resources used.
    In this section, the NPI is computed with w<sup>1</sup> = w<sup>2</sup> = 0.5.
    A lower NPI metric indicates a more optimal balance between resource utilization
    and latency. The percentage of resources used r is determined with [Eq. 2:](#page-1-2)


    <span id="page-1-2"></span>

    $$r = \frac{1}{4} \left( \frac{DSP\_u}{DSP\_t} + \frac{LUT\_u}{LUT\_t} + \frac{FF\_u}{FF\_t}
    + \frac{BRAM\_u}{BRAM\_t} \right) \times 100 \quad (2)$$


    [Table I](#page-1-3) summarizes the results obtained after testing two cryptographic
    kernels across multiple configurations, starting with the baseline code. Amongst
    all the tested cases, the baseline kernels exhibit the highest NPI values, indicating
    suboptimal performance. A naive approach to maximizing resource usage through
    built-in pragma (#pragma HLS allocation) led to poor hardware mapping and increased
    design latency, underscoring the need for a more sophisticated solution. Testing
    the baseline code with state-of-the-art (SOTA) DSE tools for HLS, such as ScaleHLS,
    showed negligible improvement over the baseline, highlighting the challenges of
    optimizing straight-line cryptographic workloads. Introducing loop structures
    into the baseline code improved hardware mapping, but at the cost of higher latency.
    However, when this code was optimized using ScaleHLS, the NPI decreased further,
    yielding better performance. Our proposed DSE framework yielded the most significant
    improvements, achieving the lowest NPI values – kernel #1 demonstrated a 3.5×
    performance gain, while kernel #2 achieved 4.5× improvement.


    <span id="page-1-3"></span>


    | Cases ↓ / Benchmarks →                     | NPI for<br>Kernel #1 | NPI for<br>Kernel
    #2 |

    |--------------------------------------------|----------------------|----------------------|

    | Baseline ❶                                 | 0.5                  | 0.499                |

    | Baseline + Allocation Pragma               | 0.5                  | 0.5                  |

    | Baseline ❶<br>+ ScaleHLS ❶<br>+ ❺          | 0.5                  | 0.4903               |

    | Looped code ❶<br>+ ❷                       | 0.4705               | 0.35                 |

    | Looped code + ScaleHLS ❶<br>+ ❷<br>+ ❺     | 0.3496               | 0.179                |

    | Baseline ❶<br>+ Cryptonite ❷<br>+ ❸<br>+ ❹ | 0.1398               | 0.111                |


    TABLE I: Comparison of NPI for two benchmarks.


    Thus, straight-line code is a poor target for high-level DSE as it lacks *parametrizable
    constructs*. SOTA DSE tools [\[11\]](#page-7-11), [\[12\]](#page-7-12) primarily
    rely on source-code loop transformations, such as tiling, padding, and perfectization,
    along with HLS pragmas that influence loop behavior to explore various resource-latency
    trade-offs. However, in the absence of loop structures, these techniques become
    ineffective. This makes scalable designs harder to synthesize. Resource limitations
    are typical in the domain of cryptography, as encryption is still necessary in
    constrained environments. Thus, re-discovering parametrizable constructs in straight-line
    code is needed to explore a broader resource/latency tradeoff, imperative to the
    practical deployment of cryptographic kernels.


    Looping straight-line code generally enables improved hardware mapping through
    regular memory access patterns and resource scheduling. Eliminating or minimizing
    data dependencies and identifying repeated computation patterns enables time-multiplexed
    DSP usage instead of static allocation. By systematically restructuring the looped
    code, one can explore various trade-offs and benefits, ultimately designing a
    highly efficient architecture. [Fig. 1](#page-1-0) presents an example of a structured
    code (❷) that also has straight-line code lacking loop structures. Such straight-line
    code structures make it challenging to incorporate loops and require extensive
    exploration of design choices to understand their impact on performance and guide
    the next phase of design iteration. Exploring such design choices for the cryptographic
    kernels has two key challenges:


    ① Formal guarantee of transformations. Correctness is paramount to cryptographic
    applications – any unintended transformation could introduce vulnerabilities,
    breaking security guarantees. Conventional DSE for ML workloads allows heuristic-driven
    optimizations since floating-point approximations (quantization and pruning) are
    often acceptable due to the robustness of the model, whereas cryptographic workloads
    require provably correct transformations that are bit-accurate to maintain data
    integrity.


    ② Looping the auto-generated straight-line code. Cryptographic primitives use
    a combination of modular arithmetic, bitwise shifts, and rotations with operands
    that are not regularly placed in memory. This makes it difficult to automatically
    structure the source code to enable resource reuse for logic synthesis. To explore
    the design space for hardware implementation of these primitives, we must first
    identify patterns in the code and reconstruct computationally efficient loop structures
    while preserving formal correctness.


    These challenges call for a specialized tool that expands DSE''s reach in cryptography
    by reintroducing parametrizable code structures into straight-line code.


    ## III. APPROACH


    <span id="page-2-0"></span>[Fig. 1](#page-1-0) presents the workflow of our DSE
    tool, Cryptonite. Cryptonite begins with straight-line primitives as inputs and
    performs DSE in two stages – ❶ E-graph Loop Synthesizer (Sec. [III-A\)](#page-2-1)
    and ❷ Hardware Implementation Explorer (Sec. [III-B\)](#page-3-0). In the first
    stage, the E-graph loop synthesizer transforms local variables into arrays to
    perform Loop Synthesis. The resultant structured code is then passed to the Hardware
    Explorer which evaluates optimization strategies based on QoR estimator to enhance
    performance. [Fig. 1](#page-1-0)❺ demonstrates the potential for integration of
    any external DSE frameworks that can make use of the output of the Egraph Loop
    Synthesizer (or) the straight-line cryptographic kernel.


    ## <span id="page-2-1"></span>*A. E-graph Loop Synthesizer*


    Local variables in straight-line primitives are replaced with array elements so
    that they can be indexed with constant values. This change helps to eliminate
    excess registers that HLS could not optimize. It also opens up opportunities to
    inject loops, as we can now reference the variables with constant indices expressed
    in terms of loop variables. However, the challenge is that replacing multiple
    local variables with one array element can introduce data dependencies. To address
    this, we construct a data-dependence graph (DDG) from the input code using the
    SVF framework [\[13\]](#page-7-13). We generate a value-flow graph for our input
    program and perform an analysis of this graph, which identifies local variables
    that are used once only after assignment. Such variables consumed by the same
    expression can safely be collapsed into a single array index without affecting
    code correctness. We also track the variable types and only group those with the
    same type into the same array and/or element. Once variables are assigned to an
    array element, we choose concrete array indices for each group and replace them
    accordingly in the source code. While assigning indices arbitrarily would already
    reduce the number of local variables, our approach goes further: we base the assignment
    on access patterns of other input arrays in the targeted


    <span id="page-2-2"></span>![](_page_2_Figure_8.jpeg)


    Fig. 2: E-graph Loop Synthesizer.


    sections of code to enhance future loop injection opportunities. [Fig. 2](#page-2-2)
    gives an example of straight-line C-code and the corresponding code with the synthesized
    arrays. We identify two essential code properties for synthesizing loops from
    straight-line code:


    ❶ *Code patterns.* To inject loops, we need repetitive code. However, syntactic
    differences between lines of code can obscure semantic patterns. To address this,
    we abstractly interpret sequential variable assignments in the clang AST as mathematical
    formulas using a C++ library called GiNaC [\[14\]](#page-7-14). Array elements
    used in the lines of code are abstracted as mathematical variables, where the
    same array corresponds to the same variable. If the mathematical structures of
    two sequential lines match, we take note of the formula as a frequently used pattern.


    ❷ *Index dependence.* Each variable in the target code section must be expressible
    in terms of a constant value. The purpose of this is to ensure that all lines
    of code can be expressed in terms of a C-code template and a loop variable (which
    will replace the constant values in the synthesized loop). To achieve this, the
    variables present must be array elements with constant indices, or the same variable
    used repeatedly.


    We sweep the target function for lines of code with the properties above. For
    these lines, we construct a C-code *template* for the structure and a symbolic
    equation for the mathematical semantics of the line. Due to property ❷, the holes
    in the C-code template will always be filled with constant values in the target
    line. Thus, we can represent the line as a pair formed by its C-code template
    and the set of constants to complete it. Using this templating method, we group
    lines of code for which the mathematical semantics match, meaning the same C-code
    template can represent them. The abstracted version of the straight-line C-code
    in [Fig. 2](#page-2-2) is under abstracted Sequence with the corresponding template
    above. Once we have considered all lines of the function, we output all sets of
    constants that correspond to the same template along with their line numbers.
    This forms sequences of abstract operations with constant arguments.


    The purpose of this template abstraction is to more easily construct an e-graph
    representation of the code. E-graphs are data structures that can compactly represent
    equivalent versions of an expression or program [\[15\]](#page-7-15). An e-graph
    is expanded to represent more equivalent versions by applying rewrite rules. Applying
    a set of rewrite rules exhaustively to an e-graph is called equality saturation.
    We discover loops using equality saturation over our e-graph representation of
    the code. This is the heart of *loop saturation*. Our approach uses *egg* [\[16\]](#page-7-16),
    a fast and extensible e-graph framework for equality saturation.


    To obtain looped programs, we need rewrite rules that introduce loops in our e-graph.
    We construct these rules using the corresponding C-code template and the program''s
    data dependence graph. An intuitive way of introducing loops is to create a C-code-like
    forloop operator containing a loop variable with a start, stop, and step. The
    issue with this approach is that it requires all constants in the abstracted operations
    to be expressed in terms of this variable. In cases where there are multiple constants
    that must be expressed in terms of a loop variable, like for the Abstracted Operations
    in [Fig. 2,](#page-2-2) this means we must synthesize a linear equation that expresses
    the replaced constants in terms of the loop variable. This kind of equation synthesis
    is unnecessary for the scope of this project. Instead of expressing all variables
    in terms of one loop variable, which is time-consuming and restrictive, we insert
    loop-like structures *within* an abstracted operation, which summarize the start,
    stop, and step conditions for that particular abstracted argument. That way, each
    argument can have its own potential loop variable. We call this operator a *range*
    operator, as it describes a variable range. The rewrite rules that insert loops
    into the e-graph actually merge operations into a loop by replacing their operands
    with the range operator. The rewrite rule used for this transformation is shown
    in [Fig. 2](#page-2-2) under part 2c (Rewrite Rule). An intermediate e-graph phase
    with one application of this rule is shown under Equality Saturation in the same
    diagram. The output looped code of the e-graph loop synthesizer is shown at the
    bottom of [Fig. 2.](#page-2-2)


    The e-graph loop synthesizer always produces the most looped version of the provided
    code. Therefore, we can only incorporate feedback by changing the code we input.
    We incorporate feedback from the final synthesis step by including and excluding
    target code sequences in the e-graph re-rolling step. In particular, we can exclude
    targeted code sequences that contain the fewest number of operations; even for
    the most compact loop representation, the overhead of representing the loop structure
    may not outweigh the DSE benefits it enables. This produces differently looped
    codes to input for the DSE step that follows.


    The approach for equality saturation consists of the following steps: ① Inject
    loops with possible variable steps for pairs of instructions. ② Apply rewrite
    rules that absorb surrounding instructions into these proposed loops. ③ Extract
    the best-looped version according to e-graph AST size. ④ Use the C-code template
    for the e-graph operation to output C-code loops from the extracted e-graph.


    ## <span id="page-3-0"></span>*B. Hardware Implementation Explorer*


    Recall that straight-line code is a poor target for high-level DSE as it lacks
    parametrizable constructs. Synthesizing loop structures enables source-code transformations
    to explore designs with different performance characteristics. However, existing
    SOTA DSE tools fall short in our domain because they rely on MLIR, specifically
    the Affine and Structured Control Flow (SCF) Dialects [\[11\]](#page-7-11), [\[12\]](#page-7-12),
    [\[17\]](#page-7-17). These can support loop transformations but restrict expressions
    to additions, subtractions, and multiplications. They further expect highly regular
    loop structures. Cryptographic algorithms, however, depend on non-affine operations
    like modulo, division, and bit-wise logic, which disrupt these transformations
    and introduce irregularities. While MLIR''s optimization infrastructure [\[18\]](#page-7-18)
    excels in ML and numerical workloads with predictable data access patterns, it
    is less effective for our needs. Without specialized dialects or extensions, these
    tools cannot fully leverage the parallelization opportunities in cryptographic
    workloads. Furthermore, the lack of open-source availability of DSE frameworks
    prevents us from evaluating their effectiveness on cryptographic workloads. To
    address this, we developed Cryptonite''s hardware implementation explorer, which
    incorporates a QoR estimator to rapidly synthesize scalable and efficient designs
    under varying constraint requirements. The explorer operates in two steps: Exploration,
    where design variants are generated, and QoR Estimation, where each variant is
    evaluated against performance and resource constraints.


    ## *B.1 Exploration*


    The code transformations described below are applied directly to the e-graph synthesized
    code structure [\(Fig. 1](#page-1-0)❷), which is well-suited for such optimizations.
    The main stages of the explorer are as follows:


    ❶ Loop Pattern Analyzer. This stage analyzes the structured output of the e-graph
    loop synthesizer to extract key features such as loop boundaries, loop-carried
    dependencies, array access patterns, loop nesting levels, and function calls.
    It also identifies and lifts redundant straight-line array accesses into loop
    structures based on detected index patterns. Sometimes, the overall code structure
    is also changed by creating *reusable* functions for repeated code patterns with
    flow dependencies. Instead of duplicating the hardware for each recurring code
    pattern, the defined function is reused, thus reducing resource utilization.


    ❷ Variable bound removal. This stage resolves variable loop bounds into static
    constants, as most loop transformations and HLS optimizations require statically
    bounded loops. Normalizing bounds also mitigates variability that can lead to
    side-channel vulnerabilities.


    ❸ Loop Exploration. This stage generates diverse design variants that capture
    a wide range of performance trade-offs by systematically applying a combination
    of loop transformations. Rather than following a fixed transformation pipeline,
    the explorer enumerates and applies multiple combinations of optimizations to
    construct a rich design space. Each input loop nest is analyzed to determine valid
    transformation opportunities, and all permissible variants are explored. While
    the loop transformation flow follows a logical order starting with loop interchange
    and padding, followed by fusion, loop perfectization, branch elimination, strength
    reduction, and tiling, it is not enforced as a fixed pipeline. Instead, this order
    helps determine legal and beneficial transformation combinations. The explorer
    selectively applies subsets of these transformations to generate a diverse set
    of loop variants. [Fig. 1](#page-1-0)❸ shows an example of an explored code variant,
    where two nested loops are combined into a single loop through loop fusion. Loop
    padding is applied to align the iteration spaces, enabling a legal loop fusion
    output. The resulting structure allows for controlled parallelism supported by
    pipelining and loop unrolling directives.


    ❹ Pragma Insertions. This stage analyzes the transformed loop structures to determine
    where and how pragma directives should be applied. These directives include, but
    are not limited to, array partitioning, loop unrolling, loop pipelining, loop
    flattening, and loop merging to improve parallelism, along with customizable parameters
    per directive applied. Additionally, dependence pragmas, leveraging dependency
    information extracted from code analysis, help resolve false hazards. Since each
    pragma directive includes configurable parameters, the explorer systematically
    generates all valid permutations of pragma configurations for each loop nest.


    <span id="page-4-1"></span>


    | Primitive          | Design Approach                                                                                                                         |
    Latency<br>(cycles)                        | DSP (%)                                                                           |
    LUT (%)                                                                                           |
    NPI                                                  | Speedup                                              |
    Latency<br>(cycles)                        | DSP (%)                                                                               |
    LUT (%)                                                                                          |
    NPI                                                | Speedup                                              |

    |--------------------|-----------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|--------------------------------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|----------------------------------------------------|------------------------------------------------------|

    | Curve<br>25519     | Original Code<br>Rolled Code<br>Latency Optimized ∗<br>Resource
    Optimized ∗<br>Latency Optimized †<br>Resource Optimized †              | 95<br>270<br>67<br>95<br>51<br>231         |
    400 (15.87)<br>10 (0.39)<br>428 (16.98)<br>40 (1.58)<br>428 (16.98)<br>14 (0.55)  |
    12414 (4.529)<br>3065 (1.11)<br>11921 (4.34)<br>4827 (1.76)<br>11716 (4.274)<br>7023
    (2.56)       | 0.5776<br>0.5<br>0.536<br>0.149<br>0.498<br>0.495    | 1×<br>0.35×<br>1.42×<br>1×<br>1.86×<br>0.41×         |
    116<br>148<br>52<br>88<br>45<br>57         | 220 (8.73)<br>102 (4.04)<br>174 (6.9)<br>104
    (4.12)<br>152 (6.03)<br>100 (3.96)       | 7165 (2.614)<br>6051 (2.207)<br>6776
    (1.472)<br>8537 (3.114)<br>7331 (2.674)<br>7442 (2.715)     | 0.84<br>0.5<br>0.3407<br>0.336<br>0.242<br>0.109   |
    1×<br>0.78×<br>2.23×<br>1.17×<br>2.57×<br>2.03×      |

    |                    | Scale-HLS                                                                                                                               |
    127                                        | 50 (1.98)                                                                         |
    14832 (5.411)                                                                                     |
    0.367                                                | 0.74×                                                |
    91                                         | 168 (6.67)                                                                            |
    10353 (3.77)                                                                                     |
    0.69                                               | 1.27×                                                |

    | Curve<br>P521      | Original Code<br>Rolled Code<br>Latency Optimized ∗<br>Resource
    Optimized ∗<br>Latency Optimized †<br>Resource Optimized †              | 78<br>155<br>70<br>105<br>63<br>89         |
    1296 (51.42)<br>288 (11.4)<br>752 (29.8)<br>144 (5.71)<br>1296 (51)<br>144 (5.71)
    | 23227 (8.474)<br>7186 (2.621)<br>10312 (3.76)<br>10159 (3.7)<br>18653 (6.80)<br>9246
    (3.37)       | 0.581<br>0.554<br>0.271<br>0.232<br>0.485<br>0.141   | 1×<br>0.5×<br>1.11×<br>0.74×<br>1.23×<br>0.87×       |
    84<br>245<br>79<br>179<br>103<br>110       | 760 (30.15)<br>416 (16.5)<br>424
    (16.82)<br>272 (10.79)<br>308 (12.22)<br>280 (11.11) | 20798 (7.588)<br>16973
    (6.192)<br>24039 (8.77)<br>18328 (6.687)<br>19544 (7.13)<br>17846 (6.511) | 0.51<br>0.618<br>0.209<br>0.307<br>0.117<br>0.093  |
    1×<br>0.34×<br>1.06×<br>0.46×<br>0.81×<br>0.76×      |

    |                    | Scale-HLS                                                                                                                               |
    104                                        | 128 (5.079)                                                                       |
    16043 (5.85)                                                                                      |
    0.262                                                | 0.75×                                                |
    -                                          | -                                                                                     |
    -                                                                                                |
    -                                                  | -                                                    |

    | Curve<br>Secp256k1 | Original Code<br>Rolled Code<br>Latency Optimized ∗<br>Resource
    Optimized ∗<br>Latency Optimized †<br>Resource Optimized †<br>Scale-HLS | 66<br>211<br>169<br>182<br>143<br>190<br>-
    | 81 (3.21)<br>63 (2.5)<br>43 (1.7)<br>39 (1.54)<br>48 (1.9)<br>32 (1.26)<br>-      |
    12105 (4.41)<br>6954 (2.53)<br>7173 (2.61)<br>9067 (3.308)<br>9227 (3.366)<br>5118
    (1.86)<br>-    | 0.5<br>0.67<br>0.468<br>0.721<br>0.54<br>0.427<br>-  | 1×<br>0.31×<br>0.39×<br>0.36×<br>0.48×<br>0.34×<br>-
    | 64<br>306<br>240<br>246<br>118<br>275<br>- | 44 (1.74)<br>7 (0.27)<br>19 (0.75)<br>11
    (0.43)<br>32 (1.26)<br>7 (0.27)<br>-         | 9292 (3.39)<br>6954 (2.53)<br>6523
    (2.37)<br>8114 (2.96)<br>7441 (2.71)<br>7684 (2.8)<br>-       | 0.5<br>0.5<br>0.47<br>0.78<br>0.386<br>0.504<br>-  |
    1×<br>0.23×<br>0.18×<br>0.26×<br>0.54×<br>0.26×<br>- |

    | Curve<br>P384      | Original Code<br>Rolled Code<br>Latency Optimized ∗<br>Resource
    Optimized ∗<br>Latency Optimized †<br>Resource Optimized †<br>Scale-HLS | 80<br>598<br>107<br>187<br>133<br>145<br>-
    | 112 (4.44)<br>32 (1.26)<br>96 (3.8)<br>56 (2.22)<br>75 (2.85)<br>64 (2.53)<br>-   |
    10182 (3.71)<br>11197 (4.085)<br>8948 (3.264)<br>13012 (4.747)<br>7284 (2.65)<br>9957
    (3.63)<br>- | 0.5<br>0.717<br>0.379<br>0.576<br>0.051<br>0.37<br>- | 1×<br>0.13×<br>0.74×<br>0.42×<br>0.6×<br>0.55×<br>-  |
    61<br>425<br>110<br>106<br>89<br>124<br>-  | 144 (5.71)<br>32 (1.26)<br>120 (4.76)<br>56
    (2.22)<br>96 (3.8)<br>40 (1.58)<br>-      | 10943 (3.99)<br>12113 (4.41)<br>9651
    (3.52)<br>9488 (3.46)<br>9689 (3.53)<br>8499 (3.10)<br>-    | 0.5<br>0.65<br>0.409<br>0.17<br>0.33<br>0.086<br>-
    | 1×<br>0.14×<br>0.55×<br>0.57×<br>0.68×<br>0.49×<br>- |


    TABLE II: Comparison of resource-latency trade-offs for Manual DSE (∗), Cryptonite


    (†) and Scale-HLS. Resource-latency trade-offs are evaluated using either a Multiplication
    Kernel (white) or Square Kernel (grey).


    It explores combinations of synthesis directives—such as array partitioning granularities,
    pipeline initiation intervals, and unroll factors—to produce a diverse set of
    implementations that span a wide range of performance and resource trade-offs.


    ❺ Design Space Synthesis. In this stage, the design variants generated by stage
    ❸ are combined with the pragma configurations identified during stage ❹. While
    the Loop DSE stage produces a diverse set of loop transformation variants, the
    Pragma Insertion stage determines applicable directives and their parameters for
    each loop structure. These pragma annotations are stored separately as metadata
    and are merged with their corresponding loop variants to construct the final design
    space. The synthesized design space is the input for downstream QoR estimation,
    enabling guided selection of the most optimal implementation under given constraints.


    ## *B.2 QoR Estimator*


    A key factor driving Cryptonite''s performance is its Design Space Exploration
    (DSE) combined with Iterative QoR-Guided Optimization. The Quality of Results
    (QoR) estimator is used to predict and evaluate the effectiveness of a hardware
    design in terms of key performance metrics before synthesis. The QoR estimator
    includes an analytical model that predicts the latency (in cycles) and DSP resource
    usage based on loop structure, dependencies, and applied optimizations. Accurately
    predicting resource usage is challenging as it depends on factors such as routing
    and overall design complexity. The explorer initially generates all feasible designs.
    The QoR estimator compares designs, eliminates dominated solutions (i.e., non-Pareto
    solutions), and retains the optimal Pareto designs. The Pareto frontier designs
    are given as feedback (QoR feedback) to apply additional optimizations to refine
    the Pareto front to enable the exploration of new points near the Pareto front.


    ## IV. EXPERIMENTAL RESULTS


    <span id="page-4-0"></span>Cryptonite is evaluated by conducting experiments leveraging
    the straight-line primitives from the Fiat Project. The primitives included in
    the results are Elliptic Curves 25519, P521, P448, Secp256k1, and P384. Curve
    25519 enables efficient Diffie-Hellman key exchange. Curve P521, offers higher
    security with increased resource consumption. Curve P448 provides a balance between
    security and performance compared to Curve P521. Curve Secp256k1 is used for secure
    key-pair generation and digital signatures. Finally, Curve P384 offers superior
    security over Curve Secp256k1, using more resources.


    Cryptonite''s DSE results are compared to manual optimization and Scale-HLS, with
    Cryptonite achieving more scalable designs compared to manual rewriting. The results
    obtained from the manual rewriting are synthesized using the Vitis HLS tool 2023.1
    version targeting the xczu9eg-ffvb1156-2-e FPGA board. While generating hardware-friendly
    loop structures is challenging, handwritten RTL benchmarks often outperform the
    RTL generated by the Vitis HLS compiler. Creating such benchmarks is beyond the
    scope of this work due to the extensive development time required. When these
    primitives are synthesized on hardware, the lack of structural variation extends
    to the RTL representation limiting the effectiveness of comparison between baseline
    code and the HLS generated design. Instead of focusing on an RTL-to-RTL comparison,
    we evaluate performance metrics such as latency and area to explore optimizations
    that balance efficiency and correctness rather than adhere to an undefined ground
    truth RTL.


    This work primarily focuses on accelerating low-level cryptographic primitives
    such as modular multiplication that are widely used in many cryptography applications
    such as elliptic curves and lattice-based cryptography. This work leverages elliptic
    curve cryptography to illustrate the approach, but none of the presented methods
    are specifically tailored to elliptic curve cryptography. The approach is designed
    to work for any straight-line C-code application, provided there are opportunities
    for loop parallelization to enable re-rolling.


    We observed that as a program is optimized to utilize fewer DSPs, it becomes more
    generalized, leading to an increase in the


    <span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)


    (c) Design Points for CurveP448 Multiplication Kernel


    Curve25519 Multiplication kernel


    ![](_page_5_Figure_3.jpeg)


    (b) Design Points for Curve25519 Multiplication Kernel


    Curve25519 Square kernel


    ![](_page_5_Figure_6.jpeg)


    (d) Design Points for Curve25519 Square Kernel


    Fig. 3: Pareto curves obtained from DSE on single cryptographic kernels.


    usage of LUTs. Since DSP blocks are specialized for handling complex operations,
    it is important to conserve them to enable the concurrent execution of different
    applications on the FPGA.


    <span id="page-5-1"></span>Cryptonite is tested for 12 distinct primitive types
    in the Fiat Cryptography project [\[10\]](#page-7-9). For brevity, we provide
    a graphical comparison of the scalability achieved by manual rewriting, Scale-HLS,
    and Cryptonite for four cryptographic primitives, considering both single and
    multi-kernel implementations. The results are presented in [Table II](#page-4-1)
    and [Fig. 3](#page-5-0) & [Fig. 4](#page-6-1) respectively. This section is further
    divided into two parts. Sec. [IV-A](#page-5-1) demonstrates the impact of a single
    kernel implementation, while Sec. [IV-B](#page-6-2) explores about the impact
    of multiple kernel implementations. ScaleHLS was unable to produce synthesizable
    designs for all input cryptographic primitives. For these cases, the ScaleHLS
    results are omitted in [Table II](#page-4-1) and [Fig. 3d.](#page-5-0) The synthesis
    results for the manually optimized and Cryptonite designs are still provided for
    comparison.


    ## *A. DSE on Single Cryptographic Kernel*


    [Table II](#page-4-1) summarizes the synthesis results from implementing a single
    kernel for the cryptographic primitives discussed earlier. Cryptonite demonstrated
    up to a 2.75× speedup and a 2.03× reduction in resource usage. Notably, the tool
    identified design points with lower resource usage than manual optimizations and
    Scale-HLS. Cryptonite also achieves the lowest latency design for primitives other
    than Curve secp256k1 and Curve P384. These results stem from the dependency of
    the DSE engine on Loop Synthesis. When the e-graph stage struggles to generate
    effective loop structures, there are limited opportunities to apply latency-oriented
    optimizations automatically. The transformations in primitives Curve secp256k1
    and Curve P384 outline repeated calls to a common arithmetic function into a shared
    master function to improve modularity and enable code reuse. This approach naturally
    reduced resource utilization by allowing time-multiplexed reuse of DSPs. However,
    these operations could


    not be pipelined due to loop-carried dependencies introduced by carry propagation
    chains. Although the transformation reduced resource usage significantly, the
    overall latency increased compared to the original straight-line code.


    [Figs. 3a– 3d](#page-5-0) present the Pareto curves corresponding to synthesis
    outcomes for four distinct kernels. The plotted points represent design candidates
    explored using Cryptonite, ScaleHLS, and manual tuning of design parameters. The
    automated DSE can often generate the best set of Pareto Optimal designs. In three
    of the four kernels, designs discovered by Cryptonite dominate both the naive
    straight-line C and fully-rolled implementations in latency and resource usage.
    Additionally, designs discovered by Scale-HLS only extend the Pareto-curves produced
    by Cryptonite, reemphasizing the optimality of the designs generated automatically.


    These results highlight the effectiveness of the loop-synthesis stage in enabling
    practical design space exploration. All three methods (manual, automated, and
    Scale-HLS) produce a range of superior hardware designs to direct straight-line
    C implementations. Furthermore, Cryptonite is frequently able to automatically
    generate a set of Pareto-optimal designs with a range of resource-latency trade-offs,
    which is competitive with both manual optimization and ScaleHLS.


    ## <span id="page-6-2"></span>*B. DSE on Multiple Cryptographic Kernels*


    Cryptonite concurrently merges Pareto optimal implementations of multiple kernels,
    for joint deployment on a resource-constrained FPGA board. [Fig. 4](#page-6-1)
    compares the outputs of different execution strategies, including straight-line
    C-codes, Pareto-optimal designs, and loop-synthesized codes, when multiple kernels
    are executed together. Total consumption of resources is given by P<sup>N</sup>
    <sup>i</sup>=1 D<sup>i</sup> , where D<sup>i</sup> is the resource count for a
    kernel in the i th permutation and the effective latency for that permutation
    is max(L0, L1, ...., Li), where L<sup>i</sup> is the Latency of a i th Kernel.
    In [Fig. 4a,](#page-6-1) the design points opt-1, opt-2, and opt-3 utilize 59.6%,
    58.88%, and 37.3% of the total DSPs (2520) available on the target FPGA, respectively.
    [Fig. 4b](#page-6-1) compares the multi-kernel DSE results obtained for Manual,
    Scale-HLS and Cryptonite optimizations for the kernels Curve25519 Mul, Curve25519
    Square and CurveP521 Mul respectively. Compared to naively synthesized straight-line
    code, Cryptonite reduces DSP usage by approximately 62.21%, while also reducing
    execution time by 27 cycles. Scale-HLS, on the other hand, achieves a 81.94% reduction
    in DSP usage, but results in a 11-cycle increase in total execution time. Meanwhile,
    the manual optimization reduces DSP usage by 29.33% and outperforms the straight-line
    code by 46 cycles in terms of execution time. Designs synthesized using naive
    straight-line C implementations of multiple kernels are unable to achieve the
    same resource utilization, highlighting Cryptonite''s ability to generate scalable
    designs automatically.


    ## V. RELATED WORK


    <span id="page-6-0"></span>The Fiat Cryptography project [\[10\]](#page-7-9) automatically
    generates code for cryptographic primitives based on proofs in the interactive
    theorem prover Coq. It outputs primitives for several curves that are provably
    correct in assembly and several high-level languages, including C. We target the
    code generated in C using our approach. The fact that this code is auto-generated
    also creates many opportunities for pattern detection. These patterns can be used
    to automatically try different ways of re-structuring the code.


    Previous works have proposed manually optimized architectures for cryptographic
    primitives, some of them particularly focused on improving efficiency to achieve
    the best performance [\[3\]](#page-7-2), [\[19\]](#page-7-19). Others have focused
    on manually designing accelerators for particular constrained settings, like Internet
    of Things (IoT) devices [\[8\]](#page-7-7), [\[9\]](#page-7-8).


    <span id="page-6-1"></span>![](_page_6_Figure_8.jpeg)


    (a) Cryptonite''s Multi-kernel DSE


    ![](_page_6_Figure_10.jpeg)


    (b) Comparison Multi-DSE results for Cryptonite, Scale-HLS and manual Rewriting


    Fig. 4: DSE on multiple cryptographic kernels.


    SEER [\[17\]](#page-7-17) serves as a super-optimization explorer for HLS, utilizing
    e-graphs to implement rewrite rules. However, this work focuses on applying MLIR-base
    optimization rather than loop generation for HLS DSE. Furthermore, the MLIR-based
    optimizations are not well-suited to cryptography and concentrate solely on latency
    minimization. Nandi *et al.* [\[20\]](#page-7-20) also uses e-graphs for loop
    re-rolling in the context of shrinking de-compiled computer-aided design (CAD)
    models. Since this work targets de-compiled code, the synthesized loops likely
    existed in the original program and were then unrolled. Since cryptographic primitives
    do not initially include loops, the source code must be transformed to eliminate
    local variables and expose patterns that can be restructured into loopable code.


    ScaleHLS [\[11\]](#page-7-11) is a tool leveraging MLIR to transform and optimize
    C/C++ code for High-Level Synthesis (HLS) at various abstraction levels. While
    it excels at enhancing the performance of kernels used in machine learning workloads,
    it is not well-suited for handling the looped codes generated from the straight-line
    codes produced by the Fiat Project. AutoScaleDSE [\[12\]](#page-7-12), an extension
    of ScaleHLS tailored for ML workloads, automates and optimizes design space exploration
    (DSE) to achieve efficient scaling of compute-intensive systems by balancing performance,
    resource utilization, and constraints. Similarly, the MLIR base optimizer, relying
    on Affine and SCF dialects, struggles to find meaningful cryptographic optimization
    before and after loop & array synthesis.


    POLSCA [\[21\]](#page-7-21) is another non-MLIR optimization framework, focusing
    on source code optimization with HLS-friendly polyhedral transformations. However,
    it also relies on structured loops and affine operations, making it unsuitable
    for cryptographic primitives. Additionally, unlike prior works [\[11\]](#page-7-11),
    [\[17\]](#page-7-17), [\[21\]](#page-7-21), which primarily target performance
    speedup, our work emphasizes achieving


    scalable designs to meet broader application requirements.


    While side-channel resistance is not the primary focus of this work, we recognize
    its critical importance when implementing cryptographic protocols. Though our
    source code transformation does introduce branches, these statements never branch
    on secret values, but rather on statically determined constants that determine
    loop length. However, these branches will not introduce sidechannel vulnerabilities
    at the software level. The aforementioned hardware implementation explorer ensures
    that critical properties of the original code, particularly those related to constant-time
    arithmetic, manual carry propagation, ambiguity in bit widths, and lack of variable
    loop bounds, are preserved. It avoids introducing control-flow variability, operator
    widening, or replacing masking logic with more complex arithmetic. By retaining
    these properties, the transformations applied by the explorer remain potentially
    sidechannel resistant, enabling safe application of HLS optimizations.


    Some side-channel vulnerabilities are unique to FPGA-based cryptographic implementations.
    Prior work, such as [\[22\]](#page-7-22), highlights how automatic hardware generation
    via HLS can inadvertently introduce timing and resource usage patterns that leak
    secret information. These findings are outside of the scope of our work; our approach
    assumes that the HLS tool will not introduce new side-channel vulnerabilities.
    MaskedHLS [\[23\]](#page-7-23) presents a domain-specific HLS framework for synthesizing
    masked side-channel-resistant designs. This illustrates that proper tool-chain
    support can mitigate these risks.


    Researchers have done significant work on hardware acceleration for cryptography
    protocols. Devadas et. al [\[24\]](#page-7-24) explores the benefits and challenges
    in this domain. This work concludes that hardware acceleration is very effective
    for cryptography. Although developing custom hardware for one protocol usually
    produces the fastest results, this may not be the best option due to the everevolving
    nature of the encryption standard. Recent works investigate accelerating post-quantum
    cryptography (PQC) workloads. One work [\[25\]](#page-7-25) proposes an open-source
    FPGA implementation of PQC primitives. Unlike our work, these are manually implemented
    and optimized for each primitive. Another suggests designing a ring processing
    unit (RPU) [\[26\]](#page-7-26) with a specialized ISA and microarchitecture tailored
    to Ring-LWE PQC protocols. Our work improves automatic design generation from
    a high-level specification.


    ## VI. CONCLUSION


    <span id="page-7-10"></span>This paper introduces Cryptonite, a tool designed
    to scale hardware implementations of cryptographic primitives in resource-constrained
    environments. By leveraging source code transformations in combination with HLS
    feedback, Cryptonite explores the design space of cryptographic primitives, generating
    diverse hardware design points. This flexibility enables the selection of optimal
    designs based on specific user constraints. The results from its multi-kernel
    implementation pave the way for more efficient hardware acceleration for cryptographic
    primitives.


    ## ACKNOWLEDGMENT


    The authors thank the anonymous reviewers for their feedback on earlier drafts
    of this paper. The work described in this paper was supported, in part, by the
    United States National Science Foundation (NSF) under grants No. 2114627, No.
    2237440 and No. 2317251. Any opinions, findings, conclusions, or recommendations
    expressed in this publication are those of the authors and do not necessarily
    reflect the views of the above sponsoring entities.


    ## REFERENCES


    - <span id="page-7-0"></span>[1] T. Ye *et al.*, "Fpga acceleration of number
    theoretic transform," in *ISC 2021*, p. 98–117, Springer-Verlag, 2021.

    - <span id="page-7-1"></span>[2] M. Bisheh-Niasar *et al.*, "High-speed ntt-based
    polynomial multiplication accelerator for post-quantum cryptography," in *IEEE
    ARITH*, 2021.

    - <span id="page-7-2"></span>[3] W. N. Chelton and M. Benaissa, "Fast elliptic
    curve cryptography on fpga," *IEEE Transactions on VLSI Systems*, 2008.

    - <span id="page-7-3"></span>[4] R. Chen, J. Liu, X. Shi, M.-H. Tsai, B.-Y. Wang,
    and B.-Y. Yang, "llvm2cryptoline: Verifying arithmetic in cryptographic c programs,"
    in *Proceedings of ESEC/FSE*, 2023.

    - <span id="page-7-4"></span>[5] M. Amara and A. Siad, "Elliptic curve cryptography
    and its applications," in *WOSSPA*, pp. 247–250, 2011.

    - <span id="page-7-5"></span>[6] N. Wu, Y. Xie, and C. Hao, "Ironman: Gnn-assisted
    design space exploration in high-level synthesis via reinforcement learning,"
    in *Proceedings of Great Lakes Symposium on VLSI*, 2021.

    - <span id="page-7-6"></span>[7] N. Wu, Y. Xie, and C. Hao, "Ironman-pro: Multiobjective
    design space exploration in hls via reinforcement learning and graph neural network-based
    modeling," *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, pp. 900–913, 2022.

    - <span id="page-7-7"></span>[8] M. Morales-Sandoval, L. Flores, R. Cumplido,
    J. J. Garcia-Hernandez, C. Feregrino, and I. Algredo-Badillo, "A compact fpga-based
    accelerator for curve-based cryptography in wireless sensor networks," *Journal
    of Sensors*, vol. 2021, pp. 1–13, 01 2021.

    - <span id="page-7-8"></span>[9] C. A. Lara-Nino, A. Diaz-Perez, and M. Morales-Sandoval,
    "Lightweight elliptic curve cryptography accelerator for internet of things applications,"
    *Ad Hoc Networks*, vol. 103, p. 102159, 2020.

    - <span id="page-7-9"></span>[10] A. Erbsen, J. Philipoom, J. Gross, R. Sloan,
    and A. Chlipala, "Simple high-level code for cryptographic arithmetic - with proofs,
    without compromises," in *IEEE SP 2019*, pp. 1202–1219, 2019.

    - <span id="page-7-11"></span>[11] H. Ye, C. Hao, J. Cheng, H. Jeong, J. Huang,
    S. Neuendorffer, and D. Chen, "Scalehls: A new scalable high-level synthesis framework
    on multi-level intermediate representation," 2021.

    - <span id="page-7-12"></span>[12] H. Jun, H. Ye, H. Jeong, and D. Chen, "Autoscaledse:
    A scalable design space exploration engine for high-level synthesis," *ACM Trans.
    Reconfigurable Technol. Syst.*, vol. 16, jun 2023.

    - <span id="page-7-13"></span>[13] Y. Sui and J. Xue, "SVF: interprocedural static
    value-flow analysis in LLVM," in *Proceedings of the 25th International Conference
    on Compiler Construction*, pp. 265–266, ACM, 2016.

    - <span id="page-7-14"></span>[14] C. Bauer, A. Frink, and R. Kreckel, "Introduction
    to the ginac framework for symbolic computation within the C++ programming language,"
    *CoRR*, vol. cs.SC/0004015, 2000.

    - <span id="page-7-15"></span>[15] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner,
    "Equality saturation: a new approach to optimization," *SIGPLAN Not.*, vol. 44,
    p. 264–276, jan 2009.

    - <span id="page-7-16"></span>[16] M. Willsey, C. Nandi, Y. R. Wang, O. Flatt,
    Z. Tatlock, and P. Panchekha, "egg: Fast and extensible equality saturation,"
    *Proc. ACM Program. Lang.*, vol. 5, no. POPL, pp. 1–29, 2021.

    - <span id="page-7-17"></span>[17] J. Cheng, S. Coward, L. Chelini, R. Barbalho,
    and T. Drane, "Seer: Super-optimization explorer for hls using e-graph rewriting
    with mlir," 2023.

    - <span id="page-7-18"></span>[18] C. Lattner, M. Amini, U. Bondhugula, A. Cohen,
    A. Davis, J. Pienaar, R. Riddle, T. Shpeisman, N. Vasilache, and O. Zinenko, "Mlir:
    A compiler infrastructure for the end of moore''s law," 2020.

    - <span id="page-7-19"></span>[19] M. Ernst, B. Henhapl, S. Klupsch, and S. Huss,
    "Fpga based hardware acceleration for elliptic curve public key cryptosystems,"
    *Journal of Systems and Software*, vol. 70, pp. 299–313, 03 2004.

    - <span id="page-7-20"></span>[20] C. Nandi, M. Willsey, A. Anderson, J. R. Wilcox,
    E. Darulova, D. Grossman, and Z. Tatlock, "Synthesizing structured cad models
    with equality saturation and inverse transformations," PLDI 2020, p. 31–44, 2020.

    - <span id="page-7-21"></span>[21] R. Zhao, J. Cheng, W. Luk, and G. A. Constantinides,
    "Polsca: Polyhedral high-level synthesis with compiler transformations," in *FPL
    2022*, pp. 235–242, 2022.

    - <span id="page-7-22"></span>[22] N. Pundir, S. Aftabjahani, R. Cammarota, M.
    Tehranipoor, and F. Farahmandi, "Analyzing security vulnerabilities induced by
    highlevel synthesis," *J. Emerg. Technol. Comput. Syst.*, vol. 18, Jan. 2022.

    - <span id="page-7-23"></span>[23] N. Sarma, A. S. Thakur, and C. Karfa, "Maskedhls:
    Domain-specific high-level synthesis of masked cryptographic designs," 2024.

    - <span id="page-7-24"></span>[24] S. Devadas, S. Langowski, N. Samardzic, S.
    Servan-Schreiber, and D. Sanchez, "Designing hardware for cryptography and cryptography
    for hardware," CCS ''22, p. 1–4, 2022.

    - <span id="page-7-25"></span>[25] R. Agrawal, L. Bu, A. Ehret, and M. Kinsy,
    "Open-source fpga implementation of post-quantum cryptographic hardware primitives,"
    in *FPL 2019*, pp. 211–217, 2019.

    - <span id="page-7-26"></span>[26] D. Soni, N. Neda, N. Zhang, B. Reynwar, H.
    Gamil, B. Heyman, M. Nabeel, A. A. Badawi, Y. Polyakov, K. Canida, M. Pedram,
    M. Maniatakos, D. B. Cousins, F. Franchetti, M. French, A. Schmidt, and B. Reagen,
    "Rpu: The ring processing unit," in *2023 ISPASS*, pp. 272–282, 2023.'
- title: "Low-Cost FlashAttention with Fused Exponential and Multiplication\n  Hardware\
    \ Operators"
  abstract: 'Attention mechanisms, particularly within Transformer architectures and
    large

    language models (LLMs), have revolutionized sequence modeling in machine

    learning and artificial intelligence applications. To compute attention for

    increasingly long sequences, specialized accelerators have been proposed to

    execute key attention steps directly in hardware. Among the various recently

    proposed architectures, those based on variants of the FlashAttention

    algorithm, originally designed for GPUs, stand out due to their optimized

    computation, tiling capabilities, and reduced memory traffic. In this work, we

    focus on optimizing the kernel of floating-point-based FlashAttention using new

    hardware operators that fuse the computation of exponentials and vector

    multiplications, e.g., e^x, V. The proposed ExpMul hardware operators

    significantly reduce the area and power costs of FlashAttention-based hardware

    accelerators. When implemented in a 28nm ASIC technology, they achieve

    improvements of 28.8% in area and 17.6% in power, on average, compared to

    state-of-the-art hardware architectures with separate exponentials and vector

    multiplications hardware operators.'
  url: http://arxiv.org/abs/2505.14314v1
  keywords: Attention, Hardware Accelerator, Fused Arithmetic Operators, Energy Efficiency
  document: '## I. INTRODUCTION


    The Transformer architecture and its application in large language models (LLMs)
    have revolutionized machine learning, particularly in sequence modeling tasks
    like natural language processing [1] and time-series analysis [2]. Their importance
    stems from the ability to selectively focus on relevant parts of the input through
    the attention mechanism, mimicking the human cognitive process of attending to
    specific information [3]. This enables models to handle long-range dependencies
    effectively, a challenge that recurrent neural networks struggled with.


    Roughly, attention achieves a dynamic, context-aware weighting of input features.
    Instead of treating all input elements equally, the attention mechanism calculates
    a score that represents the relevance of each element to the current processing
    step. This score is then used to weight the corresponding value, effectively amplifying
    the contribution of important elements and suppressing irrelevant ones. In essence,
    attention allows the model to focus to the most pertinent information, regardless
    of its position in the input sequence.


    The efficiency of attention comes at a significant computational cost. The quadratic
    complexity of the standard attention mechanism poses a significant bottleneck
    for processing long sequences in transformer models [4], leading to prohibitive
    computational costs and limiting their application to shorter contexts. This limitation
    hinders the ability to capture crucial long-range dependencies necessary for tasks
    like document summarization and code generation. To address this, researchers
    have explored various methods to reduce computational burden, including approximating
    the full attention matrix through techniques such as sparse [5], linear [6], and
    low-rank [7] attention, which aim to balance accuracy and efficiency.


    Moreover, custom hardware accelerators [8], [9] have been developed to optimize
    key components of the baseline attention computation, such as matrix arithmetic
    [10] and softmax evaluation [11], further enhancing processing speeds. These efforts
    collectively strive to make transformers more scalable and efficient for handling
    increasingly longer sequences.


    In parallel, FlashAttention [12], [13], [14], originally proposed for GPUs, addresses
    efficiently the computational bottlenecks inherent in the standard attention mechanism.
    It performs attention computation in tiles, avoiding the need to store the entire
    attention matrix. By recomputing attention scores on the fly and using optimized
    algorithms, flash attention reduces the memory footprint and improves computational
    efficiency.


    In this work, we argue that leveraging FlashAttention in specialized hardware
    is essential for maximizing performance and efficiently processing extremely long
    sequences in realworld applications. To further enhance the hardware implementation
    of floating-point FlashAttention kernels, we design novel hardware operators that
    fuse exponential computation and vector multiplication. To support agile development,
    these operators are implemented using high-level synthesis (HLS) and made publicly
    available [15], enabling efficient design space exploration and faster verification.
    The contributions of this work can be summarized as follows:


    - The discrete steps of exponential function evaluation and the subsequent vector
    floatint-point multiplication inherent in FlashAttention kernel are fused to one
    simplified ExpMul hardware operator.

    - Although the design of ExpMul includes a logarithmic quantization step, it does
    not hinder LLMs capabilities, as checked by running inference with Google''s T5
    model [1] on the GLEU [16] dataset.

    - The experimental results highlight the efficiency of the proposed approach.
    Highly parallel FlashAttention accelerators implemented in a 28nm ASIC technology,
    achieve improvements of 28.8% in area and 17.6% in power, on average, compared
    to equivalent hardware architectures that implement exponential function evaluation
    and vector multiplications separately.


    # II. COMPUTATION OF ATTENTION AND FLASHATTENTION


    The attention mechanism facilitates the dynamic weighting of input sequence elements,
    enabling the model to capture long-range dependencies and contextual relationships.
    This Algorithm 1 Attention with lazy softmax division


    1: for each query ⃗q do 2: for i = 1 : N do 3: s<sup>i</sup> ← dot(⃗q,⃗ki) 4:
    m<sup>i</sup> ← max(mi−1, si) 5: end for 6: ℓ<sup>0</sup> ← 0 7: for i = 1 : N
    do 8: ⃗o<sup>i</sup> ← ⃗oi−<sup>1</sup> + e <sup>s</sup>i−m<sup>N</sup> · ⃗v<sup>i</sup>
    9: ℓ<sup>i</sup> ← ℓi−<sup>1</sup> + e si−m<sup>N</sup> 10: end for 11: attn(⃗q,
    K, V ) ← ⃗o<sup>N</sup> /ℓ<sup>N</sup> 12: end for


    process revolves around the interaction of query, key, and value vectors, derived
    from input embeddings [3].


    #### *A. Attention Mechanism*


    Input embeddings are linearly projected into three distinct spaces, each representing
    a different aspect of the attention mechanism. These projections produce the query
    (Q), key (K), and value (V ) matrices, which are then used to compute the attention
    weights and output. In self-attention, these matrices are derived from the same
    input sequence, allowing the model to assess relationships between different positions
    within the sequence itself. In summary, attention is computed as follows:


    $$\text{Attn}(Q, K, V) = \text{softmax}(QK^T / \sqrt{d})V \tag{1}$$


    Similarity scores QK<sup>T</sup> are scaled by the square root of the key vector
    dimension d to mitigate the issue of vanishing gradients that can arise, when
    the scores become excessively large. For the rest of the paper, to simplify notation,
    we leave out this constant scaling. To detect the most relevant of tokens in (1)
    the softmax function is applied to the similarity scores for the same query. The
    final output is produced by multiplying the attention score matrix derived for
    all queries to the matrix of value vectors, where each value''s contribution to
    the output is heavily based on the corresponding scores of the attention score
    matrix.


    In practice, the attention mechanism is applied across multiple heads in parallel
    [3], allowing the model to comprehend complex relationships, make it more robust
    to erroneous generations and enable its parallel execution across heads, thus
    enhancing performance.


    The computation of attention employing the lazy softmax division concept [9],
    [14] is shown in Alg. 1. The first part computes the dot product of a query vector
    with all key vectors. Also the maximum score is identified that is needed for
    computing a numerically-stable softmax normalization. Exponentiating each score,
    as needed by softmax, may lead to infinite values that would propagate until the
    output and ruin the final result. To avoid this numerical instability, softmax
    is implemented by subtracting the maximum from all scores. This effectively removes
    the problem of overflow and numeric instability, while preserving the fundamental
    properties of the softmax function.


    In the following, the output vector ⃗o<sup>i</sup> is computed incrementally as
    the weighted sum of each value vector and the corresponding exponential score
    decremented by the maximum score. In parallel, the sum of all exponentials ℓ<sup>i</sup>
    is accumulated. The final attention vector for one query vector q is computed
    by dividing the final output vector by the corresponding sum-of-exponents ℓ<sup>N</sup>
    .


    #### *B. Attention Hardware Accelerators State-of-the-Art*


    The traditional attention mechanism faces three primary performance bottlenecks:
    high memory bandwidth demands due to the retrieval of query, key, and value matrices,
    particularly with long sequences; substantial computational overhead from the
    softmax operation across the full sequence; and limited parallelism due to reduction
    operations within softmax, such as maximum value determination and exponential
    summation.


    To mitigate memory traffic, attention accelerators employ a strategy of keeping
    key and value vectors static in local SRAM, while streaming query vectors to calculate
    attention scores [8], [17], [18]. This approach minimizes memory fetches by only
    reloading query vectors for each key and value vector. However, this method''s
    effectiveness diminishes with increasing sequence lengths, as key, value, and
    score matrices are replaced into slower DRAM, leading to performance degradation.
    This issue is particularly relevant in modern NLP tasks that often involve extensive
    sequences [4]. To reduce the memory access overhead other approaches focus on
    inmemory computation of attention [19].


    To decouple computational resources and local memory from the sequence length,
    several designs [9], [10] accumulate partial softmax results for each column of
    the attention scores. This method maintains accuracy while reducing the need to
    buffer and compute softmax results for the entire sequence at once, avoiding memory
    spills.


    Further optimization techniques involve computation skipping based on token similarity
    to reduce latency and power [20], [21], as well as quantization to minimize the
    cost of data transfers and enhance accelerator efficiency [22].


    #### III. FLASHATTENTION-BASED ACCELERATORS


    FlashAttention [12], inspired by online softmax computation [23], restructures
    attention computation into an online process. Alg. 2 illustrates its more efficient
    variant, FlashAttention-2, which further optimizes performance by postponing the
    softmax division until the end, using the corresponding sum of exponents [9],
    [14]. The key distinction from baseline attention (Algorithm 1) is that FlashAttention-2
    computes all necessary variables online within the same inner loop, eliminating
    the need to first determine the maximum of all attention scores. For large sequence
    lengths (N), this online computation of softmax components is crucial for maintaining
    performance efficiency.


    During each iteration, the dot product of the query vector and a key vector yields
    a similarity score, denoted as s<sup>i</sup> . Subsequently, m<sup>i</sup> is
    determined as the current maximum attention score. Then, ℓ<sup>i</sup> incrementally
    accumulates the sum of the exponentials of each s<sup>i</sup> decremented by the
    present maximum value. The multiplication by e mi−1−m<sup>i</sup> in the calculation
    of


    ### Algorithm 2 FlashAttention-2 with delayed softmax division


    1: for each query ⃗q do 2: for i = 1 : N do 3: s<sup>i</sup> ← dot(⃗q,⃗ki) 4:
    m<sup>i</sup> ← max(mi−1, si) 5: ℓ<sup>i</sup> ← ℓi−1e <sup>m</sup>i−1−m<sup>i</sup>
    + e si−m<sup>i</sup> 6: ⃗o<sup>i</sup> ← ⃗oi−1e <sup>m</sup>i−1−m<sup>i</sup>
    + ⃗vie si−m<sup>i</sup> 7: end for 8: attn(⃗q, K, V ) ← ⃗o<sup>N</sup> /ℓ<sup>N</sup>
    9: end for


    ℓ<sup>i</sup> effectively adjusts the prior maximum value whenever the current
    maximum m<sup>i</sup> is larger than the previous maximum mi−1. Similarly, the
    output vector ⃗o<sup>i</sup> is updated by adding the new value vector ⃗v<sup>i</sup>
    weighted by its softmax importance, to the maximum-adjusted preceding output vector,
    ⃗oi−1e mi−1−m<sup>i</sup> .At the end, to finalize attention computation for this
    query vector the output ⃗o<sup>N</sup> is divided by the total sum of exponents
    accumulated in parallel in ℓ<sup>N</sup> .


    The FlashAttention-2 kernel shown in Alg. 2 involves two for loops that can be
    unrolled to enhance parallelism and computational throughput. To avoid any serial
    dependencies, we unroll the outer loop, allowing the FlashAttention-2 kernel to
    process multiple query vectors in parallel within the same blocks of key and value
    vectors. Fig. 1 shows the derived parallel hardware structure for FlashAttention-2.
    A block of query vectors is pre-loaded locally, while key vectors are provided
    sequentially to all blocks to compute the corresponding dot products [24]. This
    process leads to the calculation of a new maximum value and an updated running
    sum of exponents. The output vectors for different query vectors are updated in
    parallel as value vectors are streamed into the accelerator. After processing
    all key and value vectors, the attention for each query vector is computed through
    a final division operation. The computation finishes once all query vectors have
    been processed. For FlashAttention-2 kernels, we assume that we can read from
    local memories in each cycle one key and one value vector of d elements each that
    corresponds to the size of the hidden dimension of attention.


    FlashAttention-2 does not require a unified softmax hardware unit, since exponentiations
    and the final division are computed separately. Many approaches have been used
    for computing those two non-linear operations in hardware. Variants include piece-wise
    linear approximations after appropriate range reduction [25], transformations
    based on logarithmic quantization [26] or other approximations [27].


    ## IV. FUSED EXPONENTIAL AND VECTOR MULTIPLICATION HARDWARE OPERATORS


    Our goal is to enhance the last part of the FlashAttention-2 kernel (line 6 in
    Alg. 2), which incrementally updates the output by performing two vector multiplications
    and one vector addition. To achieve this, we plan to fuse the exponential calculations
    with the vector multiplications, utilizing new, lowcost hardware operators.


    ![](_page_2_Figure_7.jpeg)


    Fig. 1. A block-parallel hardware architecture for FlashAttention-2 kernel.


    ## *A. Merging sum-of-exponents and output update*


    The first step in the planned simplification of the FlashAttention-2 kernel is
    to merge the incremental computation of the sum-of-exponents (performed in line
    5 of Alg.2) with the incremental computation of the output vector (line 6 in Alg.2).
    Both updates involve the same operations and can be written in a vector-merged
    form as follows:


    $$

    \begin{bmatrix} \ell\_i \\ \vec{o}\_i \end{bmatrix} = \begin{bmatrix} \ell\_{i-1}
    \cdot e^{m\_{i-1} - m\_i} + 1 \cdot e^{s\_i - m\_i} \\ \vec{o}\_{i-1} \cdot e^{m\_{i-1}
    - m\_i} + \vec{v}\_i \cdot e^{s\_i - m\_i} \end{bmatrix} \tag{2}

    $$


    Increasing by one element the output ⃗o<sup>i</sup> and the value vector ⃗vi ,
    i.e., o ∗ <sup>i</sup> = [ℓ<sup>i</sup> ⃗o<sup>i</sup> ] and v ∗ <sup>i</sup>
    = [1 ⃗v<sup>i</sup> ] the merged incremental update of the sum-of-exponents and
    the output shown in (2) can be written as:


    $$o\_i^\* = o\_{i-1}^\* \cdot e^{m\_{i-1} - m\_i} + v\_i^\* \cdot e^{s\_i - m\_i}
    \tag{3}$$


    The unified form of (3) demonstrates that the incremental computation of the output
    involves two scalar × vector multiplication operations, where the scalar value
    is determined after evaluating the exponential function. To simplify the hardware
    for this computation, we propose fusing the separate exponential and vector multiplication
    operations into a single unified hardware operator:


    $$\text{ExpMul}(x, V) = e^x \, V \tag{4}$$


    This would allow us to rewrite (3) as follows:


    $$o\_i^\* = \text{ExpMul}(m\_{i-1} - m\_i, o\_{i-1}^\*) + \text{ExpMul}(s\_i -
    m\_i, v\_i^\*) \tag{5}$$


    ## *B. Fused Exponential-Vector Multiplication Operator*


    In this work, we focus on the floating-point arithmetic implementation of FlashAttention.
    To efficiently implement the newly introduced ExpMul operator, we utilize logarithmic
    quantization techniques originally proposed for softmax hardware implementations
    [26]. These techniques allow us to replace the computationally expensive floating-point
    exponential and multiplication operations required by the ExpMul operator with
    hardware-friendly integer shift-and-add operations.


    The ExpMul operators in FlashAttention-2 receive a difference of scalar values
    that is always less or equal to 0 and a vector of floating-point data. Since the
    value of x in (4) belongs to (−∞, 0], it effectively limits e x in the range of
    (0, 1]. This characteristic fits logarithmic quantization, which applies the log<sup>2</sup>
    function and returns as a result, its rounded negative value.


    $$\text{Log2Exp}(X) = \lfloor -\log\_2 e^X \rceil = \lfloor -X \cdot \log\_2 e
    \rfloor \qquad (6)$$


    Since X is a floating-point number, the multiplication with the constant log<sup>2</sup>
    e should be performed using floating-point arithmetic.


    To simplify the computation, we convert the value of X to its fixed-point equivalent
    Xˆ. In general, converting a floatingpoint number to its fixed-point representation
    requires many integer bits to accommodate the wide range of floating-point arithmetic
    [28]. However, in this case, we can safely clip X to a much smaller dynamic range,
    as it will only be used for the computation of e <sup>X</sup> for negative values
    of X. In this range, e <sup>X</sup> quickly converges to 0, even for small negative
    X values, i.e., e <sup>−</sup><sup>15</sup> = 3.1 · 10<sup>−</sup><sup>7</sup>
    . In other words we can convert X to a fixed point number with a small number
    of bits without the need to consider its full dynamic range. Thus, instead of
    fully quantizing X we simply constrain its value before logarithmic quantization.


    $$

    \hat{X} = \text{Fixed}(\text{Clip}(X, -15, 0)),

    $$


    Converting to a fixed point number the clipped value of X requires a small number
    of bits. For this conversion, we use a 16-bit fixed point number to keep the corresponding
    hardware overhead small. Since X is going to be multiplied by a constant value
    its range after clipping will change to [−21.64, 0]. To account for this new range
    we assign 6 bits for the integer part and the rest for the fraction part.


    After the clipping and quantization of X, Eq. (6) becomes


    $$\text{Log2Exp}(X) = \lfloor -\hat{X} \cdot \log\_2 e \rfloor$$


    Since Xˆ is a fixed point number the multiplication with the constant value log<sup>2</sup>
    e can be approximated by integer shift–and–add operations [26], [25]. Following
    the approach presented in [26], we get that:


    $$\text{Log2Exp}(X) = -\lfloor \hat{X} + \hat{X} \gg 1 - \hat{X} \gg 4 \rfloor
    \tag{7}$$


    With the help of (7) the ExpMul operator of (4) can be rewritten as:


    $$\text{ExpMul}(x, V) = 2^{-\hat{L}} \, V,\\ \text{where } \hat{L} = \text{Log2Exp}(x).
    \quad (8)$$


    Multiplying the floating point number V = (−1)<sup>S</sup><sup>V</sup> 2 E<sup>V</sup>
    −b (1+ M<sup>V</sup> ), where S<sup>V</sup> , E<sup>V</sup> , M<sup>V</sup> represent
    its sign, exponent and mantissa field, respectively, with 2 −Lˆ in (8) is equivalent
    to subtracting Lˆ from the exponent of V . In case of an overflow the resulting
    FP number is set to 0. More specifically,


    $$\begin{aligned} \text{ExpMul}(x, V) &= 2^{-\hat{L}}V = 2^{-\hat{L}} \left( -1
    \right)^{S\_V} 2^{E\_V - b} \left( 1 + M\_V \right) \\ &= (-1)^{S\_V} 2^{E\_A
    - b - \hat{L}} \left( 1 + M\_V \right) \\ &= (-1)^{S\_V} 2^{E\_A - b - \text{Log2Exp(x)}}
    \left( 1 + M\_V \right) \end{aligned}$$


    The result computes both e <sup>x</sup> V and at the same time returns the result
    directly in floating-point format without requiring any addition conversion (or
    dequantization) step.


    Alg. 3 summarizes the computational steps that are performed by the ExpMul operator
    to calculate e <sup>x</sup> V and Alg. 4 using (3), (5), and (9) illustrates how
    the new hardware operator is integrated in FlashAttention-2 kernel.


    | Algorithm 3 ExpMul(x, V)                           |  |

    |----------------------------------------------------|--|

    | Input: x < 0 and a vector V of N elements          |  |

    | xV<br>Output: A vector Out where Out[i] = e<br>[i] |  |

    | 1: for i ← 1 : N do                                |  |

    | SV<br>, EV<br>, MV<br>← extract(V [i])<br>2:       |  |

    | xˆ ← Fixed(Clip(x, −15, 0))<br>3:                  |  |

    | Lˆ ← −⌊xˆ<br>+ ˆx ≫ 1 − xˆ ≫ 4⌉<br>4:              |  |

    | − L, M ˆ<br>Out[i] ← Float(SV<br>, EV<br>V )<br>5: |  |

    | 6: end for                                         |  |


    Algorithm 4 FlashAttention2 with Fused ExpMul Operators


    1: for each query ⃗q do 2: for i = 1 : N do 3: s<sup>i</sup> ← dot(⃗q,⃗ki) 4:
    m<sup>i</sup> ← max(mi−1, si) 5: o ∗ <sup>i</sup> ← ExpMul(mi−<sup>1</sup> − m<sup>i</sup>
    , o<sup>∗</sup> i−1 ) + ExpMul(s<sup>i</sup> − m<sup>i</sup> , v<sup>∗</sup> i
    ) 6: end for 7: [ℓ<sup>N</sup> ⃗o<sup>N</sup> ] ← o ∗ N 8: attn(⃗q, K, V ) ← ⃗o<sup>N</sup>
    /ℓ<sup>N</sup> 9: end for


    The proposed ExpMul operators not only remove expensive floating-point operations
    like exponent function evaluation and multiplication but perform this step with
    low-cost quantization and without any dequantization step. Traditional quantization
    approaches [22] would utilize additional quantization logic to transfer operation
    to and from the integer domain, thus paying the extra hardware cost of this transformation.
    Also, even if operating in the integer domain, the computation would still involve
    costly multiplication and exponential operations. The proposed approach removes
    both overheads and simplifies significantly the computation in FlashAttention-2.


    The parallel hardware organization of FlashAttention-2 kernel employing the proposed
    ExpMul operators is shown in Fig. 2. The organization of the new hardware unit
    is the same as the original FlashAttention-2 accelerator shown in Fig. 1. However,
    we can highlight one key difference. The exponent function hardware operators
    and floating point multipliers are replaced by a single ExpMul unit which performs
    those two operations in a fused manner using integer only arithmetic and preparing
    the result in floating point as needed by subsequent operations.


    ## V. EVALUATION


    Experimental evaluation aims to examine the impact of the new fused ExpMul hardware
    operators on real machinelearning applications and also highlight the achieved
    hardware savings.


    | TABLE I                                                                               |  |  |  |  |  |  |  |  |  |

    |---------------------------------------------------------------------------------------|--|--|--|--|--|--|--|--|--|

    | PERFORMANCE OF GOOGLE''S FLAN-T5 LLM MODEL FOR 10 BENCHMARKS OF THE GLUE DATASET
    [16]. |  |  |  |  |  |  |  |  |  |

    |                                                                                       |  |  |  |  |  |  |  |  |  |

    |                                                                                       |  |  |  |  |  |  |  |  |  |


    | Benchmarks   |             | STS-2 | MNLI-m | MNLI-mm | QQP   | QNLI  | CoLA  |
    MRPC  | RTE   | WNLI  | STS-B |

    |--------------|-------------|-------|--------|---------|-------|-------|-------|-------|-------|-------|-------|

    | Accuracy (%) | FP32        | 92.1  | 87.5   | 84.2    | 93.1  | 93.3  | 72.0  |
    86.0  | 74.3  | 62.0  | 92.0  |

    |              | FP32-ExpMul | 92.1  | 87.5   | 84.2    | 93.1  | 93.3  | 72.0  |
    86.0  | 74.3  | 62.8  | 92.0  |

    |              | BF16        | 91.1  | 87.3   | 83.3    | 93.1  | 93.3  | 72.0  |
    84.0  | 73.8  | 62.0  | 91.0  |

    |              | BF16-ExpMul | 91.2  | 87.3   | 82.1    | 93.1  | 93.3  | 69.0  |
    88.0  | 73.8  | 62.0  | 90.0  |

    | F1-score     | FP32        | 0.921 | 0.794  | 0.845   | 0.930 | 0.933 | 0.830
    | 0.900 | 0.726 | 0.690 | -     |

    |              | FP32-ExpMul | 0.931 | 0.800  | 0.833   | 0.930 | 0.920 | 0.840
    | 0.900 | 0.738 | 0.710 | -     |

    |              | BF16        | 0.910 | 0.780  | 0.845   | 0.930 | 0.900 | 0.800
    | 0.890 | 0.725 | 0.690 | -     |

    |              | BF16-ExpMul | 0.910 | 0.780  | 0.833   | 0.930 | 0.900 | 0.780
    | 0.920 | 0.733 | 0.690 | -     |


    ![](_page_4_Figure_2.jpeg)


    Fig. 2. The FlashAttention-2 kernel optimized with the proposed ExpMul operators.
    According to Alg. 3 each ExpMul operator involves clipping, integer shift-and-add
    operations and an exponent increment producing directly floating point results
    without further dequantization.


    ## *A. Verification of LLM accuracy*


    Since ExpMul involves logarithmic quantization, it introduces a numerical approximation.
    To verify that this approximation does not hinder the capabilities of a Large
    Language Model (LLM), we run ten representative NLP benchmarks from the GLUE [16]
    dataset, using Google''s Flan-T5 LLM. For comparison, we considered four implementations:
    (a) Models ''FP32'' and ''BF16'' use the default attention mechanism of T5 and
    compute all operations including separate multiplications and function evaluation
    using single-precision and BFloat16 [29] floating point arithmetic; (b) Models
    ''FP32- ExpMul'' and ''BF16-ExpMul'' employ floating point arithmetic for all
    parts of the model except for the proposed ExpMul operator, where computations
    are done in 16-bit integer arithmetic, as discussed in Section IV-B. To run inference
    with the LLM we utilized Microsoft''s PromptBench workflow [30].


    Table I presents the accuracy results for all four examined variants. All models
    exhibit comparable performance in terms of inference accuracy and the F1-score
    metric. In some cases, reduced-precision models (''FP32, BF16-ExpMul'') outperform
    their full-precision counterparts. This phenomenon is a known artifact of lower-precision
    models [31]. While it does not indicate that the model is inherently superior,
    it confirms that the approximation used to formulate the ExpMul operators does
    not degrade performance.


    # *B. Hardware Savings*


    To evaluate the hardware savings of the proposed Exp-Mul hardware operators we
    implemented two variants of the FlashAttention-2 kernel. The two designs correspond
    to the main block shown in the foreground of Figs. 1 and Fig. 2, respectively.
    The implementation involves various hidden dimension sizes (d) and two floating-point
    formats: FP32 and BFloat16. The baseline FlashAttention kernel performs exponent
    and multiplication operations separately. Exponent function evaluation is implemented
    with piece-wise linear approximation in the reduced input range inherent to attention
    and used also for the ExpMul operators.


    In practice, the total cost of the unrolled hardware serving multiple query vectors
    in parallel will be the cost for one query multiplied by the number of parallel
    query vectors served. The query vectors are preloaded separately in the architecture,
    while the key and value vectors are loaded and broadcasted to all parallel blocks.


    Both hardware blocks were implemented in C++ (publicly available at [15]) and
    synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library.
    Both designs operate at the same pipelined latency with a clock frequency of 500
    MHz. Increasing the hidden dimension of each attention accelerator increases also
    the latency of computation. For the examined sizes of the hidden dimension, For
    one key and value vectors pipelined latency with initiation interval of 1, ranges
    between 8 and 12 cycles. Next, Verilog was synthesized using the Cadence digital
    implementation flow, while power consumption was estimated with the PowerPro power
    analysis and optimization tool. The reported power consumption represents the
    average power measured after executing attention kernels for the T5 Large Language
    Model and GLUE benchmarks utilizing the PromptBench workflow.


    Figs. 3 and 4 show the area and power of the FlashAttention-2 kernel with and
    without ExpMul hardware operators, for the two examined floating point data type
    and for three hidden dimension sizes d = {16, 64, 256}. Power estimation excludes
    memory power and focuses solely on the average power consumption of the computation
    kernel. The memory power in both cases is expected to be identical, as both approaches
    implement the same FlashAttention-2 algorithm using the same computation order
    and data flows. The difference lies solely on how the computation kernel is executed
    internally.


    As shown in Fig. 3, utilizing ExpMul operators reduces the hardware area by more
    than 28.8% on average in all examined cases. These saving come from the simplified
    computation


    ![](_page_5_Figure_0.jpeg)


    Fig. 3. The hardware area at 28 nm for the FlashAttention-2 kernel, evaluated
    across various hidden dimension sizes, with and without ExpMul operators, for
    computing attention on a single query using FP32 and BFloat16 floatingpoint formats.


    ![](_page_5_Figure_2.jpeg)


    Fig. 4. The average power consumption of the FlashAttention-2 kernel, evaluated
    across various hidden dimension sizes, with and without ExpMul operators, for
    computing attention on a single query using FP32 and BFloat16 floating-point formats.
    Memory and I/O power are excluded, as they are identical in both designs.


    kernel that does not require area expensive floating point multipliers and or
    additional exponential function evaluations. In the output update module, d multiplication
    and exponentials are converted to ExpMul operators, while reforming the floatingpoint
    output requires only d exponent subtractions.


    Simplifying hardware architecture improves energy efficiency, reducing power consumption
    by over 17.8% on average. As the hidden dimension d increases, power savings increase
    due to replacing power-hungry floating point multipliers with simpler ExpMul operators
    that use integer arithmetic.


    ## VI. CONCLUSIONS


    FlashAttention kernel simplifies attention computation by fusing the softmax function
    evaluation and output computation in an online manner. This transformation, originally
    proposed for GPUs, allows attention computation in tiles with sizes independent
    of the sequence length. In this work, our goal is to leverage the FlashAttention-2
    algorithm, an optimized version of the baseline FlashAttention, to design hardware
    accelerators that not only benefit from all the advantages of FlashAttention but
    also save significant area and power. This is achieved by employing the fused
    exponential and vector multiplication operators, which lead to efficient hardware
    architectures without compromising the accuracy of state-ofthe-art LLM applications.


    ## REFERENCES


    - [1] C. Raffel *et al.*, "Exploring the limits of transfer learning with a unified
    text-to-text transformer," *Journal of Machine Learning Research*, vol. 21, no.
    140, pp. 1–67, 2020.

    - [2] Q. Wen *et al.*, "Transformers in time series: A survey," *arXiv preprint
    arXiv:2202.07125*, 2022.

    - [3] A. Vaswani *et al.*, "Attention is all you need," in *Intern. Conf. on Neural
    Information Processing Systems (NIPS)*, 2017, p. 6000–6010.

    - [4] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The longdocument transformer,"
    *arXiv preprint arXiv:2004.05150*, 2020.

    - [5] R. Child *et al.*, "Generating long sequences with sparse transformers,"
    *arXiv preprint arXiv:1904.10509*, 2019.

    - [6] A. Katharopoulos *et al.*, "Transformers are rnns: Fast autoregressive transformers
    with linear attention," in *Intern. conference on machine learning*, 2020, pp.
    5156–5165.

    - [7] S. Wang *et al.*, "Linformer: Self-attention with linear complexity," *arXiv
    preprint arXiv:2006.04768*, 2020.

    - [8] T. J. Ham *et al.*, "A3: Accelerating attention mechanisms in neural networks
    with approximation," in *IEEE Intern. Symp. on High-Performance Computer Architecture
    (HPCA)*, 2020, p. 328–341.

    - [9] H. Jang *et al.*, "Mnnfast: a fast and scalable system architecture for
    memory-augmented neural networks," in *Intern. Symp. on Computer Architecture
    (ISCA)*, 2019, p. 250–263.

    - [10] Z. Wang, G. Wang, and G. He, "COSA plus: Enhanced co-operative systolic
    arrays for attention mechanism in transformers," *IEEE Trans. on Computer-Aided
    Design of Integrated Circuits and Systems (TCAD)*, vol. 44, no. 2, p. 723–736,
    2025.

    - [11] J. R. Stevens *et al.*, "Softermax: Hardware/software co-design of an efficient
    softmax for transformers," in *ACM/IEEE Design Automation Conference (DAC)*, 2021,
    pp. 469–474.

    - [12] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast ´ and
    memory-efficient exact attention with IO-awareness," *Advances in neural information
    processing systems*, vol. 35, pp. 16 344–16 359, 2022.

    - [13] T. Dao, "Flashattention-2: Faster attention with better parallelism and
    work partitioning," *arXiv preprint arXiv:2307.08691*, 2023.

    - [14] M. N. Rabe and C. Staats, "Self-attention does not need O(n 2 ) memory,"
    *arXiv preprint arXiv:2112.05682*, 2021.

    - [15] K. Alexandridis, https://github.com/ic-lab-duth/Fused-ExpMul.git, 2025.

    - [16] A. Wang *et al.*, "Glue: A multi-task benchmark and analysis platform for
    natural language understanding," *arXiv preprint arXiv:1804.07461*, 2018.

    - [17] B. Keller *et al.*, "A 95.6-TOPS/W deep learning inference accelerator
    with per-vector scaled 4-bit quantization in 5 nm," *IEEE Journal of Solid-State
    Circuits*, vol. 58, no. 4, p. 1129–1141, 2023.

    - [18] S. Lu *et al.*, "Hardware accelerator for multi-head attention and positionwise
    feed-forward in the transformer," in *IEEE Intern. System-on-Chip Conference (SOCC)*,
    2020, pp. 84–89.

    - [19] S. Sridharan *et al.*, "X-former: In-memory acceleration of transformers,"
    *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, vol. 31, no.
    8, pp. 1223–1233, 2023.

    - [20] T. J. Ham *et al.*, "ELSA: Hardware-software co-design for efficient, lightweight
    self-attention mechanism in neural networks," in *Intern. Symp. on Computer Architecture
    (ISCA)*, 2021, p. 692–705.

    - [21] Z. Song *et al.*, "TSAcc: An efficient tempo-spatial similarity aware accelerator
    for attention acceleration," in *ACM/IEEE Design Automation Conference*, 2024.

    - [22] A. Marchisio *et al.*, "Swifttron: An efficient hardware accelerator for
    quantized transformers," in *Intern. Joint Conference on Neural Networks (IJCNN)*,
    2023, pp. 1–9.

    - [23] M. Milakov and N. Gimelshein, "Online normalizer calculation for softmax,"
    *arXiv preprint arXiv:1805.02867*, 2018.

    - [24] K. Alexandridis and G. Dimitrakopoulos, "Online alignment and addition
    in multiterm floating-point adders," *IEEE Trans. on VLSI Systems*, vol. 33, no.
    4, pp. 1182–1186, 2025.

    - [25] N. A. Koca *et al.*, "Hardware-efficient softmax approximation for selfattention
    networks," in *Intern. Symp. on Circuits and Systems (ISCAS)*, 2023, p. 1–5.

    - [26] W. Wang *et al.*, "SOLE: hardware-software co-design of softmax and layernorm
    for efficient transformer inference," in *IEEE/ACM Intern. Conference on Computer
    Aided Design (ICCAD)*, 2023, p. 1–9.

    - [27] M. E. Sadeghi *et al.*, "Peano-vit: Power-efficient approximations of nonlinearities
    in vision transformers," in *ACM/IEEE Intern. Symposium on Low Power Electronics
    and Design (ISLPED)*, 2024, pp. 1–6.

    - [28] J. Koenig *et al.*, "A hardware accelerator for computing an exact dot

    - product," in *IEEE Symp. on Comp. Arith. (ARITH)*, 2017, pp. 114–121. [29] D.
    Kalamkar *et al.*, "A study of bfloat16 for deep learning training,"

    - *arXiv preprint arXiv:1905.12322*, 2019.

    - [30] K. Zhu *et al.*, "Promptbench: A unified library for evaluation of large
    language models," *arXiv preprint arXiv:2312.07910*, 2023.

    - [31] S. Kim *et al.*, "I-bert: Integer-only bert quantization," in *International
    conference on machine learning*. PMLR, 2021, pp. 5506–5518.'
- title: Distributed quantum computing with black-box subroutines
  abstract: 'In this work, we propose a general protocol for distributed quantum computing

    that accommodates arbitrary unknown subroutines. It can be applied to scale up

    quantum computing through multi-chip interconnection, as well as to tasks such

    as estimating unknown parameters or processes for circuit depth reduction and

    constructing secure quantum cryptographic protocols. Our protocol builds upon
    a

    few techniques we develop, such as the oblivious quantum teleportation and

    control, which can circumvent quantum no-go theorems on the manipulation of

    unknown objects. Furthermore, we demonstrate that this protocol can be

    physically implemented using currently available quantum computing platforms.

    These results suggest that our framework could provide a foundation for

    developing more advanced quantum algorithms and protocols in the future.'
  url: http://arxiv.org/abs/2505.14519v1
  keywords: ''
  document: '# Distributed quantum computing with black-box subroutines


    Xiang Xu,<sup>1</sup> Yuan-Dong Liu,<sup>1</sup> Sha Shi,<sup>2</sup> Yun-Jiang
    Wang,<sup>2</sup> and Dong-Sheng Wang1, [∗](#page-0-0)


    1 Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing 100190,
    China


    School of Physical Sciences, University of Chinese Academy of Sciences, Beijing
    100049, China


    <sup>2</sup>School of Telecommunication Engineering, Xidian University, Xi''an,
    Shann Xi 710071, China


    Guangzhou Institute of Technology, Xidian University, Guangzhou 510555, China


    Hangzhou Institute of Technology, Xidian University, Hangzhou, Zhejiang 311231,
    China


    (Dated: May 21, 2025)


    In this work, we propose a general protocol for distributed quantum computing
    that accommodates arbitrary unknown subroutines. It can be applied to scale up
    quantum computing through multi-chip interconnection, as well as to tasks such
    as estimating unknown parameters or processes for circuit depth reduction and
    constructing secure quantum cryptographic protocols. Our protocol builds upon
    a few techniques we develop, such as the oblivious quantum teleportation and control,
    which can circumvent quantum no-go theorems on the manipulation of unknown objects.
    Furthermore, we demonstrate that this protocol can be physically implemented using
    currently available quantum computing platforms. These results suggest that our
    framework could provide a foundation for developing more advanced quantum algorithms
    and protocols in the future.


    #### <span id="page-0-1"></span>I. INTRODUCTION


    Since the initial establishment of universal quantum computing criteria more than
    twenty years ago, the field has witnessed remarkable advancements in experimental
    quantum computing platforms [\[1\]](#page-13-0). Currently, substantial challenges
    still exist for improving the quality of logical qubits, scaling up multi-qubit
    quantum chips, and developing more superior quantum algorithms and protocols.


    The primary universal quantum computing model is the circuit model, in which a
    quantum computing task often involves the preparation of initial states, the execution
    of a sequence of quantum gates, and the final measurements for readout [\[2\]](#page-13-1).
    Depending on how these steps are realized, more diverse models and protocols have
    been developed, expanding the application scenario of quantum computing [\[3\]](#page-13-2).
    To name a few, there are distributed quantum computing which can combine the computing
    power of a few smaller quantum chips or servers [\[4,](#page-13-3) [5\]](#page-13-4),
    blind quantum computing which can realize a user''s private task by a remote quantum
    server [\[6\]](#page-13-5), and quantum von Neumann architecture which has a higher-level
    of integration of modular units [\[7–](#page-13-6)[10\]](#page-13-7). These advanced
    architectural paradigms provide both the theoretical foundation and practical
    motivation for our current investigation.


    The focus of our study is distributed quantum computing, which is believed to
    be a vital path to scale up quantum chips [\[4,](#page-13-3) [5\]](#page-13-4).
    There are inspiring progresses in recent years, including distributed quantum
    algorithms [\[11–](#page-13-8)[15\]](#page-13-9), techniques to break circuits
    into smaller blocks such as the circuit knitting [\[16–](#page-13-10)[18\]](#page-13-11),
    and also experimental efforts to develop the interface among different types of
    qubits [\[19,](#page-13-12) [20\]](#page-13-13). However, to the best of our knowledge,
    there are two important aspects which have not been well studied. First, compared
    with the usual circuit model describing a single chip, a massive amount of communication
    is required, and generic strategies to reduce the communication cost are less
    explored. Second, the remote control or execution of unknown subroutines is necessary
    and sometimes unavoidable, and this has not been studied for distributed quantum
    computing.


    The paradigm of computation with ignorance is not only widespread but also fundamentally
    important. For instance, in estimation and learning tasks [\[21\]](#page-13-14),
    an unknown parameter or process exists a priori, and a computing scheme needs
    to be able to manipulate it. For the design of computing architecture and programming
    [\[22\]](#page-13-15), programs shall be stored and loaded on-demand as subroutines,
    and there is no need to decode a program in order to load it. In cryptography
    and secure multi-party computation [\[23\]](#page-13-16), tasks often require
    protecting sensitive information from dishonest participants.


    In literature, there is another term that describes such a feature of ignorance,
    which is "oblivious." Still, there are different usages of this term and here
    we classify two types: the weak and strong notions. For the weak notion, a quantum
    operation or protocol is considered oblivious when its execution does not depend
    on specific knowledge of its operand. Examples include the oblivious amplitude
    amplification algorithm [\[24\]](#page-13-17) and an approach of oblivious logical
    wire for measurement-based quantum com-


    <span id="page-0-0"></span><sup>∗</sup> [wds@itp.ac.cn](mailto:wds@itp.ac.cn)


    puting [\[25\]](#page-13-18). The strong notion would tie obliviousness to security
    as in the seminal protocol of oblivious transfer [\[23\]](#page-13-16), where
    a server (Bob) transmits two bit strings to a user (Alice), who can retrieve only
    one of them while Bob remains unaware of Alice''s choice. In this work, we employ
    the weak notion of obliviousness, but will also show a potential connection with
    the strong one.


    It is not easy to achieve obliviousness for quantum computing, even the weak type.
    A few no-go theorems [\[7,](#page-13-6) [26](#page-13-19)[–32\]](#page-13-20)
    exist for various tasks, but fortunately, there are strategies to circumvent them.
    Using quantum meanings for oblivious transfer is not secure [\[28,](#page-13-21)
    [29\]](#page-13-22), while under reasonable assumptions, such as the noisy storage
    model [\[33\]](#page-13-23), it can be information-theoretically secure. To use
    stored quantum program states, it was shown that [\[7\]](#page-13-6) the program
    state has to be classical, otherwise, an unknown program cannot be downloaded,
    analogous to the no-cloning theorem [\[26,](#page-13-19) [27\]](#page-13-24).
    Also different from classical data, unknown quantum states or gates cannot be
    added together [\[30](#page-13-25)[–32\]](#page-13-20), due to the unphysical
    meaning of global phase of a state or gate. Here we propose the oblivious quantum
    teleportation and control schemes that improve previous bypass strategies [\[34\]](#page-13-26).


    In this work, we develop a universal distributed quantum computing protocol with
    black-box subroutines based on a few primary oblivious quantum schemes. Our protocol
    has a few features. First and foremost, it is oblivious hence it allows application
    settings with unknown information. For instance, it can be used to construct secure
    protocols. Second, it allows a high level of integration and quantum programming,
    which in general employs quantum operations to manipulate quantum operations,
    using the theoretical framework of quantum superchannel [\[35\]](#page-13-27).
    Third, the remote execution of blackbox subroutines also reduce the communication
    cost as there is no need to upload or download the information encoded in the
    subroutines. Forth, it offers a flexible way to change the circuit depth by using
    a space-time tradeoff to realize teleportation. Therefore, we believe our protocol
    can be implemented on current quantum chips and also used for developing more
    advanced quantum algorithms.


    In Section [II](#page-1-0) we start from a primary task for the storage of quantum
    program and its oblivious execution scheme. In Section [III](#page-2-0) we present
    oblivious quantum teleportation, and in Section [IV](#page-4-0) we present oblivious
    quantum control scheme. In Section [V](#page-4-1) we study a few more oblivious
    quantum algorithms, which serve as founda-


    ### <span id="page-1-0"></span>II. OBLIVIOUS PROGRAM EXECUTION


    software development, and application protocols.


    In this section, we start from a primary oblivious quantum operation, which is
    to obliviously load a quantum program. This is based on Refs. [\[8,](#page-13-28)
    [34,](#page-13-26) [36\]](#page-13-29), and in this work we refer to this task
    as oblivious program execution (OPE).


    Quantum processes are described as completely positive, trace-preserving (CPTP)
    maps [\[37\]](#page-14-0) of the form


    $$\mathcal{E}(\rho) = \sum\_{i} K\_{i} \rho K\_{i}^{\dagger},\tag{1}$$


    also known as quantum channels, for K<sup>i</sup> known as Kraus operators and
    input states ∀ρ ∈ D(H) for a system H. This includes unitary evolution and measurements
    as special cases, which are the two essential parts for constructing quantum algorithms.
    The Stinespring dilation theorem shows that a quantum channel can be realized
    by a unitary circuit U with the final partial trace over an ancilla, and


    $$K\_i = \langle i | U | 0 \rangle \tag{2}$$


    for {|i⟩} denoting an orthonormal basis of the ancilla. Therefore, we can focus
    on the unitary case for simplicity.


    In the usual setting, a quantum computing process or algorithm includes the preparation,
    evolution, and measurement steps, and also possibly transmission of quantum states.
    The main part of a quantum algorithm, or called a program, is a unitary circuit.
    Besides processing, the storage of programs is also essential. Currently, there
    are mainly three types. A circuit U can be stored as a classical description [U],
    which could be the description of the gate sequence contained in U. It can also
    be stored as a hardware, denoted as H(U), which particularly suits the photonic
    platform. This form is not scalable, however, as will be studied in details in
    Sec. [VI.](#page-6-0)


    Given [U], the circuit U can be executed, obviously. This is the opposite of being
    oblivious. The third type is the Choi state


    $$|U\rangle = (U \otimes \mathbb{1}) |\omega\rangle,\tag{3}$$


    according to the channel-state duality [\[38,](#page-14-1) [39\]](#page-14-2)
    for the Bell state or ''ebit'' as |ω⟩ := P i |ii⟩/ √ d, d = dim(H). The form |U⟩
    allows OPE via a measurement-based scheme.Namely, it supports the write of initial
    state and the readout of observable. This also extends to the mixed-state case,
    shown in Appendix [A.](#page-11-0) For simplicity, denote the initial state as
    |0⟩, and the goal is to obtain the probabilities


    $$p\_a = \left| \langle a | U | 0 \rangle \right|^2 \tag{4}$$


    for {|a⟩⟨a|} as the projective measurement for readout. Here, the channel-state
    duality is involved to inject the initial state |0⟩ by a binary measurement {P0,
    P¯0} for P¯0 = 1 − P0, and P<sup>0</sup> = |0⟩⟨0|. Both outcomes work: for the
    case of P0, p<sup>a</sup> is obtained; for the case of P¯0 , p ′ <sup>a</sup>
    = 1−p<sup>a</sup> is obtained. This binary projective measurement is referred
    to as the initial-state injection (ISI) scheme [\[8\]](#page-13-28), see Fig.
    [1.](#page-2-1) Despite the simplicity of it, the ISI scheme also includes an
    indirect binary Bell measurement as a special case, which plays the central role
    for the construction of oblivious quantum teleportation in Sec. [III.](#page-2-0)
    The physical realization of ISI will be discussed in Sec. [VI.](#page-6-0)


    We recall that the Choi-state program was motivated by the no-programming theorem
    [\[7\]](#page-13-6), according to which, for any possible form of quantum program
    state |p<sup>U</sup> ⟩, the gate U cannot be executed as


    <span id="page-2-3"></span>

    $$G|\psi\rangle|p\_U\rangle = U|\psi\rangle|p''\_U\rangle,\tag{5}$$


    for G independent of |ψ⟩ and |p<sup>U</sup> ⟩. The reason is that ⟨p<sup>V</sup>
    |p<sup>U</sup> ⟩ = 0, ∀U ̸= V , i.e., the states |p<sup>U</sup> ⟩ are all orthogonal
    hence classical (e.g., [U]). The encoding |p<sup>U</sup> ⟩ of U acts like an encryption,
    and the downloading of U is physically equivalent to a decryption and cloning
    of it [\[9,](#page-13-30) [34\]](#page-13-26). We bypass this no-go result by
    requiring a weaker condition that only observation on U|ψ⟩ is needed, instead
    of the whole state. After all, it is the measurement outcomes that are needed.
    This does not sacrifice universality, since it can be equivalently formalized
    for gate simulation, state generation, or observable measurement [\[40\]](#page-14-3).


    The state |U t ⟩ can also be obtained by exchanging the two ports of |U⟩ due to
    the vectorization property


    $$(A \otimes \mathbb{1})|\omega\rangle = (\mathbb{1} \otimes A^t)|\omega\rangle
    \tag{6}$$


    ![](_page_2_Figure_10.jpeg)


    Figure 1. The Choi program state and the initial-state injection (ISI) scheme.
    The curve represents a Bell state, i.e., ebit.


    <span id="page-2-1"></span>![](_page_2_Figure_12.jpeg)


    <span id="page-2-2"></span>Figure 2. The standard quantum teleportation by Bell
    measurement (BM) (left) and the oblivious quantum teleportation by the indirect
    binary Bell measurement (BBM) (right).


    for any operator A ∈ B(H). Given an unknown U, it is not easy to obtain U † ,
    however [\[41,](#page-14-4) [42\]](#page-14-5). The simplest scheme is to prepare
    both |U⟩ and |U † ⟩ on the first hand. Beyond that, we can employ the rebit approach
    [\[43\]](#page-14-6) to convert U into an orthogonal operator


    $$Q = \begin{pmatrix} U\_1 & -U\_2 \\ U\_2 & U\_1 \end{pmatrix},\tag{7}$$


    and convert input state |ψ⟩ into |Φ⟩ = |R⟩|0⟩ + |I⟩|1⟩ for the real-imaginary
    part separation U = U<sup>1</sup> + iU<sup>2</sup> and |ψ⟩ = |R⟩ + i|I⟩. For any
    observable A = P <sup>a</sup> Aa|a⟩⟨a|, the measurement probability on the final
    state is


    $$p\_a \ = \text{tr}\left( (|a\rangle\langle a| \otimes \mathbb{1})Q|\Phi\rangle\langle\Phi|Q^\dagger\right)
    \tag{8}$$


    $$=\operatorname{tr}\left((|a\rangle\langle a|)U|\psi\rangle\langle\psi|U^{\dagger}\right),\tag{9}$$


    that is, the statistics is the same as the original one. This will also be used
    to construct the quantum control unit in Sec. [IV.](#page-4-0)


    ### <span id="page-2-0"></span>III. OBLIVIOUS QUANTUM TELEPORTATION


    In this section, we introduce the oblivious quantum teleportation (OQT), which
    can achieve the oblivious composition of programs U<sup>n</sup> · · ·U2U1. This
    is an alternative of the previous universal quantum teleportation [\[8\]](#page-13-28),
    which is not oblivious, and can also be viewed as a special ISI scheme but plays
    a distinct role.


    In the standard quantum teleportation (QT) [\[44\]](#page-14-7), an unknown state
    |ψ⟩ of a system S is teleported to another system B by the Bell measurement MAS(i)
    according to


    $$|\psi\rangle\_{\rm B} = \sigma\_{i,\rm B} M\_{\rm AS}(i) |\omega\rangle\_{\rm
    AB} |\psi\rangle\_{\rm S},\tag{10}$$


    with Pauli byproducts σi,<sup>B</sup> being corrected and consuming an ebit |ω⟩AB
    as resource. See Fig. [2\(](#page-2-2)left). In a certain sense, it is oblivious
    since the teleportation circuit (i.e. Bell measurement) is independent of the
    input state |ψ⟩.


    For quantum gate teleportation [\[45\]](#page-14-8), however, the scheme depends
    on the teleported gate U, i.e., it is not oblivious. The original approach is
    to classify gates as the Clifford hierarchy


    $$\mathcal{C}\_n = \{ U : UPU^\dagger \in \mathcal{C}\_{n-1} \} \tag{11}$$


    for all P in the Pauli group C1, and use lower-level gates to realize higher-level
    gates, while the Pauli byproduct depends on the teleported gate. For instance,
    in the setting of stabilizer codes the so-called T gate can be realized by gate
    teleportation [\[46\]](#page-14-9). Recently, by using the large symmetry in teleportation
    a universal gate teleportation scheme is found [\[8\]](#page-13-28), for which
    the byproduct depends on the teleported gate, and the byproduct are not Pauli
    operators anymore. In the light of the noprogramming theorem [\[7\]](#page-13-6),
    gate teleportation is possible since the teleportation depends on an input program
    U.


    We can achieve oblivious gate teleportation by a slight modification of Bell measurement.
    The idea is to construct a special type of ISI scheme with the desired injected
    state as |ω⟩. Namely, by grouping the Pauli byproduct into two sets: the trivial
    one σ<sup>0</sup> = 1 and the nontrivial set {σi} (for i ̸= 0), and use a Toffoli-type
    gate to extract the parity information of being trivial or nontrivial to a qubit
    ancilla, the projective measurement on which with


    $$P\_0 = |\omega\rangle\langle\omega|,\ P\_1 = \mathbb{1} - |\omega\rangle\langle\omega|\tag{12}$$


    will realize either an identity or a channel P formed by equal-weight σ<sup>i</sup>
    . There is no need to correct the Pauli byproduct. See Fig. [2\(](#page-2-2)right).
    The channel P relates to the completely depolarizing channel ∆ [\[2\]](#page-13-1)
    by deleting its identity operator. Then it is clear to see the oblivious teleportation
    of U onto input |ψ⟩ realizes two outcomes


    $$P\_0: U|\psi\rangle;\ P\_1: \frac{1}{d^2 - 1} \left(d\mathbb{1} - U\psi U^\dagger\right),\qquad(13)$$


    for ψ := |ψ⟩⟨ψ|. This also easily extends to the mixed


    state case. The offset due to 1 for the outcome P<sup>1</sup> can be easily dealt
    with for observable measurement.


    More importantly, it extends to a sequence of teleportation of U<sup>i</sup> .
    The outcome does not depend on the detailed order of the outcomes, but only on
    the number of 0s or 1s in the outcomes. Counting the number of 1s as s, the final
    state takes two forms


    $$\text{even } n: \ \alpha \mathbf{1} + \frac{U(\psi)}{(d^2 - 1)^s},\tag{14}$$


    $$\text{odd } n: \quad \beta \mathbf{1} - \frac{U(\psi)}{(d^2 - 1)^s},\tag{15}$$


    for α and β as normalization constants, and U = · · ·U2U<sup>1</sup> as the sequence
    of gates. That is, for any measurement outcome the final state can be used to
    compute observable quantity from |⟨ψout| · · ·U2U1|ψin⟩|<sup>2</sup> , hence showing
    the universality of OQT. Therefore, the OQT serves as the central protocol for
    program composition, as well as transmission.


    There are a few novel features of OQT. First, there is a temporal order for a
    sequence of teleportation due to the byproduct correction, yet there is no temporal
    order for OQT. This brings a huge advantage of parallelism. Second, teleportation
    can be used to reduce the circuit depth on a qubit, and this also carries over
    to OQT. If the physical carrier (such as an atom) is acted upon by a long sequence
    of gates, there can be heating issue or control problems. Teleporting the information
    to other fresh qubits can mitigate the issue. While teleportation requires more
    storage in the form of ebits, but if qubits can be refreshed quickly after measurement,
    then they can be reused. That is to say, rapid qubit reset and reuse can minimize
    space overhead, making OQT an efficient space-time tradeoff for quantum computation.


    We also see that the signal U(ψ) comes with a factor 1/(d <sup>2</sup> − 1)<sup>s</sup>
    . One would note that the state appears to be similar with the pseudo-pure state
    in the NMR platform [\[47\]](#page-14-10). This is not a problem in the fault-tolerant
    setting where accuracy is guaranteed, or when s is relatively small. However,
    if this is not the case extracting outcome from this signal could be hard. One
    strategy to increase the stability of the outcome, depending on the application
    tasks, is to use a hybrid of QT (including state and gate teleportation) and OQT.
    That is, only some gates are taken as black boxes in the simulated gate sequence
    · · ·U2U1. The usage of QT will introduce a temporal order hence increase the
    circuit depth. Therefore, it is an interesting task to design a teleportation
    network of QT


    ![](_page_4_Figure_0.jpeg)


    <span id="page-4-2"></span>Figure 3. The oblivious quantum control scheme: our
    method (left) and the original method with |λ⟩ as the ''flag'' (top right). The
    DQC1 algorithm (bottom right) can be viewed as an application.


    and OQT depending on practical figure of merits.


    #### <span id="page-4-0"></span>IV. OBLIVIOUS QUANTUM CONTROL


    In this section, we introduce oblivious quantum control (OQC), which is to add
    quantum control to unknown quantum gates. This task is usually known as control
    over unknown gates [\[30\]](#page-13-25), and here we coin the new name to highlight
    the role of being oblivious. A no-control theorem was also found, but different
    from the no-programming theorem, this no-go can be circumvented relatively easier
    [\[30,](#page-13-25) [34\]](#page-13-26), and here we find a scheme to achieve
    it.


    The task is to convert an arbitrary unknown unitary operation U to its controlled
    version


    $$

    \wedge\_U = P\_0 \otimes \mathbb{1} + P\_1 \otimes U,\tag{16}

    $$


    for P0, P<sup>1</sup> as qubit projectors onto state |0⟩, |1⟩. The reason for
    the no-control theorem [\[30\]](#page-13-25) is that the unphysical global phase
    of U will be converted to a physical one in ∧<sup>U</sup> . To fix this ambiguity,
    it was originally noted by Kitaev [\[48\]](#page-14-11) that if an eigenstate
    |λ⟩ and eigenvalue of U is known, serving as a ''flag'', then there is a way to
    realize this task. See Fig. [3.](#page-4-2) Such a ''flag'' condition appears
    simple but could be hard to realize, however. For instance, finding an eigenstate
    and eigenvalue of a large U = e itH for a certain Hamiltonian H is not an easy
    task. In this section, we present a scheme that naturally satisfies this flag
    condition by extending the usage of U.


    Our scheme is based on the following fact: the ebit |ω⟩ is an eigenstate of U
    ⊗ U <sup>∗</sup> with


    $$(U \otimes U^\*) |\omega\rangle = |\omega\rangle. \tag{17}$$


    For simplicity, let''s first denote Uˆ := U ⊗U <sup>∗</sup> and consider the task
    <sup>U</sup><sup>ˆ</sup> 7→ ∧U<sup>ˆ</sup> . This can be achieved since the ebit
    <sup>|</sup>ω⟩ serves as the flag. See Fig. [3\(](#page-4-2)left). The input for
    the data register is a product state ρ ⊗ η with η as the ancillary state, so that
    it is clear to see U 7→ ∧<sup>U</sup> is realized in the proper subspace (for
    c and ρ). The freedom of choosing η can be used in quantum algorithms.


    In our setting, the gate U can be given as a Choi state |U⟩. The flag state |ω⟩
    can be realized via ISI scheme. Observing that


    $$

    \omega^{\perp} := \frac{\mathbf{1} - |\omega\rangle\langle\omega|}{d - 1} \tag{18}

    $$


    is also an eigenstate of Uˆ, so for both outcomes in the ISI scheme, the flag
    condition is satisfied.


    Our scheme of oblivious quantum control (OQC) can be generalized to high-dimensional
    control to realize a so-called multiplexer


    $$U = \sum\_{i} P\_i \otimes U\_i,\tag{19}$$


    for P<sup>i</sup> as qudit projectors. A straightforward scheme is to decompose
    a multiplexer as a sequence of binary control as above, and then realize each
    of them. We also see that both U and U <sup>∗</sup> are needed. This can be satisfied
    by providing them on the first hand, or use the rebit approach as we have discussed
    in Sec. [II.](#page-1-0)


    ### <span id="page-4-1"></span>V. OBLIVIOUS QUANTUM ALGORITHMS


    In this section, we study a few oblivious quantum algorithms that take OQT and
    OQC as primitives, and these algorithms can be further used to construct more
    complicated algorithms or protocols.


    Quantum algorithms fundamentally operate by manipulating the amplitudes and phases
    of quantum states. Therefore, here we study the oblivious computation of state
    overlaps, phase estimation, amplitude amplification, and generation of superposition.
    Among these, the oblivious amplitude amplification (OAA) [\[24\]](#page-13-17)
    has been well established, and it actually motivates our study of oblivious version
    of quantum algorithms.


    ### A. Oblivious DQC1


    The original DQC1 algorithm [\[47\]](#page-14-10), i.e., deterministic quantum
    computing with one qubit, relies on controlled operation to evaluate trU for a
    unitary operator U. See Fig. [3,](#page-4-2) bottom right. It is closely related
    to the Hadamard test and can also be used to compute overlap between states. When
    the input ρ is a pure state |ψ⟩, the state before the Pauli X measurement M<sup>X</sup>
    is


    $$|0\rangle \frac{\mathbf{1} + U}{2} |\psi\rangle + |1\rangle \frac{\mathbf{1}
    - U}{2} |\psi\rangle. \tag{20}$$


    The norm of <sup>1</sup>+<sup>U</sup> 2 |ψ⟩ is the probability p<sup>0</sup> for
    outcome 0, which is


    $$p\_0 = \frac{1}{2} (1 + \langle \psi | \text{Re} U | \psi \rangle). \tag{21}$$


    With the same circuit, but for the Pauli Y measurement M<sup>Y</sup> , it is easy
    to obtain the corresponding probability


    $$p\_0 = \frac{1}{2}(1 + \langle \psi | \text{Im} U | \psi \rangle). \tag{22}$$


    From the probabilities above, the value of the overlap ⟨ψ|U|ψ⟩ can be obtained.
    Choosing the input |ψ⟩ as a mixed state ρ will produce tr(U ρ), and the trace
    trU is obtained for the completely mixed state 1/d, in particular. When |ψ⟩ is
    an eigenstate of U, ⟨ψ|U|ψ⟩ is the eigenvalue. This extends to Kitaev''s quantum
    phase estimation (QPE) [\[2,](#page-13-1) [48\]](#page-14-11), and is used for
    Shor''s algorithm [\[49\]](#page-14-12) and many others.


    With the oblivious quantum control scheme, we can obtain the oblivious DQC1 algorithm.
    The unitary U can be unknown or of a very large size. The controlled module ∧<sup>U</sup>
    is performed obliviously and what is evaluated is the value of the product tr(U
    ρ)tr(U <sup>∗</sup>η), instead of tr(U ρ). The ancillary state η can be chosen
    to eliminate a factor, e.g., by setting it as |0⟩ and the factor tr(U <sup>∗</sup>η)
    becomes ⟨0|U ∗ |0⟩, which could be easy to compute in practice. That is, U is
    not completely unknown but the available information is vanishingly limited.


    When both |ψ⟩ and U are unknown, this algorithm can compute ⟨ψ|U|ψ⟩. Denote |ϕ⟩
    = U|ψ⟩, this becomes ⟨ψ|ϕ⟩. However, for two unknown states |ψ⟩ and |ϕ⟩, there
    is no algorithm to compute ⟨ψ|ϕ⟩ since there is an ambiguity of the global phase.
    The ODQC1 algorithm avoid this since U is not completely unknown, and the relation
    |ϕ⟩ = U|ψ⟩ diminishes such an ambiguity. A closely related algorithm is the SWAP
    test [\[50\]](#page-14-13), which can obliviously compute |⟨ψ|ϕ⟩|<sup>2</sup>
    , instead of ⟨ψ|U|ψ⟩. For the case of two Choi states |U1⟩ and |U2⟩, this becomes
    |tr(U † <sup>1</sup>U2)| 2 . Actually, we can use teleportation to achieve probabilistic
    generation of |U † <sup>1</sup>U2⟩ from |U1⟩ and |U2⟩, and the ODQC1 algorithm
    can compute the value tr(U † <sup>1</sup>U2).


    ![](_page_5_Figure_9.jpeg)


    <span id="page-5-0"></span>Figure 4. The circuit for amplitude amplification (AA)
    algorithm and also OAA algorithm.


    #### B. Oblivious amplitude amplification


    The oblivious amplitude amplification (OAA) [\[24\]](#page-13-17) has been developed
    as the oblivious extension of amplitude amplification (AA) [\[51\]](#page-14-14),
    and serves as an important primitive for oblivious quantum computing. Combined
    with quantum phase estimation (QPE), it can also perform amplitude estimation.
    Here we review its content, and present a scheme that combines OAA with OQT to
    reduce the circuit depth.


    In order to obtain the action of a unitary U on data state |ψ⟩, a unitary circuit
    G acting on a control system and a data system with


    $$G|0\rangle|\psi\rangle = \sqrt{p}|0\rangle U|\psi\rangle + \sqrt{1-p}|\Phi''\rangle\tag{23}$$


    is constructed, for a probability parameter p ∈ (0, 1) which is independent of
    |ψ⟩, and ⟨0|Φ ′ ⟩ = 0. The goal is to prompt p close to 1. Treating G as the dilation
    of a channel, the operator <sup>√</sup>pU is a Kraus operator, and it is unnecessary
    to know other Kraus operators, i.e., the form of |Φ ′ ⟩.


    The OAA constructed a walk operator W = −GRG†R for R acting on the control unit
    and R = 2Π − 1 with Π = P<sup>0</sup> ⊗1. For instance, R is the Pauli Z operator
    for a qubit controller. Let <sup>√</sup><sup>p</sup> = sin <sup>θ</sup> for <sup>θ</sup>
    <sup>∈</sup> (0, π 2 ), it proves that


    $$W^n G|0\rangle|\psi\rangle = \sin[(2n+1)\theta]|0\rangle U|\psi\rangle + \cos[(2n+1)\theta]|\Phi''\rangle,\tag{24}$$


    and for n ∈ O(1/ <sup>√</sup>p) the success rate can be boosted arbitrarily close
    to 1 to realize U|ψ⟩. See Fig. [4.](#page-5-0) In the case that p is unknown on
    the first hand, QPE and its oblivious version can be used to estimate p, which
    can henceforth be amplified.


    It is also insightful to put OAA in the context of quantum programming (Eq. [\(5\)](#page-2-3)).
    The program U is not prestored as a state, instead, it is stored in the circuit
    operations G and W. Therefore, it can generate U|ψ⟩ obliviously without violating
    the no-programming theorem [\[7\]](#page-13-6).


    The OAA and also standard AA increase the circuit depth significantly by using
    iterative walk operators. This can be reduced to the original one (as G) by using
    OQT if we only need observable measurement on state U|ψ⟩, instead of the state
    itself. The operators G, GR, and G†R can be stored as Choi program states, and
    OQT on them can compose them together. This needs O( <sup>√</sup>p) program states,
    and this is quadratically better than an incoherent method, which measures the
    ancilla and repeat if a failure occurs.


    Furthermore, the quantum singular-value transform (QSVT) algorithm has been developed
    as a generalization of AA [\[52\]](#page-14-15), which is the special case of
    only one singular value, the parameter p. The QSVT algorithm converts a matrix
    A with the singular-value transformation A = WΣV † to another matrix B = W P(Σ)V
    † for P(Σ) denoting a proper polynomial function of Σ. The structure of QSVT is
    similar with AA, hence we can also use OQT to reduce its circuit depth by using
    stored programs for the unitary U that encodes A and the reflection operators.


    #### C. Oblivious quantum superposition


    Combining the primitives above and also the linear combination of unitary (LCU)
    operation algorithm [\[24,](#page-13-17) [53,](#page-14-16) [54\]](#page-14-17),
    we can realize oblivious quantum superposition (OQS) of unknown quantum states
    under proper conditions. This does not violate the no-go theorem [\[31\]](#page-13-31)
    since the states are not completely unknown. We present two schemes with one relying
    on AA while the other relying on OAA, while both of them rely on oblivious LCU
    that combines standard LCU and OQC.


    The LCU can be viewed as an extension of the DQC1 circuit with a multiple control
    of a set of gates U<sup>i</sup> , i.e., a multiplexer. By expressing an operator
    C = P i ciU<sup>i</sup> as a superposition of other unitary gates U<sup>i</sup>
    , the LCU circuit in general realizes C for a success probability


    $$p = \langle \psi | C^{\dagger} C | \psi \rangle \tag{25}$$


    and C = P <sup>i</sup> αiβiU<sup>i</sup> , and A|0⟩ = P <sup>i</sup> α<sup>i</sup>
    |i⟩, B|i⟩ = P j βij |j⟩, and β<sup>i</sup> := βi0. The coefficients can be chosen
    to satisfy c<sup>i</sup> = αiβ<sup>i</sup> . Given U<sup>i</sup> as oracles, the
    LCU circuit becomes oblivious. The walk operator in AA or OAA will also use U
    † i , and this can be resolved by the methods we presented for OQC in Sec. [IV.](#page-4-0)


    For the first scheme of oblivious superposition, the task is to generate a state
    |ψ⟩ ∝ P i ci |ψi⟩ with given known superposition coefficients c<sup>i</sup> while
    unknown set of states


    {|ψi⟩}, but |ψi⟩ = U<sup>i</sup> |0⟩ for a fixed state |0⟩. As the state |0⟩ is
    known, the success probability can be boosted by AA algorithm.


    For the second scheme, the task is to apply an oblivious LCU circuit C = P i ciU<sup>i</sup>
    on an unknown state. In this case, we have to request C being proportional to
    a unitary operator, C = <sup>√</sup>pU, and then this fits into the framework
    of OAA, which can be used to boost the success probability p close to 1.


    #### <span id="page-6-0"></span>VI. PHYSICAL REALIZATIONS


    In this section, we study the physical realizations of OQT, OQC, and others. We
    assess the main cost of obliviousness by comparing to their non-oblivious counterparts
    of the algorithms above, and layout the technical requirements on current quantum
    platforms.


    #### A. Cost of obliviousness


    There are common features for the primitive algorithms above, and we summarize
    them as follows:


    - The OPE, OQT schemes: they need an indirect binary projective measurement, which
    needs a Toffoli-type gate, also known as a multiple-control Toffoli gate.

    - The OQC, ODQC1, OQS schemes: they need the controlled-swap gate, also known
    as Fredkin gate, which can be further realized by Toffoli-type gates.

    - The OAA, OQS schemes: it needs a longer circuit which means more Fredkin gates.
    Using OQT to reduce the circuit depth would convert the cost to more ebits and
    also Toffoli-type gates.


    Compared with an obvious preparation of a state |ψ⟩, the action of U, and the
    measurement of an observable on U|ψ⟩, the central requirements for the oblivious
    version are the ebits and Toffoli-type gates, besides ancillary qubits. The costs
    shall be clear from the details of the algorithms studied above, and below we
    further study the cost for realizing Toffoli-type gates. Such a gate with m control
    lines and a single target is often denoted as C <sup>m</sup>NOT.


    The standard Toffoli gate is a controlled CNOT gate C<sup>2</sup>NOT, with CNOT
    as the controlled Pauli X gate. There are methods to directly realize Toffoli-type


    ![](_page_7_Figure_0.jpeg)


    <span id="page-7-0"></span>Figure 5. Circuit to realize the Toffoli gate with
    three CNOTs when it is used for measurement. The gate G is the rotation R<sup>Y</sup>
    ( π 4 ) around Y axis for <sup>π</sup> 4 angle.


    gates [\[55–](#page-14-18)[57\]](#page-14-19), which will significantly reduce
    the circuit depth. However, these techniques are still not mature yet. Instead,
    one can use gate compiling technique to decompose a Toffoli-type gate, and in
    general, a multicontrol gate [\[58,](#page-14-20) [59\]](#page-14-21). Such circuits
    are efficient and using ancillary qubits can even reduce the circuit cost. The
    circuit cost gets smaller if a Toffoli-type gate sits at the boundary of a circuit.
    As shown in Fig. [5,](#page-7-0) the Toffoli gate can be realized by qubit gates
    and three CNOTs [\[60\]](#page-14-22), instead of six, if it is used for the ISI
    and OQT measurement schemes.


    There is a systematic way to construct the OQT. A high-dimensional OQT can be
    constructed from lowdimensional ones. For instance, a two-partite OQT is shown
    in Fig. [6.](#page-7-1) The total parity k = i · j is from the local parities
    i and j, and this construction easily extends to more general settings. The cost
    is efficient with respect to the number of local parities, i.e. the circuit width.


    Alternatively, this measurement procedure for OQT can be simplified by using more
    samples. For example, the two-partite OQT in Fig. [6](#page-7-1) merely realizes
    a measurement with projectors P<sup>00</sup> and P00¯ , for P<sup>00</sup> = P0P<sup>0</sup>
    and P00¯ as its complement, while here 0 denotes a local parity information. This
    can be realized by local measurements {P0, P¯0} but with more runs to collect
    each measurement result, and finally gather the results as two sets for P<sup>00</sup>
    and P00¯ , respectively. It is easy to see, however, the number of samples grows
    exponentially with the number of local parts. So this scheme only works for relatively
    low-dimensional cases. Also note that the two-qubit OQT can also be simulated
    by the usual twoqubit Bell measurement (see Fig. [2\)](#page-2-2), but only recording
    the Pauli byproduct σ<sup>i</sup> without correcting them. This also requires
    more samples to gather the result for each byproduct σ<sup>i</sup> .


    ![](_page_7_Figure_5.jpeg)


    <span id="page-7-1"></span>Figure 6. High-dimensional indirect binary Bell measurement
    for OQT. The circuit shown contains two local parts, which can be extended to
    the multi-party cases. We do not specify the ancillary states for simplicity.


    ### B. Gates and qubits


    Here we study how to realize our schemes above in currently available platforms,
    and we find there are notable differences. At present, there are a few leading
    quantum platforms with mainly two types of qubits:


    - Matter-qubit platform: such as superconducting processors, trapped ions, and
    cold atoms. The gates are generated on-demand while the qubits are hardware in
    a chip.

    - Photon-qubit platform: linear optics. The gates are hardware in a chip while
    the qubits are generated on-demand from lasers.


    For both of them states are mostly generated ondemand, since there is no self-correcting
    quantum memory yet to store qubits for long-enough time [\[61\]](#page-14-23).
    Active quantum error-correction is required to extend the coherence time. A qubit
    can be either encoded in discrete degree of freedom or in continuous variable,
    while here we do not specify the details.


    For matter-qubit platforms, primitive gates such as qubit rotations and CNOT can
    be realized deterministically. This can be used to prepare ebits and Choi program
    states. To realize OQT, the low-dimensional case on two qubits could be simpler
    without using a Toffoli gate. Such a parity measurement is to distinguish singlet
    from triplet, which is not hard to perform. For instance, for superconducting
    qubits or spin qubits in quantum dots the energy splitting ℏω of a qubit and the
    coupling strength g between qubits can be tuned, so that an energy splitting ∆
    between singlet and triplet can be induced (See Ref. [\[62\]](#page-14-24) and
    references therein). The triplet states are nearly degenerate. For high-dimensional
    case and also OQC, Toffoli-type gates need to be realized which may have smaller
    values of fidelity.


    ![](_page_8_Figure_0.jpeg)


    <span id="page-8-1"></span>Figure 7. The schematic for a distributed quantum computing
    with three participants. The dashed lines set the locality.


    To realize teleportation, the interface between matterqubit and photons may be
    needed for long-distance communication. There are available techniques, for instance,
    states of superconducting qubits can be firstly translated to microwave resonators,
    and then use electro-optic transducer to convert into photonic states [\[19,](#page-13-12)
    [20\]](#page-13-13). For short-distance situations such as within a chip, there
    are techniques to physically move qubits [\[63,](#page-14-25) [64\]](#page-14-26).


    For photon-qubit platforms, a scalable approach is the cluster-state quantum computing
    [\[65\]](#page-14-27) rather than the circuit model due to the probabilistic nature
    of entangling gates [\[66\]](#page-14-28). It realizes a gate sequence by a sequence
    of gate teleportation. The smallest cluster is just an ebit. For polarization
    or dual-rail encoded qubits, ebits are relatively easy to prepare, e.g., generated
    via SPDC scheme [\[2\]](#page-13-1) without using CNOT gates. Starting from ebits,
    a cluster state can be generated by the so-called fusion operations [\[67\]](#page-14-29).
    For hardware, the region for generation of cluster state and the region for implementation
    of measurements can be separated. These regions or chips can be connected by optical
    fibers [\[68\]](#page-14-30), for instance. Photons can be transmitted directly
    or via teleportation, which is a merit of the photon-qubit platform.


    Another notable feature is that photonic gates are hardware. This means that a
    gate U can be stored as a hardware circuit, denoted as H(U), instead of a Choi
    state |U⟩. The hardware gate H(U) makes it possible to obliviously load it directly
    acting on input photons and realize ∧<sup>U</sup> since there is a natural direct-sum
    structure of the Hilbert space [\[30\]](#page-13-25). However, this approach is
    not scalable due to the same reason for deterministically realizing entangling
    gates. For instance, although beam splitters are universal to realize any SU(N)
    operations, the number of them scales exponentially with the number of qubits
    n for N = 2<sup>n</sup> [\[69\]](#page-14-31). Therefore, in general we need to
    employ the cluster-state approach, and a quantum program |U⟩ can be generated
    by measurements on a cluster state. This requires a massive amount of classical
    side processing of measurement feedforward, and also measurement devices.


    ![](_page_8_Figure_5.jpeg)


    <span id="page-8-2"></span>Figure 8. The schematic for the realization with OQT
    or OQC for the distributed quantum computing with three participants in Fig. [7.](#page-8-1)
    Note we only show one ∧<sup>U</sup> for the OQC scheme (II) for simplicity. The
    dashed lines set the locality.


    The realization of photonic OQT also needs a cluster state to simulate the Toffoli-type
    gates in it. Beyond this general approach, we find that there could be easier
    schemes. The primary fusion measurement divides into two flavors: the type-I fusion
    which is a 1-bit teleportation [\[70\]](#page-14-32), and the type-II fusion which
    is the usual 2-bit teleportation. They both fail with 50% percent of probability.
    It is also known that the type-II fusion is from the Hong-Ou-Mandel interference
    effect [\[71\]](#page-14-33). When the type-II fusion is used to distinguish the
    anti-symmetric singlet from the symmetric triplet, without making further observations
    among the triplet states, it actually realizes the OQT on two qubits without using
    a Toffoli gate. Therefore, it remains to see if the high-dimensional OQT can be
    efficiently simulated by multi-photon interference using a network of beam splitters,
    also known as Bell multiport beam splitter [\[72,](#page-14-34) [73\]](#page-14-35),
    whose design is highly nontrivial.


    ### <span id="page-8-0"></span>VII. DISTRIBUTED QUANTUM COMPUTATION


    In this section, we apply the oblivious quantum algorithms in the setting of distributed
    quantum computing. We first study a primary protocol and then generalize it, and
    discuss more features of our protocol.


    #### A. Primary protocol


    We first consider the two-party and tri-party cases, and then generalize to the
    multi-party setting. The protocol is illustrated in Fig. [7](#page-8-1) and Fig.
    [8.](#page-8-2) The two-party case is a reduction of it. Suppose Alice and Bob
    aims to compute some observable value O on a state U|ψ⟩, but now U = UBU<sup>A</sup>
    for U<sup>A</sup> (UB) held by Alice (Bob) in the form of Choi state |UA⟩ (|UB⟩).
    The initial state |ψ⟩ is prepared by Alice, while the measurement for observable
    O is performed by Bob. In order to gather statistics, multiple runs are needed.
    The protocol looks symmetric with respect to Alice and Bob, but it is not due
    to the inputoutput separation.


    It uses communicating ebits to perform OQT. This is due to a consideration of
    hardware realization since it may be hard to directly transmit some part of the
    stored programs in the memory unit, hence two OQT actions are needed. Physically,
    the OQT may involve interactions between flying qubits such as photons and solidstate
    qubits, e.g., Refs. [\[19,](#page-13-12) [20\]](#page-13-13).


    For the tri-party case, the dimensions of the composed programs could be different.
    We find there are two schemes:


    - I): The nonlocal part is a separate station C, as in Fig. [7,](#page-8-1) panel
    I;

    - II): The nonlocal part is a nonlocal operation on A and B, as in Fig. [7,](#page-8-1)
    panel II.


    Their realizations are shown in Fig. [8.](#page-8-2) For scheme I, a concatenation
    of the parity measurements is needed, namely, the parity i for the OQT between
    |UA⟩ and |U<sup>C</sup> ⟩ and the parity j for the OQT between |UB⟩ and |U<sup>C</sup>
    ⟩ is further summed to a total parity k. This serves as the OQT between the program
    |U<sup>C</sup> ⟩ and the combined program |UA⟩|UB⟩. The construction of such highdimensional
    OQT has been discussed in Sec. [VI.](#page-6-0)


    For scheme II, the nonlocal gate could be a product of controlled gates according
    to general gate compilation [\[58,](#page-14-20) [59\]](#page-14-21). For instance,
    if it is a ∧<sup>U</sup> for A as the control c and B as the target t and B holds
    the program |U⟩, then we can apply a remote OQC by using remote Toffoli-type gates.
    This finally reduces to remote CNOT gates, which can be realized by gate teleportation
    [\[45\]](#page-14-8). Each CNOT gate consumes one ebit. More importantly, this
    scheme avoids the OQT, despite the increase of circuit depth due to the byproduct
    correction in gate teleportation. This is a case that we discussed in Sec. [III](#page-2-0)
    for constructing a hybrid QT and OQT network.


    #### B. General protocol


    The protocol above can be extended to the multi-party setting, as illustrated
    in Fig. [9.](#page-9-0) We now describe the gen-


    ![](_page_9_Figure_9.jpeg)


    <span id="page-9-0"></span>Figure 9. The schematic for a multi-party distributed
    oblivious quantum computing. Each box could be a separable participant. The dashed
    lines are only for illustration of locality. In general, this is used in hybrid
    classical-quantum distributed algorithm.


    eral algorithmic structure of it. When there are quantum programs, i.e., Choi
    states {ωE<sup>i</sup> } for channels {Ei} as input, the quantum circuit acting
    on them is generally known as quantum superchannel or comb [\[35\]](#page-13-27).
    Such circuits can be viewed as quantum programming or quantum super-algorithms
    [\[34,](#page-13-26) [36\]](#page-13-29), although they can be described as usual
    quantum circuits. On top of the quantum circuit, there can be a classical algorithm
    A which is used to optimize the parameters ⃗θ in a quantum circuit U( ⃗θ). The
    algorithm A is a function f(⃗x) of the measurement outcome ⃗x = tr(Oρθ⃗) from
    the circuit [\[3\]](#page-13-2). The OQC and other oblivious quantum algorithms,
    as well as explicit operations, are a part of the superchannel, and the OQT can
    be used to break the whole high-depth circuit into smaller blocks for each program
    ω<sup>E</sup><sup>i</sup> , for instance. As the OQT guarantees the final outcome
    ⃗x, not necessarily the final state, this naturally fits into the framework of
    hybrid classical-quantum algorithm.


    As mentioned in Sec. [I,](#page-0-1) our protocol suits many practical settings.
    The programs ω<sup>E</sup><sup>i</sup> can be the encoding of classical or quantum
    data depending on the problem to be solved. Examples include quantum channel estimation,
    quantum machine learning and optimization, and secure quantum computing. The unknown
    objects can be the unknown channel to be estimated, some pre-stored data or subroutines,
    or private input from users, respectively.


    For instance, it is known that for quantum channel estimation or discrimination
    the sequential scheme by superchannel is necessary for non-unitary channels [\[74\]](#page-14-36).
    Using our scheme, this breaks down to three steps: firstly sending an ebit each
    to the unknown channel E<sup>i</sup> to generate a collection of ω<sup>E</sup><sup>i</sup>
    , and apply a part of superchannel to each ω<sup>E</sup><sup>i</sup> , and then
    use OQT to connect them. All the steps are transversal, i.e., can be done in parallel.
    This greatly reduces the circuit depth while consuming more


    ![](_page_10_Figure_1.jpeg)


    <span id="page-10-0"></span>Figure 10. A schematic to illustrate the space-time
    tradeoff in OQT. A parallel OQT can be realized by iterative OQT between two blocks
    if data and programs can be loaded quickly. Boxes with ⃗θ and ebits represent
    operations in the quantum superchannel.


    ebits and qubits.


    As explained in Sec. [III,](#page-2-0) a space-time tradeoff applies for the usage
    of OQT if qubits can be refreshed. It is not hard to see the minimal number of
    samples is two, shown in Fig. [10,](#page-10-0) which teleports the information
    back and forth. Now the OQT applies sequentially. A qubit needs to survive longer
    than the period of an OQT operation followed by the preparation of a program.


    #### C. Comparison with other protocols


    To better understand the features of our protocol and find more application settings,
    in this section we compare our protocol with a few other relevant quantum computing
    protocols. This is briefly summarized in Table [I](#page-11-1) and analyzed in
    details below. These protocols have close connection with quantum cryptography
    and secure quantum computing [\[23\]](#page-13-16).


    First, let us recall our protocol in the bipartite case. The protocol, abbreviated
    as distributed black-box quantum computing (DBQC) in the Table, compute overlaps
    |⟨ψo|UoUin|ψin⟩|<sup>2</sup> with a user Alice holding the input state |ψin⟩,
    the classical description of it [ψin], and the program state |Uin⟩, and a server
    Bob holding the final measurement specified by a set of {|ψo⟩}, their classical
    description [ψo], and another program state |Uo⟩.


    This is directly distinct from the ''obvious'' protocol when Alice knows [Uin]
    and Bob knows [Uo]. This is the distributed quantum computing (DQC) in the Table.
    Actually, each participant can apply a gate U<sup>i</sup> directly and use QT
    to send its system to others. A nonlocal gate can be done remotely or belongs
    to a nonlocal participant. This setting is common and a few methods have been
    developed, such as distributed schemes for some quantum algorithms [\[11](#page-13-8)[–15\]](#page-13-9)
    and the circuit knitting technique [\[16–](#page-13-10) [18\]](#page-13-11). The
    obvious setting is easier due to the lack of obliviousness, and some techniques
    could also benefit DBQC. For instance, the circuit knitting aims to break down
    a


    nonlocal gate into a sum of local ones. An example is the expansion of a two-body
    gate in terms of Pauli gates


    $$U = \sum\_{ij} u\_{ij} \sigma\_i \otimes \sigma\_j,\tag{26}$$


    as Pauli gates form an operator basis. The coefficients uij are complex and understood
    as quasi-probabilities. This will increase the sampling cost and it is easy to
    see the cost increases exponentially with the number of nonlocal gates. Therefore,
    similar with the local reduction of OQT in Sec. [VI,](#page-6-0) these techniques
    only work for relatively small circuits.


    In our protocol of DBQC, there is a weak sense of obliviousness, so we need to
    further clarify this. In oblivious transfer (OT) [\[23\]](#page-13-16) and OT-based
    quantum computing [\[75\]](#page-14-37), a server (Bob) transmits two bit strings
    to a user (Alice), who can retrieve only one of them while Bob remains unaware
    of Alice''s choice. The server can do a computation U without knowing the input
    |ψin⟩ and output |ψo⟩. The security of OT is complicated, and we defer this to
    the Appendix [B.](#page-12-0)


    In DBQC, a type of oblivious transfer of quantum information can be achieved.
    Alice hides the state UA|ψ⟩ separately as the program |UA⟩ and the ISI measurement
    of |ψ⟩. Bob hides his information in |UB⟩. Alice and Bob will broadcast their
    outcomes of OQT, and Bob also needs to send the measurement outcomes to Alice.
    Alice gets the final result. There is no way for Alice to know the whole information
    of UB, and neither for Bob to know the whole information of UA|ψ⟩ due to the no-cloning
    theorem. The security holds for the passive adversary setting whereas the participants
    are honest but curious. However, it does not hold when participants are malicious,
    e.g., Bob can send wrong measurement outcomes to Alice to spoil the task.


    In blind quantum computing (BQC) [\[6\]](#page-13-5), a user knows the computing
    task while only calls a server as an oracle to execute the circuit U. The server
    is blind or oblivious of the computing task. The final protocol we consider is
    a task in quantum von Neumann architecture (QvN) [\[34\]](#page-13-26), where
    a server can generate a program U and send it to a user as |U⟩. Given a limited
    amount of samples, the user can verify the program U without learning it [\[36,](#page-13-29)
    [76\]](#page-14-38), hence the user is oblivious. Actually, the DBQC can be viewed
    as a distributed version of QvN, with a circuit U being split into a few parts
    held by separate participants.


    |     | user                                | server                |

    |-----|-------------------------------------|-----------------------|

    | OT  | ψin⟩, [ψin],  ψo⟩, [ψo]             | U, [U]                |

    | BQC | ψin⟩, [ψin],  ψo⟩, [ψo], [U] U      |                       |

    | QvN | ψin⟩, [ψin],  ψo⟩, [ψo],  U⟩ U, [U] |                       |

    |     | DBQC  ψin⟩, [ψin],  Uin⟩            | ψo⟩, [ψo],  Uo⟩       |

    | DQC | ψin⟩, [ψin],  Uin⟩, [Uin]           | ψo⟩, [ψo],  Uo⟩, [Uo] |


    <span id="page-11-1"></span>Table I. The comparison of a few quantum computing
    protocols for the bipartite setting. The abbreviations are OT (oblivious transfer),
    BQC (blind quantum computing), QvN (quantum von Neumann architecture), DBQC (distributed
    black-box quantum computing), and DQC (distributed quantum computing). The details
    are found in the main text.


    #### VIII. CONCLUSION


    In this work, we proposed a protocol of universal distributed quantum computing
    with black-box subroutines. The protocol is based on a few oblivious quantum algorithms
    that involve black-box subroutines. We also analyzed the requirements of obliviousness
    and pointed out the potential application in various settings.


    We also would like to address the differences and connections among a few computational
    notions. The very basic notion of universality, in a narrow sense, refers to the
    ability to realize an arbitrary U within an accuracy ϵ by a process C, with the
    cost of C being efficient with respect to ϵ and a proper measure of U [\[2\]](#page-13-1).
    It does not prescribe the physical implementation mechanism for gate operations,
    nor does it inherently require programmability - the ability to store and subsequently
    execute gate operations as callable subroutines. Computation with unknown subroutines
    is oblivious, and it can further lead to security in a distributed multi-party
    setting [\[77\]](#page-14-39).


    The computational notions above are closely tied with quantum von Neumann architecture,
    which includes modular hardware units for quantum CPU, control, memory/storage,
    communication, input and output. Realizing a true quantum von Neumann architecture
    will depend heavily on future hardware developments, particularly the emergence
    of specialized quantum memory systems [\[36\]](#page-13-29). From another perspective,
    the current model of quantum computing is a form of "in-memory" computing [\[78\]](#page-15-0)
    where the traditional distinction between processing and memory units disappears.
    The von Neumann architecture can also be understood and employed from a software
    perspective, which refers to quantum programming with functional subroutines for
    the benefit of protection of programs and also higher-level integration. In this
    regard, our distributed protocol in this work can be viewed as a simulation of
    quantum von Neumann architecture. Overall, our findings not only establish meaningful
    connections with existing distributed quantum computing protocols but also open
    new avenues for advancing both theoretical and experimental research in the field.


    #### IX. ACKNOWLEDGEMENT


    This work has been funded by the National Natural Science Foundation of China
    under Grants 12447101, 12105343, and 62471368, the Natural Science Foundation
    of Guangdong Province (Grant No. 2023A1515010671), and the Shaanxi Provincial
    Young Innovative Teams in Higher Education. Suggestions from W.-Z. Cai, M. Hayashi,
    and Y.-D. Wu are greatly acknowledged.


    ## <span id="page-11-0"></span>Appendix A: Extension to quantum channels and superchannels


    In the main text, we have presented a few schemes for the unitary case. Here we
    extend them to the non-unitary channels. The channel-state duality [\[38,](#page-14-1)
    [39\]](#page-14-2) states that a channel E can be represented as a Choi state


    $$

    \omega\_{\mathcal{E}} := \mathcal{E} \otimes \mathbb{1}(\omega), \tag{A1}

    $$


    for ω := |ω⟩⟨ω| as the Bell state. From dilation a channel E can also be realized
    by a unitary circuit U requiring an additional ancilla initialized at |0⟩.


    The general operations on Choi states are superchannels [\[35,](#page-13-27) [79\]](#page-15-1).
    The unitary dilation of a superchannel Sˆ requires two unitary operators U<sup>1</sup>
    and U<sup>2</sup> such that the Kraus operator-sum form is


    $$

    \hat{\mathcal{S}}(\omega\_{\mathcal{E}}) = \sum\_{\mu} S\_{\mu} \omega\_{\mathcal{E}}
    S\_{\mu}^{\dagger} \tag{A2}

    $$


    with Kraus operators


    $$S\_{\mu} = \sum\_{m} K\_{2, m\mu} \otimes K\_{1, m} \tag{A3}$$


    and P <sup>µ</sup> S † <sup>µ</sup>S<sup>µ</sup> = 1, K1,m = ⟨m|U1|0⟩ and K2,mµ
    = ⟨m|U2|µ⟩ are also Kraus operators. The sum over m signifies the quantum memory
    between U<sup>1</sup> and U2. From an algorithmic point of view, the U<sup>1</sup>
    and U<sup>2</sup> form parametric quantum circuits that can be optimized.


    The oblivious quantum teleportation (OQT) easily extends to the non-unitary case.
    Two programs ω<sup>E</sup><sup>1</sup> and ω<sup>E</sup><sup>2</sup> can be composed
    to yield ω<sup>E</sup>2E<sup>1</sup> to guarantee observable measurement. For
    the oblivious quantum control (OQC), the situation is different. If the dilated
    circuit U for a channel E is available, we can use U and OQC to generate ∧<sup>U</sup>
    , and finally trace out the ancilla. A superposition of channels {Ei} can be realized
    as a superposition P i ciU<sup>i</sup> for a collection of U<sup>i</sup> , and
    tracing out the ancilla leads to


    $$\sum\_{ij} c\_i c\_j^\* \text{tr}\_a \left( U\_i (\rho \otimes \rho\_a) U\_j^\dagger
    \right) \tag{A4}$$


    for ρ<sup>a</sup> := |00 · · · 0⟩ as the initial ancillary states, and the trace
    tr<sup>a</sup> is over the ancilla. It contains the ''diagonal'' terms P i |ci
    | <sup>2</sup>Ei(ρ) and the interference terms for i ̸= j. The non-unitary OQC
    also applies to DQC1 which can evaluate tr(U(ρ ⊗ |0⟩⟨0|)), which is tr(K0ρ) for
    the first Kraus operator K<sup>0</sup> in U. This extends DQC1 from evaluating
    the trace trU of unitary matrix to more general matrices.


    # <span id="page-12-0"></span>Appendix B: Oblivious transfer and Bit commitment


    In our scheme of distributed quantum computing, there is a sense of oblivious
    transfer, but it is different from the traditional one. Therefore, we study this
    issue in more details.


    In classical cryptography, bit commitment (BC) and oblivious transfer (OT) are
    two primary building blocks for more advanced protocols [\[23\]](#page-13-16).
    In BC, Alice commits a bit b to Bob who cannot know its value, and Alice will
    later on reveal its value without being able to change it. In OT, Alice has two
    bits b<sup>0</sup> and b<sup>1</sup> while Bob needs one of them, bc, with c as
    his bit. Bob only gets the requested bit without knowing the other one, while
    Alice does not know his choice c. Note here, for convenience, our assignment of
    Alice and Bob is different from that in the main text.


    It is known that OT is universal for secure twoparty classical computation, and
    OT implies BC. For quantum cryptography that uses qubits to encode bits, the situation
    is different. It is known that quantum OT is equivalent to quantum BC, and they
    are information-theoretically secure in the so-called noisy storage model [\[33\]](#page-13-23),
    otherwise they are not [\[28,](#page-13-21) [29\]](#page-13-22).


    To put our study in the more broader context of cryptography, we first classify
    four categories of protocols:


    1. Transmit bits with classical means; this is the stan-


    dard classical cryptography;


    - 2. Transmit bits with quantum means; this is the quantum cryptography;

    - 3. Transmit qubits with entanglement-assisted means; this refers to quantum
    cryptography of quantum information but with possible entanglement or classical
    correlations as assistance, for instance, the commitment of qubits, or known as
    qubit commitment [\[80\]](#page-15-2);

    - 4. Transmit qubits with purely quantum means.


    From the perspective of channel capacity [\[81\]](#page-15-3), with no surprise,
    the protocols above correspond to the classical capacity of classical channel,
    the private (or secure) capacity of quantum channel, the entanglement-assisted
    quantum capacity of quantum channel, and quantum capacity of quantum channel,
    respectively, ignoring the details of assisted classical communication.


    For quantum BC, it is well known a dishonest Alice can use entanglement to cheat:
    Alice use bipartite nonorthogonal states ψ<sup>0</sup> and ψ<sup>1</sup> to encode
    0 and 1, respectively, and send one local part to Bob, and Bob cannot learn the
    committed bit from this local part, but Alice can do local operations to switch
    between ψ<sup>0</sup> and ψ1, hence cheating. This requires Alice to have the
    ability to store qubits, which is avoided in the noisy storage model [\[33\]](#page-13-23).
    The same cheating strategy carries over to the commitment of qubits [\[80\]](#page-15-2),
    which we recall here. Alice aims to commit a qubit state |ψθ⟩ = <sup>√</sup> 1
    2 (|0⟩ + e iθ|1⟩), and first uses the Bell circuit to mask it in a Choi state
    |Ψθ⟩ = <sup>√</sup> 1 2 (|00⟩ + e iθ|11⟩). Alice sends Bob half of the state which
    is completely mixed. Bob''s goal, say, is to check if Alice has made the correct
    guess of θ, but Alice still can freely change θ without being noticed by Bob.
    This cheating also applies to oblivious transfer of qubits.


    It is well known that quantum capacity is private against eavesdroppers, while
    it often assumes both the sender Alice and the receiver Bob are honest. In a sense,
    the standard quantum communication is both committing and oblivious. Namely, for
    commitment, Alice commits and sends a state |ψ⟩ directly. There is no way for
    Alice to change it anymore, neither for Bob to decode it until Alice tell him
    what it is, i.e., a classical description [ψ] of it. For oblivious transfer, Alice
    and Bob first agreed upon an assignment i 7→ |ψi⟩, for a set of index i that does
    not reveal ψ<sup>i</sup> , and Bob only knows his index while Alice holds all
    the states without knowing them. Bob tells Alice the index i and Alice then sends
    the state


    |ψi⟩ to Bob. Although Alice knows i, she would not know the state |ψi⟩. For both
    protocols, Bob can finally use [ψ] and quantum verification schemes to verify
    the state |ψ⟩. Our scheme for DBQC belongs to category (4), which does not allow
    entanglement assistance, hence the entanglement attack does not apply. However,
    it cannot tol-


    - <span id="page-13-0"></span>[1] T. D. Ladd, F. Jelezko, R. Laflamme, Y. Nakamura,
    C. Monroe, J. L. O''Brien, Quantum computers, Nature 464 (7285) (2010) 45–53.

    - <span id="page-13-1"></span>[2] M. A. Nielsen, I. L. Chuang, Quantum Computation
    and Quantum Information, Cambridge University Press, Cambridge U.K., 2000.

    - <span id="page-13-2"></span>[3] D.-S. Wang, Universal quantum computing models:
    a perspective of resource theory, Acta Phys. Sin. 73 (2024) 220302.

    - <span id="page-13-3"></span>[4] M. Caleffi, M. Amoretti, D. Ferrari, et al.,
    Distributed quantum computing: a survey, Computer Networks 254 (2024) 110672.

    - <span id="page-13-4"></span>[5] D. Barral, F. J. Cardama, G. D´ıaz-Camacho,
    et al., Review of distributed quantum computing: From single QPU to high performance
    quantum computing, Computer Science Review 57 (2025) 100747.

    - <span id="page-13-5"></span>[6] A. Broadbent, J. Fitzsimons, E. Kashefi, Universal
    blind quantum computation, in: in Proceedings of the 50th Annual Symposium on
    Foundations of Computer Science (IEEE Computer Society, Los Alamitos, CA, 2009),
    2009, pp. 517–527.

    - <span id="page-13-6"></span>[7] M. A. Nielsen, I. L. Chuang, Programmable quantum
    gate arrays, Phys. Rev. Lett. 79 (1997) 321–324.

    - <span id="page-13-28"></span>[8] D.-S. Wang, Choi states, symmetry-based quantum
    gate teleportation, and stored-program quantum computing, Phys. Rev. A 101 (2020)
    052311.

    - <span id="page-13-30"></span>[9] Y. Yang, R. Renner, G. Chiribella, Optimal
    universal programming of unitary gates, Phys. Rev. Lett. 125 (2020) 210501.

    - <span id="page-13-7"></span>[10] D.-S. Wang, A family of quantum von Neumann
    architecture, Chin. Phys. B 33 (2024) 080302.

    - <span id="page-13-8"></span>[11] J. Avron, O. Casper, I. Rozen, Quantum advantage
    and noise reduction in distributed quantum computing, Phys. Rev. A 104 (2021)
    052404.

    - [12] D. Qiu, L. Luo, L. Xiao, Distributed Grover''s algorithm, Theoretical Computer
    Science 993 (2024) 114461.

    - [13] H. Tang, B. Li, G. Wang, H. Xu, C. Li, A. Barr, P. Cappellaro, J. Li, Communication-efficient
    quantum algorithm for distributed machine learning, Phys. Rev. Lett. 130 (2023)
    150602.

    - [14] S. C. Marshall, C. Gyurik, V. Dunjko, High dimensional quantum machine
    learning with small quantum computers, Quantum 7 (2023) 1078.

    - <span id="page-13-9"></span>[15] K. Hwang, H.-T. Lim, Y.-S. Kim, D. K. Park,
    Y. Kim, Distributed quantum machine learning via classical communication, Quantum
    Science and Technology 10 (1) (2024) 015059.

    - <span id="page-13-10"></span>[16] C. Piveteau, D. Sutter, Circuit knitting with
    classical communication, IEEE Trans. Inform. Theory 70 (2023) 3310797.

    - [17] C. Ufrecht, M. Periyasamy, S. Rietsch, D. D. Scherer, A. Plinge, C. Mutschler,
    Cutting multi-control quantum


    erant dishonest participants who can use the wrong programs or send wrong measurement
    outcomes. Whether our protocol can be boosted towards secure multi-party quantum
    computation remains an interesting task.


    gates with zx calculus, Quantum 7 (2023) 1147.


    - <span id="page-13-11"></span>[18] A. Lowe, M. Medvidovi´c, A. Hayes, et al.,
    Fast quantum circuit cutting with randomized measurements, Quantum 7 (2023) 934.

    - <span id="page-13-12"></span>[19] J. Qiu, Y. Liu, L. Hu, et. al, Deterministic
    quantum state and gate teleportation between distant superconducting chips, Science
    Bulletin 70 (3) (2025) 351–358.

    - <span id="page-13-13"></span>[20] A. Almanakly, B. Yankelevich, M. Hays, et
    al., Deterministic remote entanglement using a chiral quantum interconnect, Nat.
    Phys.Https://doi.org/10.1038/s41567-025- 02811-1 (2025).

    - <span id="page-13-14"></span>[21] M. Hayashi, Quantum Information Theory: Mathematical
    Foundation, 2nd edition, Springer, 2017.

    - <span id="page-13-15"></span>[22] D. M. Harris, S. L. Harris, Digital design
    and computer architecture, Elsevier, 2013.

    - <span id="page-13-16"></span>[23] J. Katz, Y. Lindell (Eds.), Introduction to
    Modern Cryptography, Chapman and Hall/CRC (New York), 2020.

    - <span id="page-13-17"></span>[24] D. W. Berry, A. M. Childs, R. Cleve, R. Kothari,
    R. D. Somma, Exponential improvement in precision for simulating sparse hamiltonians,
    in: Proc. 46th ACM Symposium on Theory of Computing, 2014, p. 283.

    - <span id="page-13-18"></span>[25] D. T. Stephen, D.-S. Wang, A. Prakash, T.-C.
    Wei, R. Raussendorf, Computational power of symmetryprotected topological phases,
    Phys. Rev. Lett. 119 (2017) 010504.

    - <span id="page-13-19"></span>[26] W. K. Wootters, W. H. Zurek, A single quantum
    cannot be cloned, Nature 299 (1982) 802–803.

    - <span id="page-13-24"></span>[27] D. Dieks, Communication by EPR devices, Phys.
    Lett. A 92 (1982) 271.

    - <span id="page-13-21"></span>[28] D. Mayers, Unconditionally secure quantum
    bit commitment is impossible, Phys. Rev. Lett. 78 (1997) 3414–3417.

    - <span id="page-13-22"></span>[29] H.-K. Lo, H. F. Chau, Is quantum bit commitment
    really possible?, Phys. Rev. Lett. 78 (1997) 3410–3413.

    - <span id="page-13-25"></span>[30] M. Araujo, A. Feix, F. Costa, C. Brukner,
    Quantum circuits cannot control unknown operations, New J. Phys. 16 (2014) 093026.

    - <span id="page-13-31"></span>[31] M. Oszmaniec, A. Grudka, M. Horodecki, A.
    W´ojcik, Creating a superposition of unknown quantum states, Phys. Rev. Lett.
    116 (2016) 110403.

    - <span id="page-13-20"></span>[32] J. Thompson, K. Modi, V. Vedral, M. Gu, Quantum
    plug n'' play: modular computation in the quantum regime, New J. Phys. 20 (2018)
    013004.

    - <span id="page-13-23"></span>[33] S. Wehner, C. Schaffner, B. M. Terhal, Cryptography
    from noisy storage, Phys. Rev. Lett. 100 (2008) 220502.

    - <span id="page-13-26"></span>[34] D.-S. Wang, A prototype of quantum von Neumann
    architecture, Commun. Theor. Phys. 74 (2022) 095103.

    - <span id="page-13-27"></span>[35] G. Chiribella, G. M. D''Ariano, P. Perinotti,
    Transforming quantum operations: Quantum supermaps, Europhys. Lett. 83 (2008)
    30004.

    - <span id="page-13-29"></span>[36] Y.-T. Liu, K. Wang, Y.-D. Liu, D.-S. Wang,
    A Survey of Universal Quantum von Neumann Architecture, Entropy 25 (8) (2023)
    1187.

    - <span id="page-14-0"></span>[37] K. Kraus, States, Effects, and Operations:
    Fundamental Notions of Quantum Theory, Vol. 190 of Lecture Notes in Physics, Springer-Verlag,
    Berlin, 1983.

    - <span id="page-14-1"></span>[38] M.-D. Choi, Completely positive linear maps
    on complex matrices, Linear Algebra Appl. 10 (1975) 285–290.

    - <span id="page-14-2"></span>[39] A. Jamio lkowski, Linear transformations which
    preserve trace and positive semidefiniteness of operators, Rep. Math. Phys. 3
    (1972) 275.

    - <span id="page-14-3"></span>[40] D.-S. Wang, Weak, strong, and uniform quantum
    simulations, Phys. Rev. A 91 (2015) 012334.

    - <span id="page-14-4"></span>[41] S. Yoshida, A. Soeda, M. Murao, Reversing unknown
    qubit-unitary operation, deterministically and exactly, Phys. Rev. Lett. 131 (2023)
    120602.

    - <span id="page-14-5"></span>[42] Y. Mo, L. Zhang, Y. A. Chen, et al., Parameterized
    quantum comb and simpler circuits for reversing unknown qubit-unitary operations,
    npj Quantum Inf. 11 (2025) 32.

    - <span id="page-14-6"></span>[43] D. Aharonov, A simple proof that toffoli and
    hadamard are quantum universal, arXiv preprint arXiv:0301040 (2003).

    - <span id="page-14-7"></span>[44] C. H. Bennett, G. Brassard, C. Cr´epeau, R.
    Jozsa, A. Peres, W. K. Wootters, Teleporting an unknown quantum state via dual
    classical and einstein-podolsky-rosen channels, Phys. Rev. Lett. 70 (1993) 1895–1899.

    - <span id="page-14-8"></span>[45] D. Gottesman, I. L. Chuang, Demonstrating the
    viability of universal quantum computation using teleportation and single-qubit
    operations, Nature 402 (6760) (1999) 390–393.

    - <span id="page-14-9"></span>[46] S. Bravyi, A. Kitaev, Universal quantum computation
    with ideal Clifford gates and noisy ancillas, Phys. Rev. A 71 (2005) 022316.

    - <span id="page-14-10"></span>[47] E. Knill, R. Laflamme, Power of one bit of
    quantum information, Phys. Rev. Lett. 81 (1998) 5672–5675.

    - <span id="page-14-11"></span>[48] A. Kitaev, A. H. Shen, M. N. Vyalyi, Classical
    and Quantum Computation, Vol. 47 of Graduate Studies in Mathematics, American
    Mathematical Society, Providence, 2002.

    - <span id="page-14-12"></span>[49] P. W. Shor, Algorithms for quantum computation:
    discrete logarithms and factoring, in: Proceedings 35th annual symposium on foundations
    of computer science, IEEE, 1994, pp. 124–134.

    - <span id="page-14-13"></span>[50] H. Buhrman, R. Cleve, J. Watrous, R. de Wolf,
    Quantum fingerprinting, Phys. Rev. Lett. 87 (2001) 167902.

    - <span id="page-14-14"></span>[51] G. Brassard, P. Hoyer, M. Mosca, A. Tapp,
    Quantum amplitude amplification and estimation, Contem. Mathemat. 305 (2002) 53–74.

    - <span id="page-14-15"></span>[52] A. Gilyen, Y. Su, G. H. Low, N. Wiebe, Quantum
    singular value transformation and beyond: exponential improvements for quantum
    matrix arithmetics, in: Proceedings of the 51st Annual ACM SIGACT Symposium on
    Theory of Computing, 2019.

    - <span id="page-14-16"></span>[53] G. L. Long, Duality quantum computing and
    duality quantum information processing, Int. J. Theor. Phys. 50 (2011) 1305.

    - <span id="page-14-17"></span>[54] D. W. Berry, A. M. Childs, R. Cleve, R. Kothari,
    R. D. Somma, Simulating hamiltonian dynamics with a truncated taylor series, Phys.
    Rev. Lett. 114 (2015) 090502.

    - <span id="page-14-18"></span>[55] H. Levine, A. Keesling, G. Semeghini, A. Omran,
    T. T. Wang, S. Ebadi, H. Bernien, M. Greiner, V. Vuleti´c, H. Pichler, M. D. Lukin,
    Parallel implementation of highfidelity multiqubit gates with neutral atoms, Phys.
    Rev. Lett. 123 (2019) 170503.

    - [56] M. Khazali, K. Mølmer, Fast multiqubit gates by adiabatic evolution in
    interacting excited-state manifolds of rydberg atoms and superconducting circuits,
    Phys. Rev.


    X 10 (2020) 021054.


    - <span id="page-14-19"></span>[57] Y. Kim, A. Morvan, L. B. Nguyen, R. K. Naik,
    C. J¨unger, L. Chen, J. M. Kreikebaum, D. I. Santiago, I. Siddiqi, High-fidelity
    three-qubit itoffoli gate for fixed-frequency superconducting qubits, Nat. Phys.
    18 (2022) 783.

    - <span id="page-14-20"></span>[58] A. Barenco, C. H. Bennett, R. Cleve, D. P.
    DiVincenzo, N. Margolus, P. Shor, T. Sleator, J. A. Smolin, H. Weinfurter, Elementary
    gates for quantum computation, Phys. Rev. A 52 (5) (1995) 3457.

    - <span id="page-14-21"></span>[59] M. Saeedi, I. Markov, Synthesis and optimization
    of reversible circuits-a survey, ACM Comput. Surv. 45 (2013) 21.

    - <span id="page-14-22"></span>[60] R. Iten, R. Colbeck, I. Kukuljan, J. Home,
    M. Christandl, Quantum circuits for isometries, Phys. Rev. A 93 (2016) 032318.

    - <span id="page-14-23"></span>[61] B. J. Brown, D. Loss, J. K. Pachos, C. N.
    Self, J. R. Wootton, Quantum memories at finite temperature, Rev. Mod. Phys. 88
    (2016) 045005.

    - <span id="page-14-24"></span>[62] S. Stastny, G. Burkard, The singlet-triplet
    and exchange-only flopping-mode spin qubits, arXiv preprint arXiv:2503.05032 (2025).

    - <span id="page-14-25"></span>[63] W. C. Burton, B. Estey, I. M. Hoffman, A.
    R. Perry, C. Volin, G. Price, Transport of multispecies ion crystals through a
    junction in a radio-frequency paul trap, Phys. Rev. Lett. 130 (2023) 173202.

    - <span id="page-14-26"></span>[64] D. Bluvstein, S. J. Evered, A. A. Geim, et
    al., Logical quantum processor based on reconfigurable atom arrays, Nature 626
    (2024) 58.

    - <span id="page-14-27"></span>[65] R. Raussendorf, H. J. Briegel, A one-way quantum
    computer, Phys. Rev. Lett. 86 (2001) 5188–5191.

    - <span id="page-14-28"></span>[66] E. Knill, R. Laflamme, G. Milburn, A scheme
    for efficient quantum computation with linear optics, Nature 409 (2001) 46.

    - <span id="page-14-29"></span>[67] D. E. Browne, T. Rudolph, Resource-efficient
    linear optical quantum computation, Phys. Rev. Lett. 95 (2005) 010501.

    - <span id="page-14-30"></span>[68] M. AbuGhanem, Photonic quantum computers,
    arXiv preprint arXiv:2409.08229 (2024).

    - <span id="page-14-31"></span>[69] M. Reck, A. Zeilinger, H. J. Bernstein, P.
    Bertani, Experimental realization of any discrete unitary operator, Phys. Rev.
    Lett. 73 (1994) 58–61.

    - <span id="page-14-32"></span>[70] X. Zhou, D. W. Leung, I. L. Chuang, Methodology
    for quantum logic gate construction, Phys. Rev. A 62 (2000) 052316.

    - <span id="page-14-33"></span>[71] C. K. Hong, Z. Y. Ou, L. Mandel, Measurement
    of subpicosecond time intervals between two photons by interference, Phys. Rev.
    Lett. 59 (1987) 2044–2046.

    - <span id="page-14-34"></span>[72] M. Zukowski, A. Zeilinger, M. A. Horne, Realizable
    ˙ higher-dimensional two-particle entanglements via multiport beam splitters,
    Phys. Rev. A 55 (1997) 2564–2579.

    - <span id="page-14-35"></span>[73] M. C. Tichy, M. Tiersch, F. de Melo, F. Mintert,
    A. Buchleitner, Zero-transmission law for multiport beam splitters, Phys. Rev.
    Lett. 104 (2010) 220405.

    - <span id="page-14-36"></span>[74] G. Chiribella, G. M. D''Ariano, P. Perinotti,
    Memory effects in quantum channel discrimination, Phys. Rev. Lett. 101 (2008)
    180501.

    - <span id="page-14-37"></span>[75] M. Hayashi, Oblivious quantum computation
    and delegated multiparty quantum computation, arXiv preprint arXiv:2211.00962
    (2022).

    - <span id="page-14-38"></span>[76] J. Morris, V. Saggio, A. Gocanin, B. Dakic,
    Quantum verification and estimation with few copies, Adv. Quantum Technol. 5 (2022)
    2100118.

    - <span id="page-14-39"></span>[77] C. Cr´epeau, D. Gottesman, A. Smith, Secure
    multi-party


    quantum computation, in: In STOC ''02: Proc. 34rd Annual ACM Symp. Theory of Computing,
    2002, p. 643.


    - <span id="page-15-0"></span>[78] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh,
    et al., Memory devices and applications for in-memory computing, Nat. Nanotechnol.
    15 (2020) 529.

    - <span id="page-15-1"></span>[79] K. Wang, D.-S. Wang, Quantum circuit simulation
    of


    superchannels, New J. Phys. 25 (4) (2023) 043013.


    - <span id="page-15-2"></span>[80] K. Modi, A. K. Pati, A. Sen(De), U. Sen, Masking
    quantum information is impossible, Phys. Rev. Lett. 120 (2018) 230501.

    - <span id="page-15-3"></span>[81] J. Watrous, The Theory of Quantum Information,
    Cambridge University Press, 2018.'
- title: 'FLASH-D: FlashAttention with Hidden Softmax Division'
  abstract: 'The transformer''s attention mechanism has revolutionized AI and machine

    learning, with its efficient computation being crucial to its performance.

    However, calculating attention involves matrix operations interspersed with

    softmax rescaling, which inherently slows down computation and requires

    processing the entire input sequence. Building on online softmax computation,

    FlashAttention integrates softmax calculation with matrix arithmetic, enabling

    tiled computation independent of sequence length. While optimized for GPUs,

    FlashAttention''s simplicity makes it amenable to direct hardware acceleration.

    This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a

    mathematically equivalent, yet simplified, formulation that achieves: (a)

    hiding softmax division within other non-linear function evaluations; (b)

    inherently numerically stable computation of exponentials, eliminating the need

    for maximum value subtraction; and (c) a reduction in computational cost

    without introducing numerical approximations to the FlashAttention kernel.

    Importantly, the essential FlashAttention properties that facilitate efficient

    tiled implementation are fully preserved. Hardware implementation results at

    28nm demonstrate that this proposed formulation achieves a 22.8% reduction in

    area and a 20.3% reduction in power, on average, compared to state-of-the-art

    parallel hardware architectures without any performance penalty.'
  url: http://arxiv.org/abs/2505.14201v1
  keywords: AI Hardware Accelerators, Transformers, FlashAttention, Energy Efficiency
  document: '#### I. INTRODUCTION


    Current state-of-the-art ML and AI systems are characterized by deep learning
    models with billions of parameters, achieving human-level performance in tasks
    like image recognition [\[1\]](#page-6-0) and natural language processing [\[2\]](#page-6-1).
    A key innovation driving this progress is the attention mechanism [\[3\]](#page-6-2),
    which allows models to focus on relevant parts of the input data. This has led
    to breakthroughs like transformer networks and large language models that are
    revolutionizing how machines understand and interact with the world.


    The increasing demand for processing long sequences in transformer models has
    exposed the complexity of the attention mechanism [\[4\]](#page-6-3). This computational
    burden, makes inference (and training) prohibitively expensive for extended context
    lengths. Traditional attention calculates pairwise similarities between all tokens,
    leading to a massive number of operations and memory traffic.


    To reduce the number of operations either by approximating the full attention
    matrix or by selectively focusing on the most relevant parts of the input sequence,
    techniques such as sparse attention [\[5\]](#page-6-4), linear attention [\[6\]](#page-6-5),
    and low-rank attention [\[7\]](#page-6-6) have been explored, each balancing accuracy
    and computational efficiency.


    Early attention hardware accelerators optimize which attention-relevant matrices,
    such as key, value and queries, remain stationary in local SRAMs and which are
    streamed to compute the attention scores [\[8\]](#page-6-7), [\[9\]](#page-6-8),
    [\[10\]](#page-6-9). If the accelerator cannot buffer and compute intermediate
    results for the entire sequence length, these intermediate results are written
    to back memory, which negatively impacts performance. To decouple the accelerator''s
    computational resources and local memory from the sequence length, the designs
    of [\[11\]](#page-6-10), [\[12\]](#page-6-11) optimize key components of attention
    such as matrix arithmetic and softmax function evaluation. In addition to data-flow
    and computation reordering optimizations, other techniques exploit token similarity
    [\[13\]](#page-6-12), [\[14\]](#page-6-13) to skip unnecessary computation and
    to reduce latency and power consumption. In-memory computation [\[15\]](#page-6-14)
    has also been explored for optimizing computation of attention.


    In parallel, FlashAttention [\[16\]](#page-6-15), [\[17\]](#page-6-16), [\[18\]](#page-6-17),
    originally developed for GPUs, has emerged as a powerful technique for accelerating
    attention. By leveraging tiling and fusing online softmax computation with matrix
    arithmetic, it enables parallel execution and reduces memory traffic. These features
    allow FlashAttention to lower execution latency and simplify the processing of
    long sequences without compromising accuracy.


    In this work, we aim at preserving the favorable IO properties of FlashAttention
    and *simplify its core computation kernel*. To do this, we *rewrite* the *forward
    pass* of FlashAttention kernel used in transformer inference in a *mathematical
    equivalent form*, called *FLASH-D*, enabling several favorable computational features:


    - The softmax division is hidden within the reformulated non-linear function evaluations.
    This eliminates the need for explicit division, either during computation [\[16\]](#page-6-15)
    or as a post-processing step when following the lazy softmax approach [\[11\]](#page-6-10),
    [\[18\]](#page-6-17).

    - Without altering the algorithm or introducing any approximation, softmax is
    replaced by an equivalent sigmoid function of attention score differences that
    is inherently numerically stable. This transformation improves parallelism and
    removes the requirement to scale inputs by the maximum attention score, which
    is necessary otherwise for numerically stable softmax computation [\[19\]](#page-6-18).

    - New possibilities emerge for reducing multiplications or memory accesses while
    preserving the core features of FlashAttention, which enable tiled computation
    to enhance locality and decrease memory traffic [\[16\]](#page-6-15), [\[17\]](#page-6-16).


    To assess the efficiency of FLASH-D for designing FlashAttention-based hardware
    accelerators, we implemented the optimized FlashAttention2 kernel and FLASH-D
    using a fully unrolled systolic architecture, in 28nm ASIC technology. Experimental
    results show that the proposed approach achieves significant area and power savings,
    ranging from 20–28% and 16–27%, respectively, for various hidden dimension sizes
    and floating-point number formats when compared to parallel hardware implementation
    of the kernel of FlashAttention2 state-of-the-art algorithm under the same performance.


    #### II. FLASHATTENTION-BASED HARDWARE ACCELERATORS


    Attention is a fundamental concept in machine learning, particularly in deep learning
    models that utilize the transformer architecture [\[20\]](#page-6-19), [\[21\]](#page-6-20).
    It guides transformer models to take into account only relevant context: A user
    query is compared to a set of key vectors leading to an attention score matrix
    which is used to retrieve the information the user requested from a set of value
    vectors. In practice, attention is applied across multiple heads in parallel [\[3\]](#page-6-2),
    allowing the model to comprehend more complex relationships. Without loss of generality,
    we limit our descriptions to single-head attention.


    #### *A. Attention Kernel*


    For a query ⃗q and a set of key and value vectors K = ⃗k1, . . . , ⃗k<sup>N</sup>
    and V = ⃗v1, . . . , ⃗v<sup>N</sup> attention is defined as


    $$s\_i = \det(\vec{q}, \vec{k}\_i) \qquad f\_i = \frac{e^{s\_i}}{\sum\_j e^{s\_j}}
    \qquad \text{Attn}(\vec{q}, \mathbf{K}, \mathbf{V}) = \sum\_i f\_i \vec{v}\_i$$


    The attention score s<sup>i</sup> represents the similarity between a given query
    and the ith key vector computed via a dot product. To identify the most relevant
    tokens, the softmax function is applied to all scores corresponding to the same
    query vector. Softmax first exponentiates each score and then divides it by the
    sum of all exponentials. The final output is obtained by multiplying the attention
    scores of a query by all value vectors, where the contribution of each value to
    the output is determined by its corresponding attention score normalized via softmax.


    Exponentiating scores can cause infinite values that would affect the final result.
    To prevent this, *safe softmax* subtracts the maximum score from all scores, thus
    avoiding overflow while keeping its core properties f<sup>i</sup> = e <sup>s</sup>i−max/
    P j e sj−max .


    #### *B. Baseline FlashAttention Algorithm*


    Algorithmically, attention calculation faces one major bottleneck. The softmax
    operation should be applied across the entire sequence length. This requirement
    reduces the extent to which attention can be parallelized or limits the application
    of attention to shorter sequence lengths [\[22\]](#page-6-21).


    To overcome this limitation, FlashAttention [\[16\]](#page-6-15), driven by the
    online calculation of softmax function [\[23\]](#page-6-22), reorganized computations
    involved in attention kernel and enabled arbitrary tiling that reduced also memory
    traffic. The forward pass of baseline FlashAttentionm, which is the focus of this
    work, is shown in Alg. [1](#page-1-0) using vector-oriented operations. An equivalent
    block-based definition can be found in [\[16\]](#page-6-15).


    At each iteration, the dot product of the query vector and a key vector yields
    a similarity score, denoted as s<sup>i</sup> . Subsequently, m<sup>i</sup> holds
    the current maximum similarity score, while ℓ<sup>i</sup> incrementally accumulates
    the sum of the exponentials of each s<sup>i</sup> minus the present maximum score.
    The multiplication by e mi−1−m<sup>i</sup> in the calculation of ℓ<sup>i</sup>
    adjusts the prior maximum value used whenever the current maximum m<sup>i</sup>
    differs from the previous maximum mi−1. Similarly, the output vector ⃗o<sup>i</sup>
    is updated by adding the new value vector ⃗v<sup>i</sup>


    #### <span id="page-1-0"></span>Algorithm 1 FlashAttention


    1: for each query ⃗q do 2: for i = 1 : N do 3: s<sup>i</sup> ← dot(⃗q,⃗ki) 4:
    m<sup>i</sup> ← max(mi−1, si) 5: ℓ<sup>i</sup> ← ℓi−1e <sup>m</sup>i−1−m<sup>i</sup>
    + e si−m<sup>i</sup> 6: ⃗o<sup>i</sup> ← ⃗oi−<sup>1</sup> ℓi−1e mi−1−mi ℓi + ⃗v<sup>i</sup>
    e si−mi ℓi 7: end for 8: Attn(⃗q, K, V) ← ⃗o<sup>N</sup> 9: end for


    weighted by its softmax importance, to the adjusted preceding output vector, ⃗oi−1.
    The final attention vector for one query vector is returned in ⃗o<sup>N</sup>
    .


    #### *C. Flash Attention with Lazy Softmax*


    Instead of calculating the exponential sum and division before the multiplication
    with the value vectors, lazy softmax architectures [\[11\]](#page-6-10), [\[13\]](#page-6-12)
    accumulate the weighted sum and the exponent sum in parallel and perform division
    as a final step. To avoid the expensive division operation other approaches compute
    softmax in the logarithmic domain [\[12\]](#page-6-11), [\[24\]](#page-6-23).


    $$s\_i = \text{dot}(\vec{q}, \vec{k\_i}) \quad f\_i = e^{s\_i - \text{max}} \quad
    \text{Attn}(\vec{q}, \mathbf{K}, \mathbf{V}) = \frac{\sum\_i f\_i \vec{v}\_i}{\sum\_j
    e^{s\_j - \text{max}}}$$


    This postponed division approach has been adopted by FlashAttention2 algorithm
    [\[17\]](#page-6-16) that is shown in Alg. [2.](#page-1-1)


    <span id="page-1-1"></span>


    | Algorithm 2 FlashAttention2: Lazy Softmax Division  |

    |-----------------------------------------------------|

    | 1: for each query ⃗q do                             |

    | for i = 1 : N do<br>2:                              |

    | ← dot(⃗q,⃗ki)<br>si<br>3:                           |

    | mi<br>← max(mi−1, si)<br>4:                         |

    | mi−1−mi +<br>si−mi<br>ℓi<br>← ℓi−1e<br>e<br>5:      |

    | mi−1−mi +<br>si−mi<br>⃗oi<br>← ⃗oi−1e<br>⃗vie<br>6: |

    | end for<br>7:                                       |

    | Attn(⃗q, K, V) ← ⃗oN<br>/ℓN<br>8:                   |

    | 9: end for                                          |


    FlashAttention2 keeps the same basic principle of operation of baseline FlashAttention
    but removes all internal division operations (line 6 of Alg. [1\)](#page-1-0)
    and replaces them with one final vector division at the end (line 8 of Alg. [2\)](#page-1-1).
    In this way, it leads to a simpler design that can be more easily parallelized
    and implemented directly in hardware.


    The FlashAttention2 kernel shown in Alg. [2](#page-1-1) involves two for loops
    that can be unrolled to enhance parallelism and computational throughput. While
    unrolling the inner loop is a valid option, it still maintains the serial dependency
    across the three variables that constitute attention''s internal state, namely
    m<sup>i</sup> , ℓ<sup>i</sup> , and ⃗o<sup>i</sup> . A more scalable approach
    is to unroll the outer loop, allowing the FlashAttention2 kernel to process multiple
    query vectors in parallel within the same blocks of key and value vectors. In
    this case, the internal state is kept independently for each query vector, thus
    eliminating serial dependencies.


    ![](_page_2_Figure_0.jpeg)


    <span id="page-2-0"></span>Fig. 1. A parallel hardware architecture for FlashAttention2
    kernel for multiple preloaded query vectors.


    Fig. [1](#page-2-0) depicts the parallel hardware structure that corresponds to
    the outer-loop unrolling architecture of FlashAttention2. In this setup, a block
    of query vectors is loaded locally, while the key vectors are sequentially provided
    to each block for computing the dot products [\[25\]](#page-6-24), [\[26\]](#page-6-25).
    This results in the determination of a new maximum value and an updated running
    sum of exponents. The value vectors are streamed into the accelerator, allowing
    the output vectors for different query vectors to be updated simultaneously. After
    all key and value vectors are processed, a final division operation is performed
    to compute the attention for each query vector. The process concludes once all
    query vectors have been processed.


    FlashAttention2 eliminates the need for a full softmax hardware unit, as the exponentiations
    and final division are computed independently. Several methods have been proposed
    for performing these two non-linear operations in hardware. These approaches include
    piece-wise linear approximations following range reduction [\[19\]](#page-6-18),
    logarithmic quantization-based transformations [\[27\]](#page-6-26), and other
    approximations [\[28\]](#page-6-27) that convert exponential functions into simpler
    powers of two, allowing for shift-and-add algorithms.


    # III. FLASHATTENTION WITH HIDDEN SOFTMAX DIVISION


    The two algorithmic variants of FlashAttention perform identical computations.
    Their sole difference lies in the timing of the softmax division. In baseline
    FlashAttention, the division is executed incrementally during output accumulation,
    whereas in FlashAttention2, it is rescheduled to the end of the computation.


    In this work, we aim at rewriting baseline FlashAttention to effectively hide
    the softmax division within non-linear function evaluations, without resorting
    to any approximations. By doing so, we intend to introduce a novel, yet equivalent,
    algorithmic variant of FlashAttention that facilitates energyefficient hardware
    accelerators. We accomplish this transformation in two steps.


    # *A. Redefine output recursion as a weighted contribution of value vectors*


    First, we need to rewrite the recursive output computation of baseline FlashAttention.
    From line 5 of Alg. [1](#page-1-0) we know that the output is computed as follows:


    <span id="page-2-1"></span>

    $$

    \vec{o}\_{i} = \vec{o}\_{i-1} \left( \frac{\ell\_{i-1} e^{m\_{i-1} - m\_i}}{\ell\_i}
    \right) + \vec{v}\_i \left( \frac{e^{s\_i - m\_i}}{\ell\_i} \right) \tag{1}

    $$


    From the definition of ℓ<sup>i</sup> = ℓi−1e <sup>m</sup>i−1−m<sup>i</sup> + e
    si−m<sup>i</sup> in line 4 of Alg. [1](#page-1-0) we get that:


    $$\ell\_{i-1}e^{m\_{i-1}-m\_i} = \ell\_i - e^{s\_i - m\_i}$$


    Replacing this term to the output recursion [\(1\)](#page-2-1) we get that


    $$\begin{split} \vec{o}\_{i} &= \vec{o}\_{i-1} \left( \frac{\ell\_{i} - e^{s\_{i}
    - m\_{i}}}{\ell\_{i}} \right) + \vec{v}\_{i} \left( \frac{e^{s\_{i} - m\_{i}}}{\ell\_{i}}
    \right) \\ &= \vec{o}\_{i-1} \left( 1 - \frac{e^{s\_{i} - m\_{i}}}{\ell\_{i}}
    \right) + \vec{v}\_{i} \left( \frac{e^{s\_{i} - m\_{i}}}{\ell\_{i}} \right) \end{split}
    \tag{2}$$


    Setting


    <span id="page-2-3"></span><span id="page-2-2"></span>

    $$w\_i = \frac{e^{s\_i - m\_i}}{\ell\_i} \tag{3}$$


    and replacing it in [\(2\)](#page-2-2) we get an equivalent, but simplified form,
    for the recursive computation of the output in baseline FlashAttention:


    <span id="page-2-6"></span>

    $$\begin{array}{c}\hline\vec{o\_{i}} = \vec{o\_{i-1}}(1 - w\_{i}) + \vec{v\_{i}}w\_{i}
    \\\hline\end{array} \tag{4}$$


    This newly rewritten form of [\(1\)](#page-2-1) reveals that, effectively, the
    output is a weighted sum of the previously accumulated output, ⃗oi−1, and the
    new value vector, ⃗v<sup>i</sup> . The contribution of each part is determined
    by the value of w<sup>i</sup> which, according to [\(3\)](#page-2-3), corresponds
    to an incrementally formed softmax since it includes the exponent of each attention
    score divided by the running sum of exponents accumulated in ℓ<sup>i</sup> . Thus,
    by definition, w<sup>i</sup> is always positive and less than 1.


    # *B. Recursive weight computation*


    Next, we aim to establish a recursive relation that links the current weight,
    w<sup>i</sup> to the weight from the previous iteration, wi−<sup>1</sup> and the
    corresponding attention scores. Our goal is to use the planned recursive relation
    *exclusively*, *replacing* the recursive computation of ℓ<sup>i</sup> and m<sup>i</sup>
    , as well.


    To do so, based on the definition of weight w<sup>i</sup> in [\(3\)](#page-2-3),
    we can equivalently write that wi−<sup>1</sup> = e <sup>s</sup>i−1−mi−<sup>1</sup>
    /ℓi−1. Solving for ℓ<sup>i</sup> and ℓi−1, respectively, we get that:


    <span id="page-2-4"></span>

    $$\ell\_i = \frac{e^{s\_i - m\_i}}{w\_i} \qquad \ell\_{i-1} = \frac{e^{s\_{i-1}
    - m\_{i-1}}}{w\_{i-1}} \tag{5}$$


    Replacing ℓ<sup>i</sup> and ℓi−<sup>1</sup> of [\(5\)](#page-2-4) in the recursive
    equation used to compute the sum of exponents ℓ<sup>i</sup> = ℓi−1e <sup>m</sup>i−1−mi+e
    si−m<sup>i</sup> (repeated from line 4 of Alg. [1\)](#page-1-0) we get that:


    $$\frac{e^{s\_i - m\_i}}{w\_i} = \frac{e^{s\_{i-1} - m\_{i-1}}}{w\_{i-1}} e^{m\_{i-1}
    - m\_i} + e^{s\_i - m\_i} \tag{6}$$


    Simple algebraic manipulation allows us to remove the contribution of mi−<sup>1</sup>
    leading to:


    <span id="page-2-5"></span>

    $$\frac{e^{s\_i - m\_i}}{w\_i} = \frac{e^{s\_{i-1} - m\_i}}{w\_{i-1}} + e^{s\_i
    - m\_i} \tag{7}$$


    The term e −m<sup>i</sup> is common to all terms in [\(7\)](#page-2-5) and it
    can be simplified leading to:


    <span id="page-3-1"></span>

    $$\frac{e^{s\_i}}{w\_i} = \frac{e^{s\_{i-1}}}{w\_{i-1}} + e^{s\_i} \tag{8}$$


    At this point, we have expressed w<sup>i</sup> recursively, depending only on
    the previous weight wi−<sup>1</sup> and neighbor attention scores s<sup>i</sup>
    and si−<sup>1</sup> of the same query vector with respect to two consecutive key
    vectors. Furthermore, the need to compute the running maximum attention score
    is also removed. In Section [III-C,](#page-3-0) we will show that this has no
    negative effects on the numerical stability of computing w<sup>i</sup> .


    To clarify the structure of w<sup>i</sup> in [\(8\)](#page-3-1), we first factor
    out e si and simplify both sides. This transforms [\(8\)](#page-3-1) into


    <span id="page-3-2"></span>

    $$\frac{1}{w\_i} = \frac{e^{s\_{i-1} - s\_i}}{w\_{i-1}} + 1$$


    rearranging the above fraction yields


    $$w\_i = \frac{w\_{i-1}}{w\_{i-1} + e^{s\_{i-1} - s\_i}}.\tag{9}$$


    According to [\(3\)](#page-2-3), w<sup>i</sup> is always positive. Therefore,
    we can replace wi−<sup>1</sup> in [\(9\)](#page-3-2) with e ln <sup>w</sup>i−<sup>1</sup>
    and rewrite it as follows:


    $$\begin{split} w\_i &= \frac{e^{\ln w\_{i-1}}}{e^{\ln w\_{i-1}} + e^{s\_{i-1}
    - s\_i}} = \frac{1}{1 + e^{s\_{i-1} - s\_i - \ln w\_{i-1}}} \\ &= \frac{1}{1 +
    e^{-(s\_i - s\_{i-1} + \ln w\_{i-1})}} \end{split} \tag{10}$$


    Eq. [\(10\)](#page-3-3) reveals that weight w<sup>i</sup> is the result of applying
    sigmoid function σ(x) = 1/(1 + e −x ) to the difference of attention scores si−si−<sup>1</sup>
    achieved by the same query vector for consecutive key vectors incremented (skewed)
    by the natural logarithm of previous weight wi−1. In other words,


    <span id="page-3-4"></span>

    $$\boxed{w\_i = \sigma(s\_i - s\_{i-1} + \ln w\_{i-1})} \tag{11}$$


    #### <span id="page-3-0"></span>*C. FLASH-D: Hiding Softmax Division in FlashAttetion*


    The sigmoid-based computation of the newly defined weights in [\(11\)](#page-3-4)
    and the revised recursive computation of the output derived in [\(4\)](#page-2-6)
    facilitates a more compact reformulation of the FlashAttention kernel that is
    shown in Alg. [3.](#page-3-5) For each attention score s<sup>i</sup> , a new weight
    w<sup>i</sup> is computed based on previous attention score si−<sup>1</sup> and
    weight wi−1, that in turn determines how much the new value vector ⃗v<sup>i</sup>
    would affect the accumulation at the output ⃗o<sup>i</sup> . In the first iteration,
    the weight is set equal to 1.


    The output is gradually formed following the FlashAttention paradigm. For instance,
    following the evolution of the inner loop of Alg. [3,](#page-3-5) w<sup>1</sup>
    is set to 1 that leads to ⃗o<sup>1</sup> = ⃗v1. The first value vector ⃗v<sup>1</sup>
    will be weighted by its corresponding exponential in the next iteration. Specifically,
    in the next iteration, w<sup>2</sup> = 1/(1 + e −(s2−s1) ) = e <sup>s</sup><sup>2</sup>
    /(e <sup>s</sup><sup>1</sup> + e <sup>s</sup><sup>2</sup> ), which gives 1 − w<sup>2</sup>
    = e <sup>s</sup><sup>1</sup> /(e <sup>s</sup><sup>1</sup> + e <sup>s</sup><sup>2</sup>
    ). These coefficients lead to an incremental output ⃗o<sup>2</sup> = e <sup>s</sup>1⃗v1/(e
    <sup>s</sup><sup>1</sup> +e <sup>s</sup><sup>2</sup> )+e <sup>s</sup>2⃗v2/(e <sup>s</sup><sup>1</sup>
    +e <sup>s</sup><sup>2</sup> ). As the iterations progress each ⃗v<sup>i</sup>
    is multiplied to the appropriate exponential, while the sum of exponentials is
    growing including more terms e <sup>s</sup><sup>i</sup> at the denominator of
    each fraction. Computation is completed in Nth step with ⃗o<sup>N</sup> , when
    all key and value vectors have been processed.


    <span id="page-3-5"></span>Algorithm 3 FLASH-D: Division hidden in sigmoid function


    1: for each query ⃗q do 2: for i = 1 : N do 3: s<sup>i</sup> ← dot(⃗q,⃗ki) 4:
    if i ̸= 1 then 5: w<sup>i</sup> ← σ(s<sup>i</sup> − si−<sup>1</sup> − ln wi−1)
    6: else 7: w<sup>i</sup> ← 1 8: end if 9: ⃗o<sup>i</sup> ← ⃗oi−1(1 − wi) + ⃗v<sup>i</sup>
    w<sup>i</sup> 10: end for 11: Attn(⃗q, K, V) ← ⃗o<sup>N</sup> 12: end for


    The proposed Alg[.3](#page-3-5) is a one-to-one equivalent of the baseline FlashAttention
    (Alg[.1\)](#page-1-0), derived through mathematical reformulation without introducing
    any approximations at any stage. The incremental division by the sum of exponents
    in the baseline FlashAttention is effectively hidden within the sigmoid function,
    thereby merging it with the corresponding exponential function evaluations.


    <span id="page-3-3"></span>Even without reducing each attention score s<sup>i</sup>
    by the maximum attention score across all key vectors, the numerical stability
    of FLASH-D is guaranteed. This is because we can completely avoid situations that
    would lead to exponential overflow and instead return the default correct values
    for wi . In fact, the cases that could cause exponential overflows correspond
    to a range of attention score differences s<sup>i</sup> −si−1, where computing
    the weight function becomes meaningless.


    ![](_page_3_Figure_18.jpeg)


    <span id="page-3-6"></span>Fig. 2. Weight w<sup>i</sup> function for various values
    of consecutive attention score differences s<sup>i</sup> − si−1. The four weight
    graphs correspond to four different values of the previous weight wi−1.


    To clarify this argument we plot in Fig. [2](#page-3-6) weight w<sup>i</sup> for
    different attention score differences s<sup>i</sup> − si−1. The graphs correspond
    to four distinct values of the previous weight. Since wi is derived using the
    sigmoid function, its dynamic range is always between 0 and 1. The leftmost graph
    represents wi−<sup>1</sup> = 0.99, which closely follows the standard sigmoid
    function, as ln wi−<sup>1</sup> is effectively zero. As wi−<sup>1</sup> decreases,
    the weight function shifts to the right. In all cases, it is evident that when
    s<sup>i</sup> − si−<sup>1</sup> falls outside the range [-6,11], there is no need
    to compute the weight function explicitly. In such cases, w<sup>i</sup> will be
    very close to either 0 or 1, allowing it to be set directly to the smallest or
    largest values within (0, 1) by default. Consequently, for values of s<sup>i</sup>
    − si−<sup>1</sup> outside the range [-6,11], the exponential within the sigmoid
    function is skipped, thus preventing any overflow condition.


    Additionally, in such cases, output update can be simplified. When s<sup>i</sup>
    − si−<sup>1</sup> ≤ −6, w<sup>i</sup> approaches 0, meaning the output vector
    ⃗o<sup>i</sup> remains unchanged (see line 9 of Alg. [3\)](#page-3-5), eliminating
    the need of loading of ⃗v<sup>i</sup> and performing any for vector multiplication
    or addition. Conversely, when si−si−<sup>1</sup> ≥ 11, w<sup>i</sup> approaches
    1 by default, causing the output vector to effectively "forget" previous value
    contributions and update only with the new value vector ⃗v<sup>i</sup> . In this
    scenario, multiplication and addition operations are also bypassed.


    #### IV. HARDWARE ARCHITECTURE OF FLASH-D


    Flash-D preserves all the characteristics of FlashAttention variants while embedding
    the softmax division within the sigmoid non-linear function, ensuring numerical
    stability is maintained. This transformation greatly simplifies the hardware implementation
    of FLASH-D.


    #### *A. Overall Organization*


    As illustrated in Fig[.3,](#page-4-0) the hardware design of FLASH-D follows the
    same overall architecture of FlashAttention2 (Fig[.1\)](#page-2-0). Both approaches
    share the same dataflow, processing multiple query vectors in parallel in an unrolled
    manner, driven by the same key and value vectors. The primary differences lie
    in three key areas.


    First, the division at the output of FlashAttention2 kernel is not required in
    Flash-D. While division is neither removed nor approximated, it is implemented
    incrementally during the computation of each weight w<sup>i</sup> for each query
    vector.


    Second, the running sum-of-exponents ℓ<sup>i</sup> and the maximum attention score
    m<sup>i</sup> used in FlashAttention2, as shown in Fig. [1,](#page-2-0) are eliminated.
    The running sum-of-exponents ℓ<sup>i</sup> is implicitly embedded in the computation
    of each weight (see Eq. [\(3\)](#page-2-3)), so it does not need to be explicitly
    computed. Additionally, the maximum value is no longer necessary, as numerical
    stability is maintained by ensuring that the attention score difference remains
    within the active region of [-6,11], where the sigmoid function can be effectively
    computed.


    Third, the output computation module in the FLASH-D hardware requires one vector
    adder, one subtractor, and one multiplier, compared to the two multipliers and
    one adder found in FlashAttention2. This hardware simplification, where one multiplier
    is replaced by a subtractor, is made possible by the new weighted definition of
    the output recursion (line 9 of Alg. 3), which can be equivalently written as
    follows:


    $$

    \vec{o}\_i = \vec{o}\_{i-1}(1 - w\_i) + \vec{v}\_i \, w\_i = \vec{o}\_{i-1} +
    \left(\vec{v}\_i - \vec{o}\_{i-1}\right) w\_i \quad (12)

    $$


    # *B. Non-linear function evaluation in FLASH-D*


    The non-linear function evaluations in FLASH-D involve the sigmoid and natural
    logarithm functions, both of which are well-researched and have various hardware
    implementations available. Without focusing on any specific simplifications, this
    work implements both functions using standard piece-wise linear (PWL) approximations.


    This design choice is well justified for both functions. The sigmoid function
    has a well-defined structure and output range,


    ![](_page_4_Figure_13.jpeg)


    <span id="page-4-0"></span>Fig. 3. A parallel hardware architecture for FLASH-D
    kernel for multiple preloaded query vectors.


    making it a natural fit for the PWL approach. Furthermore, in FLASH-D the input
    dynamic range is constrained to [-6, 11], so no computation is required outside
    this range. The natural logarithm function is used solely to compute the natural
    logarithm of the previous weight, i.e., ln wi−1, which has a narrow input dynamic
    range of (0,1). As a result, there is no need to compute a generic logarithm;
    instead, we require one that consistently returns a negative result that follows
    the value of the previous weight. Again, the PWL approach proves to be a suitable
    choice for this case.


    In both cases, we approximated the functions using 8 line segments. The coefficients
    of each segment are produced via pwlf a Python-based PWL-fit optimization library
    [\[29\]](#page-6-28).


    # V. EVALUATION


    The experimental evaluation has two parts. First, we highlight the area and power
    benefits provided by the hardware implementation of FLASH-D (Fig. [3\)](#page-4-0)
    compared to the optimized hardware implementation of the FlashAttention2 kernel
    shown in Fig. [1.](#page-2-0) Second, our goal is to quantify how often the weight
    and output update can be skipped in real LLM applications using FLASH-D as the
    forward pass without affecting the outcome of FlashAttention.


    # *A. Hardware complexity*


    To evaluate the hardware complexity of the two designs, we implemented the main
    block shown in the foreground of Figs. [1](#page-2-0) and [3](#page-4-0) for various
    hidden dimension sizes (d) and for two reduced-precision floating-point formats:
    BFloat16 [\[30\]](#page-6-29) and FP8-E4M3 [\[31\]](#page-6-30). In practice,
    the total cost of the unrolled hardware serving multiple query vectors in parallel
    will be the cost for one query multiplied by the number of parallel query vectors
    served. The query vectors are preloaded separately in the architecture, while
    the key and value vectors are loaded and broadcasted to all parallel blocks. For
    both FLASH-D and FlashAttention2 kernels, we assume that we can read from local
    memories in each cycle one key and one value vector of d elements each.


    ![](_page_5_Figure_0.jpeg)


    <span id="page-5-1"></span>Fig. 4. The hardware area at 28 nm for FLASH-D and
    FlashAttention2 kernel for computing attention of a single query using BFloat16
    and FP8- E4M3 floating-point formats, across different hidden dimension lengths.


    ![](_page_5_Figure_2.jpeg)


    <span id="page-5-2"></span>Fig. 5. The average power for FLASH-D and FlashAttention2
    kernel for computing attention of a single query using BFloat16 and FP8-E4M3 floatingpoint
    formats, across different hidden dimension lengths. Memory and IO power is not
    included since it is identical to both designs.


    Both hardware blocks were implemented in C++[1](#page-5-0) and synthesized into
    Verilog using Catapult HLS with a 28-nm standard-cell library. For verifying the
    correctness of the C++ code, we integrated it to llama2.c [\[32\]](#page-6-31)
    and we received exactly the same replies as the original implementation for all
    examined queries. Both designs operate at the same pipelined latency with a clock
    frequency of 500 MHz. Latency depends on the size of the hidden dimension, requiring
    8, 10, and 12 cycles for d = {16, 64, 256} elements, respectively. Verilog was
    synthesized using the Cadence digital implementation flow, while power consumption
    was estimated with the PowerPro power analysis and optimization tool. The reported
    power consumption represents the average power measured after executing attention
    kernels for various Large Language Models and benchmarks from PromptBench.


    Figs. [4](#page-5-1) and [5](#page-5-2) show the area and power of the proposed
    FLASH-D hardware and the parallel hardware implementation of the FlashAttention2
    computation kernel, for the two examined reduced precision floating-point formats
    and different sizes of the hidden dimension. Power estimation excludes memory
    power and focuses solely on the average power consumption of the computation kernel.
    The memory power in both cases is expected to be identical, as both approaches
    implement the same FlashAttention algorithm using the same computation order and
    data flows. The difference lies solely on how the computation kernel is executed
    internally.


    As shown in Fig. [4,](#page-5-1) FLASH-D reduces the hardware area by more than
    20% in all examined cases. These savings are a direct result of the restructured
    FlashAttention kernel.


    <span id="page-5-3"></span>TABLE I PERCENTAGE OF SKIPPED OUTPUT UPDATES DURING
    INFERENCE ON DIFFERENT NLP BENCHMARKS


    |                                       | Benchmarks |       |       |      |      |                    |

    |---------------------------------------|------------|-------|-------|------|------|--------------------|

    | LLM                                   | CSQA       | GSM8K | QASC  | MMLU |
    Date | Object<br>Tracking |

    | Microsoft /<br>Phi-3-mini-4k-instruct | 0.8%       | 1.7%  | 2.2%  | 2%   |
    1.5% | 2%                 |

    | DeepSeek /<br>Qwen-1.5B               | 2.5%       | 2.0%  | 2.2%  | 2.7% |
    2.4% | 2.8%               |

    | Meta /<br>Llama-3.1-1B                | 1.8%       | 1.6%  | 2.6%  | 2.3% |
    1.6% | 2.3%               |

    | Google /<br>Gemma2-2B                 | 1.2%       | 0.5%  | 0.51% | 1.4% |
    0.8% | 0.83%              |


    Standalone division is eliminated and fused within the sigmoid PWL function evaluation,
    effectively merging the exponential and division operations. One vector multiplier
    is saved in the output update module, and the sum-of-exponents and maximum logic
    are entirely removed.


    The reduction in hardware complexity also improves power consumption, which is
    reduced by more than 16% on average across all examined cases. Power savings are
    expected to increase further, as the computation-skipping criterion developed
    in FLASH-D would save additional memory power, which has not been quantified in
    the presented analysis.


    # *B. How often output update can be simplified*


    To quantify how often the attention score differences fall outside the range [-6,11]
    described in Section [III-C,](#page-3-0) and thereby simplify the output update
    in FLASH-D (line 9 in Alg[.3\)](#page-3-5), we implemented the FLASH-D kernel
    in Python and integrated it into the forward pass of various contemporary LLM
    models available on HuggingFace [\[33\]](#page-6-32). We then performed inference
    on these LLMs using the utilities and benchmarks provided by Microsoft''s PromptBench
    workflow [\[34\]](#page-6-33). The results for each case are summarized in Table
    [I.](#page-5-3)


    In all cases, there is small percentage of cases that output update can be simplified
    with either keeping the previous computed output or loading the new value vector
    without any further calculations. This percentage, even if small, is always a
    win scenario and does not represent any tradeoff across energy savings vs application-level
    performance. In the future, we plan to replace this pessimistic and static range
    check with an adaptive criterion that includes both the range of attention score
    differences and the value of the previous weight to decide when output computation
    can be simplified.


    # VI. CONCLUSIONS


    In this work, our target was to simplify the computational kernel of FlashAttention
    by hiding softmax division inside sigmoid non-linear function evaluation and reduce
    unnecessary multiplications in output computation. This reformulation achieved
    these goals without compromising numerical stability and without negatively affecting
    the favorable IO and memoryaccess properties of FlashAttention algorithm. The
    proposed approach reduces the area and power of the hardware computational kernel
    by 22.8% and 20.3%, on average, respectively, when compared to the parallel hardware
    implementation of FlashAttention2 state-of-the-art algorithm. Any other approach
    that simplifies attention mechanism is orthogonal to FLASH-D and can be applied
    for increasing efficiency further.


    <span id="page-5-0"></span><sup>1</sup>Publicly available on https://github.com/ic-lab-duth/FLASH-D.git


    #### REFERENCES


    - <span id="page-6-0"></span>[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*,
    "An image is worth 16x16 words: Transformers for image recognition at scale,"
    *arXiv preprint arXiv:2010.11929*, 2020.

    - <span id="page-6-1"></span>[2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang,
    R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi *et al.*, "Deepseek-r1: Incentivizing reasoning
    capability in llms via reinforcement learning," *arXiv preprint arXiv:2501.12948*,
    2025.

    - <span id="page-6-2"></span>[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
    L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need,"
    in *Intern. Conf. on Neural Information Processing Systems (NIPS)*, 2017, p. 6000–6010.

    - <span id="page-6-3"></span>[4] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer:
    The longdocument transformer," *arXiv preprint arXiv:2004.05150*, 2020.

    - <span id="page-6-4"></span>[5] R. Child, S. Gray, A. Radford, and I. Sutskever,
    "Generating long sequences with sparse transformers," *arXiv preprint arXiv:1904.10509*,
    2019.

    - <span id="page-6-5"></span>[6] A. Katharopoulos, A. Vyas, N. Pappas, and F.
    Fleuret, "Transformers are rnns: Fast autoregressive transformers with linear
    attention," in *Intern. conference on machine learning*, 2020, pp. 5156–5165.

    - <span id="page-6-6"></span>[7] N. Tang, M. Fu, K. Zhu, and J. Wu, "Low-rank
    attention side-tuning for parameter-efficient fine-tuning," *arXiv preprint arXiv:2402.04009*,
    2024.

    - <span id="page-6-7"></span>[8] T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park,
    Y. Song, J.-H. Park, S. Lee, K. Park, J. W. Lee, and D.-K. Jeong, "A3: Accelerating
    attention mechanisms in neural networks with approximation," in *IEEE Intern.
    Symp. on High-Performance Computer Architecture (HPCA)*, 2020, p. 328–341.

    - <span id="page-6-8"></span>[9] B. Keller, R. Venkatesan, S. Dai, S. G. Tell,
    B. Zimmer, C. Sakr, W. J. Dally, C. T. Gray, and B. Khailany, "A 95.6-TOPS/W deep
    learning inference accelerator with per-vector scaled 4-bit quantization in 5
    nm," *IEEE Journal of Solid-State Circuits*, vol. 58, no. 4, p. 1129–1141, 2023.

    - <span id="page-6-9"></span>[10] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang,
    "Hardware accelerator for multi-head attention and position-wise feed-forward
    in the transformer," in *IEEE Intern. System-on-Chip Conference (SOCC)*, 2020,
    pp. 84–89.

    - <span id="page-6-10"></span>[11] H. Jang, J. Kim, J.-E. Jo, J. Lee, and J. Kim,
    "Mnnfast: a fast and scalable system architecture for memory-augmented neural
    networks," in *Intern. Symp. on Computer Architecture (ISCA)*, 2019, p. 250–263.

    - <span id="page-6-11"></span>[12] Z. Wang, G. Wang, and G. He, "COSA plus: Enhanced
    co-operative systolic arrays for attention mechanism in transformers," *IEEE Trans.
    on Computer-Aided Design of Integrated Circuits and Systems (TCAD)*, vol. 44,
    no. 2, p. 723–736, 2025.

    - <span id="page-6-12"></span>[13] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi,
    S. J. Jung, and J. W. Lee, "ELSA: Hardware-software co-design for efficient, lightweight
    selfattention mechanism in neural networks," in *Intern. Symp. on Computer Architecture
    (ISCA)*, 2021, p. 692–705.

    - <span id="page-6-13"></span>[14] Z. Song, C. Qi, Y. Yao, P. Zhou, Y. Zi, N.
    Wang, and X. Liang, "TSAcc: An efficient tempo-spatial similarity aware accelerator
    for attention acceleration," in *ACM/IEEE Design Automation Conference*, 2024.

    - <span id="page-6-14"></span>[15] S. Sridharan, J. R. Stevens, K. Roy, and A.
    Raghunathan, "X-former: In-memory acceleration of transformers," *IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems*, vol. 31, no. 8, pp. 1223–1233,
    2023.

    - <span id="page-6-15"></span>[16] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re,
    "Flashattention: Fast ´ and memory-efficient exact attention with IO-awareness,"
    *Advances in neural information processing systems*, vol. 35, pp. 16 344–16 359,
    2022.

    - <span id="page-6-16"></span>[17] T. Dao, "Flashattention-2: Faster attention
    with better parallelism and work partitioning," *arXiv preprint arXiv:2307.08691*,
    2023.

    - <span id="page-6-17"></span>[18] M. N. Rabe and C. Staats, "Self-attention does
    not need O(n 2 ) memory," *arXiv preprint arXiv:2112.05682*, 2021.

    - <span id="page-6-18"></span>[19] N. A. Koca, A. T. Do, and C.-H. Chang, "Hardware-efficient
    softmax approximation for self-attention networks," in *Intern. Symp. on Circuits
    and Systems (ISCAS)*, 2023, p. 1–5.

    - <span id="page-6-19"></span>[20] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
    I. Sutskever *et al.*, "Language models are unsupervised multitask learners,"
    *OpenAI blog*, p. 9, 2019.

    - <span id="page-6-20"></span>[21] A. Hatamizadeh, G. Heinrich, H. Yin, A. Tao,
    J. M. Alvarez, J. Kautz, and P. Molchanov, "Fastervit: Fast vision transformers
    with hierarchical attention," in *Intern. Conf. on Learning Representations (ICLR)*,
    2024.

    - <span id="page-6-21"></span>[22] S. Liu, G. Tao, Y. Zou, D. Chow, Z. Fan, K.
    Lei, B. Pan, D. Sylvester, G. Kielian, and M. Saligane, "Consmax: Hardware-friendly
    alternative softmax with learnable parameters," *arXiv preprint arXiv:2402.10930*,
    2024.

    - <span id="page-6-22"></span>[23] M. Milakov and N. Gimelshein, "Online normalizer
    calculation for softmax," *arXiv preprint arXiv:1805.02867*, 2018.

    - <span id="page-6-23"></span>[24] T. Tambe, C. Hooper, L. Pentecost, T. Jia,
    E.-Y. Yang, M. Donato, V. Sanh, P. Whatmough, A. M. Rush, D. Brooks, and G.-Y.
    Wei, "Edge-BERT: sentence-level energy optimizations for latency-aware multi-task


    NLP inference," in *Intern. Symposium on Microarchitecture (MICRO)*, 2021, p.
    830–844.


    - <span id="page-6-24"></span>[25] K. Alexandridis and G. Dimitrakopoulos, "Online
    alignment and addition in multiterm floating-point adders," *IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems*, vol. 33, no. 4, pp. 1182–1186,
    2025.

    - <span id="page-6-25"></span>[26] D. Filippas, C. Nicopoulos, and G. Dimitrakopoulos,
    "Templatized fused vector floating-point dot product for high-level synthesis,"
    *Journal of Low Power Electronics and Applications*, vol. 12, no. 4, p. 56, 2022.

    - <span id="page-6-26"></span>[27] W. Wang, S. Zhou, W. Sun, P. Sun, and Y. Liu,
    "SOLE: hardwaresoftware co-design of softmax and layernorm for efficient transformer
    inference," in *IEEE/ACM Intern. Conference on Computer Aided Design (ICCAD)*,
    2023, p. 1–9.

    - <span id="page-6-27"></span>[28] G. C. Cardarilli, L. Di Nunzio, R. Fazzolari,
    D. Giardino, A. Nannarelli, M. Re, and S. Spano, "A pseudo-softmax function for
    hardware-based ` high speed image classification," *Scientific Reports*, vol.
    11, 2021.

    - <span id="page-6-28"></span>[29] C. F. Jekel and G. Venter, *pwlf: A Python
    Library for Fitting 1D Continuous Piecewise Linear Functions*, 2019.

    - <span id="page-6-29"></span>[30] D. Kalamkar, D. Mudigere, N. Mellempudi, D.
    Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen
    *et al.*, "A study of bfloat16 for deep learning training," *arXiv preprint arXiv:1905.12322*,
    2019.

    - <span id="page-6-30"></span>[31] P. Micikevicius, D. Stosic, N. Burgess, M.
    Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu *et
    al.*, "Fp8 formats for deep learning," *arXiv preprint arXiv:2209.05433*, 2022.

    - <span id="page-6-31"></span>[32] A. Karpathy, "llama2.c," [https://github.com/karpathy/llama2.c.git,](https://github.com/karpathy/llama2.c.git)
    2023.

    - <span id="page-6-32"></span>[33] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C.
    Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,
    P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,
    Q. Lhoest, and A. M. Rush, "Transformers: State-of-the-art natural language processing,"
    in *Conference on Empirical Methods in Natural Language Processing: System Demonstrations*,
    2020, pp. 38–45.

    - <span id="page-6-33"></span>[34] K. Zhu, Q. Zhao, H. Chen, J. Wang, and X. Xie,
    "Promptbench: A unified library for evaluation of large language models," *arXiv
    preprint arXiv:2312.07910*, 2023.'
- title: "Genesis: A Compiler Framework for Hamiltonian Simulation on Hybrid CV-DV\n\
    \  Quantum Computers"
  abstract: 'This paper introduces Genesis, the first compiler designed to support

    Hamiltonian Simulation on hybrid continuous-variable (CV) and discrete-variable

    (DV) quantum computing systems. Genesis is a two-level compilation system. At

    the first level, it decomposes an input Hamiltonian into basis gates using the

    native instruction set of the target hybrid CV-DV quantum computer. At the

    second level, it tackles the mapping and routing of qumodes/qubits to implement

    long-range interactions for the gates decomposed from the first level. Rather

    than a typical implementation that relies on SWAP primitives similar to

    qubit-based (or DV-only) systems, we propose an integrated design of

    connectivity-aware gate synthesis and beamsplitter SWAP insertion tailored for

    hybrid CV-DV systems. We also introduce an OpenQASM-like domain-specific

    language (DSL) named CVDV-QASM to represent Hamiltonian in terms of

    Pauli-exponentials and basic gate sequences from the hybrid CV-DV gate set.

    Genesis has successfully compiled several important Hamiltonians, including the

    Bose-Hubbard model, $\mathbb{Z}_2-$Higgs model, Hubbard-Holstein model,

    Heisenberg model and Electron-vibration coupling Hamiltonians, which are

    critical in domains like quantum field theory, condensed matter physics, and

    quantum chemistry. Our implementation is available at

    Genesis-CVDV-Compiler(https://github.com/ruadapt/Genesis-CVDV-Compiler).'
  url: http://arxiv.org/abs/2505.13683v1
  keywords: ''
  document: "Zihan Chen<sup>∗</sup> zihan.chen.cs@rutgers.edu Rutgers University Piscataway,\
    \ New Jersey, USA\n\nHenry Chen hc867@scarletmail.rutgers.edu Rutgers University\
    \ Piscataway, New Jersey, USA\n\nYipeng Huang yipeng.huang@rutgers.edu Rutgers\
    \ University Piscataway, New Jersey, USA\n\nJiakang Li<sup>∗</sup> jiakang.li@rutgers.edu\
    \ Rutgers University Piscataway, New Jersey, USA\n\nZirui Li zl606@scarletmail.rutgers.edu\
    \ Rutgers University Piscataway, New Jersey, USA\n\nHuiyang Zhou hzhou@ncsu.edu\
    \ North Carolina State University Raleigh, North Carolina, USA\n\nEddy Z. Zhang\
    \ eddy.zhengzhang@gmail.com Rutgers University Piscataway, New Jersey, USA\n\n\
    Minghao Guo<sup>∗</sup> minghao.guo@rutgers.edu Rutgers University Piscataway,\
    \ New Jersey, USA\n\nJoel Bierman jhbierma@ncsu.edu North Carolina State University\
    \ Raleigh, North Carolina, USA\n\nYuan Liu q\\_yuanliu@ncsu.edu North Carolina\
    \ State University Raleigh, North Carolina, USA\n\n## ABSTRACT\n\nThis paper introduces\
    \ Genesis, the first compiler designed to support Hamiltonian Simulation on hybrid\
    \ continuous-variable (CV) and discrete-variable (DV) quantum computing systems.\
    \ Genesis is a two-level compilation system. At the first level, it decomposes\
    \ an input Hamiltonian into basis gates using the native instruction set of the\
    \ target hybrid CV-DV quantum computer. At the second level, it tackles the mapping\
    \ and routing of qumodes/qubits to implement long-range interactions for the gates\
    \ decomposed from the first level. Rather than a typical implementation that relies\
    \ on SWAP primitives similar to qubit-based (or DV-only) systems, we propose an\
    \ integrated design of connectivity-aware gate synthesis and beamsplitter SWAP\
    \ insertion tailored for hybrid CV-DV systems. We also introduce an OpenQASM-like\
    \ domain-specific language (DSL) named CVDV-QASM to represent Hamiltonian in terms\
    \ of Pauli-exponentials and basic gate sequences from the hybrid CV-DV gate set.\
    \ Genesis has successfully compiled several important Hamiltonians, including\
    \ the Bose-Hubbard model, Z2−Higgs model, Hubbard-Holstein model, Heisenberg model\
    \ and Electron-vibration coupling Hamiltonians, which are critical in domains\
    \ like quantum field theory, condensed matter physics, and quantum chemistry.\
    \ Our implementation is available at Genesis-CVDV-Compiler [https://github.com/ruadapt/Genesis-CVDV-Compiler.](https://github.com/ruadapt/Genesis-CVDV-Compiler)\n\
    \n#### KEYWORDS\n\nQuantum Computing, Hamiltonian Simulation, Hybrid CV-DV, Quantum\
    \ Architecture, Compiler\n\n# 1 INTRODUCTION\n\nTo date, most quantum computing\
    \ architectures are homogeneous, featuring two-state, discrete-variable (DV) realizations\
    \ as qubits. The hybrid continuous-variable discrete-variable (CV-DV) quantum\
    \ architecture is an emerging platform incorporating both qubits and qumodes.\
    \ A qumode has a countable infinity of states in principle, thereby providing\
    \ a larger Hilbert space for computation, and often has a longer lifetime than\
    \ that of a qubit. As a result, qumodes have been an attractive target for quantum\
    \ error correction. For instance, superconducting cavity architecture was the\
    \ first to achieve memory quantum error correction above the break-even point\
    \ [\\[4,](#page-12-0) [8,](#page-13-0) [13,](#page-13-1) [16,](#page-13-2) [31,](#page-13-3)\
    \ [34,](#page-13-4) [35,](#page-13-5) [39\\]](#page-13-6) with bosonic codes.\n\
    \nMost notably, a hybrid CV-DV system can simulate mixtures of fermionic and bosonic\
    \ matter. Simulating physical systems has long been considered a key killer application\
    \ of quantum computers, as originally proposed by Feynman [\\[12\\]](#page-13-7).\
    \ While DV qubits can potentially address Fermion simulation challenges, they\
    \ are poorly suited for bosonic fields due to the difficulty of mapping bosonic\
    \ systems to qubits and implementing bosonic field operators on infinitedimensional\
    \ Hilbert spaces. In contrast, bosonic field operators are natively available\
    \ in hardware with bosonic modes or qumodes. Recent work has demonstrated the\
    \ great potential of simulating Fermion-Boson mixtures on hybrid CV-DV quantum\
    \ processors for applications such as material discovery [\\[23\\]](#page-13-8),\
    \ molecular simulation [\\[47,](#page-14-0) [48\\]](#page-14-1), topological models\
    \ [\\[37\\]](#page-13-9), and lattice gauge theory [\\[6\\]](#page-13-10).\n\n\
    In this paper, we provide compiler support for mapping Hamiltonian simulation\
    \ instances onto hybrid CV-DV processors. Compilation support for hybrid CV-DV\
    \ architectures is still in its infancy. Existing tools, such as Bosonic Qiskit\
    \ [\\[40\\]](#page-13-11), StrawberryField [\\[21\\]](#page-13-12), Perceval [\\\
    [49\\]](#page-14-2), and Bosehedral [\\[33\\]](#page-13-13), offer preliminary\
    \ support for programming, simulating, and composing circuits—primarily for\n\n\
    <sup>∗</sup>These authors have equal contributions.\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\
    \nFigure 1: A typical hybrid CV-DV architecture using the superconducting technology.\
    \ The qumodes are connected in a sparse manner. Each qubit connects to a qumode,\
    \ and there is no direct connection between qubits [\\[28\\]](#page-13-14).\n\n\
    domain-specific applications like Gaussian Boson Sampling (GBS) or general bosonic\
    \ circuits. However, none support hybrid CV-DV Hamiltonian simulation. This compilation\
    \ process requires synthesizing a circuit from the Hamiltonian operator's mathematical\
    \ representation and mapping the synthesized logical circuit onto a physical circuit\
    \ while ensuring compatibility with hardware constraints (an example of a superconducting\
    \ hybrid CV-DV architecture is shown in Fig. [1\\)](#page-1-0).\n\nOur work fills\
    \ this gap. We introduce Genesis, the first comprehensive compilation framework\
    \ for Hamiltonian simulation on hybrid CV-DV computers. It consists of two levels\
    \ of compilation: Level-1 decomposes an n-qubit-m-qumode Hamiltonian into universal\
    \ basis gates, while Level-2 compilation deals with hardware constraints, including\
    \ topology constraints, multi-qubit gates, and ancilla qubit/qumode allocation.\
    \ Our framework successfully compiles important Hamiltonians, such as the Bose-Hubbard\
    \ model, Z2-Higgs model, Hubbard-Holstein model, and vibration-electron (vibronic)\
    \ coupling Hamiltonians, crucial in quantum field theory, condensed matter physics,\
    \ and quantum chemistry. We make the following contributions.\n\n- We propose\
    \ the first compiler to synthesize circuits from hybrid CV-DV Hamiltonians using\
    \ a template-rewriting approach based on product formulas and trotterization.\
    \ This formulation enables automatic rule search and allows further rule expansion\
    \ without modifying the core algorithm.\n- When synthesizing gates from a Hamiltonian,\
    \ the universality of bosonic systems often requires hybrid qubit-qumode operations.\
    \ Therefore, the synthesis often requires the allocation of ancilla qubits or\
    \ qumodes, as well as the mapping and routing of these ancilla qubits/qumodes\
    \ in the computation. This feature is implemented in our compiler.\n- We also\
    \ provide the first compilation support for multiqubit Pauli gate implementation\
    \ in a hybrid CV-DV system. As popular hybrid CV-DV architectures do not have\
    \ connectivity among qubits, rather, only connectivity exists among qumodes and\
    \ between qubits and qumodes, we propose to synthesize multi-qubit Pauli gates\
    \ by leveraging an effect similar to phase kickback in DV systems.\n- Besides\
    \ the compiler support, we propose a domain-specific language (DSL) design for\
    \ hybrid CV-DV Hamiltonian simulation, which represents Hamiltonians as Pauli\
    \ strings and basic gate sequences.\n- We conducted extensive experiments, evaluating\
    \ benchmarks with 600-1900 multi-qubit Pauli-string Hamiltonian terms and six\
    \ key Fermion-Boson Hamiltonian models. We assessed different mapping, routing,\
    \ and qumode allocation strategies, including the Floating Qubit approach.\n\n\
    #### 2 BACKGROUND AND MOTIVATION\n\n# 2.1 Advantage of Hybrid CV-DV Systems over\
    \ CV-only and DV-only Systems\n\nCompared with CV-only or DV-only systems, hybrid\
    \ CV-DV systems offer several key advantages as summarized in Table [1.](#page-1-1)\n\
    \n<span id=\"page-1-1\"></span>\n\n| System Characteristics                  \
    \                                                              | Hybrid CV-DV\
    \ | CV-Only        | DV-Only         |  |\n|-------------------------------------------------------------------------------------------------------|--------------|----------------|-----------------|--|\n\
    | Energy Truncation for Sim<br>ulating Bosonic States &<br>Operators         \
    \                           | Not-required | Not-required   | Required       \
    \ |  |\n| Support for Simulating Na<br>tive Bosonic Operators (e.g.<br>Square-root\
    \ Factors under<br>Fock basis) | Easy         | Easy           | Non-trivial [6]\
    \ |  |\n| Gaussian Resource Gener<br>ation Difficulty                        \
    \                                   | Easy         | Easy           | Easy [22]\
    \       |  |\n| Resource<br>Non-Gaussian<br>Generation Difficulty            \
    \                                         | Easy         | Difficult [46] | N/A\
    \             |  |\n| Error Channel Complexity                               \
    \                                               | Medium       | Low [28]    \
    \   | High            |  |\n\nTable 1: Hybrid CV-DV, CV-only and DV-only Systems.\n\
    \n(1) Advantage in simulation: Quantum simulation is one of the most promising\
    \ applications of quantum computers. However, simulating fermion-boson mixtures\
    \ is challenging for DV systems. While mapping fermion operations to qubit operations\
    \ is possible, representing a bosonic mode with qubits requires truncating its\
    \ infinite-dimensional Hilbert space. Moreover, implementing native bosonic operations\
    \ in DV hardware is further complicated, for example, by the quantum arithmetic\
    \ needed to realize square-root factors under the Fock basis [\\[6,](#page-13-10)\
    \ [8,](#page-13-0) [13,](#page-13-1) [14\\]](#page-13-16). In contrast, CV hardware\
    \ employs oscillators with infinite-dimensional Hilbert space and has intrinsic\
    \ support for native bosonic operators, but has limited support for modeling fermions.\
    \ A hybrid CV-DV architecture combines the strengths of both: it harnesses the\
    \ larger Hilbert space of the CV system while leveraging discrete qubits to perform\
    \ fermion-mapped operations in the simulation of fermion-boson mixtures.\n\n(2)\
    \ Advantage in providing non-Gaussian resources: Achieving universal CV-based\
    \ quantum computation requires non-Gaussian operations, such as cubic interactions\
    \ [\\[28,](#page-13-14) [46\\]](#page-14-3). However, non-Gaussian gates are challenging\
    \ to realize on CV-only platforms. Alternatively, universal control of oscillators\
    \ can also be achieved by the addition of qubit-controlled oscillator gates, which\
    \ are more straightforward and much less costly and have been demonstrated successfully\
    \ in the lab for superconducting-cavity circuits [\\[11,](#page-13-17) [42\\]](#page-13-18),\
    \ trapped-ion, and neutral-atom architectures [\\[3,](#page-12-1) [7,](#page-13-19)\
    \ [15,](#page-13-20) [17,](#page-13-21) [38\\]](#page-13-22).\n\n<span id=\"page-2-1\"\
    ></span>![](_page_2_Figure_1.jpeg)\n\nFigure 2: Compilation workflow of Genesis.\
    \ It first decomposes the Hamiltonian into CV-DV basis gate sets, as well as the\
    \ Pauli gate we defined in this paper, in the CVDV-QASM language format. Finally,\
    \ it considers connectivity constraints, performs the hardware mapping and routing\
    \ stage, and outputs physical circuits.\n\n(3) Advantage in error modeling and\
    \ QEC: In many CV implementations (such as superconducting resonators or optical\
    \ modes), photon loss (plus minor phase noise) is the primary error source, yielding\
    \ Gaussian noise channels [\\[9,](#page-13-23) [43,](#page-13-24) [47\\]](#page-14-0).\
    \ Notably, error mechanisms in CV systems are simpler for a single oscillator\
    \ with many levels than for multiple qubits of equivalent total dimensionality.\
    \ Using multiple qubits to represent one oscillator introduces complex error sources,\
    \ such as crosstalk and correlated errors across control lines, gates, and measurements.\
    \ In contrast, CV's core error model is comparatively easier to characterize and\
    \ correct. This simplicity is why bosonic error-correcting codes were the first\
    \ to achieve the memory break-even point [\\[34,](#page-13-4) [39\\]](#page-13-6).\
    \ Hybrid CV-DV systems must address both oscillator photon loss and qubit errors\
    \ but typically require only one (or a few) qubits per oscillator, reducing qubit\
    \ crosstalk and simplifying error correction and fault tolerance.\n\n#### 2.2\
    \ Hamiltonian Simulation\n\nHamiltonian Simulation plays a crucial role in understanding\
    \ the quantum dynamics of various systems and scenarios in quantum physics, chemistry,\
    \ and materials science [\\[10\\]](#page-13-25). A universal quantum simulator\
    \ is built upon the time evolution operator <sup>−</sup>, where , known as Hamiltonian,\
    \ typically represents a Hermitian operator, and represents time (here and the\
    \ rest of the paper, we take the Planck's constant as ℏ = 1). The goal of the\
    \ Hamiltonian simulation is to decompose the operator <sup>−</sup> into a sequence\
    \ of basis gates for a given hardware backend. This decomposition enables quantum\
    \ computers to approximate the system's evolution over time.\n\nA generic n-qubit-m-qumode\
    \ Hamiltonian for a hybrid CV-DV quantum system can be represented as follows:\n\
    \n$$H = \\sum\\_{k=0}^{4^n - 1} P\\_k h\\_k \\left(\\mathring{a}\\_1, \\mathring{a}\\\
    _1^\\dagger, \\mathring{a}\\_2, \\mathring{a}\\_2^\\dagger, \\cdots, \\mathring{a}\\\
    _m, \\mathring{a}\\_m^\\dagger\\right) \\tag{1}$$\n\nThe operator represents an\
    \ element of the Pauli basis (, , , ) on Qubits, where = 0, 1, . . . , 4 − 1.\
    \ The function ℎ (ˆ , ˆ † ) is a finite-degree polynomial in terms of the annihilation\
    \ ˆ and creation operator ˆ † on qumode .\n\nAn example is the following spin-Holstein\
    \ model on N qubits and N qumodes [\\[23\\]](#page-13-8). The first term acts\
    \ on the −th qubit and qumode jointly, and the second term acts on the −th qumode:\n\
    \n<span id=\"page-2-0\"></span>\n$$H = \\sum\\_{i}^{N} \\frac{g\\_i}{2} Z\\_i\
    \ (\\hat{a}\\_i^\\dagger + \\hat{a}\\_i) + \\sum\\_{i}^{N} \\frac{g\\_i}{2} I\
    \ (\\hat{a}\\_i^\\dagger + \\hat{a}\\_i) \\tag{2}$$\n\nThe compilation goal is\
    \ to map the time evolution of a Hamiltonian, i.e., <sup>−</sup>, into a minimum\
    \ and efficient set of basis gates on the hardware. The basis gates include single-qubit\
    \ gate rotation, single-qumode gates, multi-qumode, and hybrid qubit-qumode gates.\
    \ A representative gate set is shown in Table [2.](#page-3-0) Note that twoqubit\
    \ gates are not included in the basis gates, as there is no direct connection\
    \ between qubits.\n\n# 2.3 The Challenges for Compiling A Hybrid CV-DV Hamiltonian\
    \ Application\n\nCompiling Hamiltonian simulation on a CV-DV system is more challenging\
    \ than compiling that on a DV system. It must account for (a) unique qumode gate\
    \ decomposition rules, (b) multi-qubit Paulistring synthesis, and (3) architecture\
    \ constraints when performing Qubit/Qumode mapping and routing. Specifically,\
    \ we describe three fundamental differences and challenges:\n\n#### Challenge\
    \ 1: Qumode-focused Gate Synthesis\n\nWe need to synthesize qumode-only gates\
    \ for the Hermitian polynomial of annihilation and creation operators of qumodes.\
    \ For simple Hamiltonian terms, we can perform pattern matching to identify the\
    \ right basis gate to implement them. For instance, for the Hamiltonian in Equation\
    \ [2,](#page-2-0) through Trotterization [\\[29\\]](#page-13-26), we can convert\
    \ <sup>−</sup> into a product of two terms − 2 (ˆ † +ˆ ) and − 2 (ˆ † +ˆ ) for\
    \ each .\n\n<span id=\"page-2-2\"></span>The first term can be pattern-matched\
    \ to the Control Displacement gate (†− <sup>∗</sup>) in Table [2,](#page-3-0)\
    \ by setting the displacement parameter = − 2 . The second term can be pattern\
    \ matched to the displacement gate (†− <sup>∗</sup>) by parameterizing the displacement\
    \ = − 2 .\n\nWhile we have a comprehensive basis gate set of Qubit and Qumode\
    \ operators, not every term in a Hamiltonian (after Trotterization) can be directly\
    \ mapped to a basis gate or a combination of basis gates in the quantum hardware.\
    \ For instance, the term ( † ) 2 2\n\n<span id=\"page-3-0\"></span>\n\n| Type\
    \   | Gate Name                                      | Definition            \
    \                                                       |  |  |  |  |  |\n|--------|------------------------------------------------|------------------------------------------------------------------------------|--|--|--|--|--|\n\
    | Qubit  | \U0001D465, \U0001D466<br>Rotation                               |\
    \ \U0001D45F\U0001D711 (\U0001D703) = exp h<br>\U0001D703<br>i<br>−\U0001D456\
    <br>(cos\U0001D711\U0001D70E\U0001D465 + sin\U0001D711\U0001D70E\U0001D466)<br>2\
    \                     |  |  |  |  |  |\n|        | \U0001D467 Rotation       \
    \                              | \U0001D45F\U0001D467 (\U0001D703) = exp<br>\U0001D703\
    <br><br>−\U0001D456<br>2 \U0001D70E\U0001D467                                \
    \          |  |  |  |  |  |\n|        | Phase-Space Rotation                 \
    \          | R(\U0001D703) = exp<br>−\U0001D456\U0001D703\U0001D44E†\U0001D44E\
    <br>                                                     |  |  |  |  |  |\n| Qumode\
    \ | Displacement                                   | D(\U0001D6FC) = exp h \U0001D6FC\
    \U0001D44E† − \U0001D6FC<br>i<br>∗\U0001D44E                                 \
    \             |  |  |  |  |  |\n|        | Beam-Splitter                     \
    \             | BS(\U0001D703, \U0001D711)<br>= exp h<br>\U0001D703<br><br>i<br>\U0001D456\
    \U0001D711\U0001D44E<br>†\U0001D44F + \U0001D452−\U0001D456\U0001D711\U0001D44E\
    \U0001D44F†<br>−\U0001D456<br>\U0001D452<br>2     |  |  |  |  |  |\n|        |\
    \ Conditional Phase-Space Rotation CR(\U0001D703) = exp h | \U0001D703<br>i<br>†\U0001D44E\
    <br>−\U0001D456<br>2 \U0001D70E\U0001D467\U0001D44E                          \
    \                        |  |  |  |  |  |\n|        | Conditional Parity     \
    \                        | \U0001D70B<br>†\U0001D44E<br>−\U0001D456<br>2 \U0001D70E\
    \U0001D467\U0001D44E<br>CP = exp<br>                                       | \
    \ |  |  |  |  |\n| Hybrid | Conditional Displacement                       | CD(\U0001D6FC\
    ) = exp h<br><br>i<br>\U0001D6FC\U0001D44E† − \U0001D6FC<br>∗\U0001D44E<br>\U0001D70E\
    \U0001D467                                |  |  |  |  |  |\n|        | Conditional\
    \ Beam-Splitter                      | CBS(\U0001D703, \U0001D711)<br>i<br>= exp\
    \ h<br>\U0001D703<br><br>\U0001D456\U0001D711\U0001D44E<br>†\U0001D44F + \U0001D452\
    −\U0001D456\U0001D711\U0001D44E\U0001D44F†<br>−\U0001D456<br>2 \U0001D70E\U0001D467\
    <br>\U0001D452 |  |  |  |  |  |\n|        | Rabi Interaction                 \
    \              | RB(\U0001D703) = exp h<br><br>i<br>\U0001D703\U0001D44E† − \U0001D703\
    <br>∗\U0001D44E<br>−\U0001D456\U0001D70E\U0001D465                           \
    \   |  |  |  |  |  |\n\nTable 2: Basis gates in the Hybrid CV-DV System, where\
    \ terms represent Pauli terms acting on Qubits, e.g., is the Pauli-Z operator.\
    \ and † are the annihilation and creation operators acting on Qumode. Between\
    \ different Qubits and Qumode is the tensor product ⊗. We omit the ⊗ symbol following\
    \ the convention in the physics literature [\\[20\\]](#page-13-27). Singlequbit\
    \ or single-qumode gates typically take around 20 ns [\\[11\\]](#page-13-17).\
    \ A two-qumode gate or hybrid qubit-qumode gate typically runs in the range of\
    \ 400−1000 ns [\\[5,](#page-12-2) [11\\]](#page-13-17).\n\nis a Kerr non-linearity\
    \ [\\[20\\]](#page-13-27) that performs a simulation of the Kerr Effect in optics,\
    \ and it is not easy to be directly implemented in the hardware. It is, in fact,\
    \ a complex non-linear term requiring a product formula and multiple steps of\
    \ gate decomposition. In the past, such decomposition was done manually by physicists\
    \ or theorists. This manual decomposition approach may be timeconsuming and error-prone.\
    \ Furthermore, the hardware vendors typically provide an overcomplete gate set\
    \ for more flexible and robust selections of gate operations. This further complicates\
    \ the manual decomposition process.\n\n#### Challenge 2: Multi-Qubit Pauli-string\
    \ Synthesis\n\nSince we aim to support the simulation of a generic hybrid CV-DV\
    \ Hamiltonian, we need to consider qubit-only terms – Paulistring terms. The simulation\
    \ of Pauli-string terms [\\[18,](#page-13-28) [25,](#page-13-29) [26,](#page-13-30)\
    \ [36\\]](#page-13-31) in Hamiltonians has been extensively studied for DV systems.\
    \ A Paulistring representation denotes tensor products of Pauli-matrices. Unlike\
    \ qubits that are connected in a DV system, qubits are typically not directly\
    \ connected in a hybrid CV-DV system, which is built upon either superconducting\
    \ or trapped-ion devices [\\[28\\]](#page-13-14). Instead, qubits are connected\
    \ to qumodes, and qumodes are connected (more details discussed in Challenge 3).\
    \ A previous idea that uses a sequence of (control) displacement gates to cancel\
    \ out the effect on a qumode and then \"kick back\" the phase to the qubits has\
    \ successfully implemented , CNOT, and Toffoli gates. Inspired by this idea, we\
    \ develop a method to synthesize an arbitrary multi-qubit Pauli-string Hamiltonian\
    \ by leveraging a multi-qubit Controlled Displacement gate and a trajectory of\
    \ Displacement gates with respect to different eigenvectors. Moreover, this technique\
    \ could be extended to create rules for qumode operations controlled by arbitrary\
    \ multi-qubit Pauli-strings.\n\n#### Challenge 3: Limited Connectivity Constraints\n\
    \nAfter gate synthesis from a Hamiltonian, there are three types of gates: two-qumode\
    \ gates, single-qubit/qumode gates, and hybrid qubit-qumode gates. These gates\
    \ are generated at the logical\n\nlevel. When mapped to the physical hardware\
    \ layer, there may be long-range interactions. Just like DV architectures, in\
    \ the hybrid CV-DV architectures, qubits and qumodes are often not all-to-all\
    \ connected. Most current hybrid CV-DV systems are designed with either only qumodes\
    \ connected among themselves, or qubits connected among themselves, but not both\
    \ at the same time, due to crosstalk issues [\\[28\\]](#page-13-14). We focus\
    \ on the architecture where qumodes are connected, and qubits can be indirectly\
    \ connected via qumodes. An example of a superconducting CV-DV device is shown\
    \ in Fig. [1.](#page-1-0) The reason is that a qumode has an infinite energy level\
    \ and can store the information of a qubit, but not vice versa. Also, for the\
    \ simulation of Fermion-Boson-mixture, the connectivity among qumodes naturally\
    \ allows the modeling of bosonic interactions.\n\nQubit/qumode mapping and routing\
    \ are tightly connected with circuit synthesis described in Challenge 1 and 2.\
    \ First of all, the synthesis rules allow certain flexibilities, such as the commutation\
    \ of gates, which can be leveraged to improve the mapping and routing stage to\
    \ reduce the number of intrinsic SWAP gates. Moreover, the synthesis approach\
    \ often requires ancilla qumodes, and most importantly, in certain cases, such\
    \ as compiling multi-qubit entanglement operations with control displacement,\
    \ it does not require the ancilla qumode to be in the vacuum (ground) state. Rather,\
    \ it can return the ancilla qumode back to its original state after the sequence\
    \ is completed for synthesizing a qubit/qumode operation of interest. That means\
    \ any qumode can be used to assist synthesis and also can be leveraged to improve\
    \ the mapping and routing efficiency in the hardware circuit compilation stage.\
    \ This is different from mapping and routing in the traditional DV system, where\
    \ only SWAP insertion is considered. In certain cases, it can be modeled as a\
    \ traveling salesman problem (TSP) as described in our Section [3.4.1.](#page-8-0)\
    \ We develop a synergistic mapping and routing approach that considers the flexibilities\
    \ in circuit synthesis.\n\nSummary: Overall, our work is the first comprehensive\
    \ compiler framework that addresses the compilation of Hamiltonian simulation\
    \ on hybrid CV-DV architectures. It addresses both the gate synthesis and hardware\
    \ mapping problems, as well as identifies and models the unique compilation problems\
    \ that only arise in the hybrid CV-DV architecture rather than in the traditional\
    \ DV context. We show our workflow in Fig. [2,](#page-2-1) which shows the process\
    \ of how a given Hamiltonian is decomposed into physical circuits step by step\
    \ by our methods. We present our design details in Section [3](#page-4-0) corresponding\
    \ to each of the three challenges.\n\n#### 2.4 Current State of CV-DV Technologies\n\
    \nCV-DV systems can be implemented with superconducting, trappedion, or neutral\
    \ atom devices. In superconducting devices, transmons are used as two-level systems\
    \ (DV), and microwave resonators storing photons are used as harmonic oscillators\
    \ (CV). In trapped-ion devices, collective motional modes are oscillators, while\
    \ ions' internal degrees of freedom are used as qubits. With neutral atoms, atomic\
    \ motional modes in the optical tweezer are oscillators, and neutral atoms' internal\
    \ degrees of freedom are used as qubits.\n\nAll three technologies have been demonstrated\
    \ successfully in labs. Superconducting devices typically operate in GHz, trapped-ion\
    \ in MHz, and neutral atom in KHz. Among them, superconducting devices offer advantages\
    \ compared to the other two architectures, such as fast and reliable basis gate\
    \ sets. For instance, microwavecontrolled beam-splitter gates permit fast, high-fidelity\
    \ oscillator SWAPs, usually in 100 ns, with 99.92% fidelity in the single photon\
    \ subspace [\\[30\\]](#page-13-32). Hence, the superconducting architecture is\
    \ our primary evaluation platform in this work, and our work can be extended to\
    \ other hybrid CV-DV architectures.\n\n#### <span id=\"page-4-0\"></span>3 OUR\
    \ COMPILATION FRAMEWORK\n\nAs shown in Equation [1,](#page-2-2) the Hamiltonian\
    \ consists of contributions from Pauli operators (, , , ) and qumode operators\
    \ ( † , ). We synthesize them into a physical circuit with basis gates while considering\
    \ hardware connectivity constraints in CV-DV systems.\n\n## 3.1 Qumode Gate Synthesis\n\
    \n<span id=\"page-4-2\"></span>3.1.1 Pattern Matching. For sequences involving\
    \ first or secondorder Hermitian polynomials of annihilation/creation operators,\
    \ we first try to map them to the basic gates in the hybrid CV-DV gate set (Table\
    \ [2\\)](#page-3-0) through pattern matching. The corresponding basic gates are\
    \ applied directly if the operators align with specific gate patterns with parameterization\
    \ – most of the bosonic gates have continuous parameters. These sequences can\
    \ then be synthesized into implementable gate operations. For example, common\
    \ terms like † and ( † − ) can be mapped to the Phase-Space Rotation gate and\
    \ the Displacement gate, respectively, as shown below:\n\n$$e^{(-\ntau^{\n\\dagger}a)}\
    \ \\to R(t), \\text{ where } R(\\theta) = e^{-i\\theta a^{\\dagger}a}$$\n\n$$e^{(3ia^{\\\
    dagger} + 3ia)} \\to D(3i), \\text{ where } D(a) = e^{aa^{\\dagger} - \\alpha^\\\
    *a}$$\n\nThe basic gate set in Table [2](#page-3-0) defines the matching rules,\
    \ which allow further extension for wider functionally or architecturespecific\
    \ gates. However, it only works for simple and fine-grained qumode operator sequences.\
    \ For complex and large-scale annihilation/creation operator polynomials, we propose\
    \ the following recursive decomposition process to synthesize them.\n\n<span id=\"\
    page-4-3\"></span>3.1.2 Template Matching Decomposition and Approximation. Most\
    \ Hamiltonians are not directly implementable on quantum hardware and require\
    \ further decomposition and approximation. Two common methods for Hamiltonian\
    \ decomposition are Trotterization and the Baker Campbell Hausdorff (BCH) formula,\
    \ both widely used in quantum circuit synthesis [\\[1,](#page-12-3) [20,](#page-13-27)\
    \ [27,](#page-13-33) [32\\]](#page-13-34).\n\nTrotterization: The Trotter-Suzuki\
    \ decomposition approximates the time evolution operator by breaking it into simplified\
    \ exponentials:\n\n$$e^{\\left(M+N\\right)t} \\approx \\left(e^{Mt'}e^{Nt'}\\\
    right)^n,\\tag{4}$$\n\nwhere and are parts of the Hamiltonian, ′ = /, and is the\
    \ number of Trotter steps. This method is particularly effective for approximating\
    \ long-time evolution by transforming it into discrete, simplified steps. The\
    \ purpose of Trotterization is to break a Hamiltonian into a sum of terms, which\
    \ later can be used for approximation as products of small matrix exponentials.\n\
    \nBaker Campbell Hausdorff (BCH): The BCH formula is used to decompose the time\
    \ evolution operator with non-commuting terms, allowing for a systematic approximation:\n\
    \n$$e^{\\left[M,N\\right]t^{2}} \\approx e^{\\mathcal{M}t}e^{\\mathcal{N}t}e^{-\\\
    mathcal{M}t}e^{-\\mathcal{N}t}.\\tag{5}$$\n\nwhere [, ] = − is the commutator.\
    \ This approach is most suitable for short-time evolution or small non-commutative\
    \ contributions. BCH is useful, as it can be used to help create a product of\
    \ terms.\n\nBoth Trotterization and BCH can be approximated to an arbitrary precision\
    \ with a large enough order. For the BCH formula, we use the second-order approximation.\
    \ Trotter will affect the number of gates by repeating the same gate sequence\
    \ times, but each gate has a smaller parameter and has a smaller duration. The\
    \ overall evolution time is the same after Trotterization.\n\nConsider the following\
    \ commutator (assuming A and B are qumode operators):\n\n<span id=\"page-4-1\"\
    ></span>\n$$\\{\\sigma\\_2 \\otimes A, I \\otimes B\\} = \\sigma\\_2 \\otimes\
    \ AB - \\sigma\\_2 \\otimes BA = \\sigma\\_2 \\{A, B\\}.\\tag{6}$$\n\nNow also\
    \ consider that ( [, ] + {, })/2 = , where {, } = + is the anticommutator, the\
    \ implication is that we can implement a product of terms of ˆ and ˆ † operators\
    \ if {, } can be implemented. Fortunately, Kang et al. [\\[20\\]](#page-13-27)\
    \ provides an implementation of {, } on CV-DV architecture, as well as another\
    \ version of [, ] implementation, which is different from ours in Equation [6.](#page-4-1)\
    \ These altogether set a foundation for the templaterewriting-based approach we\
    \ propose in this paper. We list all these rules in Table [3.](#page-5-0) With\
    \ basic sums and products rules, we can recursively break a Hamiltonian down into\
    \ sums and products until we reach a point where the term of interest matches\
    \ a basis gate template in Table [2.](#page-3-0) We describe the recursive template\
    \ matching process in the next section.\n\nIn Table [3,](#page-5-0) For a qubit⊗qumode\
    \ system, the Hamiltonian pattern B :\n\n$$\n\\mathcal{B}\\_M = \\begin{pmatrix}\
    \ 0 & M \\\\ M^\\dagger & 0 \\end{pmatrix},\n$$\n\nas a Hermitian operator acting\
    \ on the combined Hilbert space H<sup>2</sup> ⊗ HΛ+1, where H<sup>2</sup> is the\
    \ qubit Hilbert space and HΛ+<sup>1</sup> is the qumode Hilbert space.\n\nCertain\
    \ decomposition rules in Table [3](#page-5-0) represent that the target operator\
    \ is in a block encoding form. For instance, we only need the component in the\
    \ top-left of the Hamiltonian below.\n\n$$\n\\begin{pmatrix}\n2MN & 0 \\\\\n0\
    \ & -NM - (NM)^\\dagger \\\\\n\\end{pmatrix} . \\tag{7}\n$$\n\nIn a qubit-qumode\
    \ system, an operator O = can be ex-\n\npressed as:\n\n$$\\mathcal{O} = |0\\rangle\\\
    langle 0| \\otimes A + |0\\rangle\\langle 1| \\otimes B + |1\\rangle\\langle 0|\
    \ \\otimes C + |1\\rangle\\langle 1| \\otimes D.$$\n\nProjecting onto the subspace\
    \ associated with |0⟩ by setting the ancilla qubit to |0⟩, the effective operator\
    \ becomes ⟨0|O|0⟩ = , assuming = 0. Terms involving |0⟩ ⟨1| and |1⟩ ⟨1| vanish\
    \ due to orthogonality. Thus, by initializing the ancilla and zeroing specific\
    \ components (e.g., = 0), we can isolate and simplify blocks of the operator matrix\
    \ for easier decomposition. If we take the matrix exponential of O, let = = 0,\
    \ since is on the diagonal, we can also implement via <sup>O</sup> via the block\
    \ encoding.\n\n3.1.3 Rule-Based Recursive Template Matching for Hamiltonian Decomposition.\
    \ Section [3.1.1](#page-4-2) introduces template matching rules for synthesizing\
    \ operators into gates, while Section [3.1.2](#page-4-3) extends Zihan Chen, Jiakang\
    \ Li, Minghao Guo, Henry Chen, Zirui Li, Joel Bierman, Yipeng Huang, Huiyang Zhou,\
    \ Yuan Liu, and Eddy Z. Zhang\n\n<span id=\"page-5-0\"></span>\n\n| Rules | Operator\
    \ Template                                                                   \
    \                   | Conditions                      | Decomposition Output \
    \                                                                            |\
    \ Reference      | Precision |\n|-------|--------------------------------------------------------------------------------------------------------|---------------------------------|--------------------------------------------------------------------------------------------------|----------------|-----------|\n\
    | 1     | exp(\U0001D440\U0001D461 + \U0001D441 \U0001D461) ≈<br>Trotter(\U0001D440\
    \U0001D461, \U0001D441 \U0001D461)                                           \
    \                         |                                 | (exp(\U0001D440\U0001D461\
    /\U0001D458)exp(\U0001D441 \U0001D461/\U0001D458))\U0001D458                 \
    \                                                          | Trotterization |\
    \ Approx    |\n| 2     | exp( [\U0001D440\U0001D461, \U0001D441 \U0001D461]) ≈<br>BCH(\U0001D440\
    \U0001D461, \U0001D441 \U0001D461)                                           \
    \                           |                                 | exp(\U0001D440\
    \U0001D461)exp(\U0001D441 \U0001D461)exp(−\U0001D440\U0001D461)exp(−\U0001D441\
    \ \U0001D461)                                                                \
    \ | BCH            | Approx    |\n| 3     | [\U0001D440, \U0001D441])<br>exp(\U0001D461\
    <br>2                                                                        \
    \          | \U0001D440, \U0001D441<br>Hermitian               | exp( [\U0001D456\
    \U0001D461\U0001D70E\U0001D456\U0001D441 ,\U0001D456\U0001D461\U0001D70E\U0001D456\
    \U0001D440])                                                                 \
    \            | [20]           | Exact     |\n| 4     | exp(−\U0001D456\U0001D461\
    \ 2\U0001D70E\U0001D456 {\U0001D440, \U0001D441})                            \
    \                                                        | \U0001D440, \U0001D441\
    <br>Hermitian               | exp( [\U0001D456\U0001D461\U0001D70E\U0001D457\U0001D440\
    ,\U0001D456\U0001D461\U0001D70E\U0001D458\U0001D441])                        \
    \                                                      | [20]           | Exact\
    \     |\n| 5     | exp(−\U0001D456\U0001D461 2\U0001D70E\U0001D467 [\U0001D440\
    , \U0001D441])                                                               \
    \                     |                                 | exp( [(\U0001D456\U0001D461\
    \U0001D441 ,\U0001D456\U0001D461\U0001D70E\U0001D467\U0001D440])             \
    \                                                                 | This paper\
    \     | Exact     |\n| 6     | 2\U0001D70E\U0001D467 ( (\U0001D440\U0001D441 −\
    \ (\U0001D440\U0001D441)<br>exp(\U0001D461<br>†<br>)))                       \
    \                                           | [\U0001D440, \U0001D441]<br>= 0\
    \                   | exp( [\U0001D44B · \U0001D456\U0001D461B\U0001D441<br>·\
    \ \U0001D44B,\U0001D456\U0001D461B\U0001D440 ])                              \
    \                                      | [20]           | Exact     |\n| 7   \
    \  | exp(\U0001D456\U0001D461 2\U0001D70E\U0001D467 ( (\U0001D440\U0001D441 +\
    \ (\U0001D440\U0001D441)<br>†<br>)))                                         \
    \                           | [\U0001D440, \U0001D441]<br>= 0                \
    \   | , \U0001D44B<br>exp( [\U0001D446 · \U0001D456\U0001D461B\U0001D440<br>†<br>·\
    \ \U0001D446<br>· \U0001D456\U0001D461B\U0001D441 · \U0001D44B])             \
    \                                   | [20]           | Exact     |\n| 8     |\
    \ \U0001D440\U0001D441<br>0<br>−2\U0001D456\U0001D461 ©<br>exp<br>©<br>ª<br>ª<br>­<br>­<br>®<br>®<br>−\U0001D440\
    \U0001D441<br>0                                | \U0001D440, \U0001D441<br>Hermitian\
    \               | exp(−\U0001D456\U0001D461\U0001D70E\U0001D467 [\U0001D440, \U0001D441\
    ] −<br>\U0001D456\U0001D461\U0001D70E\U0001D467 {\U0001D440, \U0001D441})    \
    \                                                           | This paper     |\
    \ Exact     |\n| 9     | «<br>«<br>¬<br>¬<br>\U0001D440\U0001D441<br>0<br>2\U0001D456\
    \U0001D461 2 ©<br>exp<br>©<br>ª<br>ª<br>­<br>­<br>®<br>®<br>−\U0001D440\U0001D441\
    <br>0           | [\U0001D440, \U0001D441]<br>= 0<br>†<br>\U0001D440\U0001D441\
    \ = (\U0001D440\U0001D441) | , \U0001D44B<br>exp( [(\U0001D446 · \U0001D456\U0001D461\
    B\U0001D440<br>†<br>· \U0001D446<br>· \U0001D456\U0001D461B\U0001D441 · \U0001D44B\
    ])                                               | [20]           | Exact    \
    \ |\n| 10    | «<br>«<br>¬<br>¬<br>exp (2\U0001D456\U0001D461B\U0001D440\U0001D441\
    \ )                                                                      | [\U0001D440\
    , \U0001D441]<br>= 0                   | \U0001D44B · exp(\U0001D461\U0001D70E\
    \U0001D466 (\U0001D440\U0001D441 − (\U0001D440\U0001D441)<br>) + \U0001D456\U0001D461\
    \U0001D70E\U0001D465<br>(\U0001D440\U0001D441 + (\U0001D440\U0001D441)<br>)) ·\
    \ \U0001D44B<br>†<br>†                             | [20]           | Exact  \
    \   |\n| 11    | 2\U0001D440\U0001D441<br>0<br>\U0001D456\U0001D461 ©<br>exp<br>©<br>ª<br>ª<br>−\U0001D441\
    \U0001D440 − (\U0001D441\U0001D440)<br>­<br>­<br>†<br>®<br>®<br>0            \
    \         | †<br>\U0001D440\U0001D441 = (\U0001D440\U0001D441)               \
    \   | , \U0001D44B<br>exp( [\U0001D446 · \U0001D456\U0001D461B\U0001D440<br>†<br>·\
    \ \U0001D446<br>· \U0001D456\U0001D461B\U0001D441 · \U0001D44B])             \
    \                                   | [20]           | Exact     |\n| 12    |\
    \ «<br>«<br>¬<br>¬<br>\U0001D44E<br>0<br>2\U0001D456\U0001D6FC ©<br>B\U0001D44E\
    \ = exp<br>©<br>ª<br>ª<br>­<br>­<br>†<br>®<br>®<br>\U0001D44E<br>0      | \U0001D6FC\
    \ = \U0001D6FC<br>∗                      | † + \U0001D44E)) ⊗ \U0001D70E\U0001D466\
    )exp(−\U0001D456(\U0001D70B/2)\U0001D44E<br>† + \U0001D44E)) ⊗ \U0001D70E\U0001D465\
    <br>exp(\U0001D456(\U0001D70B/2)\U0001D44E<br>†\U0001D44E)exp(\U0001D456(\U0001D6FC\
    \ (\U0001D44E<br>†\U0001D44E)exp(\U0001D456(\U0001D6FC (\U0001D44E<br>)  | [20]\
    \           | Approx    |\n| 13    | «<br>«<br>¬<br>¬<br>†<br>\U0001D44E<br>0<br>2\U0001D456\
    \U0001D6FC ©<br>B\U0001D44E<br>† = exp<br>©<br>ª<br>ª<br>­<br>­<br>®<br>®<br>\U0001D44E\
    <br>0 | ∗<br>\U0001D6FC = \U0001D6FC                      | † + \U0001D44E)) ⊗\
    \ \U0001D70E\U0001D466)exp(−\U0001D456(\U0001D70B/2)\U0001D44E<br>† + \U0001D44E\
    )) ⊗ \U0001D70E\U0001D465<br>†\U0001D44E)exp(\U0001D456(\U0001D6FC (\U0001D44E\
    <br>†\U0001D44E)exp(−\U0001D456(\U0001D6FC (\U0001D44E<br>exp(\U0001D456(\U0001D70B\
    /2)\U0001D44E<br>) | This paper     | Approx    |\n| 14    | «<br>«<br>¬<br>¬<br>(\U0001D443\
    1\U0001D4432 ···\U0001D443\U0001D45B ) (\U0001D6FC\U0001D44E<br>†<br>\U0001D458\
    \ −\U0001D6FC<br>∗\U0001D44E\U0001D458 )<br>\U0001D452                       \
    \                |                                 | Multi-qubit-controlled displacement:\
    \ Right hand side (RHS) of Equation (11) first line           | [28]         \
    \  | Exact     |\n| 15    | 2\U0001D456\U0001D6FC2\U0001D4431\U0001D4432 ···\U0001D443\
    \U0001D45B<br>\U0001D452                                                     \
    \                               |                                 | Multi-Pauli\
    \ Exponential: Right hand side (RHS) of Equation (9) first line              \
    \          | This Paper     | Exact     |\n| 16    | All Native Gates RHS in Table\
    \ 2                                                                        | \
    \                                | All Native Gates Left Hand Side (LHS) Table\
    \ 2                                                    | [28]           | Exact\
    \     |\n\nTable 3: Decomposition rules for operators in hybrid CV-DV systems\
    \ with corresponding conditions. and represent Pauli-X and phase gates, implemented\
    \ using the single-qubit rotation gates in Table [2.](#page-3-0) All rules are\
    \ exact except rules 1, 2, 12, and 13, which are subject to Trotter and BCH approximation\
    \ errors with respect to the order of Trotter or BCH.\n\nthese rules to decompose\
    \ operator sequences. To decompose complex Hamiltonians, recursive steps and multiple\
    \ decomposition paths are needed. To enable scalable and automated Hamiltonian\
    \ simulation, we transform the decomposition task into a Template Matching compilation.\
    \ This recursive process applies gate synthesis and decomposition rules iteratively\
    \ to input sequences of annihilation and creation operators so as to reduce a\
    \ Hamiltonian to basic gates in the Hybrid CV-DV gate set.\n\nNext, we use an\
    \ example, Hamiltonian in Equation [8,](#page-5-1) to illustrate the decomposition\
    \ process. We first apply Rule 1, the Trotterization formula, to approximate it\
    \ as a product of two exponential operators. As Rule 1 and Rule 2 are common intermediate\
    \ steps in many decomposition Rules, we omit their detailed discussion here.\n\
    \n<span id=\"page-5-1\"></span>\n$$\\begin{split} \\exp(-iHt) &= \\exp\\left(-i(\\\
    omega a^\\dagger a + \\frac{\\kappa}{2}(a^\\dagger)^2 a^2)t\\right) \\\\ &\\approx\
    \ \\left(\\exp(-i\\omega a^\\dagger a \\frac{t}{k}) \\cdot \\exp(-i\\frac{\\kappa}{2}(a^\\\
    dagger)^2 a^2 \\frac{t}{k})\\right)^k \\end{split} \\tag{8}$$\n\nRule 1 separates\
    \ the linear term † and nonlinear term ( † ) 2 2 . Each term is then decomposed\
    \ into basic gates in the Hybrid CV-DV gate set. For the linear term †, it is\
    \ synthesized as a Phase-Space Rotation gate using the rules from Section [3.1.1.](#page-4-2)\n\
    \nDecomposing the nonlinear term ( † ) 2 2 is challenging due to the lack of a\
    \ matching basis gate. This would incur recursively applying the template matching\
    \ rules from Section [3.1.2](#page-4-3) or Table [3](#page-5-0) to simplify it.\
    \ The process follows a recursive search tree, where nodes represent decomposition\
    \ states (current terms and their settings), and edges correspond to synthesis\
    \ rules from Tables [2-](#page-3-0)[3.](#page-5-0)\n\nFor a complex term, there\
    \ may exist multiple ways to break it into subterms, and in Table [3.](#page-5-0)\
    \ Fig. [3](#page-5-2) shows three possible methods for splitting ( † ) 2 2 into\
    \ and . This introduces additional complexity or opportunity to the decomposition\
    \ process due to the multipath selection.\n\n<span id=\"page-5-2\"></span>![](_page_5_Figure_10.jpeg)\n\
    \nFigure 3: Decomposing ( † ) 2 2 into three child states, i.e., splitting it\
    \ into subterms and using three methods.\n\nTo decompose ( † ) 2 2 , we select\
    \ a splitting method for , , then apply the template matching rules from Table\
    \ [3.](#page-5-0) First, we check the conditions for each rule and filter out\
    \ those that don't apply. Next, we identify the operator templates for and , such\n\
    \nas , [, ], {, }, ± () † , or B , and proceed with the decomposition accordingly.\n\
    \nWe use Depth-First Search (DFS) with backtracking to traverse the recursive\
    \ decomposition tree, exploring paths deeply and storing valid decomposition paths\
    \ for evaluation. Dead ends occur when no rule further reduces the operator sequence.\
    \ The algorithm then backtracks to the last valid node, prunes unnecessary branches,\
    \ and explores alternative paths.\n\nHandling Dead Ends: For instance, if = †\
    \ and = † 2 , Rule 11 from Table [3](#page-5-0) applies as = () † . While matches\
    \ a basic gate, ( † 2 ) requires further decomposition. The path reaches a dead\
    \ end after trying three splitting methods for without success. The algorithm\
    \ then discards this branch and backtracks to try an alternative decomposition,\
    \ such as = ( † ) 2 and = 2 .\n\nStep 1: For = ( † ) 2 and = 2 , the condition\
    \ = () † holds, allowing Rule 11 to be applied. This decomposes the term into\
    \ two subterms, ( · B · † ) and ( · B · ).\n\nStep 2: One of subterm ( † ) 2 is\
    \ further decomposed into = † and = † , satisfying the condition [, ] = 0 and\
    \ matching the Block Encoding template. Rule 10 is applied to produce two new\
    \ subterms: ( + () † ) and ( − () † ).\n\nStep 3: At this stage, = † and = † again.\
    \ Rule 6 is applied, decomposing the term into ( ·B · † ) and ( ·B ·), further\
    \ simplifying the operators.\n\nStep 4: The Block Encoding template is then decomposed\
    \ into a sequence of simplified annihilation and creation operators using Rule\
    \ 13.\n\nStep 5: Finally, the resulting sequence is synthesized into Phase-Space\
    \ Rotation and Displacement gates from the hybrid CV-DV gate set. The complete\
    \ 5-step recursive Template Matching decomposition and gate synthesis process\
    \ is illustrated in Fig. [4.](#page-6-1)\n\nDiscussion: Our mechanism is designed\
    \ to be extension-friendly, reuse-friendly, and customization-friendly. When expanding\
    \ two rule sets, such as adding new architecture-specific gates or decomposition\
    \ rules, our template matching and implementation mechanism remains unaffected.\
    \ Additionally, we can customize cost metrics, such as fidelity or decomposition\
    \ approximation, by extending our template-matching into a cost-optimized framework.\n\
    \nHamiltonian decomposition and gate synthesis are highly challenging tasks. To\
    \ the best of our knowledge, our work is the first to automatically decompose\
    \ the six general Hamiltonian models described in Section [4](#page-9-0) for the\
    \ CV-DV system. Our evaluation indicates that the decomposition and synthesis\
    \ of a given Hamiltonian operator have limited viable pathways. As a result, the\
    \ trade-offs between latency, gate count, and error rate are not a primary concern\
    \ at this stage, given the scarcity of successful decomposition pathways. However,\
    \ providing support to choose among different decomposition and synthesis pathways\
    \ should be a feasible extension. We leave this extension of our compiler as our\
    \ future work.\n\n#### <span id=\"page-6-2\"></span>3.2 Multi-qubit Pauli-string\
    \ Synthesis\n\nWe propose a scheme to synthesize an arbitrary multi-qubit Paulistring\
    \ on hybrid CV-DV platforms. It is inspired by phase kickback in DV systems, where\
    \ the phase of the control qubit is influenced by the operation on the target\
    \ qubits. In our approach, we use qumodes as a medium to pass entanglement among\
    \ qubits through\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_13.jpeg)\n\
    \nFigure 4: Recursive decomposition and gate synthesis of ( † ) 2 2 into a sequence\
    \ of basic gates. The red circle indicates the template matching rules from the\
    \ Decomposition Rules Set (Table [3\\)](#page-5-0), and the blue square represents\
    \ the template matching rules from the hybrid CV-DV gate set (Table [2\\)](#page-3-0).\n\
    \na sequence of Conditional Displacement gates, i.e., qubit-controlled qumode\
    \ Displacement, and unconditional Displacement gates such that the desired operation\
    \ is achieved on qubits while the effects on qumodes cancel.\n\nWe propose, for\
    \ the first time, the multi-Pauli exponential for the CV-DV system in this paper,\
    \ that is, an arbitrary Pauli-string exponential for a hybrid CV-DV system built\
    \ upon displacement and multi-qubit control displacement gates.\n\n<span id=\"\
    page-6-0\"></span>The detailed implementation follows a structured decomposition:\n\
    \n$$\\begin{split} U &= D^k(ia) \\, CD^{(k, P\\_{1\\dots n})}(-a) \\, D^k(-ia)\
    \ \\, CD^{(k, P\\_{1\\dots n})}(a) \\\\ &= e^{2ia\\alpha^2 P\\_1 P\\_2 \\dots\
    \ P\\_n} \\end{split} \\tag{9}$$\n\nwhere 1<sup>2</sup> · · · represents the desired\
    \ multi-qubit Pauli string <sup>1</sup> ⊗<sup>2</sup> ⊗ · · · ⊗, () is a displacement\
    \ of to the -th qumode, and (,1··· ) (−) and (,1··· ) () are controlled displacement\
    \ of ±. During the operation, displacement effects on qumodes return them to the\
    \ original states, kicking back an overall phase to the qubits, akin to phase\
    \ kickback in DV systems. This is built on Liu et al.[\\[28\\]](#page-13-14),\
    \ which used ancillary qumodes to implement CNOT and and single-qubit gates. We\
    \ extend their approach to cover arbitrary multi-qubit operations with machine\
    \ compilation.\n\n<span id=\"page-6-3\"></span>An example with one qubit and one\
    \ qumode is as follows.\n\n$$\\begin{split} U &= D^k(ia)CD^{k,P\\_1}(a,-a)D(-ia)CD^{k,Z}(-a,a)\
    \ \\\\ &= e^{2ia^2Z} \\end{split} \\tag{10}$$\n\nIf we set <sup>1</sup> as the\
    \ Pauli-Z operator, , () applies a qubitcontrolled Displacement gate to the -th\
    \ qumode, with () for\n\n|0⟩ and (−) for |1⟩ with respect to the state of the\
    \ first qubit, this exactly implements <sup>2</sup>2 .\n\nOur new multi-Pauli\
    \ exponential in Equation [\\(9\\)](#page-6-0) makes use of a multi-qubit controlled\
    \ CD gate proposed by Liu et al.[\\[28\\]](#page-13-14), as below.\n\n$$\\begin{split}\
    \ \\text{CD}^{(k, P\\_1 P\\_2 \\dots P\\_n)}(\\alpha) &= U\\_{\\text{seq}}^\\\
    dagger D(i^n \\alpha) U\\_{\\text{seq}} \\\\ &= e^{(P\\_1 P\\_2 \\dots P\\_n)\
    \ \\left( \\alpha a\\_k^\\dagger - \\alpha^\\* a\\_k \\right)} \\end{split} \\\
    tag{11}$$\n\n<span id=\"page-7-0\"></span>where seq = Î † /2 is a sequence of\
    \ Control Parity gates (in Table [2\\)](#page-3-0) conjugated by Clifford gates,\
    \ with the −th qubit controlling the −th qumode. It acts as a Control Parity gate\
    \ for = . For = , the Control Parity gate must be conjugated with Hadamard ()\
    \ gates, and for = , with and † gates.\n\nEssentially, this method combines unconditional\
    \ Displacement and Control Parity gates to construct an arbitrary multi-qubit\
    \ Paulistring term, requiring only a single ancillary qumode. The approach is\
    \ agnostic to the state of the ancilla qumode, with the ancilla qumode returning\
    \ to its initial state after the operation. This flexibility supports efficient\
    \ resource allocation and specific optimization goals, benefiting mapping and\
    \ routing.\n\nThe example in Fig. [5](#page-7-1) illustrates how an ancilla qumode\
    \ travels in the physical circuit, visiting every qubit , , · · · , , to implement\
    \ a CD gate for all qubits involved in the Pauli-string.\n\n<span id=\"page-7-1\"\
    ></span>![](_page_7_Figure_7.jpeg)\n\nFigure 5: An ancilla qumode interacts with\
    \ every qubit on the path from A to N via Control Parity (CP) gates, performs\
    \ an unconditional Displacement gate, and then travels back to A's location while\
    \ performing CP gates. This implements a CD gate in Equation [11.](#page-7-0)\
    \ Note that the ancilla qumode can be any qumode, and the CP gates can run in\
    \ any order. We just pick the qumode below qubit A to illustrate this idea.\n\n\
    Note that conjugating the Displacement gate with conjugated CP gates, or applying\
    \ it to a polynomial of annihilation and creation operators, yields multi-qubit\
    \ controlled qumode gates. These form a series of sub-rules, which are also included\
    \ in our rule database (Table [3\\)](#page-5-0) for conciseness.\n\n## 3.3 DSL\
    \ Design and Implementation\n\nAfter level-1 compilation, Hamiltonians are decomposed\
    \ into sequences of basic gates from the hybrid CV-DV gate set. Level-2 compilation\
    \ then applies hardware constraints to generate physical circuits. To streamline\
    \ this process, we introduce CVDV-QASM, an OpenQASM-like DSL that integrates Pauli-strings\
    \ and CV-DV gate sequences. Our mapping and routing stage parses CVDV-QASM into\
    \ quantum circuits. Syntax support for Pauli-string terms like \"pauli(/4) YYZI\"\
    \ enables CP gate commutation and qumode allocation flexibility during physical\
    \ circuit mapping while preserving\n\n<span id=\"page-7-2\"></span>semantics and\
    \ providing crucial information to the lower compilation stack. An example is\
    \ shown in Fig. [6:](#page-7-2)\n\n```\n// Pauli String with Parameter\npauli(pi/4)\
    \ YIYZXXIIIIIIII;\npauli(pi/4) XZYZXYIIIIIIII;\n...\n// Phase Space Rotation Gate\n\
    R(pi/4) qm[1];\nR(pi/4) qm[2];\n...\n// Control Displacement Gate\nCD(pi/4) q[2],\
    \ qm[1];\nCD(pi/4) q[3], qm[1];\n...\n// Displacement Gate\nD(pi/4) qm[2];\nD(pi/4)\
    \ qm[2];\n...\n```\nFigure 6: Hybrid CV-DV circuit DSL example. **pauli** represents\
    \ the parameter of a Pauli String; **R, CD, D** represent the gate type; **qm[i]**\
    \ and **q[i]** represent the qumode and qubit respectively. Pauli String sequence\
    \ will be further decomposed into basic gates in the compiler output of physical\
    \ circuits.\n\nThis DSL has a similar input format of Bosonic Qiskit [\\[40\\\
    ]](#page-13-11), except that our DSL supports Pauli string representation. As\
    \ described before, Pauli strings are decomposed with an ancilla qumode, while\
    \ hardware constraints, such as qumode interactions, are managed by inserting\
    \ SWAP gates. The final circuit after our mapping and routing consists only of\
    \ basic CV-DV gates and not Pauli-string gates. Detailed discussions on hardware\
    \ constraints are in Section [3.4.](#page-7-3)\n\n# <span id=\"page-7-3\"></span>3.4\
    \ Tackling Limited Connectivity Constraints\n\nIn hybrid CV-DV architectures shown\
    \ in Fig. 1, qumodes have limited connectivity and interact with qubits, adding\
    \ complexity to multi-qumode interactions. While a SWAP operation in DV systems\
    \ typically requires 3 CNOT gates, in CV systems, it begins with a Beam-Splitter\
    \ gate (, 0) with parameters = and = 0:\n\n$$BS(\\pi, 0) \\left| \\Phi\\_a, \\\
    Phi\\_b \\right\\rangle = e^{-i\\frac{\\pi}{2} \\left( a^\\dagger a + b^\\dagger\
    \ b \\right)} \\left| \\Phi\\_b, \\Phi\\_a \\right\\rangle \\tag{12}$$\n\nTo cancel\
    \ the phase change from the Beam-Splitter (BS) gate, two Phase Space rotation\
    \ gates, − 2 ˆ and − 2 ˆ , are added. Thus, a qumode SWAP gate consists of one\
    \ Beam-Splitter (BS) gate and two Phase Space rotation gates, with the latter\
    \ having very low latency compared with a BS gate. This qumode SWAP primitive\
    \ enables qumode movement for interactions with other qumodes or qubits. For qubit\
    \ interactions involving qumodes, the qumodes can be moved to qubits. Multi-qubit\
    \ interactions require primitives like Controlled Parity gates, discussed in Section\
    \ [3.2.](#page-6-2)\n\nIn physical implementations, connectivity constraints between\
    \ qumodes, qubits, and qumode-qubit pairs can be summarized as the following three\
    \ mapping challenges:\n\n- Qumode-qumode Mapping. Interactions are limited to\
    \ adjacent qumodes. For non-adjacent qumodes, qumode SWAP gates are used, with\
    \ routing optimized to minimize such SWAPs.\n- Qubit-qumode Mapping. Each qumode\
    \ interacts only with its associated qubit. For interactions with other qumodes,\
    \ adjacency is established by moving qumodes, similar to qumode-qumode mapping.\n\
    \n• Qubit-qubit Mapping. Qubits interact indirectly via an ancilla qumode, which\
    \ is moved between qubits to mediate interactions and complete gate operations.\n\
    \n<span id=\"page-8-0\"></span>3.4.1 Optimized Ancilla Qumode Routing for Qubit-qubit\
    \ Interactions. As direct qubit-qubit interactions are limited by hardware constraints,\
    \ we use ancilla qumodes to mediate interactions via phase kickback (Equation\
    \ [10\\)](#page-6-3). The main challenge is to find the ancilla qumode and optimize\
    \ its path to interact with target qubits efficiently. This Optimized Ancilla\
    \ Qumode Routing Problem seeks to minimize path costs, measured by qumode SWAP\
    \ operations, by finding the shortest path for a specific ancilla qumode to visit\
    \ all qubits in ⊆ on an undirected graph = ( , ).\n\n<span id=\"page-8-1\"></span>Fig.\
    \ [7](#page-8-1) compares an arbitrary alphabetical routing path with an optimized\
    \ ancilla qumode routing path, highlighting its significant impact on the overall\
    \ SWAP cost:\n\n![](_page_8_Figure_4.jpeg)\n\nFigure 7: In the Optimized Ancilla\
    \ Qumode Routing example, the red path represents alphabetical routing ( → → →\
    \ ) with a SWAP cost of 4. The blue path shows optimized routing ( → → → ), using\
    \ the Qumode at as the ancilla, reducing the SWAP cost to 3.\n\nThe Optimized\
    \ Ancilla Qumode Routing Problem can be reformulated as a relaxed Hamiltonian\
    \ Path Problem, similar to a modified Traveling Salesman Problem (TSP). Unlike\
    \ the closed-path TSP, this problem allows revisiting vertices and does not require\
    \ returning to the starting vertex.\n\nWe construct a complete graph ′ = ( , ′\
    \ ) with the same vertex set as the original graph . Each edge in ′ is weighted\
    \ by , the shortest path cost between and in , mapped to the edge sequence in\
    \ . As shortest paths in are polynomial-time computable, constructing ′ is also\
    \ polynomial in complexity.\n\nGiven the NP-hardness of the Hamiltonian Path Problem,\
    \ we propose a multi-level solution strategy for scalability, as heuristic solutions\
    \ can be more practical for larger problem instances.\n\nWe use Christofides Algorithm,\
    \ which constructs a minimum spanning tree, finds a minimum-weight perfect matching\
    \ among the odd-degree vertices, and combines them to form an Eulerian circuit,\
    \ which can guarantee a near-optimal solution with polynomial time complexity,\
    \ making it suitable for large-scale qumode routing scenarios. However, we also\
    \ explore the other Heuristicbased algorithms, such as Threshold Accepting Algorithm,\
    \ which can perform better in large-scale Pauli Strings compilation.\n\nAlso,\
    \ for larger-scale problems, Heuristic Algorithms can provide efficient solutions,\
    \ also making them well-suited for specific\n\nphysical circuits optimization\
    \ goal in Hybrid CV-DV systems to provide high extensibility and scalability.\n\
    \nDynamic Qubit Floating: While the only native SWAP gates are for qumodes when\
    \ addressing the connectivity constraints. Qubit movement can also be considered.\
    \ As discussed by Liu et al.[\\[28\\]](#page-13-14), CNOT gates can be implemented\
    \ in the CV-DV architecture where qubits are not directly connected. We can essentially\
    \ implement a qubit-qubit SWAP using 3 CNOT gates. However, it would require the\
    \ usage of ancilla qumode, as the qubits are indirectly interacting via the ancilla\
    \ qumodes. However, since qubit-qubit SWAP is not native, it takes 12 control\
    \ displacement gates and 12 qumode-SWAP gates. It is almost 24 times the cost\
    \ of the beamsplitter (qumode-SWAP) gate. It could only be useful when there are\
    \ frequent interactions on a set of qubits, while the set of qubits is scattered\
    \ very far from each other. This depends on the active-qubit pattern in the Pauli-string\
    \ terms and the qubit-controlled qumode gates. Although it may not be useful in\
    \ some applications, we still integrate this design into our compiler. We calculate\
    \ the average shortest distance for active qubits within a Pauli-string, and if\
    \ it exceeds a certain threshold, we trigger a clustering procedure and move qubits\
    \ into a connected component. It offers more flexibility for specific long-range\
    \ interactions. This is as if qubits can float around rather than being at their\
    \ fixed locations. We name this approach as floating qubit.\n\nGiven hardware\
    \ constraints and optimization flexibility, using an ancilla qumode or using qubit\
    \ floating for multi-qubit gate synthesis offers three key advantages: Resource\
    \ Flexibility: Any available qumode can serve as the ancilla without requiring\
    \ a specific initial state, enabling efficient resource utilization. Operational\
    \ Efficiency: The ancilla qumode can be reused across multiple gates, with Controlled\
    \ Parity gates applied in any order before and after Displacement gates. Optimization\
    \ Potential: Ancilla selection, Ancilla/Qubit routing, and task assignment can\
    \ be optimized for resource reuse and circuit partitioning.\n\n3.4.2 Mapping and\
    \ Routing for General Case. Now we handle the case when the CVDV-QASM representation\
    \ contains both Pauli strings and other CV-DV basis gates. Pauli-strings are processed\
    \ separately. The general idea is to keep a working frontier – the set of gates\
    \ (Pauli-string gates) whose dependence has been resolved, as well as a scheduled\\\
    _worklist data structure containing gates that have already been scheduled so\
    \ far. Then we repeat the following.\n\n- (1) Execute any gate, including Pauli-string\
    \ gates that are executable from the frontier. We prioritize the scheduling of\
    \ Pauli-string gates because, by Trotterization, each Paulistring gate (block)\
    \ can run in any order. We use different ranking functions to choose the set of\
    \ Pauli-strings to schedule next, and the ranking function either prioritizes\
    \ the non-identity qubit number or the time stamp of the latest finished gate\
    \ in that set of active qubits, to improve the parallelism. Then for the rest\
    \ of the CV-DV basis gates, they are executable if their involved qubits/qumodes\
    \ are connected. Once we schedule a sequence of gates, we add them to the scheduled\\\
    _worklist.and update the frontier.\n- (2) For the rest of the gates in the frontier,\
    \ pick SWAPs that will help at least one gate in the frontier. Rank all the possible\n\
    \nSWAPs. Then select one SWAP and add it to the scheduled\\_worklist.\n\n(3) Go\
    \ back to Step 1 or terminate if there is no gate in the work frontier.\n\nOnce\
    \ the program completes, the scheduled\\_worklist implies the order of the compiled\
    \ physical gates. For the cost function of ranking candidate SWAP gates, we use\
    \ a method similar to the Sabre qubit mapper [\\[24\\]](#page-13-35). For the\
    \ cost function of ranking Pauli-strings, as mentioned above, we use either the\
    \ minimum-active-qubit-number metric or the minimum-total-depth metric (sum up\
    \ the depths of all active qubits in that Pauli-string) to parallelize as much\
    \ as possible.\n\nWe also design a specific coupling map data structure for hybrid\
    \ CV-DV systems. Currently, there are three versions of hybrid CV-DV quantum processors:\
    \ superconducting, trapped ion, and neutral atom. We do not consider the neutral\
    \ atom architecture as it does not allow connectivity among qubits. Our coupling\
    \ map can support both superconducting and trapped ion CV-DV architecture.\n\n\
    #### <span id=\"page-9-0\"></span>4 EVALUATION\n\n#### 4.1 Experimental Setup\n\
    \nOur evaluation focuses on two key aspects: Pauli-string Hamiltonian (qubit-only)\
    \ in a hybrid CV-DV architecture and general compilation support for Hamiltonian\
    \ simulation. First, we analyze the performance of compiling qubit-only Pauli-string\
    \ Hamiltonians within the hybrid CV-DV architecture, evaluating our solutions\
    \ for generating multi-qubit interactions via ancilla qumode(s). Second, we demonstrate,\
    \ for the first time, the complete synthesis and compilation of given Hamiltonian\
    \ models that may consist of both Fermions and Bosons, or just Bosons. This is\
    \ for evaluating both the gate synthesis component and the hardware component.\n\
    \nMetrics. We employ three primary metrics to assess the compiler's performance:\
    \ 1-op Gate, 2-op Gate, and Depth. The 1-op Gate and 2-op Gate metrics count the\
    \ number of single-operand and two-operand gates, respectively, reflecting interactions\
    \ such as qubit-qumode or qumode-qumode operations. The Depth metric measures\
    \ the number of layers of quantum gates after compilation, representing the complexity\
    \ of the compiled circuit. Using data from Table [2,](#page-3-0) we approximate\
    \ the cost of a 2-op gate as equivalent to 20x that of a 1-op Gate and set 1-op\
    \ gate latency to 1 when evaluating depth. For general Hamiltonian simulation\
    \ compilation, the metric depth is for the final physical circuits that satisfy\
    \ connectivity constraints rather than for the logical circuit.\n\nBenchmarks.\
    \ We select benchmarks of various sizes and applications from practical, real-world\
    \ Hamiltonian models. For the fermion-only Hamiltonian, using the PySCF [\\[41\\\
    ]](#page-13-36) software package, we constructed the Hamiltonians for seven distinct\
    \ molecules: LiH, BeH2, ethylene, NH3, C2, N2, and H2O, under minimal basis STO-3G.\n\
    \nWe also consider different spin-orbital-to-electron ratios, resulting in 20\
    \ benchmarks that comprise a total of 600 to 1900 Pauli strings. For example,\
    \ H2O(8,12) and H2O(10,14) represent water molecules with 8 electrons and 12 spin\
    \ orbitals, and 10 electrons and 14 spin orbitals, respectively. The size of the\
    \ molecular system affects the overall complexity of the simulation, and different\
    \ encoding schemes can lead to variations in the number of required operations\
    \ and the resulting circuit depth.\n\nWe also include six general models, such\
    \ as the Kerr nonlinear oscillator Hamiltonian, the Z2-Higgs model, the Bose-Hubbard\
    \ model, the Hubbard-Holstein model, the Heisenberg model, and the electronic-vibration\
    \ coupling Hamiltonians. They could include interactions of boson matters or fermion-boson\
    \ matters.\n\nFor each of the Hamiltonians, the fermion creation/annihilation\
    \ operators must first be converted into qubit operators, in order to convert\
    \ them into the form in Equation [1](#page-2-2) before we perform further synthesis\
    \ and scheduling. We employ both Jordan-Wigner [\\[19\\]](#page-13-37) (JW) and\
    \ Bravyi-Kitaev [\\[2\\]](#page-12-4) (BK) encoding schemes to map fermionic operators\
    \ to qubit operators [\\[44\\]](#page-13-38). JW is highly regular, but its operator\
    \ length grows linearly with the system size (()). In contrast, the Bravyi-Kitaev\
    \ encoding employs a more intricate tree-based structure to store parity information,\
    \ thereby reducing the Pauli-weight, but with less regularity.\n\nThe Kerr nonlinear\
    \ oscillator Hamiltonian (1) is a single-qumode Hamiltonian that includes a Kerr\
    \ nonlinear term, adding complexity to its decomposition and compilation:\n\n\
    $$H\\_1 = \\omega a^\\dagger a + \\frac{\\kappa}{2} (a^\\dagger)^2 a^2. \\tag{13}$$\n\
    \nThe Z2-Higgs model (2) is a hybrid Hamiltonian featuring multimode interactions,\
    \ including qumode-qumode and qubit-qubit interactions, making it representative\
    \ of hybrid CV-DV systems:\n\n$$H\\_2 = -g\\sum\\_{i=1}^{L-1} X\\_{i, \\delta+1}\
    \ + U\\sum\\_{i=1}^{L} \\hat{\\mathfrak{n}}\\_i^2 - J\\sum\\_{i=1}^{L-1} \\left(\\\
    hat{\\mathfrak{a}}\\_i^\\dagger Z\\_{i, \\delta+1} \\hat{\\mathfrak{a}}\\_{\\\
    delta+1} + \\text{h.c.}\\right), \\quad \\text{(14)}$$\n\nwhere ˆ = † is the bosonic\
    \ number operator, and ,+<sup>1</sup> and ,+<sup>1</sup> are Pauli X and Z operators\
    \ linking sites and + 1, respectively, each can be represented using one qubit\
    \ operator. The term \"h.c.\" represents the hermitian conjugate of the term right\
    \ before the addition sign.\n\nThe Bose-Hubbard model (3) is a lattice model whose\
    \ structure varies with different lattice configurations, making it suitable for\
    \ higher-dimensional physical systems. Also, it is a pure Hamiltonian with only\
    \ qumode-qumode interaction (may need ancilla qubits for higher order annihilation\
    \ and creation operators), making it representative of pure qumode Hamiltonian\
    \ systems:\n\n$$H\\_3 = -t\\sum\\_{i,j} (b\\_i^\\dagger b\\_j + b\\_j^\\dagger\
    \ b\\_i) + \\frac{U}{2} \\sum\\_i \\hat{n}\\_i (\\hat{n}\\_i - 1) - \\mu \\sum\\\
    _i \\hat{n}\\_i,\\tag{15}$$\n\nwhere ˆ = † , and † and are creation and annihilation\
    \ operators.\n\nThe Hubbard-Holstein model (4) involves both fermionic and bosonic\
    \ operators, adding complexity due to multiple mappings of fermionic operators\
    \ to qubit Pauli operators. Here, † ( ) denotes the bosonic creation (annihilation)\
    \ operator at site , while † , (, ) represents the fermionic counterpart. The\
    \ number operator ˆ, = † ,, counts fermions with spin =↑, ↓ at site . We applied\
    \ Jordan-Wigner encoding, but, due to space constraints, do not expand it into\
    \ Pauli and Bosonic annihilation operator form:\n\n$$H\\_4 = -t\\sum\\_{i,j,\\\
    sigma} c\\_{i,\\sigma}^\\dagger c\\_{j,\\sigma} + U\\sum\\_i \\hat{n}\\_{i,\\\
    uparrow}\\hat{n}\\_{i,\\downarrow} + \\sum\\_i b\\_i^\\dagger b\\_i + g\\sum\\\
    _{i,\\sigma} \\hat{n}\\_{i,\\sigma}(b\\_i^\\dagger + b\\_i),\\tag{16}$$\n\nThe\
    \ Hamiltonian <sup>5</sup> describes the interaction between discrete electronic\
    \ states (qubits) and vibrational modes (qumodes), capturing complex electron-phonon\
    \ coupling effects in molecular systems. Such models are essential for simulating\
    \ quantum dynamics in onedimensional (1D) chromophore arrays, where electronic\
    \ excitations are strongly coupled to vibrational environments. Applications span\
    \ light-harvesting complexes, organic semiconductors, and photosynthetic systems\
    \ [\\[45\\]](#page-13-39). The full Hamiltonian is decomposed as:\n\n$$H\\_5 =\
    \ \\sum\\_{\\chi=1}^{N} \\left[ H\\_0^{(\\chi)} + H\\_1^{(\\chi)} + H\\_2^{(\\\
    chi)} \\right] \\tag{17}$$\n\nHere, ( ) 0 represents local non-interacting terms,\
    \ ( ) 1 captures dispersive coupling between qubits and modes, and ( ) 2 includes\
    \ inter-chromophore interactions and vibrationally modulated couplings. Each component\
    \ is defined below:\n\n$$H\\_0^{(\\chi)} = \\omega\\_{\\chi\\_0} b\\_{\\chi\\\
    _0}^\\dagger b\\_{\\chi\\_0} + \\omega\\_{\\chi\\_1} b\\_{\\chi\\_1}^\\dagger\
    \ b\\_{\\chi\\_1} - \\frac{\\omega\\_{qq\\chi\\_0}}{2} \\sigma\\_{\\chi\\_0}^z\
    \ \\tag{18}$$\n\n$$H\\_1^{(\\chi)} = -\\frac{\\chi\\_{\\mathcal{Y}^0}}{2} b\\\
    _{\\mathcal{Y}^0}^\\dagger b\\_{\\mathcal{Y}^0} \\sigma\\_{\\mathcal{Y}^0}^z +\
    \ \\frac{g\\_{cd,\\mathcal{Y}^0}}{2} (b\\_{\\mathcal{Y}^0} + b\\_{\\mathcal{Y}^0}^\\\
    dagger) \\sigma\\_{\\mathcal{Y}^0}^z \\tag{19}$$\n\n ( ) 2 = ,<sup>1</sup> 2 (<sup>1</sup>\
    \ + † 1 ) 0 + 0,(−1)<sup>0</sup> 4 0 (−1)<sup>0</sup> + 0 (−1)<sup>0</sup> + 0,(+1)<sup>0</sup>\
    \ 4 0 (+1)<sup>0</sup> + 0 (+1)<sup>0</sup> + 0,(−1)0,<sup>1</sup> 4 0 (−1)<sup>0</sup>\
    \ + 0 (−1)<sup>0</sup> (<sup>1</sup> + † 1 ) + 0,(+1)0,<sup>1</sup> 4 0 (+1)<sup>0</sup>\
    \ + 0 (+1)<sup>0</sup> (<sup>1</sup> + † 1 ) (20)\n\nThe Heisenberg model (6)\
    \ describes qubit-qubit interactions in a spin chain or lattice system. Here,\
    \ <sup>6</sup> represents a chain of qubits coupled via exchange interactions\
    \ in the , , directions. The absence of qubit-qumode and qumode-qumode interactions\
    \ simplifies the model, making it ideal for understanding pure qubit dynamics.\
    \ The Heisenberg Hamiltonian has broad applications in condensed matter physics,\
    \ quantum magnetism, and quantum computing, serving as a foundation for entanglement,\
    \ spin transport, and quantum phase transitions, which is given by:\n\n$$H\\_6\
    \ = -\\frac{1}{2} \\sum\\_{j=1}^{N} (J\\_\\mathbf{x} X\\_j X\\_{j+1} + J\\_y Y\\\
    _j Y\\_{j+1} + J\\_z Z\\_j Z\\_{j+1} + h Z\\_j) \\tag{21}$$\n\nThese Hamiltonians\
    \ exemplify hybrid CV-DV systems and present significant compilation challenges.\
    \ For the first time, we have fully decomposed, synthesized, and compiled their\
    \ physical circuits for hybrid CV-DV quantum computers.\n\nBaselines and Hardware\
    \ Coupling Maps. For multi-qubit interactions on a hybrid CV-DV architecture,\
    \ since there is a prior compiler study, we compare different versions of our\
    \ implementation, including the Christofides algorithm and the Threshold Accepting\
    \ Heuristic-based approach, to analyze their effectiveness in handling such Hamiltonian\
    \ Simulation scenarios. Additionally, we investigate the use of Floating Qubits\
    \ within the hybrid CV-DV architecture, exploring innovative compilation strategies\
    \ under this framework. Finally, we assess the scalability and extensibility of\
    \ our methods, offering valuable insights for future research and development.\
    \ For the coupling map, we adopt a lattice structure for qumode connectivity as\
    \ shown in Fig. [1,](#page-1-0) each qumode is connected to one qubit. Qubits\
    \ are not connected.\n\n# 4.2 Pauli String Synthesis using Ancilla Qumode and\
    \ Floating Qubit\n\nFor Pauli string compilation, hybrid CV-DV systems introduce\
    \ challenges in handling qubit-qubit interactions where direct qubit compilation\
    \ is unavailable. This necessitates adaptations in our framework. One approach\
    \ mediates multi-qubit interactions through qumodes, addressing the Optimized\
    \ Ancilla Qumode Routing Problem. We explored two routing strategies: the Christofides\
    \ Algorithm and the Threshold Accepting Algorithm, both efficient heuristics for\
    \ the common traveling salesman problem (TSP). Additionally, we investigated the\
    \ Floating Qubit method as an alternative architectural solution for hybrid systems.\
    \ Detailed results are in Table [4.](#page-11-0) Our benchmarks involve complex\
    \ and representative Hamiltonians. While the Christofides Algorithm provides an\
    \ efficient polynomialtime approximation, results indicate that the Threshold\
    \ Accepting Algorithm achieves 3-7% better optimization, reducing circuit depth\
    \ by an average of 4.8%. This demonstrates the effectiveness of heuristics in\
    \ addressing complex compilation problems. Further improvements might be possible\
    \ by incorporating additional heuristic objectives, such as architecture-specific\
    \ optimizations.\n\nWe also evaluated the Floating Qubit method but found it largely\
    \ ineffective across our benchmark suite, achieving optimal results in only two\
    \ cases. In most benchmarks, it increased the average circuit depth by 6%. This\
    \ is attributed to the significant cost of Floating Qubits, which can reach up\
    \ to 24x, making the depth minimization goal more challenging. However, flexible\
    \ Floating Qubit strategies may still be useful in scenarios requiring high adaptability,\
    \ such as resource reuse or circuit partitioning.\n\nTable [4](#page-11-0) shows\
    \ that JW outperforms BK in physical mapping and routing. It is because JW's Pauli\
    \ strings have more regularity from block to block and hence better locality,\
    \ thereby reducing the number of additional routing gates. Therefore, we use JW\
    \ encoding for fermion operators in all subsequent experiments.\n\n# 4.3 General\
    \ Hamiltonian Simulation Compilation\n\n4.3.1 End-to-end Compilation. For the\
    \ evaluation of general Hamiltonian simulation, we compiled a diverse set of six\
    \ Hamiltonian models. These models are not only representative in hybrid CV-DV\
    \ systems but also pose significant complexity during compilation, making them\
    \ ideal for evaluating our general-purpose compiler. We further tested performance\
    \ across varying lattice sizes to assess scalability. The detailed results are\
    \ shown in Table [5.](#page-12-5)\n\nZihan Chen, Jiakang Li, Minghao Guo, Henry\
    \ Chen, Zirui Li, Joel Bierman, Yipeng Huang, Huiyang Zhou, Yuan Liu, and Eddy\
    \ Z. Zhang\n\n<span id=\"page-11-0\"></span>\n\n| Molecules         | Mapping\
    \ | # Pauli Strings | Christofides Algorithm(Qumode SWAP) |           |      \
    \    | Threshold(Qumode SWAP) |           |                 | Threshold(Floating\
    \ Qubit) |           |                   |\n|-------------------|---------|-----------------|-------------------------------------|-----------|----------|------------------------|-----------|-----------------|---------------------------|-----------|-------------------|\n\
    |                   |         |                 | 1-op Gate                  \
    \         | 2-op Gate | Duration | 1-op Gate              | 2-op Gate | Duration\
    \        | 1-op Gate                 | 2-op Gate | Duration          |\n| LiH\
    \ (4, 12)       | BK      | 631             | 41472                          \
    \     | 30920     | 546597   | 39192                  | 29780     | 518510 (5.14%)\
    \  | 39360                     | 29864     | 520580 (4.76%)    |\n|          \
    \         | JW      | 631             | 36344                               |\
    \ 27384     | 458441   | 33888                  | 26156     | 432246 (5.71%) \
    \ | 34352                     | 26472     | 444074 (3.13%)    |\n| BeH2 (6, 14)\
    \      | BK      | 666             | 49520                               | 35608\
    \     | 628278   | 46576                  | 34136     | 587180 (6.54%)  | 48210\
    \                     | 35184     | 644966 (-2.66%)   |\n|                   |\
    \ JW      | 666             | 44428                               | 33652    \
    \ | 562822   | 42092                  | 32484     | 535692 (4.82%)  | 43384  \
    \                   | 33508     | 585151 (-3.97%)   |\n|                   | BK\
    \      | 789             | 54976                               | 40688     | 678566\
    \   | 52368                  | 39384     | 650664 (4.11%)  | 57602           \
    \          | 43408     | 769017 (-13.33%)  |\n| Ethylene (10, 16) | JW      |\
    \ 789             | 57312                               | 43496     | 712622 \
    \  | 54304                  | 41992     | 681511 (4.37%)  | 58066            \
    \         | 44608     | 796156 (-11.72%)  |\n| NH3 (8, 12)       | BK      | 915\
    \             | 61564                               | 45592     | 810401   | 58164\
    \                  | 43892     | 773912 (4.5%)   | 58268                     |\
    \ 43944     | 769401 (5.06%)    |\n|                   | JW      | 915       \
    \      | 53680                               | 39960     | 656519   | 50336  \
    \                | 38288     | 615952 (6.18%)  | 53002                     | 39684\
    \     | 699905 (-6.61%)   |\n|                   | BK      | 1177            |\
    \ 87200                               | 63768     | 1057647  | 82584         \
    \         | 61460     | 1011085 (4.4%)  | 85998                     | 64196  \
    \   | 1111745 (-5.11%)  |\n| C2 (10, 16)       | JW      | 1177            | 88960\
    \                               | 66968     | 1100289  | 83832               \
    \   | 64404     | 1057411 (3.9%)  | 87568                     | 67028     | 1190209\
    \ (-8.17%)  |\n| N2 (10, 16)       | BK      | 1177            | 88792       \
    \                        | 65004     | 1074193  | 84488                  | 62852\
    \     | 1039060 (3.27%) | 87078                     | 65176     | 1150102 (-7.07%)\
    \  |\n|                   | JW      | 1177            | 88752                \
    \               | 67056     | 1106544  | 84232                  | 64796     |\
    \ 1063242 (3.91%) | 89512                     | 68528     | 1216367 (-9.92%) \
    \ |\n|                   | BK      | 1219            | 80004                 \
    \              | 59516     | 1048788  | 75860                  | 57444     | 1001186\
    \ (4.54%) | 75940                     | 57484     | 997611 (4.88%)    |\n| H2O\
    \ (8, 12)       | JW      | 1219            | 70200                          \
    \     | 52696     | 862689   | 66336                  | 50764     | 817900 (5.19%)\
    \  | 70540                     | 52908     | 937287 (-8.65%)   |\n|          \
    \         | BK      | 1654            | 122864                              |\
    \ 88204     | 1543297  | 116576                 | 85060     | 1469419 (4.79%)\
    \ | 122572                    | 91040     | 1620031 (-4.97%)  |\n| H2O (10, 14)\
    \      | JW      | 1654            | 111332                              | 83068\
    \     | 1377143  | 105180                 | 79992     | 1319874 (4.16%) | 113576\
    \                    | 84904     | 1553009 (-12.77%) |\n|                   |\
    \ BK      | 1734            | 131716                              | 94452    \
    \ | 1657461  | 124404                 | 90796     | 1547533 (6.63%) | 129320 \
    \                   | 93968     | 1700487 (-2.6%)   |\n| NH3 (8, 14)       | JW\
    \      | 1734            | 118260                              | 88428     | 1484725\
    \  | 111620                 | 85108     | 1395552 (6.01%) | 120348           \
    \         | 90228     | 1622020 (-9.25%)  |\n|                   | BK      | 1884\
    \            | 175612                              | 126176    | 2212185  | 167756\
    \                 | 122248    | 2141333 (3.2%)  | 171994                    |\
    \ 131024    | 2381938 (-7.67%)  |\n| C2 (12, 18)       | JW      | 1884      \
    \      | 155140                              | 117292    | 1937459  | 147340 \
    \                | 113392    | 1857982 (4.1%)  | 163376                    | 127668\
    \    | 2361642 (-21.89%) |\n|                   |         |                 |\
    \                                     |           |          |               \
    \         |           |                 |                           |        \
    \   |                   |\n\nTable 4: Compilation latency results for multi-Pauli\
    \ exponentials. Gate latencies are assigned according to Table [2,](#page-3-0)\
    \ where single-qubit (1-op) gates have a latency of 1 unit (20 nanoseconds) and\
    \ two-qubit (2-op) gates incur a latency of 20 units. The Christofides algorithm\
    \ is used as the performance baseline. Christofides-based routing is used as the\
    \ baseline. Duration results show absolute latency and percentage reduction relative\
    \ to the baseline.\n\nTable [5](#page-12-5) shows the results at different stages\
    \ of compilation. The number of Pauli strings and hybrid CV-DV gates is from the\
    \ intermediate representation using our domain-specific language, CVDV-QASM. The\
    \ \"Total Gate Count\" and \"Duration\" are from the final compiled circuits after\
    \ decomposition, mapping, and routing.\n\nThese representative Hamiltonian models\
    \ highlight the compiler's capability to support practical quantum computing applications,\
    \ offering a valuable framework for benchmarking and reference in future research.\
    \ Key considerations included successful decomposition, gate count, and acceptable\
    \ depth metrics. Based on our Rule-Based Recursive Template Matching Mechanism,\
    \ the framework can be further optimized for architecture-specific constraints\
    \ or fidelity improvements, providing a foundation for future advancements in\
    \ Hamiltonian simulation compilation.\n\n4.3.2 Hit Rate Analysis for Decomposition\
    \ Rules. We conducted a detailed decomposition analysis by measuring the \"hit\
    \ rate\" of each rule, quantifying its relative importance during synthesis. Each\
    \ unique term in a summation Σ was counted once per Hamiltonian model, regardless\
    \ of index variations. For complex expressions (e.g., † †, ††), which often require\
    \ multiple recursive applications, we normalized rule usage per Hamiltonian model.\
    \ This normalized frequency is defined as the rule's \"hit rate.\"\n\nRule 16,\
    \ which corresponds to native basis gate synthesis, appears most frequently and\
    \ nearly all decomposition paths eventually terminate at a basis gate. It accounts\
    \ for 68.30% of rule applications along successful paths, and 70.88% overall.\
    \ To better assess the relative contribution of other rules, we exclude Rule 16\
    \ and report the normalized hit rates for Rules 1-15 in Table [6.](#page-12-6)\n\
    \nRules 1 and 2 (Trotter and BCH decompositions) are heavily used during intermediate\
    \ steps, together contributing over 28.80% of the successful hit rate. Rules 14\
    \ and 15, which address multi-Pauli exponentials and multi-qubit-controlled qumode\
    \ displacements, also show high hit rates, highlighting the importance of qubitqumode\
    \ interactions in certain Hamiltonian model. Rules 11 and 12 frequently appear\
    \ in the successful paths as well, enabling bosonic operators to be decomposed\
    \ into fine-grained gate-synthesizable forms via block-encoding templates, jointly\
    \ contributing 10.80% to the successful hit rate.\n\n4.3.3 Compilation Time Analysis.\
    \ We evaluated the compilation time of our framework using JW-mapped Pauli strings,\
    \ considering the scale of Pauli string compilation for each molecule. Results\
    \ are shown in Table [7.](#page-12-7) Measured in seconds, the compilation time\
    \ covers the entire process, from Pauli string decomposition to physical circuit\
    \ synthesis. While it generally increases with the number of Pauli strings, C2(12,\
    \ 18) shows a significantly higher time due to the increased number of electrons\
    \ and spin-orbitals, leading to a more complex system than just longer Pauli strings.\n\
    \n# 5 RELATED WORK\n\nExisting tools, including Bosonic Qiskit [\\[40\\]](#page-13-11),\
    \ StrawberryField [\\[21\\]](#page-13-12), Perceval [\\[49\\]](#page-14-2), and\
    \ Bosehedral [\\[33\\]](#page-13-13), offer preliminary support for programming,\
    \ simulating, and composing circuits for either domainspecific applications such\
    \ as Gaussian Boson Sampling (GBS) or general bosonic circuits. Perceval allows\
    \ users to build linear optical circuits from a collection of pre-defined components.\
    \ Strawberry Fields and Bosonic Qikist provide Python libraries involving basic\
    \ CV-DV gates, as well as simulate the programs written with\n\n<span id=\"page-12-5\"\
    ></span>\n\n| Hamiltonians                  | # Qubits     | # Qumodes | # Pauli\
    \ Strings (in IR) | # Gates (in IR) | Final Gate Count | Duration |\n|-------------------------------|--------------|-----------|-------------------------|-----------------|------------------|----------|\n\
    | Kerr Nonlinear Hamiltonian    | 19 (ancilla) | 1         | 0               \
    \        | 409             | 595              | 3032     |\n|                \
    \               | 20           | 20        | 19                      | 419   \
    \          | 696              | 943      |\n| \U0001D44D2-Higgs Model        \
    \        | 40           | 40        | 39                      | 839          \
    \   | 1472             | 1797     |\n|                               | 60    \
    \       | 60        | 59                      | 1259            | 2263       \
    \      | 2721     |\n|                               | 20 (ancilla) | 20     \
    \   | 0                       | 820             | 2935             | 15353   \
    \ |\n| Bose-Hubbard Model            | 40 (ancilla) | 40        | 0          \
    \             | 2440            | 15313            | 62548    |\n|           \
    \                    | 60 (ancilla) | 60        | 0                       | 4860\
    \            | 36705            | 155872   |\n| Hubbard-Holstein Model       \
    \ | 40           | 20        | 3100                    | 100             | 688011\
    \           | 4509157  |\n|                               | 20           | 40\
    \        | 96                      | 1392            | 5635             | 28551\
    \    |\n| Electronic-Vibration Coupling | 40           | 80        | 196     \
    \                | 2852            | 12655            | 54952    |\n|        \
    \                       | 60           | 120       | 296                     |\
    \ 4312            | 19684            | 81729    |\n| Heisenberg Model        \
    \      | 20           | 0         | 77                      | 0              \
    \ | 2218             | 4122     |\n|                               | 40      \
    \     | 0         | 157                     | 0               | 4676         \
    \    | 4398     |\n|                               | 60           | 0        \
    \ | 237                     | 0               | 7200             | 7074     |\n\
    \nTable 5: Compilation results for general Hamiltonian simulations. For each model,\
    \ the table reports (a) the number of qubits and qumodes utilized, (b) the component\
    \ counts in intermediate representation, i.e., the number of Pauli strings and\
    \ hybrid CV-DV gates, using our domain-specific language, CVDV-QASM, and (c) the\
    \ \"Total Gate Count\" and \"Duration\" of the final compiled circuit. Operation\
    \ durations are estimated by assigning 1 time unit (20 nanoseconds) to each single-qubit\
    \ or single-qumode gate and 20 time units to each hybrid CV-DV gate and multi-qumode\
    \ gate, according to Table [2.](#page-3-0)\n\n<span id=\"page-12-6\"></span>\n\
    \n| Rule  | Success | Total  | Rule   | Success | Total | Rule   | Success | Total\
    \  |\n|-------|---------|--------|--------|---------|-------|--------|---------|--------|\n\
    | No. 1 | 9.50%   | 3.83%  | No. 6  | 1.35%   | 0.97% | No. 11 | 0.34%   | 4.47%\
    \  |\n| No. 2 | 19.33%  | 23.77% | No. 7  | 1.35%   | 0.57% | No. 12 | 5.40% \
    \  | 12.78% |\n| No. 3 | 0.00%   | 0.00%  | No. 8  | 8.15%   | 1.47% | No. 13\
    \ | 5.40%   | 11.48% |\n| No. 4 | 8.15%   | 2.94%  | No. 9  | 0.00%   | 1.47%\
    \ | No. 14 | 15.77%  | 17.17% |\n| No. 5 | 8.15%   | 1.47%  | No. 10 | 1.35% \
    \  | 0.45% | No. 15 | 15.77%  | 17.17% |\n\nTable 6: Normalized hit rates for\
    \ decomposition Rules 1-15. Each entry includes both the Success Hit Rate (calculated\
    \ over successfully decomposed paths) and the Total Hit Rate (all search attempts,\
    \ including failures).\n\n<span id=\"page-12-7\"></span>\n\n| Molecule       \
    \   | # Pauli Strings Time(s) |        | Molecule     | # Pauli Strings Time(s)\
    \ |          |\n|-------------------|-------------------------|--------|--------------|-------------------------|----------|\n\
    | LiH (4, 12)       | 631                     | 20.39  | BeH2 (6, 14) | 666  \
    \                   | 33.33    |\n| Ethylene (10, 16) | 789                  \
    \   | 67.21  | NH3 (8, 12)  | 915                     | 59.95    |\n| C2 (10,\
    \ 16)       | 1177                    | 184.49 | N2 (10, 16)  | 1177         \
    \           | 205.48   |\n| H2O (8, 12)       | 1219                    | 100.33\
    \ | H2O (10, 14) | 1654                    | 379.83   |\n| NH3 (8, 14)       |\
    \ 1734                    | 485.46 | C2 (12, 18)  | 1884                    |\
    \ 1,152.00 |\n|                   |                         |        |       \
    \       |                         |          |\n\nTable 7: Compilation Time Analysis\n\
    \nthese basic libraries. However, none of this provides support for the simulation\
    \ of the hybrid CV-DV Hamiltonian.\n\nWhile Hamiltonian simulation on DV systems\
    \ has been extensively studied [\\[18,](#page-13-28) [25,](#page-13-29) [26\\\
    ]](#page-13-30), compilation support for hybrid CV-DV systems remains underexplored.\
    \ The unique properties of Bosonic hardware mean that Fermion-Boson interactions\
    \ have not been thoroughly investigated. This involves converting Hamiltonian\
    \ simulation algorithms into basis gates for both qubits and qumodes, implementing\
    \ multi-qubit operations, and optimizing qubit-qumode mapping and routing to address\
    \ connectivity and error mitigation challenges. Our work bridges this gap.\n\n\
    ## 6 CONCLUSIONS\n\nOur work introduces Genesis, the first comprehensive compilation\
    \ framework for Hamiltonian simulation on hybrid CV-DV quantum processors. By\
    \ leveraging a two-stage compilation approach: (1) decomposing hybrid Hamiltonians\
    \ into universal basis gates, and (2) mapping them to hardware-constrained circuits,\
    \ we enable efficient simulation of complex physical systems. Our tool has successfully\
    \ compiled important Hamiltonians, including the Bose-Hubbard model, Z2−Higgs\
    \ model, Hubbard-Holstein model, and electronvibration coupling Hamiltonians critical\
    \ in domains like quantum field theory, condensed matter physics, and quantum\
    \ chemistry. We also provide a domain-specific language (DSL) design to support\
    \ Hamiltonian simulation on a hybrid CV-DV architecture.\n\n# ACKNOWLEDGMENTS\n\
    \nThis work is supported in part by grants from the Rutgers Research Council,\
    \ NSF, and DOE. In particular, Y.L., H.Z., and J.B. are supported by the U.S.\
    \ Department of Energy (DOE), Office of Science, Office of Advanced Scientific\
    \ Computing Research (ASCR), under Award Number DE-SC0025384. Z.C., J.L., M.G.,\
    \ H.C., and E.Z. are supported by the DOE Award DE-SC0025563, the NSF Award CCF-2129872,\
    \ and Rutgers Research Council Grant. J.B., Y.L., and H.Z. are supported in part\
    \ by NSF OSI-2410675 (with a subcontract to NC State University from Duke University).\
    \ H.Z. is also supported in part by NSF grants PHY-2325080 and OMA-2120757 (with\
    \ a subcontract to NC State University from the University of Maryland). Any opinions,\
    \ findings, conclusions, or recommendations expressed in this material are those\
    \ of the authors and do not necessarily reflect the views of our sponsors.\n\n\
    ## REFERENCES\n\n- <span id=\"page-12-3\"></span>[1] Junaid Aftab, Dong An, and\
    \ Konstantina Trivisa. 2024. Multi-product Hamiltonian simulation with explicit\
    \ commutator scaling. arXiv preprint arXiv:2403.08922 (2024).\n- <span id=\"page-12-4\"\
    ></span>[2] Sergey B Bravyi and Alexei Yu Kitaev. 2002. Fermionic quantum computation.\
    \ Annals of Physics 298, 1 (2002), 210–226.\n- <span id=\"page-12-1\"></span>[3]\
    \ Colin D. Bruzewicz, John Chiaverini, Robert McConnell, and Jeremy M. Sage. 2019.\
    \ Trapped-ion quantum computing: Progress and challenges. Applied Physics Reviews\
    \ 6, 2 (May 2019). [doi:10.1063/1.5088164](https://doi.org/10.1063/1.5088164)\n\
    - <span id=\"page-12-0\"></span>[4] Philippe Campagne-Ibarcq, Alec Eickbusch,\
    \ Steven Touzard, Evan Zalys-Geller, Nicholas E Frattini, Volodymyr V Sivak, Philip\
    \ Reinhold, Shruti Puri, Shyam Shankar, Robert J Schoelkopf, et al. 2020. Quantum\
    \ error correction of a qubit encoded in grid states of an oscillator. Nature\
    \ 584, 7821 (2020), 368–372.\n- <span id=\"page-12-2\"></span>[5] Benjamin J.\
    \ Chapman, Stijn J. de Graaf, Sophia H. Xue, Yaxing Zhang, James Teoh, Jacob C.\
    \ Curtis, Takahiro Tsunoda, Alec Eickbusch, Alexander P. Read, Akshay Koottandavida,\
    \ Shantanu O. Mundhada, Luigi Frunzio, M. H. Devoret, S. M. Girvin, and R. J.\
    \ Schoelkopf. 2023. A high on-off ratio beamsplitter interaction for gates on\
    \ bosonically encoded qubits. arXiv[:2212.11929](https://arxiv.org/abs/2212.11929)\
    \ [quant-ph] [https:](https://arxiv.org/abs/2212.11929) [//arxiv.org/abs/2212.11929](https://arxiv.org/abs/2212.11929)\n\
    - <span id=\"page-13-10\"></span>[6] Eleanor Crane, Kevin C Smith, Teague Tomesh,\
    \ Alec Eickbusch, John M Martyn, Stefan Kühn, Lena Funcke, Michael Austin DeMarco,\
    \ Isaac L Chuang, Nathan Wiebe, et al. 2024. Hybrid Oscillator-Qubit Quantum Processors:\
    \ Simulating Fermions, Bosons, and Gauge Fields. arXiv preprint arXiv:2409.03747\
    \ (2024).\n- <span id=\"page-13-19\"></span>[7] Brennan de Neeve, Thanh Long Nguyen,\
    \ Tanja Behrle, and Jonathan Home. 2020. Error correction of a logical grid state\
    \ qubit by dissipative pumping. arXiv[:2010.09681](https://arxiv.org/abs/2010.09681)\
    \ [quant-ph] <https://arxiv.org/abs/2010.09681>\n- <span id=\"page-13-0\"></span>[8]\
    \ Brennan De Neeve, Thanh-Long Nguyen, Tanja Behrle, and Jonathan P Home. 2022.\
    \ Error correction of a logical grid state qubit by dissipative pumping. Nature\
    \ Physics 18, 3 (2022), 296–300.\n- <span id=\"page-13-23\"></span>[9] Yu-Hao\
    \ Deng, Yi-Chao Gu, Hua-Liang Liu, Si-Qiu Gong, Hao Su, Zhi-Jiong Zhang, Hao-Yang\
    \ Tang, Meng-Hao Jia, Jia-Min Xu, Ming-Cheng Chen, Jian Qin, Li-Chao Peng, Jiarong\
    \ Yan, Yi Hu, Jia Huang, Hao Li, Yuxuan Li, Yaojian Chen, Xiao Jiang, Lin Gan,\
    \ Guangwen Yang, Lixing You, Li Li, Han-Sen Zhong, Hui Wang, Nai-Le Liu, Jelmer\
    \ J. Renema, Chao-Yang Lu, and Jian-Wei Pan. 2023. Gaussian Boson Sampling with\
    \ Pseudo-Photon-Number-Resolving Detectors and Quantum Computational Advantage.\
    \ Phys. Rev. Lett. 131 (Oct 2023), 150601. Issue 15. [doi:10.1103/PhysRevLett.131.150601](https://doi.org/10.1103/PhysRevLett.131.150601)\n\
    - <span id=\"page-13-25\"></span>[10] Yulong Dong, K Birgitta Whaley, and Lin\
    \ Lin. 2022. A quantum hamiltonian simulation benchmark. npj Quantum Information\
    \ 8, 1 (2022), 131.\n- <span id=\"page-13-17\"></span>[11] Alec Eickbusch, Volodymyr\
    \ Sivak, Andy Z. Ding, Salvatore S. Elder, Shantanu R. Jha, Jayameenakshi Venkatraman,\
    \ Baptiste Royer, S. M. Girvin, Robert J. Schoelkopf, and Michel H. Devoret. 2022.\
    \ Fast universal control of an oscillator with weak dispersive coupling to a qubit.\
    \ Nature Physics 18, 12 (Oct. 2022), 1464–1469. [doi:10.1038/s41567-022-01776-9](https://doi.org/10.1038/s41567-022-01776-9)\n\
    - <span id=\"page-13-7\"></span>[12] Richard P Feynman. 1982. Simulating physics\
    \ with computers. International journal of theoretical physics 21 (1982), 467–488.\
    \ Issue 6/7. [doi:10.1007/BF02650179](https://doi.org/10.1007/BF02650179)\n- <span\
    \ id=\"page-13-1\"></span>[13] Christa Flühmann and Jonathan P Home. 2020. Direct\
    \ characteristic-function tomography of quantum states of the trapped-ion motional\
    \ oscillator. Physical review letters 125, 4 (2020), 043602.\n- <span id=\"page-13-16\"\
    ></span>[14] Christa Flühmann, Thanh Long Nguyen, Matteo Marinelli, Vlad Negnevitsky,\
    \ Karan Mehta, and JP Home. 2019. Encoding a qubit in a trapped-ion mechanical\
    \ oscillator. Nature 566, 7745 (2019), 513–517.\n- <span id=\"page-13-20\"></span>[15]\
    \ C. Flühmann, T. L. Nguyen, M. Marinelli, V. Negnevitsky, K. Mehta, and J. P.\
    \ Home. 2019. Encoding a qubit in a trapped-ion mechanical oscillator. Nature\
    \ 566, 7745 (Feb. 2019), 513–517. [doi:10.1038/s41586-019-0960-6](https://doi.org/10.1038/s41586-019-0960-6)\n\
    - <span id=\"page-13-2\"></span>[16] Jeffrey M Gertler, Brian Baker, Juliang Li,\
    \ Shruti Shirol, Jens Koch, and Chen Wang. 2021. Protecting a bosonic qubit with\
    \ autonomous quantum error correction. Nature 590, 7845 (2021), 243–248.\n- <span\
    \ id=\"page-13-21\"></span>[17] P. C. Haljan, K.-A. Brickman, L. Deslauriers,\
    \ P. J. Lee, and C. Monroe. 2005. Spin-Dependent Forces on Trapped Ions for Phase-Stable\
    \ Quantum Gates and Entangled States of Spin and Motion. Phys. Rev. Lett. 94 (Apr\
    \ 2005), 153602. Issue 15. [doi:10.1103/PhysRevLett.94.153602](https://doi.org/10.1103/PhysRevLett.94.153602)\n\
    - <span id=\"page-13-28\"></span>[18] Yuwei Jin, Zirui Li, Fei Hua, Yanhao Chen,\
    \ Henry Chen, Yipeng Huang, and Eddy Z. Zhang. 2023. Tetris: A compilation Framework\
    \ for VQE Applications. arXiv[:2309.01905](https://arxiv.org/abs/2309.01905) [quant-ph]\n\
    - <span id=\"page-13-37\"></span>[19] Pascual Jordan and Eugene Paul Wigner. 1993.\
    \ Über das paulische äquivalenzverbot. Springer.\n- <span id=\"page-13-27\"></span>[20]\
    \ Christopher Kang, Micheline B Soley, Eleanor Crane, SM Girvin, and Nathan Wiebe.\
    \ 2023. Leveraging hamiltonian simulation techniques to compile operations on\
    \ bosonic devices. arXiv preprint arXiv:2303.15542 (2023).\n- <span id=\"page-13-12\"\
    ></span>[21] Nathan Killoran, Josh Izaac, Nicolás Quesada, Ville Bergholm, Matthew\
    \ Amy, and Christian Weedbrook. 2019. Strawberry Fields: A Software Platform for\
    \ Photonic Quantum Computing. Quantum 3 (March 2019), 129. [doi:10.22331/q-](https://doi.org/10.22331/q-2019-03-11-129)[2019-03-11-129](https://doi.org/10.22331/q-2019-03-11-129)\n\
    - <span id=\"page-13-15\"></span>[22] Alexei Kitaev and William A Webb. 2008.\
    \ Wavefunction preparation and resampling using a quantum computer. arXiv preprint\
    \ arXiv:0801.0342 (2008).\n- <span id=\"page-13-8\"></span>[23] Johannes Knörzer,\
    \ Tao Shi, Eugene Demler, and J Ignacio Cirac. 2022. Spin-Holstein models in trapped-ion\
    \ systems. Physical Review Letters 128, 12 (2022), 120404.\n- <span id=\"page-13-35\"\
    ></span>[24] Gushu Li, Yufei Ding, and Yuan Xie. 2019. Tackling the qubit mapping\
    \ problem for NISQ-era quantum devices. Proceedings of the Twenty-Fourth International\
    \ Conference on Architectural Support for Programming Languages and Operating\
    \ Systems (2019), 1001–1014.\n- <span id=\"page-13-29\"></span>[25] Gushu Li,\
    \ Yunong Shi, and Ali Javadi-Abhari. 2021. Software-Hardware Co-Optimization for\
    \ Computational Chemistry on Superconducting Quantum Processors. In Proceedings\
    \ of the 48th Annual International Symposium on Computer Architecture (Virtual\
    \ Event, Spain) (ISCA '21). IEEE Press, 832–845. [doi:10.1109/ISCA52012.2021.00070](https://doi.org/10.1109/ISCA52012.2021.00070)\n\
    - <span id=\"page-13-30\"></span>[26] Gushu Li, Anbang Wu, Yunong Shi, Ali Javadi-Abhari,\
    \ Yufei Ding, and Yuan Xie. 2022. Paulihedral: A Generalized Block-Wise Compiler\
    \ Optimization Framework for Quantum Simulation Kernels. Proceedings of the 27th\
    \ ACM International Conference on Architectural Support for Programming Languages\
    \ and Operating Systems (2022), 554–569. [doi:10.1145/3503222.3507715](https://doi.org/10.1145/3503222.3507715)\n\
    - <span id=\"page-13-33\"></span>[27] Langyu Li. 2024. Principal Trotter Observation\
    \ Error with Truncated Commutators. arXiv preprint arXiv:2408.03891 (2024).\n\
    - <span id=\"page-13-14\"></span>[28] Yuan Liu, Shraddha Singh, Kevin C. Smith,\
    \ Eleanor Crane, John M. Martyn, Alec Eickbusch, Alexander Schuckert, Richard\
    \ D. Li, Jasmine Sinanan-Singh, Micheline B. Soley, Takahiro Tsunoda, Isaac L.\
    \ Chuang, Nathan Wiebe, and Steven M. Girvin. 2024. Hybrid Oscillator-Qubit Quantum\
    \ Processors: Instruction Set Architectures, Abstract Machine Models, and Applications.\
    \ arXiv[:2407.10381](https://arxiv.org/abs/2407.10381) [quantph] <https://arxiv.org/abs/2407.10381>\n\
    - <span id=\"page-13-26\"></span>[29] Seth Lloyd. 1996. Universal quantum simulators.\
    \ Science 273, 5278 (Aug. 1996), 1073. [https://www.proquest.com/docview/213562780/abstract/](https://www.proquest.com/docview/213562780/abstract/381415109DD14C66PQ/1)\
    \ [381415109DD14C66PQ/1](https://www.proquest.com/docview/213562780/abstract/381415109DD14C66PQ/1)\n\
    - <span id=\"page-13-32\"></span>[30] Yao Lu, Aniket Maiti, John WO Garmon, Suhas\
    \ Ganjam, Yaxing Zhang, Jahan Claes, Luigi Frunzio, Steven M Girvin, and Robert\
    \ J Schoelkopf. 2023. Highfidelity parametric beamsplitting with a parity-protected\
    \ converter. nature communications 14, 1 (2023), 5767.\n- <span id=\"page-13-3\"\
    ></span>[31] Yuwei Ma, Yuan Xu, Xianghao Mu, Weizhou Cai, Ling Hu, Weiting Wang,\
    \ Xiaoxuan Pan, Haiyan Wang, YP Song, C-L Zou, et al. 2020. Error-transparent\
    \ operations on a logical qubit protected by quantum error correction. Nature\
    \ Physics 16, 8 (2020), 827–831.\n- <span id=\"page-13-34\"></span>[32] Refik\
    \ Mansuroglu, Felix Fischer, and Michael J Hartmann. 2023. Problem-specific classical\
    \ optimization of Hamiltonian simulation. Physical Review Research 5, 4 (2023),\
    \ 043035.\n- <span id=\"page-13-13\"></span>[33] Nicolas Maring, Andreas Fyrillas,\
    \ Mathias Pont, Edouard Ivanov, Petr Stepanov, Nico Margaria, William Hease, Anton\
    \ Pishchagin, Thi Huong Au, Sébastien Boissier, Eric Bertasi, Aurélien Baert,\
    \ Mario Valdivia, Marie Billard, Ozan Acar, Alexandre Brieussel, Rawad Mezher,\
    \ Stephen C. Wein, Alexia Salavrakos, Patrick Sinnott, Dario A. Fioretto, Pierre-Emmanuel\
    \ Emeriau, Nadia Belabas, Shane Mansfield, Pascale Senellart, Jean Senellart,\
    \ and Niccolo Somaschi. 2023. A general-purpose single-photon-based quantum computing\
    \ platform. arXiv[:2306.00874](https://arxiv.org/abs/2306.00874) [quant-ph] <https://arxiv.org/abs/2306.00874>\n\
    - <span id=\"page-13-4\"></span>[34] Zhongchu Ni, Sai Li, Xiaowei Deng, Yanyan\
    \ Cai, Libo Zhang, Weiting Wang, Zhen-Biao Yang, Haifeng Yu, Fei Yan, Song Liu,\
    \ et al. 2023. Beating the breakeven point with a discrete-variable-encoded logical\
    \ qubit. Nature 616, 7955 (2023), 56–60.\n- <span id=\"page-13-5\"></span>[35]\
    \ Nissim Ofek, Andrei Petrenko, Reinier Heeres, Philip Reinhold, Zaki Leghtas,\
    \ Brian Vlastakis, Yehan Liu, Luigi Frunzio, Steven M Girvin, Liang Jiang, et\
    \ al. 2016. Extending the lifetime of a quantum bit with error correction in superconducting\
    \ circuits. Nature 536, 7617 (2016), 441–445.\n- <span id=\"page-13-31\"></span>[36]\
    \ Jennifer Paykin, Albert T. Schmitz, Mohannad Ibrahim, Xin-Chuan Wu, and A. Y.\
    \ Matsuura. 2023. PCOAST: A Pauli-based Quantum Circuit Optimization Framework.\
    \ arXiv[:2305.10966](https://arxiv.org/abs/2305.10966) [quant-ph]\n- <span id=\"\
    page-13-9\"></span>[37] Alexandru Petrescu, Hakan E Türeci, Alexey V Ustinov,\
    \ and Ioan M Pop. 2018. Fluxon-based quantum simulation in circuit QED. Physical\
    \ Review B 98, 17 (2018), 174505.\n- <span id=\"page-13-22\"></span>[38] Adam\
    \ L. Shaw, Pascal Scholl, Ran Finkelstein, Richard Bing-Shiun Tsai, Joonhee Choi,\
    \ and Manuel Endres. 2024. Erasure-cooling, control, and hyperentanglement of\
    \ motion in optical tweezers. arXiv[:2311.15580](https://arxiv.org/abs/2311.15580)\
    \ [quant-ph] <https://arxiv.org/abs/2311.15580>\n- <span id=\"page-13-6\"></span>[39]\
    \ V. V. Sivak, A. Eickbusch, B. Royer, S. Singh, I. Tsioutsios, S. Ganjam, A.\
    \ Miano, B. L. Brock, A. Z. Ding, L. Frunzio, S. M. Girvin, R. J. Schoelkopf,\
    \ and M. H. Devoret. 2023. Real-time quantum error correction beyond break-even.\
    \ Nature 616, 7955 (March 2023), 50–55. [doi:10.1038/s41586-023-05782-6](https://doi.org/10.1038/s41586-023-05782-6)\n\
    - <span id=\"page-13-11\"></span>[40] Timothy J Stavenger, Eleanor Crane, Kevin\
    \ C Smith, Christopher T Kang, Steven M Girvin, and Nathan Wiebe. 2022. C2QA -\
    \ Bosonic Qiskit. In 2022 IEEE High Performance Extreme Computing Conference (HPEC).\
    \ 1–8. [doi:10.1109/](https://doi.org/10.1109/HPEC55821.2022.9926318) [HPEC55821.2022.9926318](https://doi.org/10.1109/HPEC55821.2022.9926318)\n\
    - <span id=\"page-13-36\"></span>[41] Qiming Sun, Timothy C Berkelbach, Nick S\
    \ Blunt, George H Booth, Sheng Guo, Zhendong Li, Junzi Liu, James McClain, Elvira\
    \ R Sayfutyarova, Sandeep Sharma, et al. 2007. The python-based simulations of\
    \ chemistry framework (pyscf). arXiv preprint arXiv:1701.08223 (2007).\n- <span\
    \ id=\"page-13-18\"></span>[42] James D. Teoh, Patrick Winkel, Harshvardhan K.\
    \ Babla, Benjamin J. Chapman, Jahan Claes, Stijn J. de Graaf, John W. O. Garmon,\
    \ William D. Kalfus, Yao Lu, Aniket Maiti, Kaavya Sahay, Neel Thakur, Takahiro\
    \ Tsunoda, Sophia H. Xue, Luigi Frunzio, Steven M. Girvin, Shruti Puri, and Robert\
    \ J. Schoelkopf. 2023. Dual-rail encoding with superconducting cavities. Proceedings\
    \ of the National Academy of Sciences 120, 41 (2023), e2221736120. [doi:10.1073/pnas.2221736120](https://doi.org/10.1073/pnas.2221736120)\
    \ arXiv[:https://www.pnas.org/doi/pdf/10.1073/pnas.2221736120](https://arxiv.org/abs/https://www.pnas.org/doi/pdf/10.1073/pnas.2221736120)\n\
    - <span id=\"page-13-24\"></span>[43] G.S. Thekkadath, S. Sempere-Llagostera,\
    \ B.A. Bell, R.B. Patel, M.S. Kim, and I.A. Walmsley. 2022. Experimental Demonstration\
    \ of Gaussian Boson Sampling with Displacement. PRX Quantum 3 (May 2022), 020336.\
    \ Issue 2. [doi:10.1103/](https://doi.org/10.1103/PRXQuantum.3.020336) [PRXQuantum.3.020336](https://doi.org/10.1103/PRXQuantum.3.020336)\n\
    - <span id=\"page-13-38\"></span>[44] Jules Tilly, Hongxiang Chen, Shuxiang Cao,\
    \ Dario Picozzi, Kanav Setia, Ying Li, Edward Grant, Leonard Wossnig, Ivan Rungger,\
    \ George H. Booth, and Jonathan Tennyson. 2022. The Variational Quantum Eigensolver:\
    \ A review of methods and best practices. Physics Reports 986 (nov 2022), 1–128.\
    \ [doi:10.1016/j.physrep.](https://doi.org/10.1016/j.physrep.2022.08.003) [2022.08.003](https://doi.org/10.1016/j.physrep.2022.08.003)\n\
    - <span id=\"page-13-39\"></span>[45] Nam P Vu, Daniel Dong, Xiaohan Dan, Ningyi\
    \ Lyu, Victor Batista, and Yuan Liu. 2025. A Computational Framework for Simulations\
    \ of Dissipative Non-Adiabatic Dynamics on Hybrid Oscillator-Qubit Quantum Devices.\
    \ arXiv preprint arXiv:2502.17820 (2025).\n\n- <span id=\"page-14-3\"></span>[46]\
    \ Mattia Walschaers. 2021. Non-Gaussian Quantum States and Where to Find Them.\
    \ PRX Quantum 2 (Sep 2021), 030204. Issue 3. [doi:10.1103/PRXQuantum.2.030204](https://doi.org/10.1103/PRXQuantum.2.030204)\n\
    - <span id=\"page-14-0\"></span>[47] Christopher S Wang, Jacob C Curtis, Brian\
    \ J Lester, Yaxing Zhang, Yvonne Y Gao, Jessica Freeze, Victor S Batista, Patrick\
    \ H Vaccaro, Isaac L Chuang, Luigi Frunzio, et al. 2020. Efficient multiphoton\
    \ sampling of molecular vibronic spectra on a superconducting bosonic processor.\
    \ Physical Review X 10, 2 (2020), 021060.\n- <span id=\"page-14-1\"></span>[48]\
    \ Christopher S Wang, Nicholas E Frattini, Benjamin J Chapman, Shruti Puri, Steven\
    \ M Girvin, Michel H Devoret, and Robert J Schoelkopf. 2023. Observation of wave-packet\
    \ branching through an engineered conical intersection. Physical Review X 13,\
    \ 1 (2023), 011008.\n- <span id=\"page-14-2\"></span>[49] Junyu Zhou, Yuhao Liu,\
    \ Yunong Shi, Ali Javadi-Abhari, and Gushu Li. 2024. Bosehedral: Compiler Optimization\
    \ for Bosonic Quantum Computing. arXiv preprint arXiv:2402.02279 (2024)."
- title: "End-to-end fully-binarized network design: from Generic Learned\n  Thermometer\
    \ to Block Pruning"
  abstract: 'Existing works on Binary Neural Network (BNN) mainly focus on model''s
    weights

    and activations while discarding considerations on the input raw data. This

    article introduces Generic Learned Thermometer (GLT), an encoding technique to

    improve input data representation for BNN, relying on learning non linear

    quantization thresholds. This technique consists in multiple data binarizations

    which can advantageously replace a conventional Analog to Digital Conversion

    (ADC) that uses natural binary coding. Additionally, we jointly propose a

    compact topology with light-weight grouped convolutions being trained thanks to

    block pruning and Knowledge Distillation (KD), aiming at reducing furthermore

    the model size so as its computational complexity. We show that GLT brings

    versatility to the BNN by intrinsically performing global tone mapping,

    enabling significant accuracy gains in practice (demonstrated by simulations on

    the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed

    block-pruning technique, we successfully achieve lightweight (under 1Mb),

    fully-binarized models with limited accuracy degradation while being suitable

    for in-sensor always-on inference use cases.'
  url: http://arxiv.org/abs/2505.13462v1
  keywords: Binarized Neural Networks, thermometric encoding, structured pruning,
    nonlinear ADC
  document: '#### I. INTRODUCTION


    Designing light-weight, fully-binarized models [\[1\]](#page-4-0), [\[2\]](#page-4-1)
    is a promising avenue to boost the efficiency of deep neural network (DNN) execution
    towards always-on resourcedconstrained inference [\[3\]](#page-4-2)–[\[7\]](#page-4-3).
    For instance, the multiplyaccumulate (MAC) operation between 1-bit signed weights
    and activations can be implemented using XNORs and popcounts only, thus enabling
    advantageous hardware simplification compared to its full-precision counterpart.
    Besides, it goes without saying that model binarization also offers a 32× reduction
    of memory needs for storing model''s weights and local activations. However, despite
    remarkable progress in BNNs, most of the works only focus on the weights and activations
    while keeping regular input data to cap accuracy loss. Unfortunately, it remains
    an issue for a concrete hardware implementation on devices that only support 1-bit
    computations, without the capability of dealing with integer inputs. To design
    small, fully-binarized networks with limited performance loss, we propose the
    following two contributions:


    • a Generic Learned Thermometer (GLT) which is an input data binarization being
    jointly trained with a BNN, enabling DNNs execution with 1-bit computations only;


    This work is part of the IPCEI Microelectronics and Connectivity and was supported
    by the French Public Authorities within the frame of France 2030. • a method to
    gradually replace complex processing blocks in BNNs by lighter grouped convolutions
    while limiting accuracy drop thanks to a distributional loss.


    #### II. RELATED WORKS


    Input data binarization: The de-facto choice to represents input data with a normalized
    [0, 1] dynamic range is to consider M-bit fixed-point coding (*e*.*g*., M=8).
    In terms of hardware, it thus requires large bit-width multipliers to perform
    the scalar products, at least at the first layer of the model. To address this
    issue, [\[8\]](#page-4-4) and [\[9\]](#page-4-5) propose to directly use the base-2
    fixed-point representation of each input value (Fig. [1](#page-0-0) (a)), with
    additional depth-wise and point-wise convolutions to combine the resulting bit
    planes. However, in addition to not being robust to spurious inputs and bit-flips,
    it is clear that a base-2 fixed-point encoding can only be used in the digital
    domain. Another class of input data binarization is thermometer encoding [\[10\]](#page-4-6),
    [\[11\]](#page-4-7). For instance, [\[11\]](#page-4-7) introduces a fixed thermometer
    (FT) encoding with quantization thresholds following a linear ramp (Fig. [1](#page-0-0)
    (b)). It is noteworthy mentioning that the thresholds of these thermometric encoding
    are manually designed and therefore may not be optimal for a wide variety of targeted
    tasks. On the contrary, GLT allows to automatically learn the thresholds as model''s
    parameters that can be taken into account at the ADC level (Fig. [2\)](#page-1-0)
    to involve a nonlinear input response, as a global tone mapping.


    <span id="page-0-0"></span>![](_page_0_Figure_13.jpeg)


    CONFIDENTIAL Fig. 1: Three examples of encoding techniques for input image data
    binary representation. Here, for the sake of simplicity, input pixel dynamic range
    is considered between 0 and 255.


    BNNs pruning: Network pruning [\[12\]](#page-4-8)–[\[15\]](#page-4-9) aims at
    removing unnecessary operations, operands and weights to reduce model size and
    complexity. While most existing techniques are mainly designed for full-precision
    models, only some works address the pruning problem in the context of BNNs. For
    example, [\[14\]](#page-4-10) employs weight flipping frequency while [\[16\]](#page-4-11)
    leverages the magnitude of latent weights as the pruning criteria. However, these
    methods result in irregular structures that require additional hardware features
    to handle sparse computations. In contrast, our block-based pruning scheme replaces
    complex blocks by lightweight grouped convolution layers, hence can be directly
    mapped to canonical hardware platforms. Compared to the existing depth pruning
    method [\[15\]](#page-4-9), our method is specifically tailored for BNNs, by gradually
    pruning complex blocks combining with a distributional loss [\[17\]](#page-4-12)
    to avoid severe accuracy degradation.


    #### III. GENERIC LEARNED THERMOMETER


    Our encoding method (GLT) is illustrated in Fig. [1](#page-0-0) (c), which reports
    a typical example of thermometric thresholds provided as model''s parameters,
    being optimized for a specific inference task via model training. In practice,
    it intrinsically performs a global tone mapping operation that can be implemented
    directly during the analog-to-digital conversion stage, if relying on a programmable
    slope ADC as schematically depicted in Fig. [2.](#page-1-0) This top-level schematic
    relies on a 1bit comparator that sequentially outputs comparisons between Vpix
    (the pixel voltage to convert) and Vt<sup>i</sup> . For i ∈ [[1, M]], Vt<sup>i</sup>
    successively takes the M different threshold values t<sup>i</sup> encoded on N<sup>b</sup>
    bits through the use of a DAC composed of a multiplexer and a set of voltage references
    ranging from Vmin to Vmax. Note that for this example Nb=8 and M=8.


    <span id="page-1-0"></span>![](_page_1_Figure_3.jpeg)


    Fig. 2: Nonlinear ramp ADC top-level view schematic. *A. Detailed mathematical
    formulation*


    Let us consider an M-bit thermometer encoding with M thresholds 0 < t<sup>1</sup>
    < t<sup>2</sup> < ... < tM−<sup>1</sup> < t<sup>M</sup> < 1. The thermovector
    TV ∈ R<sup>M</sup> corresponding to a value x is defined as:


    $$TV\_i = \text{Heaviside}(x - t\_i) = \begin{cases} 1 & \text{if } x \ge t\_i,
    \\ 0 & \text{otherwise.} \end{cases} \qquad (\text{l)}$$


    where x is normalized to the interval [0, 1] for ease of later optimization. The
    choice of the threshold values {ti}i=[[1,M]] deeply impacts the quantity of relevant
    information preserved and thus model''s performance, therefore our goal is to
    find optimal thresholds by minimizing any task loss. Let us denote ˜t ∈ R M+1
    <sup>+</sup> as the learnable latent threshold parameters that are kept always
    positive during the optimization process. We first compute the normalized vector
    ¯t as follows:


    <span id="page-1-1"></span>

    $$\bar{\mathbf{t}} = \frac{\bar{\mathbf{t}}}{\sum\_{i=1}^{M+1} \tilde{t}\_i} \tag{2}$$


    With 0 < t¯<sup>i</sup> <1 and P<sup>M</sup>+1 <sup>i</sup>=1 t¯<sup>i</sup> =
    1, the thermometer thresholds {ti}i=[[1,M]] are then computed as a cumulative
    sum:


    <span id="page-1-2"></span>

    $$t\_i = \sum\_{j=1}^{i} \bar{t}\_j \tag{3}$$


    It can be easily verified that 0 < t<sup>i</sup> < ti+1 < 1 for i ∈ [[1, M]],
    and the optimal threshold values are indirectly learned through the optimization
    of ˜t. Let us denote L as the task loss; I ∈ R <sup>H</sup>×<sup>W</sup> as the
    input image data of resolution H × W that is normalized within [0, 1]; and I b
    <sup>i</sup> ∈ {0, 1} <sup>H</sup>×<sup>W</sup> as the encoded bit plane at the
    bit position i ∈ [[1, M]]. Indeed, the gradient with respect to t˜<sup>j</sup>
    can be written by the chain rule as:


    $$\begin{split} \frac{\partial \mathcal{L}}{\partial \tilde{t}\_j} &= \sum\_{x,
    y, i} \left( \frac{\partial \mathcal{L}}{\partial I\_{x, y, i}^b} \frac{\partial
    I\_{x, y, i}^b}{\partial t\_i} \right) \frac{\partial t\_i}{\partial \tilde{t}\_j}
    \\ &= \sum\_{x, y, i} \left( \frac{\partial \mathcal{L}}{\partial I\_{x, y, i}^b}
    \frac{\partial \text{Heaviside}(I\_{x, y} - t\_i)}{\partial t\_i} \right) \frac{\partial
    t\_i}{\partial \tilde{t}\_j} \\ \end{split} (4)$$


    where the term ∂t<sup>i</sup> ∂t˜<sup>j</sup> is calculated from Eqs. [2](#page-1-1)
    and [3.](#page-1-2) The gradient vanishing problem caused by Heaviside function
    could be overcame using approximation techniques [\[18\]](#page-4-13)– [\[20\]](#page-4-14).
    Intuitively, near-threshold values should have higher influence (*i*.*e*., larger
    gradient magnitude) than far-threshold counterparts. Therefore, we employ the
    ReSTE approximation proposed in [\[20\]](#page-4-14) and propose a modified version
    as follows:


    $$\frac{\partial \text{Heaviside}(u)}{\partial u} = \frac{1}{m} \text{min}\left(\frac{1}{p}
    |u|^{\frac{1-p}{p}}, m\right) \qquad (5)$$


    where p > 1 controls the narrowness of the bell-shaped gradient curve and m is
    the clipping threshold. Concretely, p = 2 and m = 5 for all of our experiments.
    Since each t<sup>i</sup> is shared for all pixels within the same bit plane, a
    small change of the latent threshold parameters may notably tamper the model''s
    behavior. Therefore, to stabilize the model convergence during training, the update
    magnitude of the latent parameters t˜<sup>j</sup> is scaled by a factor β = 2/
    √ HWM.


    ### *B. Latent parameter''s initialization and constraint*


    An important question is how to properly initialize the latent threshold parameters
    ˜t. For the sake of simplicity, the straightforward option is to choose the initial
    values such that the resulting thermometer thresholds follow a linear ramp like
    proposed in [\[11\]](#page-4-7). In details, the threshold of the i-th bit-plane
    is defined as t<sup>i</sup> = s(i−0.5)/(2<sup>N</sup>b−1) where s = 2<sup>N</sup><sup>b</sup>
    /M is the uniform step size and 1/(2<sup>N</sup><sup>b</sup> − 1) is the normalization
    factor (*e*.*g*., 1/255 with 8b image data). Besides, the initialization of the
    non-normalized latent parameters ˜t is scaled as:


    $$

    \tilde{t}\_i = \begin{cases}

    0.5sk & \text{if } i = 1, \\

    sk & \text{if } 1 < i \le M, \\

    (0.5s - 1)k & \text{if } i = M + 1.

    \end{cases} \tag{6}

    $$


    Here the scale factor k aims to balance the stabilization with the updatability
    of the GLT. For optimization purposes and with respect to training stability issues,
    the latent parameters are forced to be positive and large enough, *i*.*e*., t˜<sup>i</sup>
    > 0.05. Coherently, the scaling value k is set such that the initial latent parameters
    remain higher than the aforementioned clipping threshold, *e*.*g*., by setting
    k = M/1280.


    #### IV. BLOCK PRUNING WITH DISTRIBUTIONAL LOSS


    #### *A. Proposed method*


    BNNs usually need to be compressed further to match strict hardware specifications
    of edge devices, *e*.*g*., typically under 1Mb SRAM. In this section, we propose
    a method to gradually prune complex processing blocks and replace it with a lightweight
    convolution (LWC) block involving a ggroup 3 × 3 convolution (GConv) of strides
    2 followed by a channel shuffle (if g > 1). Figure [3](#page-2-0) illustrates
    how our idea is applied to MUXORNet-11 [\[21\]](#page-4-15). The number of channels
    is doubled after the GConv, thus allowing the pruned model to have the same data
    dimension as the baseline model. The LWC shows a far better hardware mapping than
    its corresponding block in the baseline model. We leverage the Kullback-Leibler
    divergence loss forcing the pruned model to learn similar class distributions
    as the baseline model:


    $$\mathcal{L}\_{distr} = \frac{T^2}{N} \sum\_{i=1}^{N} \sum\_{j=1}^{\mathcal{C}}
    y\_{b\_j}(\mathbf{X}\_i; T) \log \left( \frac{y\_{b\_j}(\mathbf{X}\_i; T)}{y\_{p\_j}(\mathbf{X}\_i;
    T)} \right) \tag{7}$$


    where yb<sup>j</sup> (X<sup>i</sup> ; T) and yp<sup>j</sup> (X<sup>i</sup> ; T)
    are the softened probability corresponding to class j of the baseline and the
    pruned model, given the input image X<sup>i</sup> , *i*.*e*., the pre-softmax
    divided by a given temperature T. The total loss function is as follows:


    $$\mathcal{L} = (1 - \lambda)\mathcal{L}\_{ce} + \lambda\mathcal{L}\_{distr} \tag{8}$$


    where λ is the hyperparameter controlling the balance between the KD loss Ldistr
    and the cross-entropy loss Lce. Concretely, we set T = 8 and λ = 0.5 for later
    experiments. Assume that the model contains N<sup>b</sup> blocks, we gradually
    prune them in the direction from block N<sup>b</sup> to block 1. For each stage,
    we replace each block by a LWC block such that the output dimension is kept unchanged.
    Without loss of generality, we also notice that here a block may contains a single
    or several layers. The pruned model is initialized with weights of the previous
    stage and then retrained with the aforementioned loss function. The complete pruning
    procedure is presented in Algorithm [1.](#page-2-1)


    <span id="page-2-1"></span>Algorithm 1 Block pruning algorithm for BNNs.


    - Input: (1) pre-trained baselines with first layers and classifier models (FL
    and Cls); (2) X the training set; (3) list of N<sup>b</sup> blocks to be pruned
    W with P<sup>b</sup> ∈ [[1, Nb]];

    - 1: W<sup>p</sup> ← W

    - 2: for b = N<sup>b</sup> to P<sup>b</sup> do

    - 3: Wp[b] ← Initialize LWCb()

    - 4: PrunedNet ← Append(FL, Wp, Cls)

    - 5: PrunedNet ← TrainToConvergence(PrunedNet; X) 6: end for


    Output: Optimized pruned model with FL, W<sup>p</sup> and Cls


    <span id="page-2-0"></span>![](_page_2_Figure_16.jpeg)


    CONFIDENTIAL Fig. 3: Block pruning with an auxiliary lightweight grouped convolution
    (LWC) module. Note that our pruning method can be applied to an arbitrary network.


    #### V. EXPERIMENTS


    #### <span id="page-2-2"></span>*A. Validation of the GLT*


    Implementation details: Each channel of the RGB input image is encoded separately
    into M binary planes (*i*.*e*., M ∈ {8, 16, 32}), then the resulting binary planes
    are concatenated together to feed the model. Note that here we only evaluate the
    effectiveness of the encoding method, therefore we do not consider extra architectures
    to combine binary planes like proposed in [\[8\]](#page-4-4), [\[9\]](#page-4-5).
    For Visual Wake Words (VWW) dataset [\[22\]](#page-4-16) we resize images to 132
    × 176 for training and 120 × 160 for testing. We then perform data augmentation
    using random cropping to 120 × 160 and random horizontal flipping. For STL-10
    dataset [\[23\]](#page-4-17) with 96 × 96 RGB images, we adopt random crop from
    all-sided 12-pixel zero-padded images combined with random horizontal flip and
    random cutout [\[24\]](#page-4-18) of 24 × 24 patches. We first pre-train the
    realvalued model with binarized input and ReLU activations, then use it to initialize
    the weights of the fully-binarized model with 1-b Signed weights and Heaviside
    activations with STE [\[18\]](#page-4-13) gradient. Each model is trained during
    100 epochs for VWW and 250 epochs for STL-10 with Radam optimizer [\[25\]](#page-4-19).
    The learning rate is initialized at 10<sup>−</sup><sup>3</sup> and reduced to
    10<sup>−</sup><sup>8</sup> using cosine decay scheduler.


    VWW: We first conduct a benchmark on the gammainversed images (γ = 2.2) of VWW
    with MUXORNet-11 model. Table [I](#page-3-0) shows that models with GLT achieve
    higher accuracy on both train and test sets compared to FT and even the baseline
    gamma-inversed images in the case of binarized models. Specifically, at M = 16,
    we obtain a gap of nearly 0.9% on training and 1.5% on testing. These results
    demonstrate that GLT allows model to learn more effectively during training and
    hence generalize better at inference time.


    STL-10: Table [II](#page-3-1) and [III](#page-3-2) report the accuracy of models
    after each stage of training on both the original and the gammainversed STL-10
    datasets. It is shown that in most cases, model with GLT-encoded input achieves
    highest accuracy compared to model whose input is encoded by other methods. For
    the original dataset, at M = 8, GLT has a slight gain of 1.07%


    <span id="page-3-0"></span>TABLE I: MUXNORNet-11 on gamma-inversed VWW.


    <span id="page-3-3"></span>![](_page_3_Figure_1.jpeg)


    on gamma-inversed STL-10. Black: fixed linear curve [\[11\]](#page-4-7), Red/Green/Blue:
    learned curves of R/G/B channels.


    for VGG-Small and 0.85% for MUXORNet-11 compared to FT [\[11\]](#page-4-7). This
    gain slightly decreases for larger M, as more bit planes will increase inter-channel
    redundancy. On the other hand, for the gamma-inversed dataset, the gain of GLT
    over FT is strongly boosted up to 2.5% for M = 8 and it can even retain the accuracy
    level of FT on the original post-gamma correction dataset. In particular, for
    M = 32, MUXORNet-11 with GLT even achieves a slightly higher accuracy than the
    baseline model with gamma-inversed input. Figure [4](#page-3-3) shows the curves
    of the encoding bit-count level (from 1 to M) as a function of the thresholds
    learned on gamma-inversed dataset. It is shown that although being linearly initialized,
    our GLT can successfully learn proper nonlinear curves (*i*.*e*., global tone
    mapping), providing a higher accuracy and being wellsuited to a real-world deployment
    scenario. It thus suggests the possibility of using GLT to perform inference directly
    from analog data, bypassing all the image rendering stages.


    <span id="page-3-1"></span>TABLE II: Accuracy (%) on original STL-10 dataset.


    | Input encoding                   | # planes | VGG-Small |       | MUXORNet-11
    |       |

    |----------------------------------|----------|-----------|-------|-------------|-------|

    | method                           | (M)      | FP        | Bin.  | FP          |
    Bin.  |

    | Baseline integer (8-bit)         | 32-b     | 83.02     | 79.95 | 84.24       |
    79.74 |

    | Base-2 fixed-point [8]           | 8        | 78.32     | 73.15 | 80.06       |
    75.83 |

    | Fixed Linear<br>Thermometer [11] | 8        | 79.35     | 76.40 | 81.39       |
    77.43 |

    |                                  | 16       | 80.63     | 77.49 | 82.17       |
    78.47 |

    |                                  | 32       | 81.50     | 77.81 | 82.51       |
    78.90 |

    | Ours<br>GLT                      | 8        | 79.93     | 77.47 | 81.62       |
    78.28 |

    |                                  | 16       | 80.87     | 78.02 | 82.73       |
    78.60 |

    |                                  | 32       | 81.40     | 78.24 | 82.79       |
    79.06 |


    <span id="page-3-2"></span>TABLE III: Accuracy (%) on gamma-inversed STL-10 dataset.


    | Input encoding                   | # planes | VGG-Small |       | MUXORNet-11
    |       |

    |----------------------------------|----------|-----------|-------|-------------|-------|

    | method                           | (M)      | FP        | Bin.  | FP          |
    Bin.  |

    | Baseline gamma-inversed          | 32-b     | 82.29     | 79.44 | 83.03       |
    78.44 |

    | Fixed Linear<br>Thermometer [11] | 8        | 76.51     | 73.40 | 78.39       |
    74.99 |

    |                                  | 16       | 77.99     | 75.09 | 80.81       |
    76.98 |

    |                                  | 32       | 78.86     | 75.86 | 80.85       |
    77.30 |

    | Ours<br>GLT                      | 8        | 78.24     | 75.69 | 80.90       |
    77.45 |

    |                                  | 16       | 79.54     | 76.46 | 81.78       |
    78.05 |

    |                                  | 32       | 80.35     | 77.88 | 82.06       |
    78.51 |


    ## *B. Validation of the block pruning on fully-binarized model*


    In this part, we consider the MUXORNet-11 model with GLT and M = 32, trained on
    gamma-inversed dataset (78.51% acc.), to conduct experiment. The pre-trained model
    after the first stage in [V-A](#page-2-2) is used as teacher in our KD scheme.
    We set the groups g as 1, 2, 8 for three blocks, respectively, since the last
    layers are less sensitive to model''s performance. We compare our gradual block
    pruning method with three competitors, involving 1) baseline: the pruned model
    but trained from scratch; 2) depth: depth pruning [\[15\]](#page-4-9) using our
    LWCs as auxiliary model but trained in one shot without the KD loss Ldistr; 3)
    magnitude: channel-pruning based on the magnitude of the latent weights (inspired
    by [\[16\]](#page-4-11)) in which the pruning ratio is computed to have the same
    model size as the block-pruned model. The pruned models are trained during 300
    epochs with learning rate initialized at 10−<sup>3</sup> and reduced to 10−<sup>10</sup>
    using cosine decay scheduler. Figure [5](#page-3-4) shows the trade-off of model
    size/BOPs-accuracy loss of the pruned models. At each pruning point, our method
    always offers more than 3.6% higher accuracy compared to other methods. In particular,
    when pruning the third block, we successfully reduce the model size by 70% and
    the number of BOPs [\[26\]](#page-4-20) by 16%, this nearly without accuracy drop.
    Even if we seek for a model of under 0.5Mb and 1GBOPs, our method still reaches
    73% accuracy while other methods cannot exceed 68%. These results demonstrate
    the effectiveness of our method on designing extremely tiny, fully-binarized models.


    <span id="page-3-4"></span>![](_page_3_Figure_10.jpeg)


    Fig. 5: Trade-off curve for model size/BOPs reduction and accuracy loss of pruned
    MUXORNet-11 (original acc: 78.5%). VI. CONCLUSION


    This work addresses the end-to-end design of small-sized, fully-binarized networks
    for always-on inference use cases. To do this, we first introduce the GLT encoding
    scheme to properly manage input images binarization, jointly with performing a
    trained global tone mapping. Then, we propose a gradual block-based pruning strategy
    combined with a distributional loss to limit the overall accuracy drop. Experimental
    results on VWW and STL-10 demonstrate that GLT enables an efficient training stage,
    with a better generalization on unseen dataset. Our methods enable highly accurate
    models (∼78.5% accuracy on STL-10) that require 1-bit computations only with a
    model size under 1Mb, while relying on inputs of 32 bit-planes. Future works may
    investigate the performance on practical in-sensor data (*e*.*g*., on mosaiced
    frames impaired by fixed pattern noise and dead pixels), enabling ISP-free but
    still highly accurate always-on inference modules.


    #### REFERENCES


    - <span id="page-4-0"></span>[1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv,
    and Y. Bengio, "Binarized Neural Networks," in *Advances in Neural Information
    Processing Systems (NeurIPS)*, 2016, pp. 4107–4115.

    - <span id="page-4-1"></span>[2] J. Bethge, C. Bartz, H. Yang, Y. Chen, and C.
    Meinel, "MeliusNet: An Improved Network Architecture for Binary Neural Networks,"
    in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision (WACV)*, January 2021, pp. 1439–1448.

    - <span id="page-4-2"></span>[3] J. Choi, "Review of low power image sensors for
    always-on imaging," in *2016 International SoC Design Conference (ISOCC)*, 2016,
    pp. 11–12.

    - [4] M. Gazivoda and V. Bilas, "Always-On Sparse Event Wake-Up Detectors: A Review,"
    *IEEE Sensors Journal*, vol. 22, no. 9, pp. 8313–8326, 2022.

    - [5] D. Garrett, Y. S. Park, S. Kim, J. Sharma, W. Huang, M. Shaghaghi, V. Parthasarathy,
    S. Gibellini, S. Bailey, M. Moturi, P. Vorenkamp, K. Busch, J. Holleman, B. Javid,
    A. Yousefi, M. Judy, and A. Gupta, "A 1mW Always-on Computer Vision Deep Learning
    Neural Decision Processor," in *2023 IEEE International Solid-State Circuits Conference
    (ISSCC)*, 2023, pp. 8–10.

    - [6] A. Verdant, W. Guicquero, D. Coriat, G. Moritz, N. Royer, S. Thuries, A.
    Mollard, V. Teil, Y. Desprez, G. Monnot, P. Malinge, B. Paille, G. Caubit, A.
    Bourge, L. Tardif, S. Bigault, and J. Chossat, "A 450µw@50fps wake-up module featuring
    auto-bracketed 3-scale logcorrected pattern recognition and motion detection in
    a 1.5mpix 8t global shutter imager," in *2024 IEEE Symposium on VLSI Technology
    and Circuits (VLSI Technology and Circuits)*, 2024, pp. 1–2.

    - <span id="page-4-3"></span>[7] J. Vohra, A. Gupta, and M. Alioto, "Imager with
    In-Sensor Event Detection and Morphological Transformations with 2.9pJ/pixel×frame
    Object Segmentation FOM for Always-On Surveillance in 40nm," in *2024 IEEE International
    Solid-State Circuits Conference (ISSCC)*, vol. 67, 2024, pp. 104–106.

    - <span id="page-4-4"></span>[8] R. Durichen, T. Rocznik, O. Renz, and C. Peters,
    "Binary Input ¨ Layer: Training of CNN models with binary input data," *CoRR*,
    vol. abs/1812.03410, 2018. [Online]. Available: [http://arxiv.org/abs/1812.](http://arxiv.org/abs/1812.03410)
    [03410](http://arxiv.org/abs/1812.03410)

    - <span id="page-4-5"></span>[9] L. Vorabbi, D. Maltoni, and S. Santi, "Input
    Layer Binarization with Bit-Plane Encoding," in *International Conference on Artificial
    Neural Networks*, 2023.

    - <span id="page-4-6"></span>[10] D. Bankman, L. Yang, B. Moons, M. Verhelst,
    and B. Murmann, "An Always-On 3.8 µ J/86% CIFAR-10 Mixed-Signal Binary CNN Processor
    With All Memory on Chip in 28-nm CMOS," *IEEE Journal of Solid-State Circuits*,
    vol. 54, no. 1, pp. 158–172, 2019.

    - <span id="page-4-7"></span>[11] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen,
    and Z. Zhang, "FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with
    Fractional Activations," in *The 2021 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays*, ser. FPGA ''21. New York, NY, USA: Association for Computing Machinery,
    2021, p. 171–182.

    - <span id="page-4-8"></span>[12] S. Han, J. Pool, J. Tran, and W. Dally, "Learning
    both Weights and Connections for Efficient Neural Network," in *Advances in Neural
    Information Processing Systems*, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama,
    and R. Garnett, Eds., vol. 28. Curran Associates, Inc., 2015.

    - [13] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, "What is the
    State of Neural Network Pruning?" in *Proceedings of Machine Learning and Systems*,
    I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 129–146.

    - <span id="page-4-10"></span>[14] Y. Li and F. Ren, "BNN Pruning: Pruning Binary
    Neural Network Guided by Weight Flipping Frequency," in *2020 21st International
    Symposium on Quality Electronic Design (ISQED)*, 2020, pp. 306–311.

    - <span id="page-4-9"></span>[15] J. D. De Leon and R. Atienza, "Depth Pruning
    with Auxiliary Networks for Tinyml," in *ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2022, pp. 3963–
    3967.

    - <span id="page-4-11"></span>[16] T. Chen, N. Anderson, and Y. Kim, "Latent Weight-based
    Pruning for Small Binary Neural Networks," in *2023 28th Asia and South Pacific
    Design Automation Conference (ASP-DAC)*, 2023, pp. 751–756.

    - <span id="page-4-12"></span>[17] Z. Liu, Z. Shen, M. Savvides, and K.-T. Cheng,
    "Reactnet: Towards precise binary neural network with generalized activation functions,"
    in *European Conference on Computer Vision (ECCV)*, 2020.

    - <span id="page-4-13"></span>[18] Y. Bengio, N. Leonard, and A. Courville, "Estimating
    or Propagating ´ Gradients Through Stochastic Neurons for Conditional Computation,"
    *arXiv:1308.3432 [cs]*, Aug. 2013.

    - [19] H. Le, R. K. Høier, C.-T. Lin, and C. Zach, "AdaSTE: An Adaptive Straight-Through
    Estimator To Train Binary Neural Networks," in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 460–469.

    - <span id="page-4-14"></span>[20] X.-M. Wu, D. Zheng, Z. Liu, and W.-S. Zheng,
    "Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator
    for Binary Neural Networks Training," in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, October 2023, pp. 17 055–17 064.

    - <span id="page-4-15"></span>[21] V. T. Nguyen, W. Guicquero, and G. Sicard,
    "Histogram-Equalized Quantization for logic-gated Residual Neural Networks," in
    *2022 IEEE International Symposium on Circuits and Systems (ISCAS)*, 2022, pp.
    1289–1293.

    - <span id="page-4-16"></span>[22] A. Chowdhery, P. Warden, J. Shlens, A. G. Howard,
    and R. Rhodes, "Visual wake words dataset," *ArXiv*, vol. abs/1906.05721, 2019.

    - <span id="page-4-17"></span>[23] A. Coates, A. Ng, and H. Lee, "An Analysis
    of Single-Layer Networks in Unsupervised Feature Learning," in *AISTATS*, 2011.

    - <span id="page-4-18"></span>[24] T. Devries and G. W. Taylor, "Improved regularization
    of convolutional neural networks with cutout," *ArXiv*, vol. abs/1708.04552, 2017.

    - <span id="page-4-19"></span>[25] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J.
    Gao, and J. Han, "On the Variance of the Adaptive Learning Rate and Beyond," in
    *International Conference on Learning Representations*, 2020.

    - <span id="page-4-20"></span>[26] Y. Wang, Y. Lu, and T. Blankevoort, "Differentiable
    joint pruning and quantization for hardware efficiency," in *Computer Vision -
    ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
    Part XXIX*, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T.
    Brox, and J. Frahm, Eds., vol. 12374. Springer, pp. 259–277.'
- title: "Introducing Instruction-Accurate Simulators for Performance Estimation\n\
    \  of Autotuning Workloads"
  abstract: 'Accelerating Machine Learning (ML) workloads requires efficient methods
    due

    to their large optimization space. Autotuning has emerged as an effective

    approach for systematically evaluating variations of implementations.

    Traditionally, autotuning requires the workloads to be executed on the target

    hardware (HW). We present an interface that allows executing autotuning

    workloads on simulators. This approach offers high scalability when the

    availability of the target HW is limited, as many simulations can be run in

    parallel on any accessible HW. Additionally, we evaluate the feasibility of

    using fast instruction-accurate simulators for autotuning. We train various

    predictors to forecast the performance of ML workload implementations on the

    target HW based on simulation statistics. Our results demonstrate that the

    tuned predictors are highly effective. The best workload implementation in

    terms of actual run time on the target HW is always within the top 3 % of

    predictions for the tested x86, ARM, and RISC-V-based architectures. In the

    best case, this approach outperforms native execution on the target HW for

    embedded architectures when running as few as three samples on three simulators

    in parallel.'
  url: http://arxiv.org/abs/2505.13357v1
  keywords: Autotuning, TVM, gem5, cache optimization
  document: "#### I. INTRODUCTION\n\nThe optimization of Machine Learning (ML) models\
    \ is crucial due to their high computational demands. Traditional analytical methods\
    \ often fall short given the huge search space for optimal implementations on\
    \ modern CPU architectures. To address this challenge, autotuning has emerged\
    \ as a powerful approach. Autotuning systematically evaluates multiple implementations\
    \ of the same workload using mathematical models or ML techniques to guide subsequent\
    \ selections of implementations [\\[1\\]](#page-6-0), [\\[2\\]](#page-6-1). Apache\
    \ Tensor Virtual Machine (TVM) [\\[3\\]](#page-6-2), a well-known ML compiler\
    \ framework, implements several autotuning strategies [\\[4\\]](#page-6-3), [\\\
    [5\\]](#page-6-4). Autotuning typically requires execution on real hardware, which\
    \ introduces non-determinism due to factors such as the system load [\\[6\\]](#page-6-5),\
    \ cache collisions [\\[7\\]](#page-6-6), thermal throttling [\\[8\\]](#page-6-7),\
    \ and frequency and voltage scaling [\\[9\\]](#page-6-8).\n\nTo mitigate these\
    \ issues, each benchmark is executed multiple times, outliers are removed, cooldown\
    \ periods are inserted, and caches are flushed before each repetition. Consequently,\
    \ benchmarking a single implementation takes significantly longer than its actual\
    \ run time. This is time-consuming, especially when only limited hardware (HW)\
    \ devices are available. This paper presents the following two main contributions:\n\
    \nContribution I : Simulator Interface - We present an interface allowing autotuning\
    \ workloads to be executed on simulators rather than real hardware. Figure [1](#page-0-0)\
    \ I illustrates\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_12.jpeg)\n\n\
    Fig. 1: The proposed simulator interface I and the score predictor approach using\
    \ instruction-accurate simulators II\n\nthis approach. We extract the TVM tasks\
    \ and provide them as executables for the target architecture through a simulator\
    \ interface. The number of parallel tasks is configurable, each running in a separate\
    \ instance of the simulator. Potential scenarios that profit from our simulator\
    \ interface are:\n\n- The HW is not yet available, e.g., for pre-silicon software\
    \ (SW) development.\n- The embedded HW is only available in limited quantities,\
    \ making parallel execution of simulations on highperformance computers faster\
    \ than native execution.\n- Other metrics besides run time should be optimized.\n\
    \nContribution II : Score Predictor - We aim to demonstrate that instruction-accurate\
    \ simulators can be used for performance analysis, using autotuning workloads\
    \ as an example. *To the best of our knowledge, we are the first to show how performance\
    \ analysis can be conducted with fast instructionaccurate simulators.* Many open-source\
    \ implementations of instruction-accurate simulators exist for different architectures,\
    \ e.g., QEMU [\\[10\\]](#page-6-9) or gem5 [\\[11\\]](#page-6-10). Figure [1](#page-0-0)\
    \ II illustrates this idea. A predictor gets statistics from an instruction-accurate\
    \ simulator as input and calculates a score based on reference values measured\
    \ on real hardware. The score is then returned to the autotuning framework.\n\n\
    Since an instruction-accurate simulator does not provide accurate timing, we do\
    \ not aim to predict execution latencies. Instead, our approach utilizes scores\
    \ to evaluate and compare different implementations of the same workload. These\
    \ scores are essential for guiding the autotuning process but are not suitable\
    \ for comparing different types of workloads. We employ multiple predictors for\
    \ this task: Multiple Linear Regression (MLR) [\\[12\\]](#page-6-11), Deep Neural\
    \ Networks (DNNs) [\\[13\\]](#page-6-12), Bayesian optimization [\\[14\\]](#page-6-13),\
    \ and XGBoost [\\[15\\]](#page-6-14). We tune and compare them to identify the\
    \ most accurate predictions for common ML kernels. We evaluate our approach on\
    \ different CPU architectures, namely x86, ARM, and RISC-V.\n\n#### II. BACKGROUND\
    \ AND RELATED WORK\n\n# <span id=\"page-1-6\"></span>*A. Autotuning in TVM*\n\n\
    Apache TVM [\\[3\\]](#page-6-2) is an open-source ML compiler framework designed\
    \ to optimize computations across various hardware platforms. In TVM, each operation,\
    \ called *kernel* in this work, can be expressed in different abstractions, e.g.,\
    \ in Tensor Expression (TE) or in Tensor Intermediate Representation (TIR). For\
    \ example, a kernel can be a Neural Network (NN) layer. TVM enables the application\
    \ of *transformation primitives* to optimize kernels for diverse target architectures.\n\
    \nSimilar to Halide [\\[16\\]](#page-6-15), TVM distinguishes between the *compute*\
    \ operation, which defines the functional behavior of the kernel, and the *schedule*,\
    \ which defines its implementation. This distinction allows a single computation\
    \ to have multiple schedules. The set of all possible schedules of an operation\
    \ is called *design space*. Large design spaces and complex hardware behavior\
    \ make analytical approaches impractical for finding optimal schedules for ML\
    \ kernels. Autotuning addresses this issues by empirically evaluating multiple\
    \ schedules directly on the target hardware. TVM offers three autotuning concepts:\
    \ the AutoTVM framework, the Auto-Scheduler (also called Ansor [\\[4\\]](#page-6-3)),\
    \ and the Meta-Scheduler. This work focuses on AutoTVM and the Auto-Scheduler\
    \ since they operate on the same input representation, namely TE.\n\nListing [1](#page-1-0)\
    \ provides an example of a TE compute operation definition. Tensor C (size N ×\
    \ M) contains the result of a Matrix Matrix Multiplication (MMM) between matrices\
    \ A (size N × L) and B (size L × M).\n\n```\n1 k = te.reduce_axis((0, L), name=\"\
    k\")\n2 C = te.compute((N, M), lambda i, j: \\\n3 te.sum(A[i,k]*B[k,j], axis=k),\
    \ name=\"matmul\")\n```\nListing 1: Definition of a MMM compute operation in TE\n\
    \nAutoTVM requires users to define templates with tunable parameters, such as\
    \ loop tiling factors. AutoTVM then navigates this search space by executing configurations\
    \ on real hardware and measuring the run time. It requires some expertise to define\
    \ a useful schedule template. However, predesigned templates for common operators\
    \ can be found in the repository. As an example, Listing [2](#page-1-1) illustrates\
    \ how a *split* primitive of a single axis can be described using AutoTVM.\n\n\
    ```\n1 s = te.create_schedule(C.op)\n2 y,_ = s[C].op.axis\n3 # Define schedule\
    \ template\n4 cfg = autotvm.get_config()\n5 cfg.define_split(\"split_y\", y, num_outputs=2,...)\n\
    6 # Apply schedule\n7 yo, yi = cfg[\"split_y\"].apply(s, C, y)\n```\nListing 2:\
    \ Definition of a scheduling template for AutoTVM\n\nIn contrast to AutoTVM, the\
    \ Auto-Scheduler automates the schedule generation process without requiring manual\
    \ template definitions, making it more suitable for non-experts.\n\nFigure [2](#page-1-2)\
    \ illustrates the workflows of both the Auto-Scheduler and AutoTVM. Unlike the\
    \ manually defined search space in AutoTVM, the Auto-Scheduler uses *sketches*\
    \ generated from the kernel's Directed Acyclic Graph (DAG) through\n\n<span id=\"\
    page-1-2\"></span>![](_page_1_Figure_13.jpeg)\n\nFig. 2: Autotuning using Auto-Scheduler\
    \ and AutoTVM\n\n<span id=\"page-1-3\"></span>![](_page_1_Figure_15.jpeg)\n\n\
    Fig. 3: Typical cache hierarchies of modern CPUs\n\npredefined derivation rules.\
    \ A sketch basically contains nested loops with placeholders, which are filled\
    \ during an *annotation* phase. During annotation, loop axes can also be marked\
    \ for parallelization, unrolling, or vectorization. Implementation details can\
    \ be found in [\\[4\\]](#page-6-3). In contrast to the Auto-Scheduler, AutoTVM\
    \ relies on tuners responsible for selecting subsequent programs based on selectable\
    \ tuning algorithms.\n\n# <span id=\"page-1-5\"></span>*B. Cache Hierarchy*\n\n\
    ML operations require cache optimizations to achieve high performance [\\[17\\\
    ]](#page-6-16), [\\[18\\]](#page-6-17). Modern CPUs have hierarchical memory systems\
    \ with multiple cache levels (L1, L2, L3) as shown in Figure [3.](#page-1-3) The\
    \ hierarchy can vary between different CPUs [\\[19\\]](#page-6-18). The L1 cache\
    \ is usually divided into Data (L1D) and Instruction (L1I) cache. Higher-level\
    \ caches are often shared among the cores. Most CPUs use N-way set-associative\
    \ caches, where each memory address maps to one of N *ways* within a specific\
    \ *set*. In Linux environments, information on the cache hierarchy can be accessed\
    \ via the sysfs.\n\n# <span id=\"page-1-4\"></span>*C. The gem5 Simulator*\n\n\
    gem5 [\\[11\\]](#page-6-10) is an open-source Full System Simulator (FSS) used\
    \ in computer architecture research, supporting different architectures like x86,\
    \ ARM, and RISC-V. It provides different abstraction levels. In gem5's *atomic*\
    \ mode, memory accesses are executed within a single transaction. The requester\
    \ of the memory access is blocked until the access is completed. In the *timing*\
    \ mode, gem5 provides detailed simulation of memory access timing, including latency,\
    \ queuing delays, and bandwidth constraints. gem5 also offers different CPU models.\
    \ The SimpleCPU does not model a pipeline, which makes it fast but not very accurate\
    \ in terms of timing. It can be used in both, atomic and timing mode. gem5 further\
    \ provides an InOrderCPU and O3CPU (out-of-order CPU), which are both equipped\
    \ with a CPU pipeline model. In addition, gem5 distinguishes between the *full-system*\
    \ mode and the *system call emulation* mode. The system call emulation mode aims\
    \ to simulate user-space programs. It intercepts system calls of the target software\
    \ and handles them on the host. The\n\n<span id=\"page-2-0\"></span>\n\n|   |\
    \ 1 class SimulatorRunner(Runner):              |\n|---|-----------------------------------------------|\n\
    | 2 | def __init__(self, n_parallel=16, ) :         |\n| 3 | super(SimulatorRunner,\
    \ self).__init__()       |\n| 4 | def run(self, measure_inputs, build_results):\
    \ |\n| 5 | # Build executables                           |\n| 6 | # Run executables\
    \ in parallel on simulator    |\n| 7 | return [AutoTVMRes0, AutoTVMRes1, ]   \
    \        |\n|   |                                               |\n\nListing 3:\
    \ Custom run function for AutoTVM flow\n\n```\n1 @tvm._ffi.register_func(<func_name>,\
    \ override=True)\n2 def local_run(inputs, build_results, ...) :\n3 # Build executables\n\
    4 # Run executables in parallel on simulator\n5 return [AutoSchedRes0, AutoSchedRes1,\
    \ ...]\n```\nListing 4: Custom run function for Auto-Scheduler flow\n\nsystem\
    \ call emulation mode is faster than the full-system mode and focuses on software\
    \ testing, as it does not simulate the Operating System (OS) and thus cannot capture\
    \ any OSspecific behavior.\n\n## III. IMPLEMENTATION\n\nIn the first part of this\
    \ chapter, we explain the implementation of the simulator interface. In the second\
    \ part, we use this interface to connect TVM's autotuning with an x86, an ARM,\
    \ and a RISC-V-based instruction-accurate simulator.\n\n## *A. Simulator Interface*\n\
    \nThe simulator interface allows for executing autotuning implementations on user-level\
    \ or syscall-emulation simulators instead of real hardware. This enables parallel\
    \ execution of different implementations. TVM's autotuning requires a *builder*\
    \ and a *runner*. The builder generates an object file that contains the compiled\
    \ functionality of the workload. The runner executes the workload using the tvm::runtime.\n\
    \nFor a simulator, a standalone executable is needed. The executable prepares\
    \ the input tensors, allocates space for the output tensors, and calls the compiled\
    \ workload. To integrate this behavior into AutoTVM, we implement a custom runner\
    \ called SimulatorRunner by inheriting from the Runner class (see Listing [3\\\
    )](#page-2-0). When the run function of the custom runner is called, it generates\
    \ a main function, compiles it, and links it against the compiled object file.\
    \ A function called simulator\\_run is called with the path to the executable.\
    \ This function serves as a simulator interface and can be overwritten to use\
    \ a simulator for execution. The return value needs to be a score quantifying\
    \ the performance of the workload, e.g., the run time. A parameter named n\\_parallel\
    \ defines how many simulators can be instantiated in parallel.\n\nTo integrate\
    \ a simulator into the Auto-Scheduler, we override a function in TVM's function\
    \ registry called auto\\_scheduler.local\\_runner.run. This can be seen in Listing\
    \ [4.](#page-2-1) The return value is a list of AutoSchedResults containing scores.\
    \ The remaining implementation works in the same way as for AutoTVM.\n\nWhen generating\
    \ an object file for a ML kernel, the kernel is optimized and lowered through\
    \ TVM. LLVM is used for code generation. To produce cross-compiled object files,\
    \ it is possible to specify an LLVM triple in TVM.\n\n```\ndef conv2d(N,H,W,CO,CI,KH,KW,...)\
    \ 1\n   ifm=te.placeholder((N,CI,H,W),...) 2\n   weights=te.placeholder((CO,CI,KH,KW),...)\
    \ 3\n   bias=te.placeholder((N,CO,1,1),...) 4\n   conv=topi.nn.conv2d_nchw(ifm,weights,...)\
    \ 5\n   ofm=topi.nn.relu(conv+bias) 6\n   # Return values: transferred as DLPack\
    \ tensors 7\n   return [ifm,kernel,bias,ofm] 8\n```\nListing 5: Conv2D+Bias+ReLU\
    \ kernel definition in TVM\n\nListing [5](#page-2-2) shows the definition of a\
    \ Conv2D+Bias+ReLU kernel as an example. The tensor shapes and parameters are\
    \ passed as command line arguments to the executable.\n\n## <span id=\"page-2-4\"\
    ></span>*B. gem5 Simulator Setup*\n\nTo accurately predict timing, a cycle-accurate\
    \ simulator is needed. However, cycle-accurate simulators often suffer from slow\
    \ simulation speed. Additionally, obtaining cycle-accurate simulators with the\
    \ required level of detail for commercial architectures is a challenge, as information\
    \ about precise latencies associated with, e.g., memory components, buses, or\
    \ microarchitecture details are rarely available open source.\n\n*We will demonstrate\
    \ the feasibility of using a non-timingaccurate simulator for effective predictions\
    \ based on quantitative parameters, without relying on timing information.*\n\n\
    Our focus is on single-core workloads. To achieve fast simulation performance,\
    \ we employ gem5 in *atomic* mode with the SimpleCPU model for x86, RISC-V, and\
    \ ARM architectures (see Section [II-C\\)](#page-1-4). gem5 allows to replicate\
    \ the cache architecture while making it parameterizable to account for cache\
    \ hits, misses, and replacements within the memory hierarchy (see Section [II-B\\\
    )](#page-1-5).\n\n# <span id=\"page-2-5\"></span>*C. Score Predictor - Workflow\
    \ and Notations*\n\nTo use non-timing-accurate, i.e., instruction-accurate simulators,\
    \ for score prediction, a predictor must be trained. Figure [4](#page-2-3) illustrates\
    \ the training and execution phases of this predictor. During the training phase,\
    \ workloads are not only executed in gem5 but also natively on the target CPU.\
    \ We require a distinct predictor for each architecture and *kernel type*. For\
    \ instance, Conv2D+Bias+ReLU represents one specific kernel type. Note that the\
    \ corresponding predictor can be applied to any combination of shapes and parameters\
    \ of this kernel type. A fixed combination of shapes and parameters for a given\
    \ kernel type is referred to as a *group*. The autotuning process generates various\
    \ *implementations* (schedules) for each group. In the subsequent execution phase,\
    \ we leverage the pre-trained predictor. The target CPU is not required anymore\n\
    \n<span id=\"page-2-3\"></span>![](_page_2_Figure_22.jpeg)\n\nFig. 4: Workflow\
    \ of training I and execution II of a predictor for one target architecture and\
    \ one kernel type\n\nat this stage, which enables the simulation of architectures\
    \ such as RISC-V on x86 platforms.\n\n#### <span id=\"page-3-2\"></span>*D. Score\
    \ Predictor Training*\n\nThe score predictor estimates a score S for an implementation\
    \ I. The underlying principle is that these scores correlate with the run times,\
    \ but only for different implementations of the same kernel type and group. Perfect\
    \ prediction means SI<sup>1</sup> < SI<sup>2</sup> ⇒ tI<sup>1</sup> < tI2, where\
    \ tIx is the run time of implementation x. Notably, comparisons between implementations\
    \ across different kernel types or groups are not possible using the score. Given\
    \ that timing details are unavailable, the score predictor relies solely on quantitative\
    \ information rather than latencies. The relevant statistics derived from gem5\
    \ are:\n\n- The number of the executed load/store/branch instructions divided\
    \ by the total number of instructions.\n- The total number of the executed instructions\
    \ normalized to the total number of executed instructions of the group.\n- Cache\
    \ read/write replacements/hits/misses divided by read/write accesses of each cache.\n\
    \nConsidering, e.g., L1D write (L1Dw) cache hits for implementation x, we determine:\n\
    \n$$P\\_{L1Dw}(I\\_x) = \\frac{L1D\\_{w,hits}(I\\_x)}{L1D\\_{w,access}(I\\_x)}\\\
    tag{1}$$\n\nAdditionally, all parameters are also normalized to the group:\n\n\
    $$P\\_{L1Dw,norm}(I\\_x) = \\frac{P\\_{L1Dw}(I\\_x) - \\overline{P\\_{L1Dw}(\\\
    mathbf{I})}}{\\overline{P\\_{L1Dw}(\\mathbf{I})}} \\qquad (2)$$\n\nThe values\
    \ PL1Dw(I) represent the mean values across the group. We found that the most\
    \ promising approach is to use these parameters as inputs for the predictor in\
    \ both their original form PL1Dw(Ix) and their normalized form PL1Dw,norm(Ix).\
    \ The output scores that are used for training are the measured run times normalized\
    \ to the group, similar to Equation [\\(2\\)](#page-3-0). Based on these inputs\
    \ and the scores, we train different predictors, which can be used with different\
    \ loss functions. In this work, we use Mean Squared Error (MSE), Mean Absolute\
    \ Error (MAE), and Residual Sum of Squares (RSS). Below, a brief overview of the\
    \ predictors is provided.\n\n*1) Multiple Linear Regression:* MLR is an extension\
    \ of linear regression. MLR is a simple predictor. It only models linear relationships.\
    \ It finds a relation between multiple outputs y<sup>k</sup> and multiple independent\
    \ inputs x<sup>i</sup> [\\[20\\]](#page-6-19), in our case:\n\n$$y = b\\_0 + b\\\
    _1 x\\_1 + b\\_2 x\\_2 + \\dots + b\\_n x\\_n \\tag{3}$$\n\n*2) Regression with\
    \ DNNs:* Regression using DNNs means predicting continuous outputs based on several\
    \ input features. Typically, the networks consist of multiple fully connected\
    \ layers, with the number of input neurons corresponding to the number of input\
    \ features. Each hidden layer usually applies an activation function to introduce\
    \ non-linearity and allow the network to capture complex relationships. The final\
    \ layer has a number of neurons equal to the number of output variables. A common\
    \ architecture might include a couple of hidden layers with decreasing numbers\
    \ of neurons.\n\n*3) Bayesian Optimization:* Bayesian optimization builds a surrogate\
    \ model (in this case, a Gaussian process) that approximates the real/true objective\
    \ function. It uses this model to predict which areas of the parameter space are\
    \ likely to yield the best results and evaluates the function in those regions.\
    \ An acquisition function balances the trade-off between exploring new regions\
    \ of the space (exploration) and refining promising known regions (exploitation)\
    \ [\\[21\\]](#page-6-20). Listing [6](#page-3-1) shows the choice of the used\
    \ objective function and loss function.\n\n<span id=\"page-3-1\"></span>\n\n|\
    \ def objective_function(C, RBF_scale, noise): | 1 |\n|----------------------------------------------|---|\n\
    | func = ConstantKernel(constant_value=C, ) \\  | 2 |\n| * RBF(length_scale=RBF_scale,\
    \ ) \\            | 3 |\n| + WhiteKernel(noise_level=noise, )           | 4 |\n\
    | gp = GaussianProcessRegressor(func) \\        | 5 |\n| .fit(X_train, y_train)\
    \                       | 6 |\n| predictions = gp.predict(X_test)            \
    \ | 7 |\n| return - loss_function(y_test, predictions)  | 8 |\n|             \
    \                                 |   |\n\nListing 6: Objective function for bayes\
    \ optimization\n\nThe hyperparameters of the Gaussian process are the input parameters\
    \ of the objective\\_function. The Bayesian optimization framework maximizes the\
    \ objective function through a series of iterations. Each iteration involves fitting\
    \ a Gaussian process model with new hyperparameters and updating the probabilistic\
    \ model based on the results.\n\n<span id=\"page-3-0\"></span>*4) XGBoost:* Extreme\
    \ Gradient Boosting (XGBoost [\\[22\\]](#page-6-21)) is a powerful algorithm for\
    \ regression tasks. It uses an ensemble of decision trees to predict continuous\
    \ outcomes. In XGBoost, trees are built sequentially, with each tree trying to\
    \ minimize the errors made by the previous ones by adjusting residuals. The algorithm\
    \ optimizes a loss function using gradient descent. The hyperparameters of the\
    \ XGBoost algorithm are, e.g., the learning rate, max. tree depth, and regularization\
    \ parameters.\n\n#### *E. Score Predictor Inference*\n\nFor inference, the trained\
    \ predictors are integrated into the execution pipeline, as shown in Figure [4.](#page-2-3)\
    \ A challenge arises because input parameters, such as PL1Dw,norm(Ix), cannot\
    \ be determined for new, unknown groups. This limitation is due to the Auto-Scheduler\
    \ generating implementations batchwise based on prior scores (see Section [II-A\\\
    )](#page-1-6), preventing the computation of mean values like PL1Dw(I) at the\
    \ beginning.\n\nTo address this, we allow approximating mean values using two\
    \ approaches: *static* and *dynamic* windows. In the static approach, mean values\
    \ are calculated from the first w samples. In the dynamic window approach, mean\
    \ values are adaptively adjusted over time. The batch size, and thus the window\
    \ size w, is typically large enough that no accuracy loss compared to using PL1Dw(I)\
    \ was observed in the experiments.\n\n# IV. RESULTS\n\nTo verify the quality of\
    \ our predictors, all benchmarks are executed on three different CPU architectures:\
    \ x86, ARM, and RISC-V. For x86, we use a 64 bit 2.2 GHz AMD Ryzen 7 5800X 8-Core\
    \ processor; for ARM, we use a 64 bit 1.5 GHz Raspberry Pi 4 Model B with an ARM\
    \ Cortex-A72 processor; and for RISC-V, we use a 64 bit 1.2 GHz SiFive U74-MC\
    \ processor.\n\n<span id=\"page-4-0\"></span>TABLE I: Cache sizes and hierarchy\
    \ of the used CPUs\n\n|            |                       | L1 Data |       \
    \                |                   | L1 Instruction |   |               | L2\
    \   |      | LLC (L3)        |   |   |  |  |\n|------------|-----------------------|---------|-----------------------|-------------------|----------------|---|---------------|------|------|-----------------|---|---|--|--|\n\
    |            | assoc<br>size<br>sets |         | assoc<br>size<br>sets |     \
    \              | size<br>sets   |   | assoc         | size | sets | assoc    \
    \       |   |   |  |  |\n| x86        | 32K                   | 64      |    \
    \                   | 8 32K             | 64             | 8 | 512K          |\
    \ 1024 | 8    | 32768K 32768 16 |   |   |  |  |\n| ARM        |              \
    \         |         |                       | 32K 256 2 48K 256 |            \
    \    | 3 | 1024K 1024 16 |      |      | -               | - | - |  |  |\n| RISC-V\
    \ 32K |                       | 64      |                       | 8 32K      \
    \       | 64             | 8 | 2048K 2048 16 |      |      | -               |\
    \ - | - |  |  |\n\n<span id=\"page-4-1\"></span>TABLE II: Shapes of the used Conv2D+Bias+ReLU\
    \ kernels\n\n| group | N | H   | W   | CO  | CI  | KH | KW | stride | pad   |\n\
    |-------|---|-----|-----|-----|-----|----|----|--------|-------|\n| 0     | 1\
    \ | 224 | 224 | 64  | 3   | 7  | 7  | (2,2)  | (3,3) |\n| 1     | 1 | 56  | 56\
    \  | 64  | 64  | 3  | 3  | (1,1)  | (1,1) |\n| 2     | 1 | 56  | 56  | 128 | 64\
    \  | 3  | 3  | (2,2)  | (1,1) |\n| 3     | 1 | 28  | 28  | 256 | 128 | 3  | 3\
    \  | (2,2)  | (1,1) |\n| 4     | 1 | 14  | 24  | 512 | 256 | 3  | 3  | (2,2) \
    \ | (1,1) |\n\nTable [I](#page-4-0) lists the cache hierarchies of these CPUs.\
    \ We model them in gem5 (see Section [III-B\\)](#page-2-4). All cache line sizes\
    \ are 64 B. The ARM and RISC-V CPUs feature a shared L2 cache but no L3 cache.\
    \ All gem5 simulations are executed on the x86 machine. We use five groups of\
    \ Conv2D+Bias+ReLU kernels from a ResNet [\\[23\\]](#page-6-22) architecture as\
    \ benchmarks. Table [II](#page-4-1) lists the shapes and parameters of these groups.\
    \ For training the predictors, the Auto-Scheduler generates 500 implementations\
    \ per group, with 100 implementations used for the test set.\n\nTo determine the\
    \ reference execution time tref , each implementation is executed Nexe = 15 times,\
    \ with the median value taken as the reference. Additionally, cooldown times of\
    \ tcooldown = 1s are inserted between each run to ensure more reproducible measurements.\
    \ Workloads are not executed in parallel on real hardware, as this could affect\
    \ the measurements. This means that execution on K parallel simulators is faster\
    \ than native (sequential) execution on a single device.\n\n$$K = \\left\\lceil\
    \ \\frac{t\\_{simulation}}{(t\\_{cooldown} + t\\_{ref}) \\cdot N\\_{exe}} \\right\\\
    rceil \\tag{4}$$\n\nOur measurement setup (Nexe = 15, tcooldown = 1s) results\
    \ in Kx<sup>86</sup> ∈ [7, 97], KARM ∈ [4, 31], and KRISC−<sup>V</sup> ∈ [3, 21]\
    \ for the tested workloads. This means that in the best case, *only 3 parallel\
    \ executions on the x86 machine are sufficient to achieve a speedup over native\
    \ execution on the RISC-V CPU*.\n\n#### *A. Prediction of Non-Trained Groups*\n\
    \nFirst, we demonstrate that a trained predictor can be utilized even when a specific\
    \ kernel group was not included in the training data. This capability is crucial\
    \ as we aim to develop one predictor per architecture and kernel type (e.g., Conv2D+Bias+ReLU)\
    \ that remains valid across all groups with different shapes and parameters (see\
    \ Section [III-C\\)](#page-2-5).\n\nTo achieve this, we initially train Bayesian\
    \ predictors for all architectures using all groups. Subsequently, we train additional\
    \ predictors using only groups 0, 1, 2, 4, and 5. Figure [5](#page-5-0) compares\
    \ the test set of group 3 when group 3 is included in the training (Figures [5a](#page-5-0)\
    \ to [5c\\)](#page-5-0) against using the same samples when group 3 is not included\
    \ in the training (Figures [5d](#page-5-0) to [5f\\)](#page-5-0). The x-axis represents\
    \ the individual samples. The reference time tref shows the median values of the\
    \ measured, in ascending order sorted run times of the implementations. To obtain\
    \ the predicted run time tpred, the predicted scores are sorted in ascending order,\
    \ with the corresponding measured run time plotted according to these scores.\
    \ A perfect prediction would mean that tpred == tref .\n\nAlthough not all scores\
    \ are perfectly ordered, a clear ascending trend is evident. Predictions for ARM\
    \ and RISC-V architectures appear more accurate than those for x86. This discrepancy\
    \ may arise from fewer HW optimizations on these embedded CPUs. Furthermore, each\
    \ predictor is only as good as its reference measurements. Since execution times\
    \ on x86 are significantly faster, reference measurements show greater variability\
    \ compared to longer run times on the embedded architectures. Overall, visual\
    \ inspections reveal no clear advantage between included and non-included training\
    \ groups. *This demonstrates that the predictor can effectively be used even for\
    \ groups that are not present in the training data.* To enable better comparisons\
    \ of predictions, we will introduce three different evaluation metrics in the\
    \ following.\n\n#### *B. Evaluation Metrics for the Predictors*\n\nThe most important\
    \ aspect is predicting the fastest run time. Therefore, we introduce Etop<sup>1</sup>\
    \ and the rank metric Rtop1. Etop<sup>1</sup> represents the relative error between\
    \ the run times of the fastest reference measurement and the sample with the best\
    \ predicted score:\n\n$$\\begin{aligned} \\stackrel{\\text{I.S.on.}}{E\\_{top1}}\
    \ &:= \\left(1 - \\frac{t\\_{ref}[0]}{t\\_{pred}[0]}\\right) \\cdot 100\\% \\\
    end{aligned} \\tag{5}$$\n\nRank Rtop<sup>1</sup> indicates the relative position\
    \ at which the fastest sample was ranked by the predictor:\n\n$$R\\_{top1} :=\
    \ \\frac{100\\%}{|t\\_{ref}|} \\cdot \\left( \\underset{x}{\\text{argmin}} \\\
    left( t\\_{pred}[x] == t\\_{ref}[0] \\right) + 1 \\right) \\quad (6)$$\n\nFor\
    \ example, Rtop<sup>1</sup> = 3 % means that the fastest sample was ranked within\
    \ the top 3 % of predictions.\n\nAdditionally, we need a metric to evaluate sorting\
    \ quality. Consecutive non-monotonically increasing samples should be penalized,\
    \ as well as their extent of deviation. Therefore, we introduce the Quality Score\
    \ Q:\n\n$$Q \\coloneqq \\frac{100\\%}{|t\\_{ref}|} \\cdot \\sum\\_{i} \\frac{t\\\
    _{ref}[i] - \\min(t\\_{ref}[i], t\\_{ref}[i+1])}{t\\_{ref}[i]} \\tag{7}$$\n\n\
    To prevent deviations in the slower part of the run times from being disproportionately\
    \ weighted, we evaluate Q separately for the lower 50 % of the run times (Qlow)\
    \ and the higher 50 % of the run times (Qhigh). For all metrics, a smaller value\
    \ indicates better performance.\n\n#### *C. Comparison of Predictors*\n\nNext,\
    \ we compare the different predictors (see Section [III-D\\)](#page-3-2). We tested\
    \ various loss functions, activation functions, and parameters. Given that the\
    \ XGBoost algorithm has many hyperparameters, we employed grid search [\\[24\\\
    ]](#page-6-23) for tuning. The used configurations (after tuning) are:\n\n####\
    \ • Linear Regression: RSS loss\n\n• DNN: 6 dense layers (number of neurons: 128,\
    \ 128, 64, 32, 16, 1), linear output activation, tanh hidden layer activation,\
    \ MAE loss, adam optimizer\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_1.jpeg)\n\
    \nFig. 5: Sorted run time predictions for the test set of group 3 (a)-(c) when\
    \ group 3 is included in the training vs. the same samples of group 3 (d)-(f)\
    \ when group 3 is not included in the training\n\n<span id=\"page-5-1\"></span>TABLE\
    \ III: Prediction results for x86-based CPU\n\n| ID |                        \
    \            | LinReg  |          |          | DNN      |                    \
    \             |          |          | Bayes                                  \
    \                             |                                 | XGBoost  | \
    \         |          |         |          |          |\n|----|------------------------------------|---------|----------|----------|----------|---------------------------------|----------|----------|---------------------------------------------------------------------|---------------------------------|----------|----------|----------|---------|----------|----------|\n\
    |    | Etop1(%)                           | Qlow(%) | Qhigh(%) | Rtop1(%) | Etop1(%)\
    \ | Qlow(%)                         | Qhigh(%) | Rtop1(%) | Etop1(%)         \
    \                                                   | Qlow(%)                \
    \         | Qhigh(%) | Rtop1(%) | Etop1(%) | Qlow(%) | Qhigh(%) | Rtop1(%) |\n\
    |    |                                    |         |          |          |  \
    \        |                                 |          |          |           \
    \                                                          |                 \
    \                |          |          |          |         |          |     \
    \     |\n|    |                                    |         |          |    \
    \      |          |                                 |          |          | 0\
    \ 10.7 3.0 3.0 8.5 1.4 2.7 2.2 3.0 12.5 3.0 2.7 3.0 7.0 2.7 2.4 2.0 |        \
    \                         |          |          |          |         |       \
    \   |          |\n|    | 1 11.2 3.6 3.5 3.0 3.2 2.8 2.8 2.0 |         |      \
    \    |          |          |                                 |          |    \
    \      |                                                                     |\
    \ 8.9 2.6 2.8 3.5 1.5 2.7 2.4 2.5 |          |          |          |         |\
    \          |          |\n|    | 2 15.0 3.2 3.2 3.0 7.6 2.7 2.3 2.0 |         |\
    \          |          |          |                                 |         \
    \ |          |                                                               \
    \      | 0.0 2.6 2.3 1.0 2.2 2.4 2.2 2.0 |          |          |          |  \
    \       |          |          |\n| 3  |                                    | \
    \        |          |          |          | 8.8 3.2 3.0 2.0 0.7 2.4 2.4 1.5 |\
    \          |          |                                                      \
    \               | 0.7 2.7 2.7 1.5 0.0 2.8 2.4 1.0 |          |          |    \
    \      |         |          |          |\n| 4  |                             \
    \       |         |          |          |          | 0.0 3.4 3.3 1.0 2.8 3.1 2.2\
    \ 3.0 |          |          |                                                \
    \                     | 5.4 2.5 2.6 2.0 2.0 2.8 2.4 2.0 |          |         \
    \ |          |         |          |          |\n\n- Bayes: Objective function\
    \ shown in Listing [6,](#page-3-1) MSE loss\n- XGBoost: Column subsample ratio\
    \ 0.6, learning rate 0.05, max. tree depth 3, alpha 0, lambda 0.1, gradient boost\
    \ trees 300, min. child weight 1, subsample ratio for training 0.8, MSE loss\n\
    \nFor the evaluation, all groups were included in the training. One predictor\
    \ (i.e., one of LinReg, DNN, etc.) was trained per CPU architecture. Each training\
    \ was performed 10 times with a random selection of training and test sets. Scores\
    \ were subsequently calculated based on the median predictions from the test sets.\
    \ Table [III](#page-5-1) shows the results for x86, Table [IV](#page-5-2) for\
    \ ARM, and Table [V](#page-5-3) for RISC-V.\n\nFor the x86 architecture, the DNN,\
    \ Bayesian optimization, and XGBoost yield good scores. XGBoost achieves the best\
    \ average Rtop<sup>1</sup> score, with a maximum of 2.5 %. XGBoost also had the\
    \ smallest prediction error, with an average of Etop<sup>1</sup> = 2.54 %. For\
    \ ARM and RISC-V, the scores are slightly better. For ARM, DNNs, Bayesian optimization,\
    \ and XGBoost achieve a Rtop<sup>1</sup> score smaller or equals 2.5 %. All Etop<sup>1</sup>\
    \ errors are below 5 %, and often even below 2 %. Bayesian optimization correctly\
    \ predicts the optimum in three out of five cases and demonstrates the lowest\
    \ Qlow and Qhigh. For RISC-V, even the linear model performs quite well, with\
    \ exceptions in Etop<sup>1</sup> for group 0 and 4. For other predictors, the\
    \ optimum is often included in the top 2 % of predictions. The DNN provides the\
    \ best results with Etop<sup>1</sup> ≤ 3.6 % and Rtop<sup>1</sup> ≤ 2 %.\n\nIn\
    \ summary, it is possible to train predictors for forecasting. *For RISC-V and\
    \ ARM, a prediction error* Etop<sup>1</sup> *of less than 5 % is achievable.*\
    \ If the goal is to find the best sample, it is *sufficient to re-execute the\
    \ top 2 %-3 % of the predictions* later on a real architecture.\n\n<span id=\"\
    page-5-2\"></span>TABLE IV: Prediction results for ARM-based CPU\n\n| ID |   \
    \       | LinReg  |          |                                               \
    \                    |          | DNN     |          |          |          | Bayes\
    \   |          |          | XGBoost  |         |          |          |\n|----|----------|---------|----------|-------------------------------------------------------------------|----------|---------|----------|----------|----------|---------|----------|----------|----------|---------|----------|----------|\n\
    |    |          |         |          |                                       \
    \                            |          |         |          |          |    \
    \      |         |          |          |          |         |          |     \
    \     |\n|    |          |         |          |                              \
    \                                     |          |         |          |      \
    \    |          |         |          |          |          |         |       \
    \   |          |\n|    | Etop1(%) | Qlow(%) | Qhigh(%) | Rtop1(%)            \
    \                                              | Etop1(%) | Qlow(%) | Qhigh(%)\
    \ | Rtop1(%) | Etop1(%) | Qlow(%) | Qhigh(%) | Rtop1(%) | Etop1(%) | Qlow(%) |\
    \ Qhigh(%) | Rtop1(%) |\n|    |          |         |          |              \
    \                                                     |          |         | \
    \         |          |          |         |          |          |          | \
    \        |          |          |\n|    |          |         |          | 0 9.9\
    \ 3.4 2.9 3.5 0.2 2.8 2.3 1.5 0.0 2.6 2.0 1.0 2.7 3.0 2.2 1.5 |          |   \
    \      |          |          |          |         |          |          |    \
    \      |         |          |          |\n|    |          |         |        \
    \  | 1 6.4 3.6 3.2 3.0 4.6 3.2 2.6 2.0 4.6 3.4 2.4 2.0 4.3 3.2 2.6 2.5 |     \
    \     |         |          |          |          |         |          |      \
    \    |          |         |          |          |\n|    |          |         |\
    \          | 2 7.7 3.6 2.3 5.0 0.7 3.3 2.4 1.5 4.3 3.2 2.3 2.5 0.3 3.4 2.2 1.5\
    \ |          |         |          |          |          |         |          |\
    \          |          |         |          |          |\n|    |          |   \
    \      |          | 3 7.7 2.8 2.6 2.5 1.1 2.8 2.3 1.5 0.0 2.8 2.0 1.0 4.0 3.2\
    \ 2.2 2.0 |          |         |          |          |          |         |  \
    \        |          |          |         |          |          |\n|    |     \
    \     |         |          | 4 2.2 3.5 2.5 4.0 0.2 3.1 2.6 1.5 0.0 2.6 2.2 1.0\
    \ 1.0 3.0 2.3 2.0 |          |         |          |          |          |    \
    \     |          |          |          |         |          |          |\n\n<span\
    \ id=\"page-5-3\"></span>TABLE V: Prediction results for RISC-V-based CPU\n\n\
    | ID | LinReg                             |         |          |          | DNN\
    \                             |         |          |          | Bayes        \
    \                                                       |         |          |\
    \          | XGBoost                         |         |          |          |\n\
    |----|------------------------------------|---------|----------|----------|---------------------------------|---------|----------|----------|---------------------------------------------------------------------|---------|----------|----------|---------------------------------|---------|----------|----------|\n\
    |    |                                    |         |          |          |  \
    \                               |         |          |          |            \
    \                                                         |         |        \
    \  |          |                                 |         |          |       \
    \   |\n|    |                                    |         |          |      \
    \    |                                 |         |          |          |     \
    \                                                                |         | \
    \         |          |                                 |         |          |\
    \          |\n|    | Etop1(%)                           | Qlow(%) | Qhigh(%) |\
    \ Rtop1(%) | Etop1(%)                        | Qlow(%) | Qhigh(%) | Rtop1(%) |\
    \ Etop1(%)                                                            | Qlow(%)\
    \ | Qhigh(%) | Rtop1(%) | Etop1(%)                        | Qlow(%) | Qhigh(%)\
    \ | Rtop1(%) |\n|    |                                    |         |        \
    \  |          |                                 |         |          |       \
    \   |                                                                     |  \
    \       |          |          |                                 |         |  \
    \        |          |\n|    | 0 10.9 4.0 3.9 4.0 2.2 4.0 4.0 2.0 |         | \
    \         |          |                                 |         |          |\
    \          |                                                                 \
    \    |         |          |          | 0.0 3.8 4.2 1.0 0.0 3.4 4.1 1.0 |     \
    \    |          |          |\n| 1  |                                    |    \
    \     |          |          | 0.0 4.0 4.3 1.0 0.6 3.7 4.5 1.5 |         |    \
    \      |          |                                                          \
    \           |         |          |          | 2.5 3.4 4.4 2.0 4.6 3.5 4.5 2.5\
    \ |         |          |          |\n| 2  |                                  \
    \  |         |          |          | 0.0 3.4 3.7 1.0 0.0 3.2 3.8 1.0 |       \
    \  |          |          |                                                   \
    \                  |         |          |          | 0.0 2.8 3.4 1.0 0.0 3.0 3.8\
    \ 1.0 |         |          |          |\n| 3  |                              \
    \      |         |          |          | 0.0 3.6 3.8 1.0 0.0 3.2 3.9 1.0 |   \
    \      |          |          |                                               \
    \                      |         |          |          | 2.0 2.8 3.7 1.5 4.4 3.0\
    \ 3.6 2.0 |         |          |          |\n|    |                          \
    \          |         |          |          |                                 |\
    \         |          |          | 4 11.0 4.0 4.2 3.0 3.6 3.6 4.2 1.5 10.7 3.2\
    \ 4.0 2.0 8.2 3.8 4.0 3.0 |         |          |          |                  \
    \               |         |          |          |\n\n#### V. CONCLUSION AND FUTURE\
    \ WORK\n\nIn this work, we introduced an interface for executing autotuning workloads\
    \ on simulators and explored the feasibility of using instruction-accurate simulators\
    \ for autotuning of ML workloads. We trained and compared different score predictors\
    \ for the x86, ARM, and RISC-V architectures. Our results show that the tuned\
    \ predictors can identify optimal implementations within the top 3 % of predictions.\
    \ In the case of limited availability of the target HW, our approach can even\
    \ be used to accelerate autotuning. In the best case, 3 parallel simulations on\
    \ the used x86 machine were sufficient to replace one RISC-V board. Our research\
    \ lays the groundwork for using instruction-accurate simulations for performance\
    \ estimation.\n\nFuture work will focus on benchmarking a broader range of CPUs\
    \ to train more generalized predictors. These generalized predictors can then\
    \ be applied to previously untested CPUs, enhancing our methodology's appeal for\
    \ pre-silicon software development.\n\nPREPRINT - Accepted for publication at\
    \ the 62th Design Automation Conference (DAC), June 22-25, 2025, in San Francisco.\n\
    \n© 2025 IEEE: Personal use of this material is permitted. Permission from IEEE\
    \ must be obtained for all other uses,\n\nor redistribution to servers or lists,\
    \ or reuse of any copyrighted component of this work in other works. including\
    \ reprinting/republishing this material for advertising or promotional purposes,\
    \ collecting new collected works for resale\n\n#### REFERENCES\n\n- <span id=\"\
    page-6-0\"></span>[1] K. Datta *et al.*, \"Stencil computation optimization and\
    \ auto-tuning on state-of-the-art multicore architectures,\" in *SC'08: Proceedings\
    \ of the 2008 ACM/IEEE conference on Supercomputing*. IEEE, 2008, pp. 1– 12.\n\
    - <span id=\"page-6-1\"></span>[2] L. Lin and M. Gen, \"Auto-tuning strategy for\
    \ evolutionary algorithms: balancing between exploration and exploitation,\" *Soft\
    \ Computing*, vol. 13, pp. 157–168, 2009.\n- <span id=\"page-6-2\"></span>[3]\
    \ T. Chen *et al.*, \"TVM: An automated End-to-End optimizing compiler for deep\
    \ learning,\" in *13th USENIX Symposium on Operating Systems Design and Implementation\
    \ (OSDI 18)*, 2018, pp. 578–594.\n- <span id=\"page-6-3\"></span>[4] L. Zheng\
    \ *et al.*, \"Ansor: Generating High-Performance tensor programs for deep learning,\"\
    \ in *14th USENIX symposium on operating systems design and implementation (OSDI\
    \ 20)*, 2020, pp. 863–879.\n- <span id=\"page-6-4\"></span>[5] Y.-S. Hsieh and\
    \ Y.-P. You, \"DLOOPT: An Optimization Assistant on AutoTVM for Deep Learning\
    \ Operators,\" *Journal of Signal Processing Systems*, vol. 95, no. 5, pp. 585–607,\
    \ 2023.\n- <span id=\"page-6-5\"></span>[6] W. Grunewald and T. Ungerer, \"Towards\
    \ extremely fast context switching in a block-multithreaded processor,\" in *Proceedings\
    \ of EUROMICRO 96. 22nd Euromicro Conference. Beyond 2000: Hardware and Software\
    \ Design Strategies*. IEEE, 1996, pp. 592–599.\n- <span id=\"page-6-6\"></span>[7]\
    \ S. Zhuravlev, J. C. Saez, S. Blagodurov, A. Fedorova, and M. Prieto, \"Survey\
    \ of scheduling techniques for addressing shared resources in multicore processors,\"\
    \ *ACM Computing Surveys (CSUR)*, vol. 45, no. 1, pp. 1–28, 2012.\n- <span id=\"\
    page-6-7\"></span>[8] T. Benoit-Cattin, D. Velasco-Montero, and J. Fernandez-Berni,\
    \ \"Impact ´ of thermal throttling on long-term visual inference in a CPU-based\
    \ edge device,\" *Electronics*, vol. 9, no. 12, p. 2106, 2020.\n- <span id=\"\
    page-6-8\"></span>[9] D. Brodowski, N. Golde, R. J. Wysocki, and V. Kumar, \"\
    CPU frequency and voltage scaling code in the Linux (TM) kernel,\" *Linux kernel\
    \ documentation*, vol. 66, 2013.\n- <span id=\"page-6-9\"></span>[10] F. Bellard,\
    \ \"QEMU, a fast and portable dynamic translator.\" in *USENIX annual technical\
    \ conference, FREENIX Track*, vol. 41, no. 46. California, USA, 2005, pp. 10–5555.\n\
    - <span id=\"page-6-10\"></span>[11] N. Binkert *et al.*, \"The gem5 simulator,\"\
    \ *ACM SIGARCH computer architecture news*, vol. 39, no. 2, pp. 1–7, 2011.\n-\
    \ <span id=\"page-6-11\"></span>[12] D. C. Montgomery, E. A. Peck, and G. G. Vining,\
    \ *Introduction to linear regression analysis*. John Wiley & Sons, 2021.\n- <span\
    \ id=\"page-6-12\"></span>[13] J. Du and Y. Xu, \"Hierarchical deep neural network\
    \ for multivariate regression,\" *Pattern Recognition*, vol. 63, pp. 149–157,\
    \ 2017.\n- <span id=\"page-6-13\"></span>[14] P. I. Frazier, \"Bayesian optimization,\"\
    \ in *Recent advances in optimization and modeling of contemporary problems*.\
    \ Informs, 2018, pp. 255–278.\n- <span id=\"page-6-14\"></span>[15] T. Chen *et\
    \ al.*, \"Xgboost: extreme gradient boosting,\" *R package version 0.4-2*, vol.\
    \ 1, no. 4, pp. 1–4, 2015.\n- <span id=\"page-6-15\"></span>[16] J. Ragan-Kelley,\
    \ C. Barnes, A. Adams, S. Paris, F. Durand, and S. Amarasinghe, \"Halide: a language\
    \ and compiler for optimizing parallelism, locality, and recomputation in image\
    \ processing pipelines,\" *Acm Sigplan Notices*, vol. 48, no. 6, pp. 519–530,\
    \ 2013.\n- <span id=\"page-6-16\"></span>[17] V. Kelefouras and G. Keramidas,\
    \ \"Design and implementation of deep learning 2D convolutions on modern CPUs,\"\
    \ *IEEE Transactions on Parallel and Distributed Systems*, 2023.\n- <span id=\"\
    page-6-17\"></span>[18] R. Li, Y. Xu, A. Sukumaran-Rajam, A. Rountev, and P. Sadayappan,\
    \ \"Analytical characterization and design space exploration for optimization\
    \ of CNNs,\" in *Proceedings of the 26th ACM International Conference on Architectural\
    \ Support for Programming Languages and Operating Systems*, 2021, pp. 928–942.\n\
    - <span id=\"page-6-18\"></span>[19] S. Kumar and P. Singh, \"An overview of modern\
    \ cache memory and performance analysis of replacement policies,\" in *2016 IEEE\
    \ International Conference on Engineering and Technology (ICETECH)*. IEEE, 2016,\
    \ pp. 210–214.\n- <span id=\"page-6-19\"></span>[20] G. K. Uyanık and N. Guler,\
    \ \"A study on multiple linear regression ¨ analysis,\" *Procedia-Social and Behavioral\
    \ Sciences*, vol. 106, pp. 234– 240, 2013.\n- <span id=\"page-6-20\"></span>[21]\
    \ P. I. Frazier, \"A tutorial on Bayesian optimization,\" *arXiv preprint arXiv:1807.02811*,\
    \ 2018.\n- <span id=\"page-6-21\"></span>[22] T. Chen, T. He, M. Benesty, and\
    \ V. Khotilovich, \"Package 'xgboost',\" *R version*, vol. 90, no. 1-66, p. 40,\
    \ 2019.\n- <span id=\"page-6-22\"></span>[23] Z. Wu, C. Shen, and A. Van Den Hengel,\
    \ \"Wider or deeper: Revisiting the ResNet model for visual recognition,\" *Pattern\
    \ recognition*, vol. 90, pp. 119–133, 2019.\n- <span id=\"page-6-23\"></span>[24]\
    \ S. Putatunda and K. Rama, \"A comparative analysis of hyperopt as against other\
    \ approaches for hyper-parameter optimization of XGBoost,\" in *Proceedings of\
    \ the 2018 international conference on signal processing and machine learning*,\
    \ 2018, pp. 6–10."
